DOI,Link,Abstract
"10.1140/epjds/s13688-019-0196-6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066153901&doi=10.1140%2fepjds%2fs13688-019-0196-6&partnerID=40&md5=b5731660c2461bcdb157f8b7f8d32561","Mobility is one of the fundamental requirements of human life with significant societal impacts including productivity, economy, social wellbeing, adaptation to a changing climate, and so on. Although human movements follow specific patterns during normal periods, there are limited studies on how such patterns change due to extreme events. To quantify the impacts of an extreme event to human movements, we introduce the concept of mobility resilience which is defined as the ability of a mobility system to manage shocks and return to a steady state in response to an extreme event. We present a method to detect extreme events from geo-located movement data and to measure mobility resilience and transient loss of resilience due to those events. Applying this method, we measure resilience metrics from geo-located social media data for multiple types of disasters occurred all over the world. Quantifying mobility resilience may help us to assess the higher-order socio-economic impacts of extreme events and guide policies towards developing resilient infrastructures as well as a nation’s overall disaster resilience strategies. © 2019, The Author(s)."
"10.1140/epjds/s13688-019-0193-9","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065925699&doi=10.1140%2fepjds%2fs13688-019-0193-9&partnerID=40&md5=9f18d671a59069caa808bef726ea47a1","The recent rise of the political extremism in Western countries has spurred renewed interest in the psychological and moral appeal of political extremism. Empirical support for the psychological explanation using surveys has been limited by lack of access to extremist groups, while field studies have missed psychological measures and failed to compare extremists with contrast groups. We revisit the debate over the psychological and moral appeal of extremism in the U.S. context by analyzing Twitter data of 10,000 political extremists and comparing their text-based psychological constructs with those of 5000 liberal and 5000 conservative users. The results reveal that extremists show a lower positive emotion and a higher negative emotion than partisan users, but their differences in certainty is not significant. In addition, while left-wing extremists express more language indicative of anxiety than liberals, right-wing extremists express lower anxiety than conservatives. Moreover, our results mostly lend support to Moral Foundations Theory for partisan users and extend it to the political extremists. With the exception of ingroup loyalty, we found evidences supporting the Moral Foundations Theory among left- and right-wing extremists. However, we found no evidence for elevated moral foundations among political extremists. © 2019, The Author(s)."
"10.1140/epjds/s13688-019-0194-8","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065919884&doi=10.1140%2fepjds%2fs13688-019-0194-8&partnerID=40&md5=20f394b0856adc9d99fb41f93be1ab4c","The power of the press to shape the informational landscape of a population is unparalleled, even now in the era of democratic access to all information outlets. However, it is known that news outlets (particularly more traditional ones) tend to discriminate who they want to reach, and who to leave aside. In this work, we attempt to shed some light on the audience targeting patterns of newspapers, using the Chilean media ecosystem. First, we use the gravity model to analyze geography as a factor in explaining audience reachability. This shows that some newspapers are indeed driven by geographical factors (mostly local news outlets) but some others are not (national-distribution outlets). For those which are not, we use a regression model to study the influence of socioeconomic and political characteristics in news outlets adoption. We conclude that indeed larger, national-distribution news outlets target populations based on these factors, rather than on geography or immediacy. © 2019, The Author(s)."
"10.1140/epjds/s13688-019-0195-7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065637466&doi=10.1140%2fepjds%2fs13688-019-0195-7&partnerID=40&md5=41684185a2e38fc5e7748eef14eb03df","Identifying influential nodes in a network is a fundamental issue due to its wide applications, such as accelerating information diffusion or halting virus spreading. Many measures based on the network topology have emerged over the years to identify influential nodes such as Betweenness, Closeness, and Eigenvalue centrality. However, although most real-world networks are made of groups of tightly connected nodes which are sparsely connected with the rest of the network in a so-called modular structure, few measures exploit this property. Recent works have shown that it has a significant effect on the dynamics of networks. In a modular network, a node has two types of influence: a local influence (on the nodes of its community) through its intra-community links and a global influence (on the nodes in other communities) through its inter-community links. Depending on the strength of the community structure, these two components are more or less influential. Based on this idea, we propose to extend all the standard centrality measures defined for networks with no community structure to modular networks. The so-called “Modular centrality” is a two-dimensional vector. Its first component quantifies the local influence of a node in its community while the second component quantifies its global influence on the other communities of the network. In order to illustrate the effectiveness of the Modular centrality extensions, comparison with their scalar counterparts is performed in an epidemic process setting. Simulation results using the Susceptible-Infected-Recovered (SIR) model on synthetic networks with controlled community structure allows getting a clear idea about the relation between the strength of the community structure and the major type of influence (global/local). Furthermore, experiments on real-world networks demonstrate the merit of this approach. © 2019, The Author(s)."
"10.1140/epjds/s13688-019-0191-y","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065148735&doi=10.1140%2fepjds%2fs13688-019-0191-y&partnerID=40&md5=6291c3ac3d4ce4b72fe7e4f41b525a02","To complement traditional dietary surveys, which are costly and of limited scale, researchers have resorted to digital data to infer the impact of eating habits on people’s health. However, online studies are limited in resolution: they are carried out at country or regional level and do not capture precisely the composition of the food consumed. We study the association between food consumption (derived from the loyalty cards of the main grocery retailer in London) and health outcomes (derived from publicly-available medical prescription records of all general practitioners in the city). The scale and granularity of our analysis is unprecedented: we analyze 1.6B food item purchases and 1.1B medical prescriptions for the entire city of London over the course of one year. By studying food consumption down to the level of nutrients, we show that nutrient diversity and amount of calories are the two strongest predictors of the prevalence of three diseases related to what is called the “metabolic syndrome”: hypertension, high cholesterol, and diabetes. This syndrome is a cluster of symptoms generally associated with obesity, is common across the rich world, and affects one in four adults in the UK. Our linear regression models achieve an R 2 of 0.6 when estimating the prevalence of diabetes in nearly 1000 census areas in London, and a classifier can identify (un)healthy areas with up to 91% accuracy. Interestingly, healthy areas are not necessarily well-off (income matters less than what one would expect) and have distinctive features: they tend to systematically eat less carbohydrates and sugar, diversify nutrients, and avoid large quantities. More generally, our study shows that analytics of digital records of grocery purchases can be used as a cheap and scalable tool for health surveillance and, upon these records, different stakeholders from governments to insurance companies to food companies could implement effective prevention strategies. © 2019, The Author(s)."
"10.1140/epjds/s13688-019-0190-z","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065017590&doi=10.1140%2fepjds%2fs13688-019-0190-z&partnerID=40&md5=10f6c91f8bf1b5f4ab088a44c378943c","During the past decades the importance of soft skills for labour market outcomes has grown substantially. This carries implications for labour market inequality, since previous research shows that soft skills are not valued equally across race and gender. This work explores the role of soft skills in job advertisements by drawing on methods from computational science as well as on theoretical and empirical insights from economics, sociology and psychology. We present a semi-automatic approach based on crowdsourcing and text mining for extracting a list of soft skills. We find that soft skills are a crucial component of job ads, especially of low-paid jobs and jobs in female-dominated professions. Our work shows that soft skills can serve as partial predictors of the gender composition in job categories and that not all soft skills receive equal wage returns at the labour market. Especially “female” skills are frequently associated with wage penalties. Our results expand the growing literature on the association of soft skills on wage inequality and highlight their importance for occupational gender segregation at labour markets. © 2019, The Author(s)."
"10.1140/epjds/s13688-019-0192-x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064260228&doi=10.1140%2fepjds%2fs13688-019-0192-x&partnerID=40&md5=33aab53cbf2ea6083741c0473f5c56ee","In this paper, to reveal the differences of gender-specific preference and the factors affecting potential mate choice in online dating, we analyze the users’ behavioral data of a large online dating site in China. We find that for women, network measures of popularity and activity of the men they contact are significantly positively associated with their messaging behaviors, while for men only the network measures of popularity of the women they contact are significantly positively associated with their messaging behaviors. Secondly, when women send messages to men, they pay attention to not only whether men’s attributes meet their own requirements for mate choice, but also whether their own attributes meet men’s requirements, while when men send messages to women, they only pay attention to whether women’s attributes meet their own requirements. Thirdly, compared with men, women attach great importance to the socio-economic status of potential partners and their own socio-economic status will affect their enthusiasm for interaction with potential mates. Further, we use the ensemble learning classification methods to rank the importance of factors predicting messaging behaviors, and find that the centrality indices of users are the most important factors. Finally, by correlation analysis we find that men and women show different strategic behaviors when sending messages. Compared with men, for women sending messages, there is a stronger positive correlation between the centrality indices of women and men, and more women tend to send messages to people more popular than themselves. These results have implications for understanding gender-specific preference in online dating further and designing better recommendation engines for potential dates. The research also suggests new avenues for data-driven research on stable matching and strategic behavior combined with game theory. © 2019, The Author(s)."
"10.1140/epjds/s13688-019-0189-5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063671526&doi=10.1140%2fepjds%2fs13688-019-0189-5&partnerID=40&md5=781265428610f5e29432f72733338a96","How diverse are sharing economy platforms? Are they fair marketplaces, where all participants operate on a level playing field, or are they large-scale online aggregators of offline human biases? Often portrayed as easy-to-access digital spaces whose participants receive equal opportunities, such platforms have recently come under fire due to reports of discriminatory behaviours among their users, and have been associated with gentrification phenomena that exacerbate preexisting inequalities along racial lines. In this paper, we focus on the Airbnb sharing economy platform, and analyse the diversity of its user base across five large cities. We find it to be predominantly young, female, and white. Notably, we find this to be true even in cities with a diverse racial composition. We then introduce a method based on the statistical analysis of networks to quantify behaviours of homophily, heterophily and avoidance between Airbnb hosts and guests. Depending on cities and property types, we do find signals of such behaviours relating both to race and gender. We use these findings to provide platform design recommendations, aimed at exposing and possibly reducing the biases we detect, in support of a more inclusive growth of sharing economy platforms. © 2019, The Author(s)."
"10.1140/epjds/s13688-019-0188-6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063642794&doi=10.1140%2fepjds%2fs13688-019-0188-6&partnerID=40&md5=d5dad621e3f7adf134e598b3d6e8adcc","In this paper we develop a methodology, based on Mutual Information and Transfer of Entropy, that allows to identify, quantify and map on a network the synchronization and anticipation relationships between financial traders. We apply this methodology to a dataset containing 410 , 612 real buy and sell operations, made by 566 non-professional investors from a private investment firm on 8 different assets from the Spanish IBEX market during a period of time from 2000 to 2008. These networks present a peculiar topology significantly different from the random networks. We seek alternative features based on human behavior that might explain part of those 12 , 158 synchronization links and 1031 anticipation links. Thus, we detect that daily synchronization with price (present in 64.90% of investors) and the one-day delay with respect to price (present in 4.38% of investors) play a significant role in the network structure. We find that individuals reaction to daily price changes explains around 20% of the links in the Synchronization Network, and has significant effects on the Anticipation Network. Finally, we show how using these networks we substantially improve the prediction accuracy when Random Forest models are used to nowcast and predict the activity of individual investors. © 2019, The Author(s)."
"10.1140/epjds/s13688-019-0185-9","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062952031&doi=10.1140%2fepjds%2fs13688-019-0185-9&partnerID=40&md5=2945b6d40bcc2e0aa75c30b90ea0ce3d","In human relations individuals’ gender and age play a key role in the structures and dynamics of their social arrangements. In order to analyze the gender preferences of individuals in interaction with others at different stages of their lives we study a large mobile phone dataset. To do this we consider four fundamental gender-related caller and callee combinations of human interactions, namely male to male, male to female, female to male, and female to female, which together with age, kinship, and different levels of friendship give rise to a wide scope of human sociality. Here we analyse the relative strength of these four types of interaction using call detail records. Our analysis suggests strong age dependence for an individual of one gender choosing to call an individual of either gender. We observe a strong bonding with the opposite gender across most of their reproductive age. However, older women show a strong tendency to connect to another female that is one generation younger in a way that is suggestive of the grandmothering effect. We also find that the relative strength among the four possible interactions depends on phone call duration. For calls of medium and long duration, opposite gender interactions are significantly more probable than same gender interactions during the reproductive years, suggesting potential emotional exchange between spouses. By measuring the fraction of calls to other generations we find that mothers tend to make calls more to their daughters than to their sons, whereas fathers make calls more to their sons than to their daughters. For younger callers, most of their calls go to the same generation contacts, while older people call the younger people more frequently, which supports the suggestion that affection flows downward. Our study primarily rests on resolving the nature of interactions by examining the durations of calls. In addition, we analyse the intensity of the observed effects using a score based on a null model. © 2019, The Author(s)."
"10.1140/epjds/s13688-019-0187-7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062548865&doi=10.1140%2fepjds%2fs13688-019-0187-7&partnerID=40&md5=4b611390885b2f60cf17348a07463183","The tendency of people to form socially cohesive groups that get together in urban spaces is a fundamental process that drives the formation of the social structure of cities. However, the challenge of collecting and mining large-scale data able to unveil both the social and the mobility patterns of people has left many questions about urban social groups largely unresolved. We leverage an anonymized mobile phone dataset, based on Call Detail Records (CDRs), which integrates the usual voice call data with text message and Internet activity information of one million mobile subscribers in the metropolitan area of Milan to investigate how the members of social groups interact and meet onto the urban space. We unveil the nature of these groups through an extensive analysis, along with proposing a methodology for their identification. The findings of this study concern the social group behavior, their structure (size and membership) and their root in the territory (locations and visit patterns). Specifically, the footprint of urban groups is made up by a few visited locations only; which are regularly visited by the groups. Moreover, the analysis of the interaction patterns shows that urban groups need to combine frequent on-phone interactions with gatherings in such locations. Finally, we investigate how their preferences impact the city of Milan telling us which areas encourage group get-togethers best. © 2019, The Author(s)."
"10.1140/epjds/s13688-019-0186-8","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062279025&doi=10.1140%2fepjds%2fs13688-019-0186-8&partnerID=40&md5=efeec71400c5f5d13004209f46502114","Car sharing is one the pillars of a smart transportation infrastructure, as it is expected to reduce traffic congestion, parking demands and pollution in our cities. From the point of view of demand modelling, car sharing is a weak signal in the city landscape: only a small percentage of the population uses it, and thus it is difficult to study reliably with traditional techniques such as households travel diaries. In this work, we depart from these traditional approaches and we leverage web-based, digital records about vehicle availability in 10 European cities for one of the major active car sharing operators. We discuss which sociodemographic and urban activity indicators are associated with variations in car sharing demand, which forecasting approach (among the most popular in the related literature) is better suited to predict pickup and drop-off events, and how the spatio-temporal information about vehicle availability can be used to infer how different zones in a city are used by customers. We conclude the paper by presenting a direct application of the analysis of the dataset, aimed at identifying where to locate maintenance facilities within the car sharing operation area. © 2019, The Author(s)."
"10.1140/epjds/s13688-019-0184-x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062273331&doi=10.1140%2fepjds%2fs13688-019-0184-x&partnerID=40&md5=2c68e73c347b3240cf2c5f0d65c959b7","Religion is considered as a notable origin of interpersonal relations, as well as an effective and efficient tool to organize a huge number of people towards some challenging targets. At the same time, a believer prefers to make friend with other people of the same faith, and thus people of different faiths tend to form relatively isolated communities. The segregation between different religions is a major factor for many social conflicts. However, quantitative understanding of religious segregation is rare. Here we analyze a directed social network extracted from weibo.com (the largest directed social network in China, similar to twitter.com), which is consisted of 6875 believers in Christianity, Buddhism, Islam and Taoism. This religion network is highly segregative. Comparative analysis shows that the extent of segregation for different religions is much higher than that for different races and slightly higher than that for different political parties. Furthermore, we study the few cross-religion links and find 46.7% of them are probably related to charitable issues. Our findings provide quantitative insights into religious segregation and valuable evidence for religious syncretism. © 2019, The Author(s)."
"10.1140/epjds/s13688-019-0182-z","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061713419&doi=10.1140%2fepjds%2fs13688-019-0182-z&partnerID=40&md5=2117d9fe6d86defa0c67a5ff697205d0","The global community decorates their homes based on personal decisions and contextual influences of their larger cultural and economic surroundings. The extent to which spatial patterns emerge in residential decoration practices has been traditionally difficult to ascertain due to the private nature of interior home spaces. Yet, measuring these patterns can reveal the presence of geographic culture hearths and/or globalization trends. In this work, we collected over one million geolocated images of interior living spaces from a popular home rental website, Airbnb (http://airbnb.com), and used transfer learning techniques to automatically detect the presence of key stylistic objects: plants, books, decor, wall art and predominance of vibrant colors. We investigated patterns of home decor practices for 107 cities on six continents, and performed a deep dive into six major U.S. cities. We found that world regions show statistically significant variation in decorative element prevalence, indicating differences in geographic cultural trends. At the U.S. neighborhood level, elements were only weakly spatially clustered and found to not correlate with socio-economic neighborhood variables such as income, unemployment rates, education attainment, residential property value, and racial diversity. These results may suggest that American residents in different socio-economic environments put similar effort into personalizing and caring for their homes. More broadly, our results represent a new view of worldwide human behavior and a new application of machine learning techniques to the exploration of cultural phenomena. © 2019, The Author(s)."
"10.1140/epjds/s13688-019-0183-y","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061621407&doi=10.1140%2fepjds%2fs13688-019-0183-y&partnerID=40&md5=1b9d624e18789ca0f14d994fda8ff0ac","The diffusion of new information and communication technologies—social media in particular—has played a key role in social and political activism in recent decades. In this paper, we propose a theory-motivated, spatiotemporal learning approach, ActAttn, that leverages social movement theories and a deep learning framework to examine the relationship between protest events and their social and geographical contexts as reflected in social media discussions. To do so, we introduce a novel predictive framework that incorporates a new design of attentional networks, and which effectively learns the spatiotemporal structure of features. Our approach is not only capable of forecasting the occurrence of future protests, but also provides theory-relevant interpretations—it allows for interpreting what features, from which places, have significant contributions on the protest forecasting model, as well as how they make those contributions. Our experiment results from three movement events indicate that ActAttn achieves superior forecasting performance, with interesting comparisons across the three events that provide insights into these recent movements. © 2019, The Author(s)."
"10.1140/epjds/s13688-019-0181-0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060906667&doi=10.1140%2fepjds%2fs13688-019-0181-0&partnerID=40&md5=9922919ebd33c8a05602f7cdd44537c3","The Modified Mercalli intensity scale (Mercalli scale for short) is a qualitative measure used to express the perceived intensity of an earthquake in terms of damages. Accurate intensity reports are vital to estimate the type of emergency response required for a particular earthquake. In addition, Mercalli scale reports are needed to estimate the possible consequences of strong earthquakes in the future, based on the effects of previous events. Emergency offices and seismological agencies worldwide are in charge of producing Mercalli scale reports for each affected location after an earthquake. However, this task relies heavily on human observers in the affected locations, who are not always available or accurate. Consequently, Mercalli scale reports may take up to hours or even days to be published after an earthquake. We address this problem by proposing a method for early prediction of spatial Mercalli scale reports based on people’s reactions to earthquakes in social networks. By tracking users’ comments about real-time earthquakes, we create a collection of Mercalli scale point estimates at municipality (i.e., state subdivisions) level granularity. We introduce the concept of reinforced Mercalli support, which combines Mercalli scale point estimates with locally supported data (named ‘local support’). We use this concept to provide Mercalli scale estimates for real-world events by providing smooth point estimates using a spatial smoother that incorporates the distribution of municipalities in each affected region. Our method is the first method based on social media that can provide spatial reports of damages in the Mercalli intensity scale. Experimental results show that our method is accurate and provides early spatial Mercalli reports 30 minutes after an earthquake. Furthermore, we show that our method performs well for earthquake spatial detection and maximum intensity prediction tasks. Our findings indicate that social media is a valuable source of spatial information for quickly estimating earthquake damages. © 2019, The Author(s)."
"10.1140/epjds/s13688-018-0179-z","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059688337&doi=10.1140%2fepjds%2fs13688-018-0179-z&partnerID=40&md5=ade760c61226c946da2635b0401fc7c9","Pattern detection in network models provides insights to both global structure and local node interactions. In particular, studying patterns embedded within remittance and migration flow networks can be useful in understanding economic and sociologic trends and phenomena and their implications both in regional and global settings. We illustrate how topo-algebraic methods can be used to detect both local and global patterns that highlight simultaneous interactions among multiple nodes, giving a more holistic perspective on the network fabric and a higher order description of the overall flow structure of directed networks. Using the 2015 Asian net migration and remittance networks, we build and study the associated directed clique complexes whose topological features correspond to specific flow patterns in the networks. We generate diagrams recording the presence, persistence, and perpetuity of patterns and show how these diagrams can be used to make inferences about the characteristics of migrant movement patterns and remittance flows. © 2019, The Author(s)."
"10.1186/s40537-019-0199-y","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066050998&doi=10.1186%2fs40537-019-0199-y&partnerID=40&md5=d54054874f4d8cf6d291bfcca039cb77","Big data architectures have been gaining momentum in recent years. For instance, Twitter uses stream processing frameworks like Apache Storm to analyse billions of tweets per minute and learn the trending topics. However, architectures that process big data involve many different components interconnected via semantically different connectors. Such complex architectures make possible refactoring of the applications a difficult task for software architects, as applications might be very different with respect to the initial designs. As an aid to designers and developers, we developed OSTIA (Ordinary Static Topology Inference Analysis) that allows detecting the occurrence of common anti-patterns across big data architectures and exploiting software verification techniques on the elicited architectural models. This paper illustrates OSTIA and evaluates its uses and benefits on three industrial-scale case-studies. © 2019, The Author(s)."
"10.1186/s40537-019-0204-5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066021315&doi=10.1186%2fs40537-019-0204-5&partnerID=40&md5=5b14492d41dbb1e75fcc08ac3f48d39e","Data validation is about verifying the correctness of data. When organisations update and refine their data transformations to meet evolving requirements, it is imperative to ensure that the new version of a workflow still produces the correct output. We motivate the need for workflows and describe the implementation of a validation tool called Diftong. This tool compares two tabular databases resulting from different versions of a workflow to detect and prevent potential unwanted alterations. Row-based and column-based statistics are used to quantify the results of the database comparison. Diftong was shown to provide accurate results in test scenarios, bringing benefits to companies that need to validate the outputs of their workflows. By automating this process, the risk of human error is also eliminated. Compared to the more labour-intensive manual alternative, it has the added benefit of improved turnaround time for the validation process. Together this allows for a more agile way of updating data transformation workflows. © 2019, The Author(s)."
"10.1186/s40537-019-0202-7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065849860&doi=10.1186%2fs40537-019-0202-7&partnerID=40&md5=d73174419d206b5936ee2be064d6ab4e","No sleep! No sick days! No Holidays! alongside 24/7 around the clock tireless work, welcome to the world of Artificial Intelligence, how can humans match the phenomena of AI. Humanity is in the grip of an unstoppable, and exciting future with the future development of AI, humans must decide whether to ride on the coat-tails of AI or resist the inevitable change of the world as we know it. The real game changer will be the commercial availability of Quantum Computing. Humans must learn to work, live, learn and interact with AI or become second class citizens. Big data is the lifeblood of AI and Quantum Computing processing power will enable future computers to process incredible amounts of big data. AI is taking over the job’s human do, receptionists, drivers, chefs and in the future doctors and accountants. Quantum Supremacy will soon be achieved, and AI will soon reach and far exceed the “Singularity”. Will humans grasp the opportunity to develop with AI or resist? humans have the chance to develop with AI through brain computer interface (BCI) technology. Security and regulations have to be put in place, with questions of “Who is responsible for AI security and regulations”? and “can AI be trusted as an autonomous entity” also the ethical use of AI has to be addressed, “What about the rights and ethics of AI”?. The human race is on an inevitable path of AI dominance the question is “will humans and AI be friends or adversaries”? © 2019, The Author(s)."
"10.1186/s40537-019-0200-9","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065846315&doi=10.1186%2fs40537-019-0200-9&partnerID=40&md5=9d38369aaafd280689243812df61e9ad","Pattern mining is a powerful tool for analysing big datasets. Temporal datasets include time as an additional parameter. This leads to complexity in algorithmic formulation, and it can be challenging to process such data quickly and efficiently. In addition, errors or uncertainty can exist in the timestamps of data, for example in manually recorded health data. Sometimes we wish to find patterns only within a certain temporal range. In some cases real-time processing and decision-making may be desirable. All these issues increase algorithmic complexity, processing times and storage requirements. In addition, it may not be possible to store or process confidential data on public clusters or the cloud that can be accessed by many people. Hence it is desirable to optimise algorithms for standalone systems. In this paper we present an integrated approach which can be used to write efficient codes for pattern mining problems. The approach includes: (1) cleaning datasets with removal of infrequent events, (2) presenting a new scheme for time-series data storage, (3) exploiting the presence of prior information about a dataset when available, (4) utilising vectorisation and multicore parallelisation. We present two new algorithms, FARPAM (FAst Robust PAttern Mining) and FARPAMp (FARPAM with prior information about prior uncertainty, allowing faster searching). The algorithms are applicable to a wide range of temporal datasets. They implement a new formulation of the pattern searching function which reproduces and extends existing algorithms (such as SPAM and RobustSPAM), and allows for significantly faster calculation. The algorithms also include an option of temporal restrictions in patterns, which is available neither in SPAM nor in RobustSPAM. The searching algorithm is designed to be flexible for further possible extensions. The algorithms are coded in C++, and are highly optimised and parallelised for a modern standalone multicore workstation, thus avoiding security issues connected with transfers of confidential data onto clusters. FARPAM has been successfully tested on a publicly available weather dataset and on a confidential adult social care dataset, reproducing results obtained by previous algorithms in both cases. It has been profiled against the widely used SPAM algorithm (for sequential pattern mining) and RobustSPAM (developed for datasets with errors in time points). The algorithm outperforms SPAM by up to 20 times and RobustSPAM by up to 6000 times. In both cases the new algorithm has better scalability. © 2019, The Author(s)."
"10.1186/s40537-019-0203-6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065817760&doi=10.1186%2fs40537-019-0203-6&partnerID=40&md5=e1d12b78f8811b00a7bed650b9e5e68e","Moving objects such as people, animals, and vehicles have generated a large amount of spatiotemporal data by using location-capture technologies and mobile devices. This collected data needs to be processed, visualized and analyzed to transform raw trajectory data into useful knowledge. In this study, we build a system to deliver a set of traffic insights and recommendations by applying two techniques, clustering, and sequential pattern mining. This system has three stages, the first stage preprocesses and samples the dataset into 168 subsets, the second stage applies two clustering techniques, the hierarchical density-based spatial clustering (HDBSCAN) and the Random Swap clustering (RS). We compare these two clustering algorithms in terms of processing time and quality of clusters. In the comparative analysis, the Silhouette coefficient shows that RS clustering outperforms HDBSCAN in terms of clusters quality. Moreover, the analysis shows that RS outperforms K-means in terms of the mean of square error (MSE) reduction. After that, we use a Google Maps approach to label the traffic districts and apply sequential pattern mining to extract taxi trips flow. The system can detect 146 sequential patterns in different areas of the city. In the last stage, we visualize traffic clusters generated from the RS algorithm. Furthermore, we visualize the taxi trips heatmap per weekday and hour of the day in Porto city. This system can be integrated with the current traffic control applications to provide useful guidelines for taxi drivers, passengers, and transportation authorities. © 2019, The Author(s)."
"10.1186/s40537-019-0201-8","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065572191&doi=10.1186%2fs40537-019-0201-8&partnerID=40&md5=d4b4617f340ec214a43292cbd871bd5d","We study the the spread and adoption of libraries within Python projects hosted in public software repositories on GitHub. By modelling the use of Git pull, merge, commit, and other actions as deliberate cognitive activities, we are able to better understand the dynamics of what happens when users adopt new and cognitively demanding information. For this task we introduce a large corpus containing all commits, diffs, messages, and source code from 259,690 Python repositories (about 13% of all Python projects on Github), including all Git activity data from 89,311 contributing users. In this initial work we ask two primary questions: (1) What kind of behavior change occurs near an adoption event? (2) Can we model future adoption activity of a user? Using a fine-grained analysis of user behavior, we show that library adoptions are followed by higher than normal activity within the first 6 h, implying that a higher than normal cognitive effort is involved with an adoption. Further study is needed to understand the specific types of events that surround the adoption of new information, and the cause of these dynamics. We also show that a simple linear model is capable of classifying future commits as being an adoption or not, based on the commit contents and the preceding history of the user and repository. Additional work in this vein may be able to predict the content of future commits, or suggest new libraries to users. © 2019, The Author(s)."
"10.1186/s40537-019-0196-1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065422484&doi=10.1186%2fs40537-019-0196-1&partnerID=40&md5=05dae6735315f5a1131c7074a3d38dde","Hive has long been one of the industry-leading systems for Data Warehousing in Big Data contexts, mainly organizing data into databases, tables, partitions and buckets, stored on top of an unstructured distributed file system like HDFS. Some studies were conducted for understanding the ways of optimizing the performance of several storage systems for Big Data Warehousing. However, few of them explore the impact of data organization strategies on query performance, when using Hive as the storage technology for implementing Big Data Warehousing systems. Therefore, this paper evaluates the impact of data partitioning and bucketing in Hive-based systems, testing different data organization strategies and verifying the efficiency of those strategies in query performance. The obtained results demonstrate the advantages of implementing Big Data Warehouses based on denormalized models and the potential benefit of using adequate partitioning strategies. Defining the partitions aligned with the attributes that are frequently used in the conditions/filters of the queries can significantly increase the efficiency of the system in terms of response time. In the more intensive workload benchmarked in this paper, overall decreases of about 40% in processing time were verified. The same is not verified with the use of bucketing strategies, which shows potential benefits in very specific scenarios, suggesting a more restricted use of this functionality, namely in the context of bucketing two tables by the join attribute of these tables. © 2019, The Author(s)."
"10.1186/s40537-019-0198-z","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065421602&doi=10.1186%2fs40537-019-0198-z&partnerID=40&md5=51f08fe056ff75c3eee3b2da810a1e42","Background: Despite the well-documented and numerous recent successes of deep learning, the application of standard deep architectures to many classification problems within empirical software engineering remains problematic due to the large volumes of labeled data required for training. Here we make the argument that, for some problems, this hurdle can be overcome by taking advantage of low-shot learning in combination with simpler deep architectures that reduce the total number of parameters that need to be learned. Findings: We apply low-shot learning to the task of classifying UML class and sequence diagrams from Github, and demonstrate that surprisingly good performance can be achieved by using only tens or hundreds of examples for each category when paired with an appropriate architecture. Using a large, off-the-shelf architecture, on the other hand, doesn’t perform beyond random guessing even when trained on thousands of samples. Conclusion: Our findings suggest that identifying problems within empirical software engineering that lend themselves to low-shot learning could accelerate the adoption of deep learning algorithms within the empirical software engineering community. © 2019, The Author(s)."
"10.1186/s40537-019-0195-2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064625188&doi=10.1186%2fs40537-019-0195-2&partnerID=40&md5=008ead4640f0549f5cec8399c584d587","The global popularity of social media platforms has given rise to unprecedented amounts of data, much of which reflects the thoughts, opinions and affective states of individual users. Systematic explorations of these large datasets can yield valuable information about a variety of psychological and sociocultural variables. The global nature of these platforms makes it important to extend this type of exploration across cultures and languages as each situation is likely to present unique methodological challenges and yield findings particular to the specific sociocultural context. To date, very few studies exploring large social media datasets have focused on the Arab world. This study examined social media use in Arabic and English across the United Arab Emirates (UAE), looking specifically at indicators of subjective wellbeing (happiness) across both languages. A large social media dataset, spanning 2013 to 2017, was extracted from Twitter. More than 17 million Twitter messages (tweets), written in Arabic and English and posted by users based in the UAE, were analyzed. Numerous differences were observed between individuals posting messages (tweeting) in English compared with those posting in Arabic. These differences included significant variations in the mean number of tweets posted, and the mean size of users networks (e.g. the number of followers). Additionally, using lexicon-based sentiment analytic tools (Hedonometer and Valence Shift Word Graphs), temporal patterns of happiness (expressions of positive sentiment) were explored in both languages across all seven regions (Emirates) of the UAE. Findings indicate that 7:00 am was the happiest hour, and Friday was the happiest day for both languages (the least happy day varied by language). The happiest months differed based on language, and there were also significant variations in sentiment patterns, peaks and troughs in happiness, associated with events of sociopolitical and religio-cultural significance for the UAE. © 2019, The Author(s)."
"10.1186/s40537-019-0190-7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064264403&doi=10.1186%2fs40537-019-0190-7&partnerID=40&md5=aa1a9b276fde05916f311bd8cebc57d6","Today, the diagnosis of Alzheimer’s disease (AD) or mild cognitive impairment (MCI) has attracted the attention of researchers in this field owing to the increase in the occurrence of the diseases and the need for early diagnosis. Unfortunately, the nature of high dimension of neural data and few available samples led to the creation of a precise computer diagnostic system. Machine learning techniques, especially deep learning, have been considered as a useful tool in this field. Inspired by the concept of unsupervised feature learning that uses artificial intelligence to learn features from raw data, a two-stage method was presented for an intelligent diagnosis of Alzheimer’s disease. At the first stage of learning, scattered filtering, an uncontrolled two-layer neural network was used to directly learn features from raw data. At the second stage, SoftMax regression was used to categorize health statuses based on the learned features. The proposed method was validated by the data sets of Alzheimer’s Brain Images. The results showed that the proposed method achieved very good diagnostic accuracy and was better than the existing methods for brain image data sets. The proposed method reduces the need for human work and makes it easy to intelligently diagnose for big data processing, because the learning features are adaptive. In our experiments with the Alzheimer’s Disease Neuroimaging Initiative (ADNI) data, a dual and multi-class classification was conducted for AD/MCI diagnosis and the superiority of the proposed method in comparison with the advanced methods was shown. © 2019, The Author(s)."
"10.1186/s40537-019-0193-4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063892870&doi=10.1186%2fs40537-019-0193-4&partnerID=40&md5=d8a41bf8135b6cfb04eb7e85f21cdf0a","The publication and dissemination of raw data are crucial elements in commercial, academic, and medical applications. With an increasing number of open platforms, such as social networks and mobile devices from which data may be collected, the volume of such data has also increased over time move toward becoming as Big Data. The traditional model of Big Data does not specify any level for capturing the sensitivity of data both structured and unstructured. It additionally needs to incorporate the notion of privacy and security where the risk of exposing personal information is probabilistically minimized. This paper introduced security and privacy layer between HDFS and MR Layer (Map Reduce) known as new proposed Secured Map Reduce (SMR) Layer and this model is known as SMR model. The core benefit of this work is to promote data sharing for knowledge mining. This model creates a privacy and security guarantee, resolve scalability issues of privacy and maintain the privacy-utility tradeoff for data miners. In this SMR model, running time and information loss have a remarkable improvement over the existing approaches and CPU and memory usage are also optimized. © 2019, The Author(s)."
"10.1186/s40537-019-0194-3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063687586&doi=10.1186%2fs40537-019-0194-3&partnerID=40&md5=af5206f9e48282b9a724e6b3eb3e3c1c","We address the problem of detecting highly raised crowd density in situations such as indoor dance events. We propose a new method for estimating crowd density by anonymous, non-participatory, indoor Wi-Fi localization of smart phones. Using a probabilistic model inspired by statistical mechanics, and relying only on big data analytics, we tackle three challenges: (1) the ambiguity of Wi-Fi based indoor positioning, which appears regardless of whether the latter is performed with machine learning or with optimization, (2) the MAC address randomization when a device is not connected, and (3) the volatility of packet interarrival times. The main result is that our estimation becomes more—rather than less—accurate when the crowd size increases. This property is crucial for detecting dangerous crowd density. © 2019, The Author(s)."
"10.1186/s40537-019-0188-1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063275399&doi=10.1186%2fs40537-019-0188-1&partnerID=40&md5=64ab7aa6b19456bee2c062ec83d432d8","Due to their conceptual simplicity and flexibility, non-parametric mixture models are widely used to identify latent clusters in data. However, when it comes to Big Data, such as Landsat imagery, such model fitting is computationally prohibitive. To overcome this issue, we fit Bayesian non-parametric models to pre-smoothed data, thereby reducing the computational time from days to minutes, while disregarding little of the useful information. Tree based clustering is used to partition the clusters into smaller and smaller clusters in order to identify clusters of high, medium and low interest. The tree-based clustering method is applied to Landsat images from the Brisbane region, which were the actual sources of motivation for development of the method. The images are taken as a part of the red imported fire-ant eradication program that was launched in September 2001 and which is funded by all Australian states and territories, along with the federal government. To satisfy budgetary constraints, modelling is performed to estimate the risk of fire-ant incursion in each cluster so that the eradication program focuses on high risk clusters. The likelihood of containment is successfully derived by combining the fieldwork survey data with the results obtained from the proposed method. © 2019, The Author(s)."
"10.1186/s40537-019-0192-5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063272245&doi=10.1186%2fs40537-019-0192-5&partnerID=40&md5=b0adac24891dae46e4c6660a082d1d56","The purpose of this study is to examine existing deep learning techniques for addressing class imbalanced data. Effective classification with imbalanced data is an important area of research, as high class imbalance is naturally inherent in many real-world applications, e.g., fraud detection and cancer detection. Moreover, highly imbalanced data poses added difficulty, as most learners will exhibit bias towards the majority class, and in extreme cases, may ignore the minority class altogether. Class imbalance has been studied thoroughly over the last two decades using traditional machine learning models, i.e. non-deep learning. Despite recent advances in deep learning, along with its increasing popularity, very little empirical work in the area of deep learning with class imbalance exists. Having achieved record-breaking performance results in several complex domains, investigating the use of deep neural networks for problems containing high levels of class imbalance is of great interest. Available studies regarding class imbalance and deep learning are surveyed in order to better understand the efficacy of deep learning when applied to class imbalanced data. This survey discusses the implementation details and experimental results for each study, and offers additional insight into their strengths and weaknesses. Several areas of focus include: data complexity, architectures tested, performance interpretation, ease of use, big data application, and generalization to other domains. We have found that research in this area is very limited, that most existing work focuses on computer vision tasks with convolutional neural networks, and that the effects of big data are rarely considered. Several traditional methods for class imbalance, e.g. data sampling and cost-sensitive learning, prove to be applicable in deep learning, while more advanced methods that exploit neural network feature learning abilities show promising results. The survey concludes with a discussion that highlights various gaps in deep learning from class imbalanced data for the purpose of guiding future research. © 2019, The Author(s)."
"10.1186/s40537-019-0191-6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063251133&doi=10.1186%2fs40537-019-0191-6&partnerID=40&md5=7d53f654543d74e2ad62e6a2f6fbb9c6","Customer churn is a major problem and one of the most important concerns for large companies. Due to the direct effect on the revenues of the companies, especially in the telecom field, companies are seeking to develop means to predict potential customer to churn. Therefore, finding factors that increase customer churn is important to take necessary actions to reduce this churn. The main contribution of our work is to develop a churn prediction model which assists telecom operators to predict customers who are most likely subject to churn. The model developed in this work uses machine learning techniques on big data platform and builds a new way of features’ engineering and selection. In order to measure the performance of the model, the Area Under Curve (AUC) standard measure is adopted, and the AUC value obtained is 93.3%. Another main contribution is to use customer social network in the prediction model by extracting Social Network Analysis (SNA) features. The use of SNA enhanced the performance of the model from 84 to 93.3% against AUC standard. The model was prepared and tested through Spark environment by working on a large dataset created by transforming big raw data provided by SyriaTel telecom company. The dataset contained all customers’ information over 9 months, and was used to train, test, and evaluate the system at SyriaTel. The model experimented four algorithms: Decision Tree, Random Forest, Gradient Boosted Machine Tree “GBM” and Extreme Gradient Boosting “XGBOOST”. However, the best results were obtained by applying XGBOOST algorithm. This algorithm was used for classification in this churn predictive model. © 2019, The Author(s)."
"10.1186/s40537-019-0189-0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062986974&doi=10.1186%2fs40537-019-0189-0&partnerID=40&md5=c2f33dc8422a80c1d4363b1d2d66efa0","Prevention of hospital readmissions has the potential of providing better quality of care to the patients and deliver significant cost savings. A review of existing readmission analysis frameworks based on data type, data size, disease conditions, algorithms and other features shows that existing frameworks do not address the issue of using large amounts of data that is fundamental to readmission prediction analysis. Available patient data for readmission risk analysis has high dimensionality and number of instances. Further, there is more new data produced everyday which can be used on a continuous basis to improve the prediction power of risk models. This study proposes a High Performance Computing Cluster based Big Data readmission risk analysis framework which uses Nave Bayes classification algorithm. The study shows that the over-all evaluation time using Big Data and a parallel computing platform can be significantly decreased, while maintaining model performance. © 2019, The Author(s)."
"10.1186/s40537-019-0187-2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062610127&doi=10.1186%2fs40537-019-0187-2&partnerID=40&md5=7fa9795bc420b1d087f4fa58e612c54e","Critical systems that produce big data streams can require human operators to monitor these event streams for changes of interest. Automated systems which oversee many tasks can still have a need for the ‘human-in-the-loop’ operator to evaluate whether an intervention is required due to a lack of suitable training data initially offered to the system which would allow a correct course of actions to be taken. In order for an operator to be capable of reacting to real-time events, the visual depiction of the event data must be in a form which captures essential associations and is readily understood by visual inspection. A similar requirement can be found during inspections on activity protocols in a large organization where a code of correct conduct is prescribed and there is a need to oversee whether the activity traces match the expectations, with minimal delay. The methodology presented here addresses these concerns by providing an adaptive window sizing measurement for subsetting the data, and subsequently produces a set of network diagrams based upon event label co-occurrence networks. With an intuitive method of network construction the amount of time required for operators to learn how to monitor complex event streams of big datasets can be reduced. © 2019, The Author(s)."
"10.1186/s40537-019-0183-6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062549653&doi=10.1186%2fs40537-019-0183-6&partnerID=40&md5=074a943b0014a262d4d2fd1887c1e2fb","The Internet of Things (IoT) facilitates creation of smart spaces by converting existing environments into sensor-rich data-centric cyber-physical systems with an increasing degree of automation, giving rise to Industry 4.0. When adopted in commercial/industrial contexts, this trend is revolutionising many aspects of our everyday life, including the way people access and receive healthcare services. As we move towards Healthcare Industry 4.0, the underlying IoT systems of Smart Healthcare spaces are growing in size and complexity, making it important to ensure that extreme amounts of collected data are properly processed to provide valuable insights and decisions according to requirements in place. This paper focuses on the Smart Healthcare domain and addresses the issue of data fusion in the context of IoT networks, consisting of edge devices, network and communications units, and Cloud platforms. We propose a distributed hierarchical data fusion architecture, in which different data sources are combined at each level of the IoT taxonomy to produce timely and accurate results. This way, mission-critical decisions, as demonstrated by the presented Smart Healthcare scenario, are taken with minimum time delay, as soon as necessary information is generated and collected. The proposed approach was implemented using the Complex Event Processing technology, which natively supports the hierarchical processing model and specifically focuses on handling streaming data ‘on the fly’—a key requirement for storage-limited IoT devices and time-critical application domains. Initial experiments demonstrate that the proposed approach enables fine-grained decision taking at different data fusion levels and, as a result, improves the overall performance and reaction time of public healthcare services, thus promoting the adoption of the IoT technologies in Healthcare Industry 4.0. © 2019, The Author(s)."
"10.1186/s40537-019-0171-x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062467374&doi=10.1186%2fs40537-019-0171-x&partnerID=40&md5=bb929f0e1a2fff68869a8dbc9ee758c0","Background: Global environmental pollution caused by human activities has become a threat to public health. Children are especially susceptible to adverse environmental conditions owing to their unique physiological and behavioral characteristics. A number of studies have demonstrated associations between the incidence of some childhood diseases and adverse environmental conditions. Shanghai is the largest and most important economic center in China. After rapid population expansion in recent decades, the shortage of pediatric medical resources is becoming a serious public health problem. This study aimed to identify and characterize the social and environmental effect of adverse environmental conditions on overall pediatric admissions at hospitals in Shanghai, China. Methods: This was a multi-center study spanning from January, 2013 to November, 2014. Daily pediatric admission data (~ 12,000 overall pediatric admissions/day) of three tertiary pediatric hospitals were collected from the large-scale health information exchange network of Shanghai. We linked the admission data with local environmental data. A seasonal decomposition method was applied to a time-trend analysis of the admission data; a generalized additive model was applied to model the association between environmental measurements and admissions data. Results: Admissions to outpatient and emergency departments were highly influenced by calendar factors; however, these same factors showed opposite effects on different clinical departments. The effect of nitrogen dioxide was a 0.27% increase (95% confidence interval (CI) 0.23% to 0.32%) in outpatient admissions and 0.78% (95% CI 0.68% to 0.88%) increase in emergency admissions. Concentrations of fine particles ≤ 2.5 micrometers in diameter (PM2.5) and carbon monoxide (CO) showed multi-faceted effects on pediatric admissions. PM2.5 and CO concentrations were significantly associated with decreased current-day outpatient admissions but also significantly associated with increased current-day emergency admissions at all three hospitals. Conclusions: Based on the health information exchange network of Shanghai, we conducted a large-scale, multi-center retrospective study of the association between adverse environmental conditions and pediatric admissions. Our study contributes to environmental health research in children and may guide decision-making regarding pediatric resource planning and policies. © 2019, The Author(s)."
"10.1186/s40537-019-0184-5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062288940&doi=10.1186%2fs40537-019-0184-5&partnerID=40&md5=48d262f195134078722304b60a8ad47d","This study represents an efficient method for extracting product aspects from customer reviews and give solutions for inferring aspect ratings and aspect weights. Aspect ratings often reflect the user’s satisfaction on aspects of a product and aspect weights reflect the degree of importance of the aspects posed by the user. These tasks therefore play a very important role for manufacturers to better understand their customers’ opinion on their products and services. The study addresses the problem of aspect extraction by using aspect words based on conditional probability combined with the bootstrap technique. To infer the user’s rating for aspects, a supervised approach called the Naïve Bayes classification method is proposed to learn the aspect ratings in which sentiment words are considered as features. The weight of an aspect is estimated by leveraging the frequencies of aspect words within each review and the aspect consistency across all reviews. Experimental results show that the proposed method obtains very good performance on real world datasets in comparison with other state-of-the-art methods. © 2019, The Author(s)."
"10.1186/s40537-019-0181-8","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062269026&doi=10.1186%2fs40537-019-0181-8&partnerID=40&md5=d13bb800d3d7f9ab659897f92236d50b","The United States healthcare system produces an enormous volume of data with a vast number of financial transactions generated by physicians administering healthcare services. This makes healthcare fraud difficult to detect, especially when there are considerably less fraudulent transactions (documented and readily available) than non-fraudulent. The ability to successfully detect fraudulent activities in healthcare, given such discrepancies, can garner up to $350 billion in recovered monetary losses. In machine learning, when one class has a substantially larger number of instances (majority) compared to the other (minority), this is known as class imbalance. In this paper, we focus specifically on Medicare, utilizing three ‘Big Data’ Medicare claims datasets with real-world fraudulent physicians. We create a training and test dataset for all three Medicare parts, both separately and combined, to assess fraud detection performance. To emulate class rarity, which indicates particularly severe levels of class imbalance, we generate additional datasets, by removing fraud instances, to determine the effects of rarity on fraud detection performance. Before a machine learning model can be distributed for real-world use, a performance evaluation is necessary to determine the best configuration (e.g. learner, class sampling ratio) and whether the associated error rates are low, indicating good detection rates. With our research, we demonstrate the effects of severe class imbalance and rarity using a training and testing (Train_Test) evaluation method via a hold-out set, and provide our recommendations based on the supervised machine learning results. Additionally, we repeat the same experiments using Cross-Validation, and determine it is a viable substitute for Medicare fraud detection. For machine learning with the severe class imbalance datasets, we found that, as expected, fraud detection performance decreased as the fraudulent instances became more rare. We apply Random Undersampling to both Train_Test and Cross-Validation, for all original and generated datasets, in order to assess potential improvements in fraud detection by reducing the adverse effects of class imbalance and rarity. Overall, our results indicate that the Train_Test method significantly outperforms Cross-Validation. © 2019, The Author(s)."
"10.1186/s40537-019-0186-3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062263911&doi=10.1186%2fs40537-019-0186-3&partnerID=40&md5=536caad63ebfd831f5208684a0d14ea6","The goal of this paper is to reduce the classification (inference) complexity of tree ensembles by choosing a single representative model out of ensemble of multiple decision-tree models. We compute the similarity between different models in the ensemble and choose the model, which is most similar to others as the best representative of the entire dataset. The similarity-based approach is implemented with three different similarity metrics: a syntactic, a semantic, and a linear combination of the two. We compare this tree selection methodology to a popular ensemble algorithm (majority voting) and to the baseline of randomly choosing one of the local models. In addition, we evaluate two alternative tree selection strategies: choosing the tree having the highest validation accuracy and reducing the original ensemble to five most representative trees. The comparative evaluation experiments are performed on six big datasets using two popular decision-tree algorithms (J48 and CART) and splitting each dataset horizontally into six different amounts of equal-size slices (from 32 to 1024). In most experiments, the syntactic similarity approach, named SySM—Syntactic Similarity Method, provides a significantly higher testing accuracy than the semantic and the combined ones. The mean accuracy of SySM over all datasets is 0.835 ± 0.065 for CART and 0.769 ± 0.066 for J48. On the other hand, we find no statistically significant difference between the testing accuracy of the trees selected by SySM and the trees having the highest validation accuracy. Comparing to ensemble algorithms, the representative models selected by the proposed methods provide a higher speed for big data classification along with being more compact and interpretable. © 2019, The Author(s)."
"10.1186/s40537-019-0185-4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062147929&doi=10.1186%2fs40537-019-0185-4&partnerID=40&md5=3a69fd0db19c366a435cb459066a2ccf","As Big Data processing often takes a long time and needs a lot of resources, sampling and approximate computing techniques may be used to generate a desired Quality of Result. On the other hand, due to not considering data variety, available sample-based approximation approaches suffer from poor accuracy. Data variety is one of the key features of Big Data which causes various parts of data to have different impact on the final result. To address this problem, we develop a data variety aware approximation approach called Gapprox. Our idea is to use a kind of cluster sampling to improve the accuracy of estimation. Our approach can decrease the amount of data to be processed to achieve the desired Quality of Result with acceptable error bound and confidence interval. We divide the input data into some blocks considering the intra/inter cluster variance. The size of the block and the sample size are determined in such a way that by processing small amount of input data, an acceptable confidence interval and error bound is achieved. We compared our work with two well-known state of the art. The experimental results show that our result surpasses the state of the art and improve processing time up to 17× compared to ApproxHadoop and 8× compared to Sapprox when the user can tolerate an error of 5% with 95% confidence. © 2019, The Author(s)."
"10.1186/s40537-019-0180-9","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061813355&doi=10.1186%2fs40537-019-0180-9&partnerID=40&md5=acb95251d69d4c219d689465e193e2ee","In the age of data driven solution, the customer demographic attributes, such as gender and age, play a core role that may enable companies to enhance the offers of their services and target the right customer in the right time and place. In the marketing campaign, the companies want to target the real user of the GSM (global system for mobile communications), not the line owner. Where sometimes they may not be the same. This work proposes a method that predicts users’ gender and age based on their behavior, services and contract information. We used call detail records (CDRs), customer relationship management (CRM) and billing information as a data source to analyze telecom customer behavior, and applied different types of machine learning algorithms to provide marketing campaigns with more accurate information about customer demographic attributes. This model is built using reliable data set of 18,000 users provided by SyriaTel Telecom Company, for training and testing. The model applied by using big data technology and achieved 85.6% accuracy in terms of user gender prediction and 65.5% of user age prediction. The main contribution of this work is the improvement in the accuracy in terms of user gender prediction and user age prediction based on mobile phone data and end-to-end solution that approaches customer data from multiple aspects in the telecom domain. © 2019, The Author(s)."
"10.1186/s40537-018-0153-4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061789303&doi=10.1186%2fs40537-018-0153-4&partnerID=40&md5=bc37f1f3459fe1bb86a67e95bcf75ddc","Nature create variables using its character component, and variables are sharing characters from a vary small to relatively large scale. This results, variables to have from a vary different to a more similar character, and leads to have a relation ship. Literature suggested different relation measures based on the nature of variable and type of relation ship exist. Today, due to having high variety of frequently produced large data size, currently suggested variable filtering and selection methods have gaps to full fill the need. This research desires to fill this gap by comparing literature suggested methods to finding out a better variable selection and dimension reduction methods. The result from regression analysis using all literature suggested factors shows that none of the predictors for development status of enterprise are significant, and only 10 predictors for number of employer in an enterprise are significant out of 81 factors. Since, variable selection and dimension reduction methods are applied to find out predictors of a response by removing variable redundancy, and complexity of incorporating large number variable. Based on statistical power, for the results from variable selection methods, specially association and correlation methods showed that, CANOVA more efficiently detects non-linear or non-monotonic correlation between a continuous–continuous and a continuous-categorical variables. Spearman’s correlation coefficient more efficiently detects a monotonic correlation between a continuous with a continuous, and a continuous with a categorical variable. Pearson correlation coefficient more efficiently detects the linear correlation between continuous variables. MIC efficiently detects non-linear or non-monotonic relation between continuous variables. Chi-square test of independence efficiently detects relation between a continuous with a continuous, and categorical with categorical variables, but the non linear or non monotonic relation between a continuous with a categorical are not well detected. On the other hand, the result from lasso and stepwise methods reveals that, the relation between the predictor and response due to interaction effect not detected by correlation and association methods are detected by stepwise variable selection method, and the multicollinearity is detected and removed by lasso method. Regressing the response variable “number of employer in an enterprise” based on variables selected by lasso and stepwise method does bring greater model fitness (based on adjusted R-squared value) than variables selected by association and correlation methods. Similarly, regressing the response variable “development status of an enterprise” based on variables selected by association and correlation methods does bring 12 significant variables, where none of variables are significant from variables selected by lasso and stepwise methods. As a result, 51 predictors for number of employment in an enterprise, and 40 predictors for development status of an enterprise are detected as significantly related variables. And, lasso and stepwise methods are preferred to select predictors of a continuous response variable “number of employers in an enterprise”, and association and correlation methods are preferred to select predictors of a categorical response variable “development status of an enterprise”. Finally, the reduced regression models result reveals that, 20 predictors have causal relation with number of employment in an enterprise, and 12 predictors have causal relation with development status of an enterprise. On the other hand, based on model fitness, information lost, and number of significant factors, principal factor is preferred and applied in dimension reduction for a categorical response variable “development status of an enterprise”, and factor score based regression is preferred and applied for a continuous response variable “number of employers in an enterprise”. However, the comparison of the results in variable selection and dimension reduction indicates that, variable selection methods gave more gain in model fitness than dimension reduction methods. Hence, the suggested variable selection methods are more preferred than dimension reduction methods, and applied to find out predictors. In general, the suggested procedure for variable selection methods are recommended when small number of variables are studied, and the suggested dimension reduction methods are recommended for large number of variant variables (Big data case). © 2019, The Author(s)."
"10.1186/s40537-019-0179-2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061476911&doi=10.1186%2fs40537-019-0179-2&partnerID=40&md5=2ae864421350a4921e9af5d0f9f12ace","Deep Learning is an increasingly important subdomain of artificial intelligence, which benefits from training on Big Data. The size and complexity of the model combined with the size of the training dataset makes the training process very computationally and temporally expensive. Accelerating the training process of Deep Learning using cluster computers faces many challenges ranging from distributed optimizers to the large communication overhead specific to systems with off the shelf networking components. In this paper, we present a novel distributed and parallel implementation of stochastic gradient descent (SGD) on a distributed cluster of commodity computers. We use high-performance computing cluster (HPCC) systems as the underlying cluster environment for the implementation. We overview how the HPCC systems platform provides the environment for distributed and parallel Deep Learning, how it provides a facility to work with third party open source libraries such as TensorFlow, and detail our use of third-party libraries and HPCC functionality for implementation. We provide experimental results that validate our work and show that our implementation can scale with respect to both dataset size and the number of compute nodes in the cluster. © 2019, The Author(s)."
"10.1186/s40537-019-0178-3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061379894&doi=10.1186%2fs40537-019-0178-3&partnerID=40&md5=dd68c294efc335c21ada858c19107a57","Currently, the data to be explored and exploited by computing systems increases at an exponential rate. The massive amount of data or so-called “Big Data” put pressure on existing technologies for providing scalable, fast and efficient support. Recent applications and the current user support from multi-domain computing, assisted in migrating from data-centric to knowledge-centric computing. However, it remains a challenge to optimally store and place or migrate such huge data sets across data centers (DCs). In particular, due to the frequent change of application and DC behaviour (i.e., resources or latencies), data access or usage patterns need to be analyzed as well. Primarily, the main objective is to find a better data storage location that improves the overall data placement cost as well as the application performance (such as throughput). In this survey paper, we are providing a state of the art overview of Cloud-centric Big Data placement together with the data storage methodologies. It is an attempt to highlight the actual correlation between these two in terms of better supporting Big Data management. Our focus is on management aspects which are seen under the prism of non-functional properties. In the end, the readers can appreciate the deep analysis of respective technologies related to the management of Big Data and be guided towards their selection in the context of satisfying their non-functional application requirements. Furthermore, challenges are supplied highlighting the current gaps in Big Data management marking down the way it needs to evolve in the near future. © 2019, The Author(s)."
"10.1186/s40537-019-0174-7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061206031&doi=10.1186%2fs40537-019-0174-7&partnerID=40&md5=3872441a957f0213e009932d09255f90","This paper studies how the presence of universities and hospitals influences local home prices and rents. We analyze the data on ZIP code level and on the level of individual homes. Our ZIP code-level analysis uses median home price data from 13,105 ZIP codes over 21 years and rent data from 15,918 ZIP codes over 7 years to compare a ZIP code’s appreciation, volatility and vacancies to the size of a university or hospital within that ZIP code. Our home-level analysis uses data from 2,786,895 homes for sale and 267,486 homes for rent to study the impact of the distance from the nearest university or hospital to individual home prices. While our results generally agree with our expectations that larger, closer institutions yield higher prices, we also find some interesting results that challenge these expectations, such as positive correlations between volatility and university/hospital size in some ZIP codes, a positive correlation between rent and distance from a hospital for some homes, and lower correlations of rent vs. distance from a university compared to price vs. distance. © 2019, The Author(s)."
"10.1186/s40537-019-0170-y","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061204628&doi=10.1186%2fs40537-019-0170-y&partnerID=40&md5=83ffa5db0c3c51ae9ed11dbb6a06772e","Big data analytics is gaining substantial attention due to its innovative contribution to decision making and strategic development across the healthcare field. Therefore, this study explored the adoption mechanism of big data analytics in healthcare organizations to inspect elements correlated to behavioral intention using the technology acceptance model and task-technology fit paradigm. Using a survey questionnaire, we analyzed 224 valid responses in AMOS v21 to test the hypotheses. Our results posit that the credentials of the technology acceptance model together with task-technology fit contribute substantially to the enhancement of behavioral intentions to use the big data analytics system in healthcare, ultimately leading towards actual use. Meanwhile, trust in and security of the information system also positively influenced the behavioral intention for use. Employee resistance to change is a key factor underlying failure of the innovative system in organizations and has been proven in this study to negatively moderate the relationship between intention to use and actual use of big data analytics in healthcare. Our results can be implemented by healthcare organizations to develop an understanding of the implementation of big data analytics and to promote psychological empowerment of employees to accept this innovative system. © 2019, The Author(s)."
"10.1186/s40537-019-0172-9","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061198380&doi=10.1186%2fs40537-019-0172-9&partnerID=40&md5=23e15d55fb2327909bce8df00dec3399","In many real-world contexts, there is a pressing need to automatically screen for potential perpetrators, such as school shooters, whose prevalence in the population is extremely low. We first explain one possible obstacle in addressing this challenge, which is the confusion between “recognition” and “localization” during a search process. Next, we present a pragmatic screening methodology to the problem along Jaynes Bayesian hypothesis testing procedure. According to this approach, we should first focus our efforts on reducing the size of the haystack rather than on the identification of the needle. The third and major methodological contribution of the paper is in proposing that we may reduce the size of the haystack through the identification and use of unique data cues we describe as “impostors’ cues”. An experiment performed on an artificial data set of 7000 texts, shows that when incorporating these cues in the hypothesis testing procedure, they significantly improve the automatic screening of objects characterized by an attribute of a low prevalence (i.e. a psychopathic signature). The relevance of the proposed approach for Big Data and Homeland security is explained and discussed. © 2019, The Author(s)."
"10.1186/s40537-019-0177-4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061193339&doi=10.1186%2fs40537-019-0177-4&partnerID=40&md5=ca0f9eed51803230615afcc0e6ba8cd9","Background: Big Data analytics such as credit scoring and predictive analytics offer numerous opportunities but also raise considerable concerns, among which the most pressing is the risk of discrimination. Although this issue has been examined before, a comprehensive study on this topic is still lacking. This literature review aims to identify studies on Big Data in relation to discrimination in order to (1) understand the causes and consequences of discrimination in data mining, (2) identify barriers to fair data-mining and (3) explore potential solutions to this problem. Methods: Six databases were systematically searched (between 2010 and 2017): PsychINDEX, SocIndex, PhilPapers, Cinhal, Pubmed and Web of Science. Results: Most of the articles addressed the potential risk of discrimination of data mining technologies in numerous aspects of daily life (e.g. employment, marketing, credit scoring). The majority of the papers focused on instances of discrimination related to historically vulnerable categories, while others expressed the concern that scoring systems and predictive analytics might introduce new forms of discrimination in sectors like insurance and healthcare. Discriminatory consequences of data mining were mainly attributed to human bias and shortcomings of the law; therefore suggested solutions included comprehensive auditing strategies, implementation of data protection legislation and transparency enhancing strategies. Some publications also highlighted positive applications of Big Data technologies. Conclusion: This systematic review primarily highlights the need for additional empirical research to assess how discriminatory practices are both voluntarily and accidentally emerging from the increasing use of data analytics in our daily life. Moreover, since the majority of papers focused on the negative discriminative consequences of Big Data, more research is needed on the potential positive uses of Big Data with regards to social disparity. © 2019, The Author(s)."
"10.1186/s40537-019-0175-6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061190785&doi=10.1186%2fs40537-019-0175-6&partnerID=40&md5=50d2ebb6f8e4dc6c342891489a42c71a","Diabetes is a chronic disease or group of metabolic disease where a person suffers from an extended level of blood glucose in the body, which is either the insulin production is inadequate, or because the body’s cells do not respond properly to insulin. The constant hyperglycemia of diabetes is related to long-haul harm, brokenness, and failure of various organs, particularly the eyes, kidneys, nerves, heart, and veins. The objective of this research is to make use of significant features, design a prediction algorithm using Machine learning and find the optimal classifier to give the closest result comparing to clinical outcomes. The proposed method aims to focus on selecting the attributes that ail in early detection of Diabetes Miletus using Predictive analysis. The result shows the decision tree algorithm and the Random forest has the highest specificity of 98.20% and 98.00%, respectively holds best for the analysis of diabetic data. Naïve Bayesian outcome states the best accuracy of 82.30%. The research also generalizes the selection of optimal features from dataset to improve the classification accuracy. © 2019, The Author(s)."
"10.1186/s40537-018-0165-0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061177902&doi=10.1186%2fs40537-018-0165-0&partnerID=40&md5=bf887c5835bd3be9263480c0a8c2928c","Introduction: The problem studied in this paper is the road insecurity, which is manifested by the big number of injuries and deaths recorded annually around the world. These victims of road accidents worry the whole community, hence the duty and the need to find solutions for reducing the number of victims and material damage. The overall purpose of this case study is to treat the problem of corporeal accidents known in France. Case description: The case study presented in this paper is intended to help decision-makers to find and understand the all significant relationships and correlations that exist between the conditions that led to these corporeal accidents. In fact, the two French ministries of interior and transport have jointly created a unique database on corporeal accidents, called BAAC “Accident Analysis Bulletin Corporal”, with the aim of allowing different exploitations of this database by different concerned administrations and research organizations. Our intervention consists to adopt a hybrid approach based on data mining techniques combined to the multicriteria decision methods. This approach allows to extract the most relevant association rules. The results thus obtained can be easily exploited by the decision-makers to choose the appropriate policies in the perspective of improving road safety. Discussion and evaluation: The proposed approach ranks all association rules in order of importance for several quality measures of association rules. The alternatives at the top of the ranking are the results to retaining for the analysis. The approach is applied to the BAAC database of 2016, which led to the selection of three association rules. These association rules reveal that there are narrow correlations between the following elements: Driver with Pedestrian, Normal Surface of Road with Normal Atmospheric Condition and the Pavement with Pedestrian. These correlations can be justified by excess speed and carelessness of the drivers. Conclusion: The improvement of the road safety needs mainly to work more intensively on the behavioral side of the road users. For future work, it is planned to apply the proposed approach to the French traffic accident database, containing all the data on road accidents collected over several years. In addition, other measures will be used in the ranking of the association rules. © 2019, The Author(s)."
"10.1186/s40537-019-0176-5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061107878&doi=10.1186%2fs40537-019-0176-5&partnerID=40&md5=a09332330e6665216106524877cf96b3","Internet traffic measurement and analysis generate dataset that are indicators of usage trends, and such dataset can be used for traffic prediction via various statistical analyses. In this study, an extensive analysis was carried out on the daily internet traffic data generated from January to December, 2017 in a smart university in Nigeria. The dataset analysed contains seven key features: the month, the week, the day of the week, the daily IP traffic for the previous day, the average daily IP traffic for the two previous days, the traffic status classification (TSC) for the download and the TSC for the upload internet traffic data. The data mining analysis was performed using four learning algorithms: the Decision Tree, the Tree Ensemble, the Random Forest, and the Naïve Bayes Algorithm on KNIME (Konstanz Information Miner) data mining application and kNN, Neural Network, Random Forest, Naïve Bayes and CN2 Rule Inducer algorithms on the Orange platform. A comparative performance analysis for the models is presented using the confusion matrix, Cohen’s Kappa value, the accuracy of each model, Area under ROC Curve, etc. A minimum accuracy of 55.66% was observed for both the upload and the download IP data on the KNIME platform while minimum accuracies of 57.3% and 51.4% respectively were observed on the Orange platform. © 2019, The Author(s)."
"10.1186/s40537-019-0173-8","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061085474&doi=10.1186%2fs40537-019-0173-8&partnerID=40&md5=9623ffc6bbcf3dd36ddc6ce2b218b279","The Management of mobile networks has become so complex due to a huge number of devices, technologies and services involved. Network optimization and incidents management in mobile networks determine the level of the quality of service provided by the communication service providers (CSPs). Generally, the down time of a system and the time taken to repair [mean time to repair (MTTR)] has a direct impact on the revenue, especially on the operational expenditure (OPEX). A fast root cause analysis (RCA) mechanism is therefore crucial to improve the efficiency of the operational team within the CSPs. This paper proposes a quadri-dimensional approach (i.e. services, subscribers, handsets and cells) to build a service quality management (SQM) tree in a Big Data platform. This is meant to speed up the root cause analysis and prioritize the elements impacting the performance of the network. Two algorithms have been proposed; the first one, to normalize the performance indicators and the second one to build the SQM tree by aggregating the performance indicators for different dimensions to allow ranking and detection of tree paths with the worst performance. Additionally, the proposed approach will allow CSPs to detect the mobile network dimensions causing network issues in a faster way and protect their revenue while improving the quality of the service delivered. © 2019, The Author(s)."
"10.1186/s40537-018-0167-y","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060859806&doi=10.1186%2fs40537-018-0167-y&partnerID=40&md5=166368283345eccf9f7a14a3682aba74","Annually, lots of research papers are published in scientific journals around the world. The knowledge of the status of research is a prerequisite for research planning and policy making. This type of knowledge could be gained through a scientometrics study on the published literature that analyzes research products in a scientific field. Always healthcare was a permanent concern of researchers and also rapidly expanding field of Big Data analytics has started to play a pivotal role in the evolution of healthcare practices and research. It leads attracting attention from academia, industry and even governments around the world to “Big data in Healthcare”. Therefore, this paper has done a meta-analysis on published researches methodology in this field in the period of 2008–2018. Statistical finding shows the “Meta-analysis and evidence” is the most used methodology in published papers. We applied data mining techniques for predicting using methodologies in the various databases to achieving knowledge discovery in the field. Naïve Bayes classifier in RapidMiner has been applied and results show eight main categories for words used in papers while “Developing methods to evaluate of care” averagely is the most intended using methodology for publishing papers and “Agent-based modeling” in nature is most using methodology and could be better predicted. © 2019, The Author(s)."
"10.1186/s40537-018-0166-z","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060160748&doi=10.1186%2fs40537-018-0166-z&partnerID=40&md5=16face7eaf67c30c75acbb05c357dd3f","In addressing the challenge of Big Data Analytics, what has been of notable significance is the analysis of online search traffic data in order to analyze and predict human behavior. Over the last decade, since the establishment of the most popular such tool, Google Trends, the use of online data has been proven valuable in various research fields, including -but not limited to- medicine, economics, politics, the environment, and behavior. In the field of politics, given the inability of poll agencies to always well approximate voting intentions and results over the past years, what is imperative is to find new methods of predicting elections and referendum outcomes. This paper aims at presenting a methodology of predicting referendum results using Google Trends; a method applied and verified in six separate occasions: the 2014 Scottish Referendum, the 2015 Greek Referendum, the 2016 UK Referendum, the 2016 Hungarian Referendum, the 2016 Italian Referendum, and the 2017 Turkish Referendum. Said referendums were of importance for the respective country and the EU as well, and received wide international attention. Google Trends has been empirically verified to be a tool that can accurately measure behavioral changes as it takes into account the users’ revealed and not the stated preferences. Thus we argue that, in the time of intelligence excess, Google Trends can well address the analysis of social changes that the internet brings. © 2019, The Author(s)."
"10.1186/s40537-019-0168-5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060132547&doi=10.1186%2fs40537-019-0168-5&partnerID=40&md5=e432e1921515f190659ef986622bdc13","In recent years, mobility data from smart cards, mobile phones and sensors have become increasingly available. However, they often lack some of the key information including the purposes of trips for each individual user. Information on trip purposes is crucial for projecting the future travel patterns as well as understanding the characteristics of each area of a city and how it is changing. This paper proposes a method called EAT-CD (Extraction of Activity Types and Change Detection). It estimates the volume of passengers by activity types (e.g. commuting, leisure) using non-negative matrix factorization and detects changes in the number of visitors for each activity (e.g. increase in shopping trips triggered by the development of a new commercial facility). Validity of EAT-CD is tested through empirical analysis using smart card data of public transportation in Western Japan. The results showed that EAT-CD is effective in deriving activity patterns, which showed strong correlation with travel survey data. The results also confirmed that EAT-CD detects changes in travel patterns (e.g. start and end of semesters) and land uses (e.g. establishment of new facilities). © 2019, The Author(s)."
"10.1186/s40537-019-0169-4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059779764&doi=10.1186%2fs40537-019-0169-4&partnerID=40&md5=0d6bd745aa37fb1d0acc363ef9266651","The present work is a part of the ESTenLigne project which is the result of several years of experience for developing e-learning in Sidi Mohamed Ben Abdellah University through the implementation of open, online and adaptive learning environment. However, this platform faces many challenges, such as the increasing amount of data, the diversity of pedagogical resources and a large number of learners that makes harder to find what the learners are really looking for. Furthermore, most of the students in this platform are new graduates who have just come to integrate higher education and who need a system to help them to take the relevant courses that take into account the requirements and needs of each learner. In this article, we develop a distributed courses recommender system for the e-learning platform. It aims to discover relationships between student’s activities using association rules method in order to help the student to choose the most appropriate learning materials. We also focus on the analysis of past historical data of the courses enrollments or log data. The article discusses particularly the frequent itemsets concept to determine the interesting rules in the transaction database. Then, we use the extracted rules to find the catalog of more suitable courses according to the learner’s behaviors and preferences. Next, we deploy our recommender system using big data technologies and techniques. Especially, we implement parallel FP-growth algorithm provided by Spark Framework and Hadoop ecosystem. The experimental results show the effectiveness and scalability of the proposed system. Finally, we evaluate the performance of Spark MLlib library compared to traditional machine learning tools including Weka and R. © 2019, The Author(s)."
"10.1186/s40537-018-0162-3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059746810&doi=10.1186%2fs40537-018-0162-3&partnerID=40&md5=0ceb5cfbfc4d51b0186152664d1e128d","Smart manufacturing is strongly correlated with the digitization of all manufacturing activities. This increases the amount of data available to drive productivity and profit through data-driven decision making programs. The goal of this article is to assist data engineers in designing big data analysis pipelines for manufacturing process data. Thus, this paper characterizes the requirements for process data analysis pipelines and surveys existing platforms from academic literature. The results demonstrate a stronger focus on the storage and analysis phases of pipelines than on the ingestion, communication, and visualization stages. Results also show a tendency towards custom tools for ingestion and visualization, and relational data tools for storage and analysis. Tools for handling heterogeneous data are generally well-represented throughout the pipeline. Finally, batch processing tools are more widely adopted than real-time stream processing frameworks, and most pipelines opt for a common script-based data processing approach. Based on these results, recommendations are offered for each phase of the pipeline. © 2019, The Author(s)."
"10.1016/j.bdr.2019.04.003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065178332&doi=10.1016%2fj.bdr.2019.04.003&partnerID=40&md5=5591877f84f7775fe4cfc23cda63ac71","Analysts face many steep challenges when performing sensemaking tasks on collections of textual information larger than can be reasonably analyzed without computational assistance. To scale up such sensemaking tasks, new methods are needed to interactively integrate human cognitive sensemaking activity with machine learning. Towards that goal, we offer a human-in-the-loop computational model that mirrors the human sensemaking process, and consists of foraging and synthesis sub-processes. We model the synthesis loop as an interactive spatial projection and the foraging loop as an interactive relevance ranking combined with topic modeling. We combine these two components of the sensemaking process using semantic interaction such that the human's spatial synthesis actions are transformed into automated foraging and synthesis of new relevant information. Ultimately, the model's ability to forage as a result of the analyst's synthesis activities makes interacting with big text data easier and more efficient, thereby facilitating analysts’ sensemaking ability. We discuss the interaction design and theory behind our interactive sensemaking model. The model is embodied in a novel visual analytics prototype called Cosmos in which analysts synthesize structure within the larger corpus by directly interacting with a reduced-dimensionality space to express relationships on a subset of data. We then demonstrate how Cosmos supports sensemaking tasks with a realistic scenario that investigates the affect of natural disasters in Adelaide, Australia in September 2016 using a database of over 30,000 news articles. © 2019 Elsevier Inc."
"10.1016/j.bdr.2019.04.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065047775&doi=10.1016%2fj.bdr.2019.04.002&partnerID=40&md5=e6e6ed3891c3d8f33a23e73a0fbf7733","Recently, there have been several proposals in the area of geo-distributed big data processing. In this work, we aim to address a limitation of the existing solutions, namely to optimize task allocation across geographically distributed data centers, in a way that both the total traffic and the running time of the whole processing in complex multi-stage flows are targeted. Apart from proposing concrete efficient solutions for this combinatorial problem, we advocate to take a critical stand on the broadly spread claim that transferring distributed data to a single or fewer places is too costly. In our proposal, we judiciously reduce the participation of some data centers in the flow execution, and we show that, in a wide range of settings, this yields significant benefits. We show that a stochastic solution is superior to a fast greedy one, at the expense of optimization time of up to a few minutes. Compared to a state-of-the-art solution, we manage to decrease total traffic by 44% and running time by 37% on average. In several cases, the improvements can reach 1-2 orders of magnitude. Moreover, we provide evidence that simple heuristics are inferior. Our experimental evaluation comprises both extensive simulations and real runs in Spark. © 2019 Elsevier Inc."
"10.1016/j.bdr.2019.04.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064607186&doi=10.1016%2fj.bdr.2019.04.001&partnerID=40&md5=b6149a3052da8da3140580030b9300fe","The increasing presence of geo-distributed sensor networks implies the generation of huge volumes of data from multiple geographical locations at an increasing rate. This raises important issues which become more challenging when the final goal is that of the analysis of the data for forecasting purposes or, more generally, for predictive tasks. This paper proposes a framework which supports predictive modeling tasks from streaming data coming from multiple geo-referenced sensors. In particular, we propose a distance-based anomaly detection strategy which considers objects described by embedding features learned via a stacked auto-encoder. We then devise a repair strategy which repairs the data detected as anomalous exploiting non-anomalous data measured by sensors in nearby spatial locations. Subsequently, we adopt Gradient Boosted Trees (GBTs)to predict/forecast values assumed by a target variable of interest for the repaired newly arriving (unlabeled)data, using the original feature representation or the embedding feature representation learned via the stacked auto-encoder. The workflow is implemented with distributed Apache Spark programming primitives and tested on a cluster environment. We perform experiments to assess the performance of each module, separately and in a combined manner, considering the predictive modeling of one-day-ahead energy production, for multiple renewable energy sites. Accuracy results show that the proposed framework allows reducing the error up to 13.56%. Moreover, scalability results demonstrate the efficiency of the proposed framework in terms of speedup, scaleup and execution time under a stress test. © 2019 Elsevier Inc."
"10.1016/j.bdr.2019.02.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064084654&doi=10.1016%2fj.bdr.2019.02.002&partnerID=40&md5=3d88d538ac5230c00c6ea2453715f606","Nowadays, storing and retrieving information over the Cloud is critical for the survival and growth of organizations and people. In this context, the possibility to store a huge amount of data and files on remote third-party Cloud storage providers is becoming an even more concrete practice. Unfortunately, there is not any guarantee regarding data availability and reliability of such providers. In such a context, if the Cloud wants to use the storage as a service of another Cloud, it has only to trust it. A possible solution consists of using a Multi-Cloud Storage (MCS) system. However, how organizations should compose their own MCS system on geo-distribution basis is not trivial at all. In this paper, we specifically discuss how to optimize the overall system in terms of data storage and retrieval by testing and validating a MCS system composed of three major Cloud Storage providers; Dropbox, Google Drive and Copy. Experiments have proved that the choice of the Cloud storage providers where to store files depends on the data transfer performance according to the file chunk size. In addition, we demonstrated that not always the provider that offers the best performance in upload also offers the best performance in download. © 2019 Elsevier Inc."
"10.1080/23270012.2019.1608326","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065800950&doi=10.1080%2f23270012.2019.1608326&partnerID=40&md5=b641e0ce5ab9af386d36297a002cba61","In this paper, a holistic hierarchical analytical model is proposed to assess the performance of enablers in an integrated logistics system. Due to the ambiguous and complex environment, various refinements are needed to assess enablers and prioritize for the criteria such as economic, operational, and environment. The proposed hierarchical model is developed by a systematic approach that includes fuzzy analytical hierarchy process (FAHP), triangular fuzzy numbers (TFN), an evidential reasoning algorithm (ERA), and expected utility theory (EUT). The FAHP is used to analyze and obtain the weights of the criteria by considering the expert’s opinions. ERA is used to synthesize the enablers based on the selected criteria. These enablers are represented using subjective assessment along with a set of evaluation grades for a qualitative attribute. EUT helps in obtaining crisp values of enablers for their performance estimation. With these set of methodologies, a hierarchical model is proposed that prevent low flexibility and inadequate appropriateness of the proposed model. Further, the model helps in scenario generation for the logistics professionals who are facing various problems in integrating logistics and incorporating sustainability due to lack of appropriate methodologies and evaluation techniques. Finally, sensitivity analysis is used for overall model validation. © 2019, © 2019 Antai College of Economics and Management, Shanghai Jiao Tong University."
"10.1080/23270012.2019.1608327","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065790363&doi=10.1080%2f23270012.2019.1608327&partnerID=40&md5=6ba1a7df899d3d46c7e3e0631a701c6e","This paper focuses on coordinating contingent assistance between two lateral suppliers controlled by a central firm, when one supplier is exposed to supply disruption. By comparing two scenarios where the central firm can/cannot coordinate contingent assistance, we find the coordination of contingent assistance is more efficient, thus the central firm should do that. In the scenario without coordination, if the holding cost of the disrupted supplier is low, while the opportunity cost of the reliable supplier is high relatively, allowing the reliable supplier to hold decision power of assistance price can generate more assistance quantity for the disrupted supplier and bring more profits for the central firm. However, if the holding cost is high, and the opportunity cost is low relatively, the disrupted supplier can receive more assistance quantity, and the central firm can get more profits, by letting the disrupted supplier have the power to decide assistance price. © 2019, © 2019 Antai College of Economics and Management, Shanghai Jiao Tong University."
"10.1080/23270012.2019.1595187","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064053446&doi=10.1080%2f23270012.2019.1595187&partnerID=40&md5=01500d44d68129725ed4e9f3e0a1a151","In this paper, we investigate the incentive equilibrium strategies of two neighboring regions facing transboundary industrial pollution under abatement investment and emission permits trading in a differential game setting. Our paper can be viewed as an extension of the work of Yeung [2007. Dynamically consistent cooperative solution in a differential game of transboundary industrial pollution. Journal of Optimization Theory and Applications, 134, 143–160] in the context of the transboundary industrial pollution. Compared with the work of Yeung [2007. Dynamically consistent cooperative solution in a differential game of transboundary industrial pollution. Journal of Optimization Theory and Applications, 134, 143–160], our research significant features (i) introduce the emission permits trading into the transboundary industrial pollution control;(ii) take into account the pollution abatement investment; (iii) examine the incentive equilibrium strategies of transboundary industrial pollution control; and (iv) design an allocation mechanism for regions’ cooperative profits. Furthermore, we illustrate the results of the paper with a numerical example. The utility of this paper is how to make incentive equilibrium strategies in a situation where the neighboring regions facing transboundary industrial pollution under abatement investment and emission permits trading in a differential game setting. © 2019, © 2019 Antai College of Economics and Management, Shanghai Jiao Tong University."
"10.1080/23270012.2019.1596846","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063895394&doi=10.1080%2f23270012.2019.1596846&partnerID=40&md5=c1a58935e5ab57a522bd0a8ee3a8e15c","A complex product subjects to multiple failure modes such as minor and catastrophic failure with some probability. This paper investigates the effects of minor failure and catastrophic failure on the periodic replacement policy for a complex product supported by a warranty period. Cost models are developed and the expected optimal replacement policies are developed analytically such that long run expected life-cycle cost rate is minimized. Structural properties of the optimal replacement policies are derived for a product which fails with multiple failure modes and the failure rate is an increasing function of time. Finally, a numerical experiment is performed to show the important features of our study. © 2019, © 2019 Antai College of Economics and Management, Shanghai Jiao Tong University."
"10.4018/IJBAN.2019040102","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064574079&doi=10.4018%2fIJBAN.2019040102&partnerID=40&md5=eb74459775c784ea5508140da993f62a","Customer segmentation is the process of forming smaller groups of customers according to their characteristics. Now companies can develop proper marketing strategies for each group to get the desired results. This type of direct marketing is practiced by most organizations from the size of smallest start-up to the Fortune 500 leaders. Clustering is the ideal data mining technique for customer segmentation. In this article, the authors have proposed a clustering algorithm based on the self-organizing map and minimum spanning tree for customer segmentation. The authors have used several synthetic and real-life datasets to evaluate the clustering performance of their approach. To demonstrate the effectiveness of the authors’ proposed approach, they have trained few classifiers with the groups extracted from a direct marketing campaign of a Portuguese banking institution and show that the classification accuracy is better compared to the results obtained in some previous work where the full dataset has been used to train the same classifiers. Copyright © 2019, IGI Global."
"10.4018/IJBAN.2019040101","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064573318&doi=10.4018%2fIJBAN.2019040101&partnerID=40&md5=faaf34507790e59fde81162498e92959","This article attempts to develop a model by integrating interpretive structural modeling (ISM) and quality function deployment (QFD) methodology by establishing the relationship between the Indian retail service quality dimensions and service quality enablers. The integrated approach is employed to translate customers’ requirements/needs into specific service design factors/requirements in the Indian retail context. The retail service quality dimensions are identified using factor analysis and are considered as the customer demands in QFD process. Thirteen retail enablers were identified through an extensive literature survey and expert opinions. The enablers identified for the study were treated as design requirement for employing quality function deployment (QFD) in order to prioritize the design requirements. The results found showed that retail enablers ‘Image of the Store’ and ‘Value Conscious Consumers’ can be emphasized more in a priority basis by the Indian retailers followed by retail enablers ‘Location of store’ and ‘Globalization/Competition’. Copyright © 2019, IGI Global."
"10.4018/IJBAN.2019040105","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064565143&doi=10.4018%2fIJBAN.2019040105&partnerID=40&md5=af6d15461f8c8f06e7251af032de581f","Currency counterfeiting is a serious matter of concern for government and national finance organizations. The policy makers have formed a zero tolerance policy for counterfeiting, but still the incidence of counterfeit notes is becoming an alarming situation. The counterfeited banknotes reduce the value of real money and an increase in the money supply is not just a solution, because it leads to inflation. Counterfeiting poses tangible as well as intangible impacts and affects business organizations and thereby a nation’s economy. Therefore, the research study aims to identify the factors that affects the counterfeiting in Indian banknotes. Questionnaire as a data collection instrument was used to collect data from the north-west region of Delhi. The factors were then identified through exploratory factor analysis and standardized using a confirmatory factor analysis. At last, the relative effect of each factor was measured using multiple regression analysis as a part of confirmatory factor analysis. IBM SPSS 20 and IBM AMOS is used for factor analysis. Copyright © 2019, IGI Global."
"10.4018/IJBAN.2019040103","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064551822&doi=10.4018%2fIJBAN.2019040103&partnerID=40&md5=a5de4c1a50225a413a7de88534ff629f","Behavioural finance has gained research interest among researchers because of investor behavior and market anomalies. Investor behaviour varies with demographics and geographic characteristics. Further, investor behavior towards a gold exchange trade fund is gaining research interest due to various factors. Not much research has been carried out in this direction, with the exception of some comparisons. Therefore, the performance of a gold exchange traded fund needs to be assessed from the investor behavior perspective. Additionally, the investors behavior contains uncertainties. Thus, there is a need for intelligent techniques for identifying the investors behavior despite the presence of uncertain behavioral characteristics. Therefore, to study uncertain behavior characteristic in gold exchange traded fund, in this article the authors employ a fuzzy rough set. They employ fuzzy rough quick reduct algorithm to find the superfluous attributes. Further decision rules are generated to identify the chief feature of investors’ behavior towards gold exchange traded fund. Copyright © 2019, IGI Global."
"10.4018/IJBAN.2019040104","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064540302&doi=10.4018%2fIJBAN.2019040104&partnerID=40&md5=b2ce8d5a2d38ca2628755cecac3f4e02","Sentiment analysis manages the computational treatment of conclusion, notion, and content subjectivity. In this article, three sentiment classes such as positive, negative and neutral emotions have been demonstrated by appropriate features from raw unstructured data followed by data preprocessing steps. Applying best in class social analytics methodology to examine the sentiments embedded with purchaser remarks, encourages both producer and individual customers. Machine learning methods such as Naïve Bayes, maximum entropy classification, Deep Neural Networks were used upon the data, extracted from some websites such as Samsung and Apple for sentiment classification. In the online business arena, the application of sentiment classification explores a great opportunity. The subsidy of such an investigation is that associations can apply the proposed social examination framework to exploit the entire social information on the web and therefore improve their proper blueprint promoting strategies corresponding business. Copyright © 2019, IGI Global."
"10.1057/s41270-018-00047-y","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060712893&doi=10.1057%2fs41270-018-00047-y&partnerID=40&md5=ff4efdfe64ce4ef0ce2ae1432120e116","Research on salespeople’s preferences for rewards lacks consistency, especially regarding the importance of reward recognition. Contrary to the theoretical expectation, they found the recognition is less ranked by salespeople. These researches used ordinal or cardinal pairwise comparisons and explain these founding inconsistencies using context, such as industry standards and culture. This study argues that these results’ discrepancies may be attributed to the methodology used by researchers. To show our view, this paper demonstrates that ordinal and cardinal pairwise comparisons fail the conditions of Arrow’s impossibility theorem. With a case of 99 call center agents, we compare the salespeople’s reward preference results using five methods: (1) ordinal pairwise comparison method, (2) cardinal pairwise comparison method, (3) analytical hierarchy process (AHP), (4) fuzzy AHP, and fuzzy AHP-TOPSIS (technical order preference by similarity to the ideal solution). Results show except of salary that takes first place for each method, finding of two classes of methods are distinguishable. The first class includes the ordinal and cardinal methods, in which salespeople’s preferences for recognition rank last. The second class, which contains AHP, fuzzy AHP, and fuzzy AHP-TOPSIS, orders recognition in three or four positions. © 2019, Springer Nature Limited."
"10.1057/s41270-018-0045-7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056308995&doi=10.1057%2fs41270-018-0045-7&partnerID=40&md5=2bb06a47977f65e71e8b6f94ae40a878","Clustering observations into groups is perhaps one of the more common marketing analytic techniques. Many variable-selection procedures are available for clustering, and some have exhibited good performance in simulation studies. Unfortunately, the best-performing methods often fail because they emphasize the clustering power of individual variables. For this reason, we recommend extreme caution when using the existing procedures, and we argue that enumeration of all-possible variable subsets is a preferred strategy. We also address a common decision problem—the selection of the number of clusters—and develop an index which can help guide the joint selection of variables and clusters. By way of an empirical example, we illustrate the variable-selection problem and demonstrate the use of the proposed index to jointly select variables and clusters in K-means partitioning. © 2018, Springer Nature Limited."
"10.1057/s41270-018-0041-y","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055738847&doi=10.1057%2fs41270-018-0041-y&partnerID=40&md5=d6f0ad0c4a62f7bc065c5649e52db4dd","This study uses three advertising attitudinal constructs (i.e., practice, institution, and global) that have been developed in the West to determine if Chinese thought processes can replicate Western results. It proceeds through a comparison of Chinese future elites and American college students. The results of a causal modeling application on these constructs provide evidence of the usage of both Western and Eastern thought processes among Chinese respondents. However, the latter evidence a duality that suggests that the practice of advertising is ignored in forming overall attitudes toward advertising. Basically, the Chinese appear to be able to ignore dissonance-producing properties. © 2018, Springer Nature Limited."
"10.1057/s41270-018-0044-8","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055557600&doi=10.1057%2fs41270-018-0044-8&partnerID=40&md5=a0d3944c5fbe76a5664ef1781974e184","Companies find it necessary to develop new social customer-relationship management (CRM) capabilities to facilitate their customer-related performance. This study seeks systematically to conceptualize and measure social CRM capability, defined as a firm’s efficiency in integrating and converting social media marketing resources into desired sales revenue and customer-relationship outcomes. This definition focuses on the firm’s competency in obtaining, generating, organizing, and integrating information from customers’ social media engagement to maintain and improve the custome-relationship and its own financial performance. Adding to the conventional input–output stochastic frontier model, this study proposes including social media resource inputs and customer-related outcomes to measure social CRM capabilities. An empirical application suggests that social CRM capability is critical; investing in social media technology can lead to substantial CRM benefits and greater market value for the firm. Marketers should focus on developing strategies that emphasize customer-relationship building through social media, which allows for more customer involvement and interactions between the customer and the business. © 2018, Springer Nature Limited."
"10.1089/big.2018.0076","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063286719&doi=10.1089%2fbig.2018.0076&partnerID=40&md5=f38b57975077354f48fa63e3607d44f0","Predicting the results of sport matches and competitions is a growing research field, benefiting from the increasing amount of available data and novel data analytics techniques. Excellent forecasts can be achieved by advanced statistical and machine learning methods applied to detailed historical data, especially in very popular sports such as football (soccer). Here, we show that despite the large number of confounding factors, the results of a football team in longer competitions (e.g., a national league) follow a basically linear trend that is also useful for predictive purposes. In support of this claim, we present a set of experiments of linear regression compared to alternative approaches on a database collecting the yearly results of 746 teams playing in 22 divisions spanning up to five different levels from 11 countries, in 25 football seasons, for a total of 181,160 matches grouped in 9386 seasonal time series. © 2019, Mary Ann Liebert, Inc."
"10.1089/big.2018.0069","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063286213&doi=10.1089%2fbig.2018.0069&partnerID=40&md5=9dff3687ffa9640ed6cebedca3136fd2","Recently, professional team sport organizations have invested their resources to analyze their own and opponents' performance. So, developing methods and algorithms for analyzing team sports has become one of the most popular topics among data scientists. Analyzing football is hard because of its complexity, number of events in each match, and constant flow of circulation of the ball. Finding roles of players with the purpose of analyzing the performance of a team or making a meaningful comparison between players is crucial. In this article, an automatic big data clustering method, based on a swarm intelligence algorithm, is proposed to automatically cluster the data set of players' performance centers in different matches and extract different kinds of roles in football. The proposed method created using particle swarm optimization algorithm has two phases. In the first phase, the algorithm searches the solution space to find the number of clusters and, in the second phase, it finds the positions of the centroids. To show the effectiveness of the algorithm, it is tested on six synthetic data sets and its performance is compared with two other conventional clustering methods. After that, the algorithm is used to find clusters of a data set containing 93,000 objects, which are the centers of players' performance in about 4900 matches in different European leagues. © 2019, Mary Ann Liebert, Inc."
"10.1089/big.2018.0054","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063285321&doi=10.1089%2fbig.2018.0054&partnerID=40&md5=b52174d0a7cd301b3072381df86d3add","We investigate how to learn functions that rate game situations on a soccer pitch according to their potential to lead to successful attacks. We follow a purely data-driven approach using techniques from deep reinforcement learning to valuate multiplayer positionings based on positional data. Empirically, the predicted scores highly correlate with dangerousness of actual situations and show that rating of player positioning without expert knowledge is possible. © 2019, Mary Ann Liebert, Inc."
"10.1089/big.2018.0094","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063261828&doi=10.1089%2fbig.2018.0094&partnerID=40&md5=5617a24b106c576123276e2e9acbb386","Social media provides a platform for individuals to craft personal brands and influence their perception by others, including potential employers. Yet there remains a need for more research investigating the relationship between individuals' online identities and offline outcomes. This study focuses on the context of college football recruiting, specifically on the relationship between recruits' Twitter activities and coaches' scholarship offer decisions. Based on impression management theory, we analyze content posted by recruits and apply machine learning to identify instances of self-promotion and ingratiation in 5.5 million tweets. Using negative binomial regression, we discover that an athlete's level of engagement on Twitter is a positive and significant predictor of the number of offers. Also, both self-promotion and ingratiation are positively related to attracting new offers. Our results highlight the growing importance of social media as a recruiting tool and suggest that recruits' online self-presentation may have significant offline impacts. This research can benefit athletes and coaches by informing communication strategies during recruitment, and may also yield insight into the consequences of online impression management in other types of recruitment beyond sports. © 2019, Mary Ann Liebert, Inc."
"10.1089/big.2018.0067","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063252401&doi=10.1089%2fbig.2018.0067&partnerID=40&md5=ea479a33dd0bfdbbf9b46986bf6d673c","In professional soccer, nowadays almost every team employs tracking technology to monitor performance during trainings and matches. Over the recent years, there has been a rapid increase in both the quality and quantity of data collected in soccer resulting in large amounts of data collected by teams every single day. The sheer amount of available data provides opportunities as well as challenges to both science and practice. Traditional experimental and statistical methods used in sport science do not seem fully capable to exploit the possibilities of the large amounts of data in modern soccer. As a result, tracking data are mainly used to monitor player loading and physical performance. However, an interesting opportunity exists at the intersection of data science and sport science. By means of tracking data, we could gain valuable insights in the how and why of tactical performance during a soccer match. One of the most interesting and most frequently occurring elements of tactical performance is the pass. Every team has around 500 passing interactions during a single game. Yet, we mainly judge the quality and effectiveness of a pass by means of observational analysis, and whether the pass reaches a teammate. In this article, we present a new approach to quantify pass effectiveness by means of tracking data. We introduce two new measures that quantify the effectiveness of a pass by means of how well a pass disrupts the opposing defense. We demonstrate that our measures are sensitive and valid in the differentiation between effective and less effective passes, as well as between the effective and less effective players. Furthermore, we use this method to study the characteristics of the most effective passes in our data set. The presented approach is the first quantitative model to measure pass effectiveness based on tracking data that are not linked directly to goal-scoring opportunities. As a result, this is the first model that does not overvalue forward passes. Therefore, our model can be used to study the complex dynamics of build-up and space creation in soccer. © 2019, Mary Ann Liebert, Inc."
"10.1016/j.bdr.2019.02.003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062982944&doi=10.1016%2fj.bdr.2019.02.003&partnerID=40&md5=4f1c4b5e4ff9f76926e2c9cf194b6d1d","In this paper we demonstrate the use of multivariate topological algorithms to analyse and interpret Lattice Quantum Chromodynamics (QCD) data. Lattice QCD is a long established field of theoretical physics research in the pursuit of understanding the strong nuclear force. Complex computer simulations model interactions between quarks and gluons to test theories regarding the behaviour of matter in a range of extreme environments. Data sets are typically generated using Monte Carlo methods, providing an ensemble of configurations, from which observable averages must be computed. This presents issues with regard to visualisation and analysis of the data as a typical ensemble study can generate hundreds or thousands of unique configurations. We show how multivariate topological methods, such as the Joint Contour Net, can assist physicists in the detection and tracking of important features within their data in a temporal setting. This enables them to focus upon the structure and distribution of the core observables by identifying them within the surrounding data. These techniques also demonstrate how quantitative approaches can help understand the lifetime of objects in a dynamic system. © 2019 Elsevier Inc."
"10.1016/j.bdr.2019.02.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061666824&doi=10.1016%2fj.bdr.2019.02.001&partnerID=40&md5=4bdda36e82f13027c102013629e57a72","Geolocated time series are time series that correspond to specific locations. They can represent, for example, visitor check-ins at certain venues or readings of sensors installed at various places. The amount and significance of such time series have increased in many domains over the last years. However, although several works exist for time series visualization and visual analytics in general, there is a lack of efficient techniques for visual exploration and analysis of geolocated time series in particular. In this paper, we present two approaches that rely on hybrid spatial-time series indices to allow for efficient map-based visual exploration and summarization of geolocated time series data. In particular, we use the BTSR-tree index and we introduce a new variant of the iSAX index, called geo-iSAX. The former is a spatial-first hybrid index that extends the R-tree by maintaining bounds for the time series indexed at each node. Following a similar rationale, geo-iSAX is a time series-first hybrid index that maintains spatial MBRs of the geolocated time series indexed in each node. We describe the structure of these indices and show how they can be directly exploited to produce map-based visualizations of geolocated time series at different levels of granularity. We empirically validate our approach using two real-world datasets, as well as a synthetic one that is used to test the scalability of our methods. © 2019 Elsevier Inc."
"10.1016/j.bdr.2018.11.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057459265&doi=10.1016%2fj.bdr.2018.11.001&partnerID=40&md5=3abc0cc60da13777ba1c413a22cd000f","Label-Aware Distributed Ensemble Learning (LADEL) is a programming model and an associated implementation for distributing any classifier training to handle Big Data. It only requires users to specify the training data source, the classification algorithm and the desired parallelization level. First, a distributed stratified sampling algorithm is proposed to generate stratified samples from large, pre-partitioned datasets in a shared-nothing architecture. It executes in a single pass over the data and minimizes inter-machine communication. Second, the specified classification algorithm training is parallelized and executed on any number of heterogeneous machines. Finally, the trained classifiers are aggregated to produce the final classifier. Data miners can use LADEL to run any classification algorithm on any distributed framework, without any experience in parallel and distributed systems. The proposed LADEL model can be implemented on any distributed framework (Drill, Spark, Hadoop, etc.) to speed up the development of its data mining capabilities. It is also generic and can be used to distribute the training of any classification algorithm of any sequential single-node data mining library (Weka, R, scikit-learn, etc.). Distributed frameworks can implement LADEL to distribute the execution of existing data mining libraries without rewriting the algorithms to run in parallel. As a proof-of-concept, the LADEL model is implemented on Apache Drill to distribute the training execution of Weka's classification algorithms. Our empirical studies show that LADEL classifiers have similar and sometimes even better accuracy to the single-node classifiers and they have a significantly faster training and scoring times. © 2018 Elsevier Inc."
"10.1007/s41019-019-0090-z","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065043238&doi=10.1007%2fs41019-019-0090-z&partnerID=40&md5=d42539f2f6bc9e875620a7174fb88cc6","With the popularity of knowledge graphs growing rapidly, large amounts of RDF graphs have been released, which raises the need for addressing the challenge of distributed subgraph matching queries. In this paper, we propose an efficient distributed method to answer subgraph matching queries on big RDF graphs using MapReduce. In our method, query graphs are decomposed into a set of stars that utilize the semantic and structural information embedded RDF graphs as heuristics. Two optimization techniques are proposed to further improve the efficiency of our algorithms. One algorithm, called RDF property filtering, filters out invalid input data to reduce intermediate results; the other is to improve the query performance by postponing the Cartesian product operations. The extensive experiments on both synthetic and real-world datasets show that our method outperforms the close competitors S2X and SHARD by an order of magnitude on average. © 2019, The Author(s)."
"10.1007/s41019-019-0085-9","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065042329&doi=10.1007%2fs41019-019-0085-9&partnerID=40&md5=1bce7544591df685c80c9fd206b33070","In this paper, we propose a dynamic forecasting framework, named DMDP (dynamic multi-source default probability prediction), to predict the default probability of a company. The default probability is a very important factor to assess the credit risk of listed companies on a stock market. Aiming at aiding financial institutions in decision making, our DMDP framework not only analyzes financial data to capture the historical performance of a company, but also utilizes long short-term memory model to dynamically incorporate daily news from social media to take the perceptions of market participants and public opinions into consideration. The study of this paper makes two key contributions. First, we make use of unstructured news crawled from social media to alleviate the impact of financial fraud issue made on default probability prediction. Second, we propose a neural network method to integrate both structured financial factors and unstructured social media data with appropriate time alignment for default probability prediction. Extensive experimental results demonstrate the effectiveness of DMDP in predicting default probability for the listed companies in mainland China, compared with various baselines. © 2019, The Author(s)."
"10.1007/s41019-019-0086-8","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065038886&doi=10.1007%2fs41019-019-0086-8&partnerID=40&md5=fe3dad292b88d316ffb67d719ab3d0e5","Ever since the social networks became the focus of a great number of researches, the privacy risks of published network data have also raised considerable concerns. To evaluate users’ privacy risks, researchers have developed methods to de-anonymize the networks and identify the same person in the different networks. However, the existing solutions either require high-quality seed mappings for cold start, or exhibit low accuracy without fully exploiting the structural information, and entail high computation expense. In this paper, we propose a fast and effective seedless network de-anonymization approach simply relying on structural information, named RoleMatch. RoleMatch equips with a new pairwise node similarity measure and an efficient node matching algorithm. Through testing RoleMatch with both real and synthesized social networks, which are anonymized by several popular anonymization algorithms, we demonstrate that the RoleMatch receives superior performance compared with existing de-anonymization algorithms. © 2019, The Author(s)."
"10.1007/s41019-019-0087-7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065029539&doi=10.1007%2fs41019-019-0087-7&partnerID=40&md5=a4443c893af5decd16febc0f0803ca88","The abstractive method and extractive method are two main approaches for automatic document summarization. In this paper, to fully integrate the relatedness and advantages of both approaches, we propose a general unified framework for abstractive summarization which incorporates extractive summarization as an auxiliary task. In particular, our framework is composed of a shared hierarchical document encoder, a hierarchical attention mechanism-based decoder, and an extractor. We adopt multi-task learning method to train these two tasks jointly, which enables the shared encoder to better capture the semantics of the document. Moreover, as our main task is abstractive summarization, we constrain the attention learned in the abstractive task with the labels of the extractive task to strengthen the consistency between the two tasks. Experiments on the CNN/DailyMail dataset demonstrate that both the auxiliary task and the attention constraint contribute to improve the performance significantly, and our model is comparable to the state-of-the-art abstractive models. In addition, we cut half number of labels of the extractive task, pretrain the extractor, and jointly train the two tasks using the estimated sentence salience of the extractive task to constrain the attention of the abstractive task. The results do not decrease much compared with using full-labeled data of the auxiliary task. © 2019, The Author(s)."
"10.1007/s41019-019-0088-6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065028678&doi=10.1007%2fs41019-019-0088-6&partnerID=40&md5=eab14edbb7514f13d6ff99dbb376ec9d","A novel parallel algorithm is presented for generating random scale-free networks using the preferential attachment model. The algorithm, named cuPPA, is custom-designed for “single instruction multiple data (SIMD)” style of parallel processing supported by modern processors such as graphical processing units (GPUs). To the best of our knowledge, our algorithm is the first to exploit GPUs, and also the fastest implementation available today, to generate scale-free networks using the preferential attachment model. A detailed performance study is presented to understand the scalability and runtime characteristics of the cuPPA algorithm. Also another version of the algorithm called cuPPA-Hash tailored for multiple GPUs is presented. On a single GPU, the original cuPPA algorithm delivers the best performance, but is challenging to port to multi-GPU implementation. For multi-GPU implementation, cuPPA-Hash has been used as the parallel algorithm to achieve a perfect linear speedup up to 4 GPUs. In one of the best cases, when executed on an NVidia GeForce 1080 GPU, the original cuPPA generates a scale-free network of two billion edges in less than 3 s. On multi-GPU platforms, cuPPA-Hash generates a scale-free network of 16 billion edges in less than 7 s using a machine consisting of 4 NVidia Tesla P100 GPUs. © 2019, The Author(s)."
"10.1007/s41019-019-0084-x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065022122&doi=10.1007%2fs41019-019-0084-x&partnerID=40&md5=4fd20c94f36081035146584a8dda1d7a","We analyze the performance of regular decomposition, a method for compression of large and dense graphs. This method is inspired by Szemerédi’s regularity lemma (SRL), a generic structural result of large and dense graphs. In our method, stochastic block model (SBM) is used as a model in maximum likelihood fitting to find a regular structure similar to the one predicted by SRL. Another ingredient of our method is Rissanen’s minimum description length principle (MDL). We consider scaling of algorithms to extremely large size of graphs by sampling a small subgraph. We continue our previous work on the subject by proving some experimentally found claims. Our theoretical setting does not assume that the graph is generated from a SBM. The task is to find a SBM that is optimal for modeling the given graph in the sense of MDL. This assumption matches with real-life situations when no random generative model is appropriate. Our aim is to show that regular decomposition is a viable and robust method for large graphs emerging, say, in Big Data area. © 2019, The Author(s)."
"10.1177/2399808317716163","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062958695&doi=10.1177%2f2399808317716163&partnerID=40&md5=a3cc5dc5d56233e92a0e81effa01c8f5","The ideal Renaissance city is designed as a star-shaped fortress, where the streets and squares are organized to speed the movement of people and soldiers. Symmetry and accessibility represent the key features for the organization of the urban space. The resulting city is hierarchized and does not always guarantee an optimal degree of connectivity. Starting from the space syntax definition, we suggest a method to compute urban graphs from the Euclidean representation, the corresponding line graph and the contraction of nodes with the same urban function. We analyze the urban graphs of five historic cities and compare the analysis with the corresponding results from space syntax. Analysis of the spectral gap and the relative asymmetry distribution show a similar structure for these cities. The irregular or reticular housing structure seems to ensure connectivity and accessibility more than the regular grids. However, connectivity is ensured by the most peripheral streets, which in the space syntax representation play a marginal role. © The Author(s) 2017."
"10.1177/2399808317715639","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058968986&doi=10.1177%2f2399808317715639&partnerID=40&md5=33c4f89c997da51ec32525d7a64b497d","This study attempts to understand cultural impacts on nursing unit design through a comparative study on Chinese nursing unit typologies and their U.S. counterparts. The focus is to investigate whether seemingly westernized Chinese nursing units still retained certain characteristics of Chinese socio-cultural preferences; and how configurational differences of Chinese and American nursing unit design reflect the different work styles and organizational communication styles driven by national culture. This study’s contributions are twofold. Firstly, it demonstrates the impact of national culture on nursing unit design. The spatial configuration is a manifesto of culture and is congruent with culture. Secondly, from a methodological point of view, this study has translated abstract cultural schema, organizational constructs, and complex spatial relationships into quantitative spatial metrics. It makes the comparison of various building configurations from different cultures possible. The method and conceptual framework described here can be applied to understanding cultural differences in other building types as well. © The Author(s) 2017."
"10.1177/2399808317719709","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046074044&doi=10.1177%2f2399808317719709&partnerID=40&md5=6345ead9b283cc7d29cc2b074e7f905b","Two modes of participatory engagement in local land use planning are contrasted and compared: the asynchronous mode enabled by Geoweb applications for collecting and deliberating public preferences, and the same-place/same-time mode exemplified by public meetings. Using data from a public participation process that took place between June 2014 and July 2015 in Poznań, Poland, the article compares the scalability of each mode and evaluates it from the planners’ perspective. The findings show that Geoweb applications scale public participation more effectively than public meetings. The ability to attract a relatively large number of diverse participants contributed to the positive evaluation of participation outcomes by planners. The results of online participation have been reflected in the land use plan provisions and improved the transparency and access to planning documents. Several issues related to online participation including: bridging the educational gap, the digital divide, and focusing the attention of participants on a specific problem at hand remain still unresolved. © The Author(s) 2017."
"10.1177/2399808317720797","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041618220&doi=10.1177%2f2399808317720797&partnerID=40&md5=27854ae43f15444d715de5856fe02bfc","This study aims to conduct a multi-temporal change analysis of land use and land cover in New York City via a cellular automata-based Markov chain model that uses fuzzy set theory and multi-criteria evaluation to predict the city’s future land use changes for 2030 and 2050 under potential sea level rise and long-term rainfall-runoff flooding impacts driven by climate change. To determine the future natural forcing impacts on land use in New York City, this study highlights the need for integrating spatiotemporal modeling analyses, such as a statistical downscaling model driven by climate change with remote sensing and GIS to support urban growth assessment. The research findings indicate that the mean rainfall will increase in the future and sea levels will rise near New York City; however, open space is expected to decrease by 1.51% and 2.51% and the urban area is expected to expand by about 1.36% and 2.63% in 2030 and 2050 respectively, taking into account the climate change and sea level rise. © The Author(s) 2017."
"10.1177/2399808317719072","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041587325&doi=10.1177%2f2399808317719072&partnerID=40&md5=23eabde673b13cc9062df63099fb584b","Urban sprawl and income segregation are two undesired urban patterns that occur during urban development. Empirical studies show that income level and inequality are positively correlated with urban sprawl and income segregation, respectively. However, the relationship between urban sprawl and income segregation is not only rarely investigated but also shows ambiguous empirical results when it is. Therefore, in this study, we built a stylized agent-based model with individual behaviours based on Alonso’s bid rent theory and ran simulations with different combinations of income level and income inequality. We measured the overall emergent patterns with indicators for urban sprawl and income segregation. The model confirms the established positive correlations between income level and urban sprawl and between income inequality and segregation. Furthermore, the model shows a negative correlation between urban sprawl and income segregation under free market conditions. The model indicates that without any policy implementation, a city will either suffer from urban sprawl or income segregation. Thus, this study serves as a starting point to study the effects of different urban planning policies on these two urban problems. © The Author(s) 2017."
"10.1177/2399808317719071","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041565329&doi=10.1177%2f2399808317719071&partnerID=40&md5=589fd5ddeea8c6b93161df78de3b989a","A method for market clearing in land use models with a microsimulation approach for location choice of agents is proposed. The method, based on the Bid-auction theory and random utility models, assumes that agents individually adjust their perceived expected utility by observing market prices before entering auctions for a real estate good, hence modifying their overall willingness to pay for locations. The adjustment translates into a correction of each agent’s bid level that follows the direction of supply-demand equilibrium, as they attempt to ensure their location. In each period, auctions for each available real estate good are simulated and prices are computed as the expected maximum bid of all agents in the market. The proposed method is tested for the city of Brussels, validated against real data and compared with results obtained when the bid adjustment is not included. Simulation results reproduce price trends that were observed in reality between the year 2001 and 2008, outperforming results obtained without a quasi-equilibrium bid adjustment approach. The proposed method is feasible to be implemented in large scale microsimulations and agent-based models because it does not require solving large fixed-point equilibrium problems. © The Author(s) 2017."
"10.1177/2399808317719708","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041520431&doi=10.1177%2f2399808317719708&partnerID=40&md5=ed893ec1237e7b11c6085b807dedf94a","This paper develops a new optimal location model for siting affordable housing units to maximize the accessibility of low-income workers to appropriate jobs by public transportation. Transit-accessible housing allows disadvantaged populations to reduce their reliance on automobiles, which can lead to savings on transportation-related expenditures. The housing location model developed here maximizes transit accessibility while reducing the clustering of affordable housing units in space. Accessibility is maximized using a high-resolution space-time metric of public transit accessibility, originally developed for service equity analysis. The second objective disperses subsidized housing projects across space using a new minimax dispersion model based on spatial interaction principles. The multiobjective model trades off accessibility maximization and affordable housing dispersion, subject to upper and lower bounds on the land acquisition and construction budget. The model is tested using data for Tempe, AZ including actual data for vacant parcels, travel times by light rail and bus, and the location of low-wage jobs. This model or similar variants could provide insightful spatial decision support to affordable-housing providers or tax-credit administrators, facilitating the design of flexible strategies that address multiple social goals. © The Author(s) 2017."
"10.1177/2399808317715640","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032785579&doi=10.1177%2f2399808317715640&partnerID=40&md5=9b94d4824a14fee76cc1154d63734aed","The influence of urban design on economic vitality has been analyzed by a number of researchers and is also a key focus of many planning/design theories. However, most quantitative studies are based on just one city or a small set of cities, rather than a large number of cities that are representative of an entire country. With the increasing availability of new data, we aim to alleviate this gap by examining the impact of urban design upon economic vitality for the 286 largest cities in China by looking at a grid of geographical units that are 1 km by 1 km. We use these units and a set of new data (emerging big data and new data that reflecting urban developments and human mobility) to look at the impact of urban form indicators, such as intersection density (urban design), level of mixed use, and access to amenities and transportation, on economic vitality represented by activities using social media data. Our results show that these urban design indicators have a significant and positive relationship with levels of economic vitality for cities at every administrative level. The results contribute to a holistic understanding of how to improve economic vitality in cities across China at a detailed level, particularly at a time when China’s economic growth will depend largely on growth of the service sector in urban areas. We think these results can help decision makers, developers, and planners/designers to improve economic vitality in cities across China. © The Author(s) 2017."
"10.1177/2399808317720446","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032490439&doi=10.1177%2f2399808317720446&partnerID=40&md5=eae03f3db1c2366b1f5cd5c2dacb1a58","Planning decisions have considerable impacts on both natural and built environments. The impacts of these decisions may remain for many decades and many are irreversible. In order to gain a better understanding of these long-standing impacts, planners require a systematic approach to evaluate the planning policy instruments utilised. The literature on planning evaluation shows that most studies have taken a conformance-based evaluation approach, where the success of a planning policy instrument is based on the degree of conformity between the policy outcomes and its intended objectives. While evaluating such criteria is necessary, it is hardly ever sufficient largely because of unintended effects. This paper proposes an impact-based approach to planning evaluation that incorporates all the impacts, intended and otherwise, that a planning policy instrument may bring about, irrespective of the initial objectives of the policy. Using a number of economic and planning theories, this paper argues that, in addition to conformance and performance, other normative evaluation criteria, such as, efficiency, equity, social and political acceptability, and institutional arrangements, should be included to emphasise the importance of planning decisions and their substantial impacts on quality of life, social justice, and sustainability. © The Author(s) 2017."
"10.1177/2399808317709280","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061752114&doi=10.1177%2f2399808317709280&partnerID=40&md5=481ce136f907f080d690e42ad5b21701","Cellular automata-based models have traditionally employed regular grids to represent the geographical environment when simulating urban growth or land use change. Over the last two decades, the scientific community has introduced the use of other spatial structures in an attempt to represent the processes simulated by these models more realistically. Cadastre parcels are a good choice when simulating urban growth at local scales, where pixels or regular cells do not represent the geographic space properly. Furthermore, the implementation and calibration of key factors such as accessibility and suitability have not been sufficiently explored in models employing irregular structures. This paper presents a fully calibrated model to simulate urban growth: Model for Urban Growth simulation using Irregular Cellular Automata. The model uses the irregular structure of the cadastre and its smallest unit: the cadastral parcel. The factors included are based on the traditional Neighbourhood, Accessibility, Suitability and Zoning Status modelling schema, frequently employed in other models. Each factor was implemented and calibrated for the irregular structure employed by the model, and a new approach was explored to introduce a random component that would reproduce illegal growth. Several versions of Model for Urban Growth simulation using Irregular Cellular Automata were produced to calibrate the model within the period 2000–2010. The results obtained from the simulations were compared against observed growth for 2010, adapting the traditional confusion matrix to irregular space. A new metric is proposed, called growth simulation accuracy, which measures how well the model locates urban growth. © The Author(s) 2017."
"10.1177/2399808317705659","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061735954&doi=10.1177%2f2399808317705659&partnerID=40&md5=04837dc937871ae97af74e774ca40bb2","The question of whether multilevel buildings are memorized as volumetric map or collection of floors is central to spatial cognition and wayfinding studies about multilevel buildings. The stacked-floor buildings used in previous studies may limit people’s ability to integrate floors into a volumetric mental map. In this study, we assessed wayfinding and cognitive performances of 31 participants in a multilevel shopping mall with five atriums which provided adequate visual access and smooth floor transitions. (1) In the wayfinding task, we observed path choice for 31 participants in this mall. The participants’ choice for all path segments, also vertical path segments, clearly gravitated toward the most accessible spaces in the whole building, rather than most accessible space within individual floors. (2) Participants were also asked to identify the locations where they can see maximum number of stores. The identified locations can be reliably predicted by objectively measured three-dimensional visibility information, but not two-dimensional visibility information. (3) In the pointing task, participants can accurately point to out-of-sight targets in the same floor and in the different floor, in both azimuth and elevation direction. In sum, those findings suggest that people can memorize a multilevel atrium building as a volumetric map. This study also demonstrates the usefulness of developing three-dimensional configurational variables to explain human spatial behavior and spatial cognition. © The Author(s) 2017."
"10.1177/2399808317705881","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061735292&doi=10.1177%2f2399808317705881&partnerID=40&md5=95965d50dd0cabe917c72855ef2a7b56","Open data have come of age with many cities, states, and other jurisdictions joining the open data movement by offering relevant information about their communities for free and easy access to the public. Despite the growing volume of open data, their use has been limited in planning scholarship and practice. The bottleneck is often the format in which the data are available and the organization of such data, which may be difficult to incorporate in existing analytical tools. The overall goal of this research is to develop an open data-based community planning support system that can collect related open data, analyze the data for specific objectives, and visualize the results to improve usability. To accomplish this goal, this study undertakes three research tasks. First, it describes the current state of open data analysis efforts in the community planning field. Second, it examines the challenges analysts experience when using open data in planning analysis. Third, it develops a new flow-based planning support system for examining neighborhood quality of life and health for the City of Atlanta as a prototype, which addresses many of these open data challenges. © The Author(s) 2017."
"10.1177/2399808317710466","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061717337&doi=10.1177%2f2399808317710466&partnerID=40&md5=8d7209467633f7b8b1a174042ae2e2b7","This paper examines the importance of place-making in economic development by evaluating the relationship between specific urban design features – based on Jacobs’ “four generators of diversity” and Ewing and Cervero’s “Five-D’s” – and business sales volume. Despite the increased recognition of the importance of walkable urbanism in recent years, relatively little research has assessed the potential economic development benefits of walkable places. While a few authors have assessed the impact of urban design on property values, this paper fills a gap by examining links between components of walkable built environments and individual business characteristics. This paper uses a Hierarchical Linear Modeling framework to explicitly look at the relationship between neighborhood built environment features at the Census tract level and the sales volume per employee of individual businesses in 2010. The cities of Phoenix and Boston are used as contrasting study sites in order to inspect how larger regional characteristics influence the built environment–performance link. The results indicate that specific features of walkable built environments are positively associated with business performance. However, the relationship between walkable built environments and business performance varies considerably depending on the type of business and city-level context being studied, indicating that significant nuance must be used when considering place-based economic interventions. Although no causal statements can be made about the built environment and business performance, the results of this paper indicate that (in some contexts) design-based place-making initiatives could be used to generate sustainable local economic development. © The Author(s) 2017."
"10.1177/2399808317716935","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061710115&doi=10.1177%2f2399808317716935&partnerID=40&md5=48bd6806ea2546efa0868fca0d36276d","Global regression models, such as Ordinary Least Squares, are generally used to explore driving factors of surface urban heat island (SUHI) effects across large cities on a national level, but issues of spatial non-stationarity or local variations have rarely been taken into account. Our study quantifies SUHI effects for 274 cities in China with MODIS LST products and explores spatially varying relationships between SUHI intensity (SUHII) and their driving factors using geographically weighted regression (GWR). The results show that GWR models have stronger explanatory power and lower spatial autocorrelations of residuals compared with ordinary least square models; the application of GWR models finds that the relationships between SUHII and the driving factors vary across China. Spatially varying coefficients from GWR models could contribute to the development of local-specific urban planning or policies in different regions. The findings from our investigation suggest that GWR has the potential to serve as a useful tool for environmental investigations on a national scale. © The Author(s) 2017."
"10.1177/2399808317715263","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061698819&doi=10.1177%2f2399808317715263&partnerID=40&md5=3ec11febe3b36f6d4ab89f33f8b9dcf5","Regional designing is employed to envision regional futures that aim to guide decisions on the environment in the region over a longer period of time. However, longitudinal studies on the long-term use and effect of regional designing are lacking. This paper investigates the impacts of regional designing in the complex and fragmented setting of a cross-border region. Since the late 1980s, the region was subject to four regional design episodes that each had different impacts: from a new perception of the region to initiating regional collaboration and effects on the Dutch professional debate. The study showed that regional designing is a powerful means to overcome difficulties that arise from the fragmented setting of a cross-border region. Moreover, it revealed that the context in which regional designing is embedded determines in what areas regional designing will have its impact. Both plans and people are important in the transference of regional design outcomes to other planning arenas and conditions, such as status and available funding, improve the chances of transference. © The Author(s) 2017."
"10.1177/2399808317712715","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061698671&doi=10.1177%2f2399808317712715&partnerID=40&md5=b5de80b129f3428065122d5952c9e262","Fast urbanization is a common feature of many developing human societies. In many cases, past and present, explosive population growth in cities outstrips the rate of provision of housing and urban services and leads to the formation of informal settlements or slums. Slums are extremely varied in terms of their histories, infrastructure, and rates of change, but they share certain common features: informal land use, lack of physical accesses, and nonexistent or poor quality urban services. Currently, about 1 billion people worldwide live in slums, a number that could triple by 2050 if no practical solutions are enacted to reverse this trend. Underlying most problems of slums is the issue of lack of physical accesses to places of work and residence. This prevents residents and businesses from having an address, obtaining basic services such as water and sanitation, and being helped in times of emergency. Here, we show how the physical layout of any neighborhood can be classified quantitatively in terms of its access topology in a way that is independent of its geometry. Topological indices capturing levels of access to structures within a city block can then be used to define a constrained optimization problem, whose solution generates an access network that makes each structure in the settlement accessible to services with minimal disruption and cost. We discuss the general applicability of these techniques to several informal settlements in developing cities and demonstrate various technical aspects of our solutions. Finally, we discuss how these techniques could be used on a large scale to speed up human development processes in cities throughout the world while respecting their local identity and history. © The Author(s) 2017."
"10.1177/2399808317714799","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041563114&doi=10.1177%2f2399808317714799&partnerID=40&md5=173f7619441e1869a5539912ae43cbe8","Many studies have demonstrated that the effect of urban street spatial shape on sound propagation cannot be ignored. Most previous studies are based on idealised spatial models and but not systematically and comprehensively examine the real and complex street space. This paper takes the actual streets of a high-density city as research objects, select reliable spatial parameters, obtain the acoustic propagation data using computer simulation, identify the sound propagation characteristics and establish sound propagation models of urban streets. In total, 144 samples have been tested, 13 spatial parameters, including the width information, height information, section information and plan information of streets, have been selected, and three acoustic indices, which include the sound attenuation, reverberation time and early decay time, have been analysed in this paper. The sound propagation in the urban street is consistent with the propagation characteristics of the semi-free sound field, i.e. the sound attenuation is linearly correlated with the logarithm of the sound propagation distance. This linear correlation becomes more pronounced for the greater Plan enclosure degree and more even distribution of façades. The trend of sound attenuations decreases with the increasing Cross-sectional enclosure degree, increasing Plan enclosure degree in the Near Zone or decreasing Vehicle lane width. Reverberation time is primarily distributed between 1.0 s and 3.0 s and tends to be stable when the propagation distance increases. The mean reverberation time increases with the increasing Mean façade height, Sidewalk width, Cross-sectional enclosure degree in the Near Zone or Standard deviation of Plan enclosure degree in the Near Zone. The typical early decay time distribution curve is clearly divided into two areas along the propagation direction. In the first area, the early decay time value is notably small and nearly equals zero. With a sudden increase, the early decay time maintains at a relatively stable value in the second area (stable area) of 0 to 3.0 s. The mean early decay time in the stable area increases with the increasing Vehicle lane width, increasing Cross-sectional enclosure degree or decreasing Standard deviation of Plan enclosure degree in the Near Zone. © The Author(s) 2017."
"10.1177/2399808317714798","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041494946&doi=10.1177%2f2399808317714798&partnerID=40&md5=42ab3573e9c120a6906c60e2d5bd32c1","There is a lack of understanding of how certain characteristics of the urban environment influence an individual’s spatial cognition and familiarity with surrounding areas, and, subsequently, their travel behaviours and how these change over time. This paper aims to address this research gap in exploring the dynamics of individuals’ spatial cognitions by observing the changes of respondents’ familiar areas over time, and investigating the possible determinants that constitute respondents’ familiar areas. Panel data, containing two-week travel diaries and maps of familiar areas, were collected in four different waves over a seven-month period for 55 individuals in Stockholm, Sweden. The reported familiar areas for each individual were digitised into quantifiable variable form and further analysed by applying dynamic binary probit and linear regression models. The results show that, while familiar area is largely influenced by one’s previous knowledge of the area, it is also continuously corrected by events in between. Different land use characteristics have different impacts on different social groups’ travel patterns, thus contributing to the variability in the size of one’s familiar areas. © The Author(s) 2017."
"10.1177/2399808317712515","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041422869&doi=10.1177%2f2399808317712515&partnerID=40&md5=3e283f6710101ad37190723550a38bd3","There has been a recent shift in England towards empowering citizens to shape their neighbourhoods. However, current methods of participation are unsuitable or unwieldy for many people. In this paper, we report on ChangeExplorer, a smart watch application to support citizen feedback, to investigate the extent to which digital wearables can address barriers to participation in planning. The research contributes to both technology-mediated citizen involvement and urban planning participation methods. The app leverages in-situ, quick interactions encouraging citizens to reflect and comment on their environment. Taking a case study approach, the paper discusses the design and deployment of the app in a local planning authority through interviews with 19 citizens and three professional planners. The paper discusses the potential of the ChangeExplorer app to address more conceptual issues, and concludes by assessing the degree to which the technology raises awareness of urban change and whether it could serve as a gateway to more meaningful participatory methods. © The Author(s) 2017."
"10.1080/23270012.2019.1570364","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060879419&doi=10.1080%2f23270012.2019.1570364&partnerID=40&md5=7623746b9a7060f706a36ddcb24f2533","Airline operators face accidental damages on their fleet of aircraft as part of operational practice. Individual occurrences are hard to predict; consequently, the approach towards repairing accidental damage is reactive in aircraft maintenance practice. However, by aggregating occurrence data and predicting future occurrence rates, it is possible to predict future long-term (strategic) demand for maintenance capacity. In this paper, a novel approach for integration of reliability modelling and inventory control is presented. Here, the concept of a base stock policy has been translated to the maintenance slot capacity problem to determine long-term cost-optimal capacity. Demand has been modelled using a superposed Non-homogeneous Poisson Process (NHPP). A case study has been performed on damage data from a fleet of Boeing 777 aircraft. The results prove the feasibility of adopting an integrated approach towards strategic capacity identification, using real-life data to predict future damage occurrence and associated maintenance slot requirements. © 2019, © 2019 The Author(s). Published by Informa UK Limited, trading as Taylor & Francis Group."
"10.1080/23270012.2019.1566032","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060599503&doi=10.1080%2f23270012.2019.1566032&partnerID=40&md5=13a32a4b570a5c3f05488aea6aa7397e","This paper describes an industrial structure and its equation system of a circular economy for material circulation and builds a system dynamic model for resources recycling utilization based on Compartment Model Theory. A circulation multiplier and its computational formula are defined for measuring the efficiency of resources recycling utilization. The simulated results indicate that the resources recycling utilization can not only realize the amount accumulation of natural resources and improve the resources recycling efficiency but can minimize discharges into natural environment by means of adjustment to each compartment parameter in the circular economy. © 2019, © 2019 Antai College of Economics and Management, Shanghai Jiao Tong University."
"10.1080/23270012.2019.1568921","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060565962&doi=10.1080%2f23270012.2019.1568921&partnerID=40&md5=e46f7c31dfc9bfd914ff4ccefca0911c","As an integrated part in supply chain, third-party logistics (3PL) has intrinsic connections with upstream manufacturer and downstream retailer. Using a Stackelberg game model consisting of a manufacturer, a retailer and a 3PL to explicitly capture the interaction of firms' operations decisions, this paper attempts to better understand the role of integrated logistics and procurement service (ILPS) provided by a 3PL firm in supply chain management. Compared with a supply chain without ILPS, a Pareto region, in which all the supply chain members benefit from working with a 3PL firm offering ILPS, is disclosed. We also show that the Pareto region is more likely to occur with higher demand uncertainty. Finally, we reveal that the manufacturer obtains the highest profit in the Pareto region, and that the retailer can improve his profit share as the standard deviation of demand increases. © 2019, © 2019 Antai College of Economics and Management, Shanghai Jiao Tong University."
"10.1080/23270012.2019.1566033","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060244487&doi=10.1080%2f23270012.2019.1566033&partnerID=40&md5=bb5ddebfa0b590cbddc940bb7a54900b","Heavy seasonal rain makes waterway flood and is one of the preeminent reason behind flooding. Flooding causes various perils with outcomes including danger to human life, harm to building, streets, misfortune to horticultural fields and bringing about human uprooting. Thus, prediction of flood is of prime importance so as to reduce exposure of people and destruction of property. This paper focuses on applying different neural networks approach, i.e. Multilayer Perceptron, Radial Basis functional neural network, Local Linear Radial Basis Functional Neural Network and Artificial Neural Network with Whale Optimization to predict flood in terms of rainfall, gauge, area, velocity, pressure, average temperature, average wind speed that are setup through field and lab investigation from the contextual analysis of river “Daya” and “Bhargavi”. It has always been a troublesome undertaking to predict flood as many factors have influence on it although with this neural network models the prediction accuracy can be optimized using back propagation method which is a widely applied over traditional learning method for neural system because of its preeminent learning ability. The flood prediction system is built with the four models and a comparison is made which provides us the answer to which model is effective for the prediction. © 2019, © 2019 Antai College of Economics and Management, Shanghai Jiao Tong University."
"10.5334/dsj-2019-001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060889163&doi=10.5334%2fdsj-2019-001&partnerID=40&md5=539d6f2213ecd366ec2a09e94ca892e1","This study demonstrates the use of mobile phone data to derive country-wide mobility patterns. We identified significant locations of users such as home, work, and other based on a combined measure of frequency, duration, time, and day of mobile phone interactions. Consecutive mobile phone records of users are used to identify stay and pass-by locations. A stay location is where users spend a significant amount of their time measured through their mobile phone usage. Trips are constructed for each user between two consecutive stay locations in a day and then categorized by purpose and time of the day. Three measures of entropy are used to further understand the regularity of user’s spatiotemporal mobility patterns. The results show that user’s in a high entropy cluster has high percentage of non-home based trips (77%), and user’s in a low entropy cluster has high percentage of commuting trips (49%), indicating high regularity. A set of doubly constrained trip distribution models is estimated. To measure travel cost, the concept of a centroid point that assumes the origins and destinations of all trips are concentrated at an arbitrary location such as the centroid of a zone is replaced by multiple origins and destinations represented by cell tower locations. Note that a cell tower location can only be used as trips origin/destination location when a stay is detected. The travel cost measured between cell tower locations has resulted in shorter trip distances and the model estimation shows less sensitivity to the distance-decay effect. © 2019 The Author(s)."
"10.5334/dsj-2019-006","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060883591&doi=10.5334%2fdsj-2019-006&partnerID=40&md5=8ad422fef625a3891fedc6f9ec5eefbe","Research Data Management at Bielefeld University is considered as a cross-cutting task among central facilities and research groups at the faculties. While initially started as project “Bielefeld Data Informium” lasting over seven years (2010–2015), it is now being expanded by setting up a Competence Center for Research Data. The evolution of the institutional RDM is based on the three-pillar principle: 1. Policies, 2. Technical infrastructure and 3. Support structures. The problem of data quality and the issues with reproducibility of research data is addressed in the project Conquaire. It is creating an infrastructure for the processing and versioning of research data which will finally allow publishing of research data in the institutional repository. Conquaire extends the existing RDM infrastructure in three ways: with a Collaborative Platform, Data Quality Checking, and Reproducible Research. © 2019 The Author(s)."
"10.5334/dsj-2019-002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060865217&doi=10.5334%2fdsj-2019-002&partnerID=40&md5=ac63252e77fd3e50d1e1a9b4df3c9ecc","Progress in paleoclimatology increasingly occurs via data syntheses. We describe additions to a collection prepared for use in paleoclimate state estimation, specifically the Last Millennium Reanalysis (LMR). The 2290 additional series include 2152 tree ring chronologies and 138 other series. They supplement the collection used previously and together form a database titled LMRdb 1.0.0. The additional data draws from lake core, ice core, coral, speleothem, and tree ring archives, using published data primarily from the NOAA Paleoclimatology archive and a set of tree ring width chronologies standardized from raw International Tree Ring Data Bank ring width series. In contrast to many previous paleo compilations, the data were not selected (screened) on the basis of their environmental correlation, multi-century length, or other attributes. The inclusion of proxies sensitive to moisture and other environmental variables expands their use in data assimilation. A preliminary calibration using linear regression with mean annual temperature reveals characteristics of the proxy series and their relationship to temperature, as well as the noise and error characteristics of the records. The additional records are structured as individual files in the NOAA Paleoclimatology format and archived at NOAA Paleoclimatology (Anderson et al. 2018) and will continue to be improved and expanded as part of the LMR Project. The additions represent a four-fold increase in the number of records available for assimilation, provide expanded geographic coverage, and add additional proxy variables. Applications include data assimilation, proxy system model development, and paleoclimate reconstruction using climate field reconstruction and other methods. © 2019 The Author(s)."
"10.5334/dsj-2019-003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060862609&doi=10.5334%2fdsj-2019-003&partnerID=40&md5=0a2b8f2843b30e5305aa29046f8a915e","As data repositories make more data openly available it becomes challenging for researchers to find what they need either from a repository or through web search engines. This study attempts to investigate data users’ requirements and the role that data repositories can play in supporting data discoverability by meeting those requirements. We collected 79 data discovery use cases (or data search scenarios), from which we derived nine functional requirements for data repositories through qualitative analysis. We then applied usability heuristic evaluation and expert review methods to identify best practices that data repositories can implement to meet each functional requirement. We propose the following ten recommendations for data repository operators to consider for improving data discoverability and user’s data search experience: 1. Provide a range of query interfaces to accommodate various data search behaviours. 2. Provide multiple access points to find data. 3. Make it easier for researchers to judge relevance, accessibility and reusability of a data collection from a search summary. 4. Make individual metadata records readable and analysable. 5. Enable sharing and downloading of bibliographic references. 6. Expose data usage statistics. 7. Strive for consistency with other repositories. 8. Identify and aggregate metadata records that describe the same data object. 9. Make metadata records easily indexed and searchable by major web search engines. 10. Follow API search standards and community adopted vocabularies for interoperability. © 2019 The Author(s)."
"10.5334/dsj-2019-004","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060778609&doi=10.5334%2fdsj-2019-004&partnerID=40&md5=ff0cd16c4d77a64b3f5d299d97900cf9","VAMDC bridged the gap between atomic and molecular (A&M) producers and users by providing an interoperable e-infrastructure connecting A&M databases, as well as tools to extract and manipulate those data. The current paper highlights how the new paradigms for data citation produced by the Research Data Alliance in order to address the citation issues in the data-driven science landscape, have successfully been implemented on the VAMDC e-infrastructure. © 2019 The Author(s)."
"10.5334/dsj-2019-005","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060703153&doi=10.5334%2fdsj-2019-005&partnerID=40&md5=f154d90e8958e6cf00436a3e78c04bed","Science conducted in cross-institutional, interdisciplinary, long-term research projects requires active sharing of data, documents and further information. Thus, within the Collaborative Research Centre/Transregio 32 ‘Patterns in Soil-Vegetation-Atmosphere Systems’, funded by the German Research Foundation, research data management (RDM) services have been available since early 2007. These services were established to support all researchers during their entire individual research studies. They cover provision of general guidance, support and training for RDM. To fulfil the scientists’ needs and requests with regard to storage, backup, documentation, search and sharing of data with other project members, a project-specific RDM system was designed and implemented. This system was developed and continuously modified in collaboration with the scientists to facilitate their system acceptance. Besides the mentioned services, the system supports further common services such as controlled access to data, rights management, data publication with DOI and data statistics (on repository and single data level). All RDM services provided for the scientists are thus bundled and available to the users in one system: a ‘one-stop-shop’. After more than ten years of RDM service provision for the CRC/TR32, the repository statistics clearly visualize the use of the diverse RDM system services. Furthermore, it has been shown that an RDM adapted to the needs of interdisciplinary researchers can be fruitful and indispensable when scientists conduct their research study e.g. with a time lag. RDM services established at an early stage can contribute to a successful long-term research project. © 2019 The Author(s)."
"10.1007/s41019-019-0086-8","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064948311&doi=10.1007%2fs41019-019-0086-8&partnerID=40&md5=bbecad37660341086af5ea37f3e32070","Ever since the social networks became the focus of a great number of researches, the privacy risks of published network data have also raised considerable concerns. To evaluate users’ privacy risks, researchers have developed methods to de-anonymize the networks and identify the same person in the different networks. However, the existing solutions either require high-quality seed mappings for cold start, or exhibit low accuracy without fully exploiting the structural information, and entail high computation expense. In this paper, we propose a fast and effective seedless network de-anonymization approach simply relying on structural information, named RoleMatch. RoleMatch equips with a new pairwise node similarity measure and an efficient node matching algorithm. Through testing RoleMatch with both real and synthesized social networks, which are anonymized by several popular anonymization algorithms, we demonstrate that the RoleMatch receives superior performance compared with existing de-anonymization algorithms. © 2019, The Author(s)."
"10.1007/s41019-019-0087-7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064901393&doi=10.1007%2fs41019-019-0087-7&partnerID=40&md5=15ed76a4f3002a2ed9b6abb821d837be","The abstractive method and extractive method are two main approaches for automatic document summarization. In this paper, to fully integrate the relatedness and advantages of both approaches, we propose a general unified framework for abstractive summarization which incorporates extractive summarization as an auxiliary task. In particular, our framework is composed of a shared hierarchical document encoder, a hierarchical attention mechanism-based decoder, and an extractor. We adopt multi-task learning method to train these two tasks jointly, which enables the shared encoder to better capture the semantics of the document. Moreover, as our main task is abstractive summarization, we constrain the attention learned in the abstractive task with the labels of the extractive task to strengthen the consistency between the two tasks. Experiments on the CNN/DailyMail dataset demonstrate that both the auxiliary task and the attention constraint contribute to improve the performance significantly, and our model is comparable to the state-of-the-art abstractive models. In addition, we cut half number of labels of the extractive task, pretrain the extractor, and jointly train the two tasks using the estimated sentence salience of the extractive task to constrain the attention of the abstractive task. The results do not decrease much compared with using full-labeled data of the auxiliary task. © 2019, The Author(s)."
"10.1007/s41019-019-0085-9","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064892252&doi=10.1007%2fs41019-019-0085-9&partnerID=40&md5=4cbf09551e38399819bcd29a541328c7","In this paper, we propose a dynamic forecasting framework, named DMDP (dynamic multi-source default probability prediction), to predict the default probability of a company. The default probability is a very important factor to assess the credit risk of listed companies on a stock market. Aiming at aiding financial institutions in decision making, our DMDP framework not only analyzes financial data to capture the historical performance of a company, but also utilizes long short-term memory model to dynamically incorporate daily news from social media to take the perceptions of market participants and public opinions into consideration. The study of this paper makes two key contributions. First, we make use of unstructured news crawled from social media to alleviate the impact of financial fraud issue made on default probability prediction. Second, we propose a neural network method to integrate both structured financial factors and unstructured social media data with appropriate time alignment for default probability prediction. Extensive experimental results demonstrate the effectiveness of DMDP in predicting default probability for the listed companies in mainland China, compared with various baselines. © 2019, The Author(s)."
"10.1007/s41019-019-0090-z","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064848890&doi=10.1007%2fs41019-019-0090-z&partnerID=40&md5=afe4b73ff1f1923be320f56885f57135","With the popularity of knowledge graphs growing rapidly, large amounts of RDF graphs have been released, which raises the need for addressing the challenge of distributed subgraph matching queries. In this paper, we propose an efficient distributed method to answer subgraph matching queries on big RDF graphs using MapReduce. In our method, query graphs are decomposed into a set of stars that utilize the semantic and structural information embedded RDF graphs as heuristics. Two optimization techniques are proposed to further improve the efficiency of our algorithms. One algorithm, called RDF property filtering, filters out invalid input data to reduce intermediate results; the other is to improve the query performance by postponing the Cartesian product operations. The extensive experiments on both synthetic and real-world datasets show that our method outperforms the close competitors S2X and SHARD by an order of magnitude on average. © 2019, The Author(s)."
"10.1007/s41019-019-0084-x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064848079&doi=10.1007%2fs41019-019-0084-x&partnerID=40&md5=4393a92e76100dc8a897b1c214742107","We analyze the performance of regular decomposition, a method for compression of large and dense graphs. This method is inspired by Szemerédi’s regularity lemma (SRL), a generic structural result of large and dense graphs. In our method, stochastic block model (SBM) is used as a model in maximum likelihood fitting to find a regular structure similar to the one predicted by SRL. Another ingredient of our method is Rissanen’s minimum description length principle (MDL). We consider scaling of algorithms to extremely large size of graphs by sampling a small subgraph. We continue our previous work on the subject by proving some experimentally found claims. Our theoretical setting does not assume that the graph is generated from a SBM. The task is to find a SBM that is optimal for modeling the given graph in the sense of MDL. This assumption matches with real-life situations when no random generative model is appropriate. Our aim is to show that regular decomposition is a viable and robust method for large graphs emerging, say, in Big Data area. © 2019, The Author(s)."
"10.1007/s41019-019-0088-6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064820177&doi=10.1007%2fs41019-019-0088-6&partnerID=40&md5=9bdf70631ee1eed95ce0fce16ae4a51e","A novel parallel algorithm is presented for generating random scale-free networks using the preferential attachment model. The algorithm, named cuPPA, is custom-designed for “single instruction multiple data (SIMD)” style of parallel processing supported by modern processors such as graphical processing units (GPUs). To the best of our knowledge, our algorithm is the first to exploit GPUs, and also the fastest implementation available today, to generate scale-free networks using the preferential attachment model. A detailed performance study is presented to understand the scalability and runtime characteristics of the cuPPA algorithm. Also another version of the algorithm called cuPPA-Hash tailored for multiple GPUs is presented. On a single GPU, the original cuPPA algorithm delivers the best performance, but is challenging to port to multi-GPU implementation. For multi-GPU implementation, cuPPA-Hash has been used as the parallel algorithm to achieve a perfect linear speedup up to 4 GPUs. In one of the best cases, when executed on an NVidia GeForce 1080 GPU, the original cuPPA generates a scale-free network of two billion edges in less than 3 s. On multi-GPU platforms, cuPPA-Hash generates a scale-free network of 16 billion edges in less than 7 s using a machine consisting of 4 NVidia Tesla P100 GPUs. © 2019, The Author(s)."
,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064230802&partnerID=40&md5=e58183667cc3cfc7b75963b9c831c50b","Machine learning presents unique challenges and tremendous opportunities for today’s marketer, and while many applications have already become common practice, the future holds exciting use cases, some of which are in development and others yet to be imagined. Leveraging the vast amount of data available in the exhaust stream of digital marketing and advertising, and coupling this with almost limitless data storage and processing capacity, the move from rules-based to intelligent analysis is driving efficiencies across a number of marketing initiatives and capabilities. From intelligent bidding and the serving of advertisements across the most common digital channels to advanced segmentation, audience creation, attribution and more, machine learning has already established itself across a large and complex marketing ecosystem. Recent applications in purchase intent and churn modelling, data-driven retargeting and even data-driven creative are using machine learning to provide competitive advantage now and into the future. © Henry Stewart Publications."
,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064228937&partnerID=40&md5=b07554bb6e311f669b31c2b33208e5ca","Marketing insights generated from analytical methods have been used by firms for decades to promote the consumption of indulgent products. Customer insights generated from marketing science also show great promise in encouraging health engagement and improving health outcomes through intervention programmes. This paper presents a health insurance case study that replicates the effects of leveraging marketing analytics into preventive care, by building targeting segmentation to better understand the customer’s profile as well as design the optimal interaction and communication methods based on each segment’s preferences and characteristics. The findings indicate that marketing analytics enable a significantly greater improvement in health engagement and outcomes, as compared with control groups with non-specific interactions or ones which are non-personalised. The results provide further evidence of the benefits of leveraging marketing analytics in health engagement. © Henry Stewart Publications."
,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064225683&partnerID=40&md5=3b8358f200088d3d09e3bc244000a2ce","Harnessing data in today’s business environment can be complex and confusing. For this reason, organisations need a data strategy to guide them. The days of a monolithic, one-size-fits-all data strategy are long gone. Organisations must be nimble in their approaches, willing to accept challenges and failures, and adapt in real time to changing consumer behaviours, multiple marketing initiatives, and pressure to achieve operational efficiency amid ever-growing volumes of structured, semi-structured and unstructured data. This paper analyses how organisations need an executable data strategy that breaks down data strategy into its component parts and enables an agile approach to realising the necessary conditions for using data with purpose. © Henry Stewart Publications."
,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064200996&partnerID=40&md5=213092332491fec3b78b393c5535d461","As the world is becoming more digital, connected and intelligent, there is a growing desire among companies to become more data-driven. The unstoppable appetite for higher speed, sound and image resolution in this digital revolution is accompanied by an exponential increase in the volume of data. The key challenge, however, is how to use such data to serve our goals better. It is unwise to rely solely on technologies to cope with this challenge; rather, organisations must strike a balance between people and technologies. Technologies are here to help humans to take decisions, not take responsibility for decision-making. Championing this transformation in organisational culture is not easy and it is essential to lead by example. This article provides a foundation for building a data-driven culture and serves as a guideline for data-driven champions and other individuals who want to make such a change. It provides a generic framework on how data can play a critical role in research, problem solving and decision-making; provides an overview of common challenges; and suggests mitigations for daily practice. © Henry Stewart Publications."
,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064176086&partnerID=40&md5=87850ee9695f91733a34c882c7d1a22e","In today’s competitive business landscape, both business-to-business and business-to-consumer marketers are seeking insights from their marketing initiatives to see what is working and what is not. The faster these insights are acted upon, the faster companies can gain a competitive edge. Companies are also delving into the customer journey in order to optimise the user experience. Those yet to do so must start implementing the type of marketing analytics that will take them and their business to the next level. They need to be provided with the actionable insights that will empower them to make business decisions seamlessly. This paper analyses how the emergence of artificial intelligence and machine learning in the marketing sphere is sure to give marketing management the necessary power to shine. © Henry Stewart Publications."
,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064174443&partnerID=40&md5=d73cca94a101ad5460393a60a73c169f","As brands shift from product-focused to customer-centric business models, their interactions and relationships with consumers are both driven and defined by data. As digitisation accelerates, increasing competition and putting more power in the hands of consumers, brands must use clean, accurate, high-quality data to deliver valuable, positive experiences that will result in a sustainable advantage. This paper outlines five key factors companies must consider when using data to create customer-centric experiences and empower marketers with the data-driven mandates generated by these continually evolving elements. © Henry Stewart Publications."
,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064172987&partnerID=40&md5=0922ad8f87e505010182121173393fb3","Digital communications are becoming increasingly personalised. Numerous case studies demonstrate that the personalisation of digital communications drives sales. The majority of these case studies look at both one-to-one messaging and one-to-one media channels. The study described in this paper set out to understand the impact of localisation for out-of-home (OOH) communications using one-to-many broadcast media. The authors collaborated with SUBWAY® to test how different levels of localisation drove sales of SUBWAY subs. The results of this study contribute to understanding the sweet spot for localised messaging in OOH communications. © Henry Stewart Publications."
,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064171884&partnerID=40&md5=6c444cf9899f40064e22321edd1d638a","The ability to understand emotion from human communication is essential for marketers and data analysts, particularly in a world of growing non-verbal communication, such as reviews, complaints, surveys social media and chatbots. Thanks to advancements in artificial intelligence (AI) text analytics, a lot of information typically missed in sentiment analysis can now be analysed and early warning of issues flagged much earlier. This paper looks at how AI text analytics will succeed sentiment analysis and how it can identify additional customer emotion and intent by using concepts rather than simple keywords in its analysis. The paper also looks at how the most successful AI text analytics feature human-in-the-loop technology that optimises the combination of automation and human involvement. This makes the need for data scientists to run day-to-day models obsolete as the human input can come from the business user instead. The paper presents examples of how AI text analytics and emotions analytics can be applied, and argues that AI text analytics can reduce reliance on data scientists, provide early warnings of issues and analyse a much deeper level of sentiment and customer intent with higher levels of accuracy. © Henry Stewart Publications."
,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064169949&partnerID=40&md5=54f9150f9db31b842b8ef7525c76382b","This paper examines the evolution of conversational platforms. Over the next three to five years, the technology will likely evolve rapidly, allowing conversational platforms to move beyond established customer service use cases to applications that facilitate more informed customer decision-making and create more immersive brand engagement experiences. Marketers will need to sort through the legal, regulatory and brand trust issues associated with the Big Data generated by conversational platforms, as well as integrate conversational platforms into their user experience roadmap, data strategies and marketing technology stack. © Henry Stewart Publications."
,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064169490&partnerID=40&md5=d7fe8633b27ba76d5349bcdf44c94a34","Analytics and marketing technology (martech) are key tools for marketing and essential for the marketing function to serve as a nimble, effective and customer-centric organisation. Together, analytics and martech pave the way for marketing to serve as a strategic member of the organisation, manage and measure marketing performance, and facilitate customer, market and product decisions. Despite knowing what it takes to make analytics an engine of growth, many marketing organisations remain analytically challenged. In the majority of cases, this can be attributed to a lack of quality data, people, skills or predictive tools. This paper explores the evolution and role of analytics and martech and how marketing can use analytics to enable meaningful customer-related decisions with a positive impact on profitability, targeting, loyalty and share of wallet. © Henry Stewart Publications."
,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064165260&partnerID=40&md5=ef872fe21f1e24c7061db0a85b9712af","The EU’s General Data Protection Regulation (GDPR) has a substantial impact on all areas of digital marketing and analytics, including first and third-party data collectiontargeting and segmentation, campaign execution and analysis. Since its introduction, the digital marketing industry has made various changes to its practices to comply with the new law, changing the landscape for marketers, advertisers and publishers. This paper reviews the nature and impact of these changes, and makes several recommendations for marketers who may still be unclear on how to balance their own interests with those of the individuals with whom they wish to communicate through their marketing. It also discusses strategies to mitigate the impact of the restrictions imposed by the GDPR on data collection and processing, particularly through third parties. © Henry Stewart Publications."
,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064156009&partnerID=40&md5=6fe33967d99bc094f927ce91066c02c1","To deliver a seamless, personalised customer experience, marketers require high-quality, complete and accurate data. In practice, however, data are frequently dispersed throughout the enterprise, locked away in siloed applications, in inconsistent formats or of poor quality. Ownership is not centralised, making it very difficult to remedy the situation and unlock the full potential of state-of-the-art industry tools and technologies. The magnitude of the challenge is beyond the scope and capabilities of the chief information officer. For this reason, some organisations have created a new role: the chief data officer (CDO). This paper explores a number of issues including the role of the CDO, the responsibility of the CDO compared with that of the chief information officer or chief digital officer, how organisational data maturity correlates to use of the CDO, how the chief marketing officer (CMO) has yet to leverage this new function, the challenges associated with such a collaboration and how various organisations are viewing the marketing data challenge. The paper goes on to describe the impact of the General Data Protection Regulation as a catalyst for data-quality initiatives and models for collaboration between the CMO and CDO. This paper forms the preliminary framework for a larger research initiative to be completed in the second and third quarters of 2019. © Henry Stewart Publications."
,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064152577&partnerID=40&md5=b9f6cde64edb2ca80510a6a9d7d5f14b","Big Data can be used to make sense of highly unstructured consumer data, and improve both the reliability and validity of measures that have historically required manual coding. Furthermore, using available secondary data allows for much faster coding. This research proposes a new and more robust way to measure the degree of variety-seeking exhibited by consumers. It employs the Million Song Dataset, a database of consumer-generated tags describing musical styles, to create measures of musical variety with minimal manual coding. Using a sample of 10,511 SiriusXM subscribers, the research compares this novel method of measuring variety-seeking behaviours with a more simple model, and finds that the novel method is a more accurate predictor of churn. © Henry Stewart Publications."
,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064150235&partnerID=40&md5=f62779eea8cf52d651339989117e66ac","This paper addresses the problem of paid media budgeting and forecasting using a data-driven approach. As there is no specific analytic framework to estimate advertising investment, such calculations tend to be ad hoc in nature. This paper presents the delta opportunity index as a potential approach to paid media budgeting. The technique combines internal client analytics data as a proxy for the offer variable and external data such as search engine query volumes as a proxy for the demand variable. From there, the combined data sources are standardised as ratios to enable the calculation of the delta opportunity index. The final output provides a media budget distribution estimate to be used by media planners and marketers. © Henry Stewart Publications."
,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064148803&partnerID=40&md5=01ff6721e4cb65ace6701a9b426cc75e","This paper discusses the similarities and significant differences between the Personal Information Protection and Electronic Documents Act 2000 (PIPEDA), which sets out the ground rules for how businesses in Canada must handle personal information in the course of commercial activity, and the California Consumer Privacy Act 2018 (CCPA), which enhances privacy rights and consumer protections within the US State of California. Using PIPEDA as a baseline, the paper examines the operating practices required by these laws, and explores the issues that organisations must consider when complying with these laws. © Henry Stewart Publications."
"10.1057/s41270-019-00055-6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066044139&doi=10.1057%2fs41270-019-00055-6&partnerID=40&md5=a2cdfd8cccbd84daa8d9d9f09d39ce69","Customer engagement behaviors (CEBs) with marketing content or contacts, such as downloading a whitepaper or attending an event, can increasingly be recorded for individual customers. We develop a five-step process for monitoring CEBs. This engagement monitoring process requires understanding which CEBs relate to relevant outcomes and detecting when engagement levels are aberrant. The process is illustrated with a case study, which uses 4 years of data tracing 160,448 customers from 784 key accounts of a leading professional service provider that uses content and contact marketing. As marketer move away from traditional yearly marketing plans in an effort to operate in real-time in the face of vast change in the communication environment, process control of this kind can allow marketers to be agile without losing strategic discipline. © 2019, Springer Nature Limited."
"10.1057/s41270-019-00054-7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065914199&doi=10.1057%2fs41270-019-00054-7&partnerID=40&md5=342c42fd6ac5b2d45b43ff09224e1b80","The internationalization process has been constantly moving with an exceptional transformation led by the digital era. In order to face the challenges of the internationalization, enterprises need to understand thoroughly the new business environment, including foreign markets and customer insights to offer innovative products and services. Indeed, big data as the new source of knowledge helps enterprises convert business information into competitive advantages in the global market. Although the topic of big data receives much attention from researchers, the adoption of big data in internationalization is still an emerging research interest, especially in small and medium-sized enterprises (SMEs). The purpose of this paper is to provide a concept-centric literature review that synthesizes and evaluates recent studies to examine the current state and future research directions of big data adoption in the internationalization process. The result of this paper indicates the status and future research directions of big data adoption in internationalization with the focus on international marketing. The paper finds out that the adoption of big data in international marketing is still in the early stage of maturity. Future research directions are also proposed based on the identified research gaps and the analysis of literature review. © 2019, Springer Nature Limited."
"10.1057/s41270-019-00053-8","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065907449&doi=10.1057%2fs41270-019-00053-8&partnerID=40&md5=f6e39b4f9631ecc6d542a29bb6b21db2","This study aims to provide social marketers and researchers with some innovative perspectives on the application of social media in disseminating and promoting new sustainability knowledge to targeted audiences including academics, community sustainability stakeholders, and policy-makers. An online survey was used to examine the audiences’ attitudes of and motivations for engaging in LinkedIn and Twitter sites that disseminate sustainability knowledge. Also, the fact-based measurements from LinkedIn and Twitter showing the participants’ reactions to the contents and formats were analyzed. The results suggest that the infographics message format received the highest engagement and response rates. Participants used Twitter to obtain general sustainability knowledge while being engaged in LinkedIn for specific advice on the implementation of community sustainability plans. © 2019, Springer Nature Limited."
"10.1057/s41270-019-00051-w","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065841545&doi=10.1057%2fs41270-019-00051-w&partnerID=40&md5=d8f43b8a3c33f4a03722c227dd4569bb","This paper examines the effect of shopping goals and switching cost on customer commitment from the notional lens of the self-determination theory (SDT) through a survey-based data retrieved from Nigerian online shoppers. Findings indicate that (i) utilitarian shopping goal influences hedonic shopping goal; (ii) utilitarian and hedonic shopping goals influence shoppers’ commitment to online shopping; and (iii) switching cost moderates the effect of utilitarian shopping goal on customer commitment. Thus, this paper provides an extended understanding of online shopping acceptance from the viewpoint of the SDT by demonstrating how intrinsic and extrinsic motivators influence shoppers’ commitment to online shopping. We also extend the SDT by illustrating the moderating role of switching cost in the link between utilitarian shopping goal and customer commitment. © 2019, Springer Nature Limited."
"10.1080/23270012.2019.1614488","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065781062&doi=10.1080%2f23270012.2019.1614488&partnerID=40&md5=c01f38f11fe3608940d37ded9a701413","Traditional inventory models are mostly ignorant of the life cycle dynamics of a technology product; hence, they often fail to identify different dimensions of inventory research. This paper attempts to investigate the relationship between adoption behavior of customers using life cycle dynamics and associated trade credit policies in order to optimize the total inventory cost. The demand model used in this paper treats sales as a function of awareness diffusion and adoption. Awareness is considered as a function of feedback effects from users/customers. Retailer’s optimal strategies for short life cycle product under credit financing were determined analytically. Finally, numerical examples have been used to support the theoretical results. Theoretical results have further been used to gain some managerial insights. © 2019, © 2019 Antai College of Economics and Management, Shanghai Jiao Tong University."
"10.1080/23270012.2019.1613684","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065745567&doi=10.1080%2f23270012.2019.1613684&partnerID=40&md5=5b783657d490522d0a0b4b04e2719b88","Music in advertising plays a crucial role in making the audience feel beyond the multi-level visual experience. The intrinsic link between brand publicity and advertising music has long been a puzzle. This paper discusses the impact of the consistency between the emotional characteristics of music and brand personality on brand experience and expands the discussion to brand experience under market competition. We use the examples of Canon and Apple for our study. The results shows that: (1) the higher the degree of consistency between the emotional experience from music and brand personality, the greater the positive effect on brand experience; (2) this positive effect is not as significant for functional brands as it is for representative brands; (3) the consistency between the emotional experience from music and brand personality has a greater impact on brand experience for representative brands than functional brands. The results provide practical guidance for branding campaigns. © 2019, © 2019 Antai College of Economics and Management, Shanghai Jiao Tong University."
"10.1057/s41270-019-00052-9","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065567498&doi=10.1057%2fs41270-019-00052-9&partnerID=40&md5=1777dbe23d059d4749e5490dc8bd26ea","This paper presents the bibliometric and visualization method applied to a dataset of 729 documents published in the collaborative economy research field. Four steps are described in details: (1) the delimitation of the field of study; (2) the selection of databases, keywords, and search criteria; (3) the extraction, cleaning, and formatting; and finally (4) the co-citation analysis and visualization. The method validation section shows the results obtained by applying our methodological procedure to an author network analysis as well as a source title network analysis. This study is unique which presents a co-citation analysis coupled with a network visualization applied to the rapidly growing research area of the collaborative economy as a whole and not only of the collaborative tourism and hospitality research, as has been previously. The originality of this method lies firstly in the fact that the data were extracted from two databases (Scopus and Web of Science) instead of one as is commonly done in analytic studies. Secondly, VOSviewer was our main analytical tool performing the co-citation analysis and the network visualizations. © 2019, Springer Nature Limited."
"10.1057/s41270-019-00050-x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065169988&doi=10.1057%2fs41270-019-00050-x&partnerID=40&md5=23f5205076152ec5452410e9f500803b","Unlike consumer goods, durable consumer goods have a long product life, so it is difficult to survey repurchase intentions. Therefore, recommendation intention tends to be adopted as factors that contribute to repurchase intention. However, it has been insufficient to consider influence of elapsed time since purchasing, which is a problem peculiar to durable consumer goods. In Japan, customers replace their cars about every 10 years on average. In this study, it was clarified that recommendation intention doesn’t have a constant influence on repurchase intention during this period. This result provides suggestions on how to handle recommendation intention in durable consumer goods. © 2019, Springer Nature Limited."
"10.1057/s41270-019-00049-4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063999902&doi=10.1057%2fs41270-019-00049-4&partnerID=40&md5=c0609b600788882dff38c5f72988362a","This paper has two main objectives. The first contributes to the development of marketing theory by proposing a measurement instrument, allowing for the testing of knowledge on the effective marketing research processes, while the second focuses on the verification of that knowledge across two groups: managers and researchers in business organizations from the perspective of a two-community theory of management. In particular, the level of compliance between information-users and information-producers on their knowledge of these processes is investigated. With these objectives in mind, theoretical assumptions of the effective marketing research processes from the methodological perspective are discussed. Next, methodological problems appearing in marketing research processes are characterized. Further discussion speaks about a need for stronger integrity of researchers and managers on the basis of methodological knowledge of effective research processes in order to enhance these processes and improve activities related to planning marketing strategies and decision-making. Finally, when comparing that knowledge on the basis of the empirical study conducted, we implemented the following analytical strategies based on Multi-Group Confirmatory Factor Analysis (MGCFA) and Multiple Indicators-Multiple Causes (MIMIC) model in reference to a sample 391 of respondents working in multinational companies in a European country (Poland). The results confirmed the psychometric quality of the developed scale of testing knowledge on effective marketing research processes, but also revealed that managers and researchers share slightly different points of view on the effective marketing research processes. Managers exhibit an even greater level of instability than researchers when evaluating these processes, while their insufficient knowledge is due to a lack of conviction that marketing research can be effective in general. Moreover, managers assume that marketing research and research information does not always play a significant role in the activities related to planning marketing strategies and decision-making. Our findings also have practical implications for organizations that wish to improve their internal information policy and make the marketing research more effective with equal engagement of both groups. © 2019, Springer Nature Limited."
"10.1504/IJBIDM.2019.098843","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064088927&doi=10.1504%2fIJBIDM.2019.098843&partnerID=40&md5=ea64d0a2d0ab82ad4c96abe0822ac01a","Existing work in the domain of distributed data mining mainly focuses on achieving the speedup and scaleup properties rather than improving performance measures of the classifier. Improvement in speedup and scaleup is obvious when distributed computing platform is used. But its computing power should also be used for improving performance measures of the classifier. This paper focuses on the same by considering reoccurrence of features and handling minority classes. Since it is very time consuming to run such complex algorithms on large datasets sequentially, distributed versions of the algorithms are designed and tested on the Hadoop cluster. Base associative classifier is designed based on multi-class, multi-label associative classification (MMAC) algorithm. Since no similar distributed algorithms exist, proposed algorithms are compared with the base classifier and have shown improvement in classifier performance measures. Copyright © 2019 Inderscience Enterprises Ltd."
"10.1504/IJBIDM.2019.098840","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064078709&doi=10.1504%2fIJBIDM.2019.098840&partnerID=40&md5=b48cec7b97d91c0cdb7bf76d9b648942","In this era of online retailing, personalisation of web content has become very essential. Recommender system is a tool for extraction of relevant information to render personalisation in web information retrieval systems. With an inclination towards customer oriented service, there is a need to understand the adaptability of customers, to provide products/services of interest at the right time. In this paper, a model for distributed context aware cross layer recommender system incorporating the principle of product diffusion is proposed. The offline-online modelled recommender system learns offline about the adaptation time of users using the principle of product diffusion and then, uses online explore-then-exploit strategy to make effective recommendations to the user at the most probable time of consumption. Also, an algorithm based on product adaptability is proposed for recommending new items to the most probable users. The extensive experiments and results demonstrate the efficiency, scalability, reliability and enhanced retrieval effectiveness of the proposed recommender system model. Copyright © 2019 Inderscience Enterprises Ltd."
"10.1504/IJBIDM.2019.098838","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064073572&doi=10.1504%2fIJBIDM.2019.098838&partnerID=40&md5=c85cce15c6a2d239563444943654272f","The prediction of heart disease is a difficult task, which needs much experience and knowledge. In order to reduce the risk of heart disease prediction, in this paper we proposed a rough set theory-based feature selection and FGA-NN classifier. The overall process of the proposed system consists of two main steps, such as: 1) feature reduction; 2) heart disease prediction. At first, the kernel fuzzy c-means clustering with roughest theory (KFCMRS) algorithm is applied to the high dimensional data to reduce the dimension of the attribute. After that, the medical data classification is done through FGA-NN classifier. To improve the prediction performance, hybridisation of firefly and genetic algorithm (FGA) is utilised with NN for weight optimisation. At last, the experimentation is performed by means of Cleveland, Hungarian, and Switzerland datasets. The experimentation result proves that the FGA-NN classifier outperformed the existing approach by attaining the accuracy of 83%. Copyright © 2019 Inderscience Enterprises Ltd."
"10.1504/IJBIDM.2019.098839","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064069559&doi=10.1504%2fIJBIDM.2019.098839&partnerID=40&md5=355792233a642197085962036331afb0","In the privacy preserving data mining, the utility mining casts a very vital part. The objective of the suggested technique is performed by concealing the high sensitive item sets with the help of the hiding maximum utility item first (HMUIF) algorithm, which effectively evaluates the sensitive item sets by effectively exploiting the user defined utility threshold value. It successfully attempts to estimate the sensitive item sets by utilising optimal threshold value, by means of the grey wolf optimisation (GWO) algorithm. The optimised threshold value is then checked for its performance analysis by employing several constraints like the HF, MC and DIS. The novel technique is performed and the optimal threshold resultant item sets are assessed and contrasted with those of diverse optimisation approaches. The novel HMUIF considerably cuts down the calculation complication, thereby paving the way for the enhancement in hiding performance of the item sets. Copyright © 2019 Inderscience Enterprises Ltd."
"10.1504/IJBIDM.2019.098841","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064068647&doi=10.1504%2fIJBIDM.2019.098841&partnerID=40&md5=4359fc8a7b08c5869b230241600a393a","There is an exponential growth in the available electronic information in the last two decades. It causes a huge necessity to quickly understand high volume text data. This paper describes an efficient algorithm and it works by assigning scores to sentences in the document which is to be summarised. It also focuses on document extracts; a particular kind of computed document summary. The proposed approach uses fuzzy classifier and deep learning algorithm. Fuzzy classifier produces score for each sentence and the deep learning (DL) also produces score for each sentence. The combination of score from both fuzzy classifier and DL produces the hybrid score. Finally, the summarised text can be generated based on this hybrid score. In our proposed approach, we have achieved an average precision rate of 0.92 and average recall rate of 0.88 and the compression rate is 10% according to the experimental analysis. Copyright © 2019 Inderscience Enterprises Ltd."
"10.1504/IJBIDM.2019.096809","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058824506&doi=10.1504%2fIJBIDM.2019.096809&partnerID=40&md5=d32bb467282397507c819d623d571672","Malwares enters into the victim system by injecting the code into victim system executable files or well-known files or folders. In this paper, the proposed dynamic runtime protection technique (DRPT) will ensure for protection of all the modes of the malware entering into the system. In the affected system, the behaviours of the injected file are monitored and controlled and the malware spreads either through online or offline modes via files. The DRPT unpack the malware, continuously monitors and analyses the windows application programming interface (API) calls in the imported and exported dynamic link library (DLL’s) of the malwares to find the injection code. DRPT also protects against the malware spread into the other files and the stealing of information from the victim machine. The DRPT tested with 1,517 executable files, among which 811 malicious files have been taken with different malware families. The result of DRPT shows true positive of 94.20% and false positive of 0.05%. Copyright © 2019 Inderscience Enterprises Ltd."
"10.1504/IJBIDM.2019.096812","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058821544&doi=10.1504%2fIJBIDM.2019.096812&partnerID=40&md5=81466a76c334e518ac717e7917400676","Steganography deal with hiding information science, which offers an ultimate security in defence, profitable usages, thus sending the imperceptible information, will not be bare or distinguished by others. The aim of this paper is to propose a novel steganographic method in JPEG images to highly enrich a data security by RSA algorithm and attains higher payload by modified quantisation table. The goals of this paper are to be recognised through: 1) modify the quantisation table of the JPEG-JSTEG tool, hiding secret message with its middle frequency to offer great embedding capacity; 2) for challenge, secure RSA algorithm is used to prevent data from extraction. A broad experimental evaluation compares the performance of our proposed work with existing JSTEG was conducted. This algorithm resulted in greater PSNR values and steganogram histogram is more similar. Experimental results reported that the proposed system is a state-of-the model, contributing abundant payload and beating the statistical revealing. Besides, our method has better in all the parameters than JPEG-JSTEG method. Copyright © 2019 Inderscience Enterprises Ltd."
"10.1504/IJBIDM.2019.096845","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058817326&doi=10.1504%2fIJBIDM.2019.096845&partnerID=40&md5=8e606b78fd60fbfa48afbf18d7e9ceb4","Honda is one of the most popular algorithm for rear end collision avoidance system, but does not intend to avoid all the accidents, this due to that it gives a smaller range of warning which cannot avoid the accidents at high speed, to overcome this issue we proposing the optimisation of warning range with a genetic algorithm to minimise the probability of collision. The results show that proposed algorithm avoids 7% more accident than the standard Honda algorithm. We presented the MATLAB implementation of proposed work with examples to show the efficiency of our work. Copyright © 2019 Inderscience Enterprises Ltd."
"10.1504/IJBIDM.2019.096793","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058816277&doi=10.1504%2fIJBIDM.2019.096793&partnerID=40&md5=db20a9ac615058d2e22d13f5ae712d14","In the present work we use the technologies of machine learning and OLAP for more accurate forecasting of such phenomena as a thunderstorm, hail, heavy rain, using the numerical model of convective cloud. Three methods of machine learning: support vector machine, logistic regression and ridge regression are used for making the decision on whether or not a dangerous convective phenomenon occurs at present atmospheric conditions. The OLAP technology is used for development of the concept of multidimensional data base intended for distinguishing the types of the phenomena (thunderstorm, heavy rainfall and light rain). Previously developed complex information system is used for collecting the data about the state of the atmosphere and about the place and at the time when dangerous convective phenomena are recorded. Copyright © 2019 Inderscience Enterprises Ltd."
"10.1504/IJBIDM.2019.096842","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058815245&doi=10.1504%2fIJBIDM.2019.096842&partnerID=40&md5=edfad34c0e52cd3766fd78260af54dbe","Association rule mining discovers interesting information from large databases. Frequency, reliability and domain knowledge form the multiple criteria for evaluation these association rules. Data envelopment analysis (DEA) is a popular technique used to rank association rules based on the previously mentioned multiple criteria. A decision maker might be interested to have a priority list of these ranked rules based on business and situational requirements. We present an approach to prioritise DEA ranked association rules based on the preference and desirability of the decision maker for different criteria. A modified generalised fuzzy evaluation method (MGFEM) obtains vector-valued fuzzy scores of a group of decision makers and aggregate them to form a preference. A fuzzy logic-based decision support mechanism prioritises these rules based on the decision maker’s desirability using the membership function and preference obtained from MGFEM. An example of DEA ranked association rules is presented to explain this innovative approach for prioritisation. Copyright © 2019 Inderscience Enterprises Ltd."
"10.1504/IJBIDM.2019.096813","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058811393&doi=10.1504%2fIJBIDM.2019.096813&partnerID=40&md5=07330d8be57218ee511ad927dcbe8fd9","Business intelligence systems provide an effective solution from large volumes of data for multidimensional online computing and analysis. Usually, in a decision-making process, organisations and enterprises, require several internal and/or external cubes which are often heterogeneous. Most of the time, the structure of these cubes is unknown to the decision-makers. To analyse a phenomenon, the decision-maker seeks among sets of cubes, in a collection, the cube which responds better to his need. In this context, we propose an approach that enables decision-makers to express their needs via a query expressed in a natural language, returns top-K relevant cubes and designs/constructs new cubes when no, or few deployed cubes are relevant. We propose a tool called RD-cubes-query implementing our approach in a ROLAP architecture. We use this tool in some experiments to validate our approach. Copyright © 2019 Inderscience Enterprises Ltd."
"10.1504/IJBIDM.2019.096796","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058807857&doi=10.1504%2fIJBIDM.2019.096796&partnerID=40&md5=76c69c6b5e325baa068e529273b67ff6","In the paper, a minimal constraint based cuckoo search (CS) algorithm is proposed for solving transmission congestion problem by considering both increase and decrease in generation power. Thus, the proposed algorithm is used to optimise the real power changes of generator while transmission congestion occurred. Then, the power loss, generator sensitivity factor and congestion management cost of the system is evaluated by the proposed algorithm according to the transmission congestion. The proposed method is implemented in MATLAB working platform and their congestion management performance is analysed. The performance of the proposed method is compared with the other existing methods such as fuzzy adaptive bacterial foraging (FABF), simple bacterial foraging (SBF), particle swarm optimisation (PSO), and artificial neural network (ANN)-CS respectively. The congestion management cost is reduced up to 26.169%. Through the analysis of comparison, it is shown that the proposed technique is better and outperforms other existing techniques in terms of congestion management measures. Copyright © 2019 Inderscience Enterprises Ltd."
"10.1504/IJBIDM.2019.096801","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058807513&doi=10.1504%2fIJBIDM.2019.096801&partnerID=40&md5=6dcc737051364d26aa694ee8bfd711b0","Due to the exponential growth of biomedical repositories such as PubMed and Medline, an accurate predictive model is essential for knowledge discovery in Hadoop environment. Traditional decision tree models such as multivariate Bernoulli model, random forest and multinominal naïve Bayesian tree use attribute selection measures to decide best split at each node of the decision tree. Also, the efficiency of document analysis in Hadoop framework is limited mainly due to the class imbalance problem and large candidate sets. In this paper, we proposed a two phase map-reduce framework with text preprocessor and classification model. In the first phase, mapper based preprocessing method was designed to eliminate irrelevant features, missing values and outliers from the biomedical data. In the second phase, a map-reduce based multi-class ensemble decision tree model was designed and implemented on the preprocessed mapper data to improve the true positive rate and computational time. The experimental results on the complex biomedical datasets show that the performance of our proposed Hadoop based multi-class ensemble model significantly outperforms state-of-the-art baselines. Copyright © 2019 Inderscience Enterprises Ltd."
"10.1504/IJBIDM.2019.096794","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058805108&doi=10.1504%2fIJBIDM.2019.096794&partnerID=40&md5=6785f4b262c142a986cea19619b768c4","This paper utilises a novel experimental dataset on consumer choice to investigate and benchmark the performance of alternative statistical models under conditions of extreme uncertainty. We compare the results of logistic regression, discriminant analysis, naïve Bayes classifier, neural network, support vector machine (SVM), decision tree, and random forest (RF) to discover that the RF model robustly registers the highest classification accuracy. Variable importance analysis reveals that apart from demographic and situational factors, consumer choice is highly dependent on social network effects and subject to numerous non-linearities, thus making machine learning approaches the best modelling choice. Copyright © 2019 Inderscience Enterprises Ltd."
"10.1504/IJBIDM.2019.096807","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058803542&doi=10.1504%2fIJBIDM.2019.096807&partnerID=40&md5=7594a71e5efef72728f13db0eba5fcff","In e-learning environment, optimal learning patterns are discovered for realising and understanding the effective learning styles. The value of uncertain and imprecise knowledge collected has to be categorised into classes known as membership grades. Rough set theory is potential in categorising data into equivalent classes and fuzzy logic may be applied through soft thresholds for refining equivalence relation that quantifies correlation between each class of elucidated data. In this paper, probabilistic variable precision fuzzy rough set technique (PVPFRST) is proposed for deriving robust approximations and generalisations that handles the types of uncertainty namely stochastic, imprecision and noise in membership functions. The result infers that the degree of accuracy of PVPFRST is 21% superior to benchmark techniques. Result proves that PVPFRST improves effectiveness and efficiency in identifying e-learners styles and increases the performance by 27%, 22% and 25% in terms of discrimination rate, precision and recall value than the benchmark approaches. Copyright © 2019 Inderscience Enterprises Ltd."
"10.1504/IJBIDM.2019.096835","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058802260&doi=10.1504%2fIJBIDM.2019.096835&partnerID=40&md5=efbac08c5c017d1c8e65d58e05c83e63","During business transactions there are lot of opportunity for data theft and data misinterpretation. Mostly, the legitimate users act like malicious users and try to misuse their privileges. So, it is very important to know their intentions and different strategies they apply for business data theft. In this paper, we develop an information analytics based technique for inferring attacker intent objectives and strategies (AIOS). The input to the model is the alert logs in real-world attack-defense scenario and output are the discovered attack strategies or patterns. The implementation of this model is done on a real-world attack-defence scenario to increase the learning efficiency of the technique. Experimental results on expected impact and attack path shows that the technique provides better results than conventional intrusion detection systems. Copyright © 2019 Inderscience Enterprises Ltd."
"10.1504/IJBIDM.2019.096843","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058799224&doi=10.1504%2fIJBIDM.2019.096843&partnerID=40&md5=f620a9a0458bf11266230ca718848804","As the driver’s mental state is among the primary factors responsible for collision, most of the sectors now install an automated approach to prevent the collision. The automatic approaches halt the vehicle unit on sensing the state of collision. Anti-collision system is an automatic approach that monitors the various factors of a vehicle unit on a gradual basis. The system is powered with Honda algorithm that computes the minimum safe distance from these factors. As the efficiency of algorithm is subjected to accuracy of sensor readings, the environmental tragedies and precision of sensors can deliver false readings that affects the performance of algorithm. In this paper genetic algorithm is implemented to optimise these values. The optimised modification of readings discards the probability of false reading obtained due to environmental noise. The evaluation parameters indicate that GA performs with almost same accuracy for optimisation. Copyright © 2019 Inderscience Enterprises Ltd."
"10.1504/IJBIDM.2019.096803","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058791639&doi=10.1504%2fIJBIDM.2019.096803&partnerID=40&md5=f5f572b028d72b1372506624c53493f4","Efficient mapping of virtual machine request to the available physical machine is an optimisation problem in data centres. It is solved by aiming to minimise the number of physical machines and utilising them to their maximum capacity. Another avenue of optimisation in data centre is the energy consumption. Energy consumption can be reduced by using fewer physical machines for a given set of VM requests. An attempt is made in this work to propose an energy efficient VM placement algorithm that is also network affinity aware. Considering the network affinity between VMs during the placement will reduce the communication cost and the network overhead. The proposed algorithm is evaluated using the Cloudsim toolkit and the performance in terms of energy consumed, communication cost and number of active PMs, is compared with the standard first fit greedy algorithm. Copyright © 2019 Inderscience Enterprises Ltd."
"10.1504/IJBIDM.2019.096808","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058790965&doi=10.1504%2fIJBIDM.2019.096808&partnerID=40&md5=20187c8a4f60bb110801b3efe3330bab","Learning-to-rank has been an exciting topic of research exclusively in hypothetical and the productions in the information retrieval practices. Usually, in the learning-based ranking procedures, it is expected the training and testing data are recovered from the identical data delivery. However those existing research methods do not work well in case of multiple documents retrieved from the cross domains (different domains). In this case ranking of documents would be more difficult where the contents are described in multiple documents from different cross domains. The main goal of this research method is to rank the documents gathered from the multiple domains with improved learning rate by learning features from different domains. The feature level information allocation and instance level information relocation are achieved with four learners namely RankNet, ranking support vector machine (SVM), RankBoost and AdaRank. The estimation results presented that the AdaRank algorithm achieves good performance. Copyright © 2019 Inderscience Enterprises Ltd."
"10.1504/IJBIDM.2019.096836","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058780940&doi=10.1504%2fIJBIDM.2019.096836&partnerID=40&md5=43960fc2a21a54814186d5281b296d0d","Web pages are heterogeneous and complex and there exists complicated associations within one web pages and linking to the others. The high interactions between terms in pages demonstrate vague and ambiguous meanings. Efficient and effective clustering methods are needed to discover latent and coherent meanings in context are necessary. This paper proposes an efficient clustering approach for fair semantic web content retrieval based on tri-level ontology construction model with hybrid dragonfly algorithm. Initially the query processing phase, by making use of systematic adaptive hierarchy method (SAHM) efficient ontology selection process is carried out by means of matching keywords retrieved form user query. Secondly, fuzzy sensitive near-neighbour influence (FSNI) based clustering approach relied on the ontology driven fuzzy linguistic measure, applied to estimate the uncertainty that may be relevant to the semantic content which belongs to the user quires. The proposed FSNI clustering approach with HDA algorithm performance is be evaluated and compared with existing clustering approaches in terms of retrieval accuracy and surfing time. Copyright © 2019 Inderscience Enterprises Ltd."
"10.1504/IJBIDM.2019.096844","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058780493&doi=10.1504%2fIJBIDM.2019.096844&partnerID=40&md5=4161ca689a855e1a8b4658e2fd40906d","The main objective of this research work is to develop an R-based fault identification model for power system in a cluster analysis environment. Cluster analysis-based data mining techniques model has been implemented to locate the three-phase transmission lines fault in IEEE 30 bus power system. Power World version 18 software was used to simulate the IEEE 30 bus power system and the three-phase transmission lines fault. The bus voltages at fault were collected and import to R statistical software to identify the faults at buses. Through cluster analysis using squared euclidean distance method, fault has been identified at each bus. Then the data also imported to R statistical package to compute the cophenetic distance of dendrogram and check the accuracy of clustering. This meant that the application of data mining techniques yields a huge potential in solving complex problems related to power system, it not only yield an accurate result but also fast computation. The proposed innovative successful model was able to locate the fault at each bus by bus nominal voltage comparison method. Copyright © 2019 Inderscience Enterprises Ltd."
"10.1504/IJBIDM.2019.096804","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058775420&doi=10.1504%2fIJBIDM.2019.096804&partnerID=40&md5=36693755fd51e17da3cd6a2b6b2cc2a2","In this work, we have proposed an approach for providing very high security to the cloud system. Our proposed method comprises of three phases namely authentication phase, cloud data centre selection phase and user related service agreement phase. For the purpose of accessing data from the cloud server, we will need a secure authentication key. In the authentication phase, the user authentication is verified and gets the key then encrypts the file using blowfish algorithm. Before encryption the input data is divided into column-wisely with the help of pattern matching approach. In the approach, the encryption and decryption processes are carried out by employing the blowfish algorithm. We can optimally select the cloud data centre to store the data; here the position is optimally selected with the help of bat algorithm. In the final phase, the user service agreement is verified. The implementation will be done by cloud sim simulator. Copyright © 2019 Inderscience Enterprises Ltd."
"10.4018/IJBAN.2019010105","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059785699&doi=10.4018%2fIJBAN.2019010105&partnerID=40&md5=8f19c576926bdb407c35a6b4493d401e","In this article, the authors present a model for the pricing of a substitute and complementary products (four commodities) while the seller delivers the products individually, in the form of a package to the final customer, and compares the profits from the sale of each scenario. Also, a comparative study has been completed. In the package sale, two complementary products are delivered in one package, but each item is sold in separate sales. The demand function for each product is a function of the price. To illustrate the validity of the model, numerical examples have been used and a sensitivity analysis has been done on the important parameters in the problem. Copyright © 2019, IGI Global."
"10.4018/IJBAN.2019010101","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059776498&doi=10.4018%2fIJBAN.2019010101&partnerID=40&md5=7ef6619cbe75e3096f839e1734398fe4","Discrete choice modeling is one of the main tools of estimation utilities and preference probabilities among multiple alternatives in economics, psychology, social sciences, and marketing research. One of popular DCM tools is the Best-Worst Scaling, also known as Maximum Difference. Data for such modeling is given by respondents presented with several items, and each respondent chooses the best alternative. Estimation of utilities is usually performed in a multinomial-logit modeling which produces utilities and choice probabilities. This article describes how to obtain probability estimation adjusted to possible absence of items in actual purchasing. We apply Markov chain modeling in the form of Chapman-Kolmogorov equations and its steady-state solution for stochastic matrix can be obtained analytically. An adjustment to choice probability with network effects is also considered. Numerical example by marketing research data is used. Copyright © 2019, IGI Global."
"10.4018/IJBAN.2019010102","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059760081&doi=10.4018%2fIJBAN.2019010102&partnerID=40&md5=e9233dc559d73e53be0e05632aa93338","This article presents a stochastic opinion dynamics model where (a) the opinion of each agent in a network is modeled as a probability distribution as against a point object, (b) consensus is defined as the stability region of the ensuing set of stochastic difference equations, and (c) compromise solutions can be derived between agents who don't have a consensus. The model is well suited for tracking opinion dynamics over large online systems such as Twitter and Yelp where opinions need to be extracted from the user-generated text data. Theoretical conditions for the existence of consensus and the impact that stubborn agents have on opinion dynamics are also presented. Copyright © 2019, IGI Global."
"10.4018/IJBAN.2019010104","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059742104&doi=10.4018%2fIJBAN.2019010104&partnerID=40&md5=658d17b2332a1847c03649ac9306a30d","This article has proposed a modified Kruskal's method to increase the efficiency of a genetic algorithm to determine the path of least distance starting from a central point to solve the open vehicle routing problem. In a vehicle routing problem, vehicles start from a central point and several customers placed in different locations to serve their demands and return to the central point. In the case of the open vehicle routing problem, the vehicles do not go back to the central point after serving the customers. The challenge is to reduce the number of vehicles used and the distance travelled simultaneously. The proposed method applies genetic algorithms to find the set of customers those are covered by a particular vehicle and the authors have applied the proposed modified Kruskal's method for local routing optimization. The results of the new method are analyzed in comparison with some of the evolutionary methods. Copyright © 2019, IGI Global."
"10.4018/IJBAN.2019010103","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059740091&doi=10.4018%2fIJBAN.2019010103&partnerID=40&md5=45cd26041a03ce01549b6528fafc829a","In wireless sensor networks, repairing partitions is of high priority. Various methods have been proposed for detecting partitions in the networks. One approach for reconnecting a partitioned network is to repair partitions using mobile nodes. For reconnecting the partitions approaches like transmission range adjustment and message ferry methods have been proposed but these are based on the degree of connectivity with neighbors. In the proposed method, we consider a partition detection system where the base station knows the position of the sensor nodes and the base station communicates with the nodes at regular intervals. The failure of the base station to communicate with a group of sensor nodes located together is the proof that some partitions have occurred. There could be two or more partitions occurring at a time and so multiple mobile nodes are to be employed. The aim of the algorithm is to coordinate among the mobile nodes and the partitioned network and to reconnect the partition. Here the safety of nodes, security of the network and scalability are considered. Copyright © 2019, IGI Global."
"10.1177/2399808317726332","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066038340&doi=10.1177%2f2399808317726332&partnerID=40&md5=2350c642a918d29864858ce06327e941","Location and accessibility are core concepts for land-value research. However, the perspective is still limited in their conceptual and methodological application to cities from the Global South. The objective of this research is to bridge concepts and definitions to comprehensively operationalize accessibility indicators and uncover its relation with residential land-values in Guatemala City. We developed a multivariate regression model using the following access metrics: (1) geographic-access indices that were computed using time-based analyses per transport mode; (2) geometric-access metrics estimated via Space Syntax at various spatial scales; (3) a proposed geometric via geographic-access metric computed as potential access to network centrality. A variable selection process allowed to assess the information contribution of each variable in building a parsimonious model. We assessed the model in the context of model variations that represent common approaches used in existing literature. Geographic access to the core business district has the highest impact on the land-values, followed by proximity to urban areas with high geometric-access, measured as geometric via geographic access. Geometric accessibility at neighbourhood and city-wide scales add spatialized information that contributes to a parsimonious model and reduces spatial dependence. The model yielded the highest goodness of fit and prediction accuracy compared with the model variations. We concluded that Guatemala City land-values follow a predominant monocentric structure. Additionally, potential access to vital urban areas as identified via Space Syntax denotes the presence of economic activities, or potential for such, which were not explicitly addressed through the geographic-access metrics. The results have limitations but pose methodological possibilities relevant for research and practice in similar Latin American cities. © The Author(s) 2017."
"10.1177/2399808317722168","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066036249&doi=10.1177%2f2399808317722168&partnerID=40&md5=e3c1ace17477beeca3a5d8b20fdcff28","The purpose of this paper is to investigate the effect of the urban densification policies made after the Islamic Revolution on the urban spatial structure of Tehran as the most important metropolis in Iran. The Hot Spot approach based on the Getis Ord Local G statistical test and Arc GIS 10.2 software was employed in this study. The advantage of the Geo-statistic technique used in this study is that this model does not require the exact location of the city centre to map and determine its spatial structure. The results show that the spatial structure of Tehran was affected by the non-spatial densification policies for 30 years (until Tehran’s Master Plan in 2007). Furthermore, these policies were greatly dependent on the financial benefits from the sale of the FAR permission and fines related to the ignorance of the lawful regulations. There is a spatial imbalance between the population and activity distribution patterns in the structure of Tehran. However, the negative spatial consequences of the densification policies are declining capacity of the city centre and the inner wards in retaining the population, and growing population density in the northern outer wards of Tehran. © The Author(s) 2017."
"10.1177/2399808317727004","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066031440&doi=10.1177%2f2399808317727004&partnerID=40&md5=00bc945c6b2468812e143fd1622b0d99","Although planning support systems are being more widely adopted by professional planners, there are very few examples of planning support system infrastructures designed to support planning practices on an ongoing basis. This paper reports the result of an exploratory qualitative study of the Southern California Association of Governments’ Scenario Planning Model, an innovative new planning support system infrastructure. Interviews with professionals who served as participants in a two-year development process were conducted to explore the six dimensions that theories from the planning support systems, innovation diffusion, and organizational information technology fields suggest are important to understanding the adoption and use of a planning support system infrastructure: user considerations, perceived benefits, technical details, the development process, jurisdiction characteristics, and planning style. Drawing on these interviews, the article proposes seven lessons for the creation of planning support system infrastructures: utilize participatory design, support a variety of planning practices, address indirect costs to users, encourage collaboration among multiple users within each organization, ensure that all stakeholders have appropriate access, be mindful of the framing of new technologies, and embrace their transformational potential. Although the Scenario Planning Model has benefited from California’s unique planning mandates, advances in web-based geospatial technologies mean that many regions may draw on these lessons to create similar planning support system infrastructures, which have the potential to improve local and regional planning practices through enhanced information, analysis, and communication. © The Author(s) 2017."
"10.1177/2399808319848760","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065652442&doi=10.1177%2f2399808319848760&partnerID=40&md5=7479373baefa7b08bf217abeac5b36d1","Cities are constantly evolving complex systems. Detection methods of land use distribution have to keep pace with their rapidly changing landscapes. While traditional analysis relies on surveys and census data typically refreshed at most yearly, the widespread use of mobile devices allows cell phone activity measurements to be used as sensors for the functional clustering of urban districts. These activity-based proprietary measurements are recently complemented by publicly available geosocial network records that even enable content-aware analysis. As a bridge between separate methods, in this work we analyze the relation of conversation content and functional spatial clusters of cities using a double dataset approach. We look at the differentiating power of the content of local conversations in activity-driven land use detection based on mobile phone records. In addition to intra-city analysis of three metropolises, we present a comparative study of London, New York City, and Los Angeles sharing the common language of English, but having very different cultural backgrounds. We show that the share of words with a similar temporal pattern to that of local mobile activities is different across cities, as well as between functional clusters. We find that the conversational content can effectively differentiate both functional clusters of a single city, and similar locations of the same function across many cities, like business areas that otherwise have a common temporal heartbeat. Moreover, we identify words related to activity types as the most important features emerging from the content-based, data-driven classification. © The Author(s) 2019."
"10.1177/2399808319843919","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065636667&doi=10.1177%2f2399808319843919&partnerID=40&md5=c46e0d895832d8890f6e16a33d11ae9b","This article is about an investigation exploring the vernacular environment of the villages adjacent to the rice fields of Mazandaran in order to identify the configuration and characteristic of its vernacular dwellings, and applied grammatical rules. In this regard, based on case study research, 4 villages and 44 houses within them have been investigated employing different methods including measurement, mapping, visual recording, one on one, and focus group interviews. Houses have been analysed based on the framework that shape grammars provided towards identification of compositional rules. Criteria and influential factors for creation of rules have been achieved through cultural and climatic studies that justify their appropriateness. This process has been successfully examined through plan creation of four different types of vernacular houses. The result shows that the set of rules contributed by this research can be efficient for generation of vernacular houses within the region of this study. © The Author(s) 2019."
"10.1177/2399808319846517","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065543950&doi=10.1177%2f2399808319846517&partnerID=40&md5=0ea394da44e5034c9e561ec6a1651dc3","In recent years, deep learning and computer vision have been applied to solve complex problems across many domains. In urban studies, these technologies have been instrumental in the development of smart cities and autonomous vehicles. However, a knowledge gap is present when it comes to informal urban regions in less developed countries. How can deep learning and artificial intelligence untangle the complexities of informality to advance urban modelling? In this paper, we introduce a framework for multipurpose realistic-dynamic urban modelling using deep convolutional neural networks. The purpose of the framework is twofold: (1) to sense and detect informality and slums in urban scenes from aerial and street-level images and (2) to detect pedestrian and transport modes. The model has been trained on images of urban scenes in cities across the globe. The framework shows strong validation performance in the identification of planned and unplanned regions, despite broad variations in the classified images. The algorithms of the URBAN-i model are coded in Python and the trained models can be applied to images of any urban setting, including informal settlements and slum regions. © The Author(s) 2019."
"10.1177/2399808319847204","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065543223&doi=10.1177%2f2399808319847204&partnerID=40&md5=26dede36f8bc4852ccaaee845d704ee6","The purpose of the paper is to investigate how a three-dimensional pedestrian network reshapes connectivity and helps to integrate the built environment of high-density cities. Using the case of Hong Kong, first, we elaborate how a continuous three-dimensional network constitutes an entirely different urban morphological spatial hierarchy compared to two-dimensional because of the footbridge system, underground connected with metro stations, and paths connected with mall developments. Second, we construct a three-dimensional pedestrian network model classifying segments into 23 categories with multi-height levels (e.g. sidewalk, footbridge, underground, crosswalk, ramp, paths on the building roof). Then we map the three-dimensional network for Hong Kong territory in a geographic information system, finding that the three-dimensional pedestrian network is 2.4 times in length and 8.5 times in link size greater than the road network. Connectivity comparison through a betweenness measure found striking differences between the two networks and indicated that footbridges and underground links could enhance walkability when they are well connected with the ground-level networks. Since road networks are widely used as a proxy for pedestrian analysis, we suggest that active travel optimisation planning, especially in high-density cities, requires a bespoke three-dimensional pedestrian model. The three-dimensional pedestrian network, enabling multi-level city living in a vertical metropolis, is a fundamental consideration in urban planning and design practices for high-density cities. © The Author(s) 2019."
"10.1177/2399808319843534","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065422355&doi=10.1177%2f2399808319843534&partnerID=40&md5=73c477ed8519251d4d17050552768efc","We propose an upgraded gravitational model which provides population counts beyond the binary (urban/non-urban) city simulations. Numerically studying the model output, we find that the radial population density gradients follow power-laws where the exponent is related to the preset gravity exponent γ. Similarly, the urban fraction decays exponentially, again determined by γ. The population density gradient can be related to radial fractality and it turns out that the typical exponents imply that cities are basically zero-dimensional. Increasing the gravity exponent leads to extreme compactness and the loss of radial symmetry. We study the shape of the major central cluster by means of another three fractal dimensions and find that overall its fractality is dominated by the size and the influence of γ is minor. The fundamental allometry, between population and area of the major central cluster, is related to the gravity exponent but restricted to the case of higher densities in large cities. We argue that cities are shaped by power-law proximity. We complement the numerical analysis by economics arguments employing travel costs as well as housing rent determined by supply and demand. Our work contributes to the understanding of gravitational effects, radial gradients, and urban morphology. The model allows to generate and investigate city structures under laboratory conditions. © The Author(s) 2019."
"10.1177/2399808319846904","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065416207&doi=10.1177%2f2399808319846904&partnerID=40&md5=2bf91baf37e95d16e2768ef803d0d540","Well established in criminological scholarship is the way that crime is neither spatially nor temporally uniformly distributed. Rather crime is distributed in a manner that means it is both particular places and particular times that are subject to the majority of crime events. Furthermore, we know that crime varies over the course of a day and week, with periods of time the same place can function as a crime generator, a crime attractor or as a crime detractor. What is less evident is the way in which some places ‘flip’ from a largely crime free space to one that provides the necessary preconditions for crime to occur. In this paper we focus on commercial precincts as the context, given their functional importance in our daily lives; providing places of employment, recreation and a locus for social encounters. By spatially integrating crime incident data and census information and employing circular, clustering and spline regression techniques we examine the intensity, tempo and timing of crime in these locales and generate a temporal typology for 2286 commercial precincts across Queensland, Australia. © The Author(s) 2019."
"10.1177/2399808319845006","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065387742&doi=10.1177%2f2399808319845006&partnerID=40&md5=9d35ee2a1d65843eac4c901c31cb0314","An increasing number of studies have examined neighbourhood built environment attributes associated with cycling. Some of them suggest non-linear relationships between built environment attributes and cycling. This study examined the strength and shape of associations of cycling for transport with objectively measured built environment attributes. Data were from 9146 Australian adults who took part in the 2009 South-East Queensland Travel Survey. Participants (aged 18–64 years) completed a 24-hour travel survey, in which they reported modes of travel. Residential density, Walk Score and a Space Syntax measure of street integration were calculated at a neighbourhood level using geographic information systems. Multilevel logistic regression analyses examined associations of bicycle use with each built environment attribute, which was modelled continuously and categorically. All continuous measures of the built environment attributes were associated with bicycle use. Each one-decile increment in residential density, Walk Score, and street integration was associated with 13%, 16%, and 10% higher odds of bicycle use, respectively. However, the associations appeared to be non-linear, with significant odds ratios observed only for the higher categories of each built environment attribute relative to the middle category. This study found that adults living in high-density neighbourhoods with more destinations nearby and well-connected streets were more likely to cycle for transport. However, medium-level density, access to destinations and street connectivity may not be enough to facilitate bicycle use. Further studies are needed to investigate urban design threshold values above which cycling can be promoted. © The Author(s) 2019."
"10.1177/2399808319846903","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065386823&doi=10.1177%2f2399808319846903&partnerID=40&md5=1b29b898890b08b32364755b9322d126","Studies of socio-spatial segregation in recent decades have shifted the focus from residential areas to people’s workplaces and other activity places. This study attempts to elucidate workplace segregation using cellular network data and to examine urban China with a special focus on the segregation of rural migrants, using residents living in the migrant enclaves of urban villages as a proxy. Furthermore, this study identifies factors that affect the variations of workplace segregation. The study shows that rural migrants who work in manufacturing industries and live in suburban areas suffer from higher workplace segregation from other social groups compared with those who work in service jobs and reside in the central-city areas, indicating that migrant enclaves in central-city areas play a significant role in housing rural migrants. It provides them with considerable access to service jobs and, thus, alleviates workplace segregation. Our results show that the use of big data can effectively capture the dynamics of population composition in activity places and provide a useful perspective for a deeper understanding of socio-spatial segregation. © The Author(s) 2019."
"10.1177/2399808319843928","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064914274&doi=10.1177%2f2399808319843928&partnerID=40&md5=8a591b76eb1e0ef90ae5ec2fdfdce8bf","Urban redevelopment is the reconstruction or upgrade of current urban built-up areas; it revitalizes old towns and contributes to sustainable development. This paper proposes a methodological framework that integrates open-source street networks and point-of-interest data and aims to identify and evaluate urban redevelopment at the block level from the perspective of urban form and function. It is found that (1) urban blocks can be categorized into eight groups regarding the spatial form of road junctions that have emerged within them over time, and blocks of each group share common features that can be automatically identified; (2) there are more blocks that have been morphologically redeveloped than functionally redeveloped, and the two types of redevelopments also significantly overlap with one another; and (3) the evaluation of urban redevelopment identification results presents a high accuracy rate that verifies the validity of the proposed framework. Based on the identification results, the impact factors of urban redevelopment are explored on both the inter- and intracity levels. The intercity analysis indicates that Chinese cities with a lower administrative level, lower urbanization rate, and higher density of road junctions tend to be associated with a higher proportion of urban redevelopment. Meanwhile, the intracity analysis attempts to determine which kinds of urban blocks are more likely to undergo urban redevelopment, which are found to be the blocks with lower points of interest density, a smaller distance to city centers, higher transit accessibility, a higher land use mixed index, and larger size. © The Author(s) 2019."
"10.1177/2399808319841615","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064813373&doi=10.1177%2f2399808319841615&partnerID=40&md5=5b55590bf974628bb87de6ff7d03b18a","Measuring the size of a crowd in a specific location can be of crucial importance for crowd management, in particular in emergency situations. Here, using two football stadiums as case studies, we present evidence that data generated through interactions with the social media platform Instagram can be used to generate estimates of the size of a crowd. We present a detailed analysis of the impact of varying the time period and spatial area considered for the collection of Instagram data. Crucially, we demonstrate how to address issues that arise from changes in the usage of a social media platform such as Instagram. Our findings show how social media datasets carrying location-based information may help provide near to real-time measurements of the size of a crowd. © The Author(s) 2019."
"10.1177/2399808319842181","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064809659&doi=10.1177%2f2399808319842181&partnerID=40&md5=bdf637e2fd93ea195afa8ffcbf791f69","Unlike their nonspatial counterparts, spatial multi-sided platforms are matchmaking platforms with an additional layer of complexity: their customers expect to meet in space, not only virtually. This additional challenge will be studied in this paper in the context of a two-sided ride-sharing platform, which serves drivers and passengers. As with any two-sided platform, there is an interdependence between both groups of customers: More drivers are more attractive for passengers, and vice versa. This interdependence creates the old chicken-and-egg problem, only that here drivers and passengers need to be matched not for a virtual transaction, but by their ability to meet physically and travel jointly. We argue, and illustrate by simulations, that in spatial multi-sided markets there is not a single critical mass frontier that needs to be reached in order to make the system self-sustained (as in nonspatial markets), and that this frontier is varying from one location to the next, depending on the density and distribution of the demand and supply over space and time. Identification of the critical mass frontier will allow for better evaluation of implementation policies and regulations. © The Author(s) 2019."
"10.1177/2399808319841910","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064270546&doi=10.1177%2f2399808319841910&partnerID=40&md5=1745cfff0f786a212ff01086e7217943","Customer profiles that include gender and age information are important to businesses and can be used to promote sales and provide personalized services. This information is gathered in e-commerce by analyzing customer visit records in virtual web space. However, such practice is difficult in brick-and-mortar businesses because the data that can be utilized to infer customer profiles are limited in physical spaces. In this paper, we attempt to infer the gender and age of customers using indoor positioning data generated by the Wi-Fi engine. To achieve this, we first construct a synthesized features vector to distinguish different profiles. This vector contains both customer spatial–temporal mobility characteristics and interest preferences. A hidden Markov model group detection method is then applied to detect customers who shop together because they usually show the same shopping behavior and it is difficult to distinguish their profiles. Finally, a random forest inference model is proposed to infer profiles of customers who shop alone. The indoor positioning data collected in the Longhu Tianjie Plaza in Chongqing were used as a case study. The result shows that customer profiles are indeed inferable from indoor positioning data. The accuracy of the gender inference model reaches 73.9%, while that of the age inference model is 67.9%. This demonstrates the potential value of new “big data” for promoting precision marketing and customer management in brick-and-mortar businesses. © The Author(s) 2019."
"10.1177/2399808319840666","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064048462&doi=10.1177%2f2399808319840666&partnerID=40&md5=343c55764d7916c5f201de4cb8233b89","Early attempts to classify shopping activity often took a relatively simple approach, largely driven by the lack of reliable data beyond fascia name and retail outlet counts by centre. There seems to be a consensus amongst contemporary scholars, commercial research consultancies and retailers that more comprehensive classifications would generate better-informed debate on changes in the urban economic landscape, as well as providing the basis for a more effective comparison of retail centres across time and space, particularly given the availability of new data sources and techniques and in the context of the transformational changes presently affecting the retail sector. This paper seeks to demonstrate the interrelationship between supply and demand for retailing services by integrating newly available data sources within a rigorously specified classification methodology. This in turn provides new insight into the multidimensional and dynamic taxonomy of consumption spaces within Great Britain. First, such a contribution is significant in that it moves debate within the literature past simple linear scaling of retail centre function to a more nuanced understanding of multiple functional forms; and second, in that it provides a nationally comparative and dynamic framework through which the evolution of retail structures can be evaluated. Using non-hierarchical clustering techniques, the results are presented in the form of a two-tier classification with 5 distinctive ‘coarse’ clusters and 15 more detailed and nested sub-clusters. The paper concludes that more nuanced and dynamic classifications of this kind can help deliver more effective insights into changing role of retailing and consumer services in urban areas across space and through time and will have implications for a variety of stakeholders. © The Author(s) 2019."
"10.1177/2399808319832305","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062997647&doi=10.1177%2f2399808319832305&partnerID=40&md5=785643b09012f223f6ce625432c92ccf","The built environment plays an important role in shaping physical activities and furthering a healthy lifestyle. An obesogenic environment, which promotes obesity through uninviting neighborhood design, can cause sedentary living and environmentally induced inactivity, particularly by reducing walking. Existing measures of walkability consider the distribution and features of potential destinations, but fail to account for key aspects of the built environment design, pedestrian preferences, or various reasons for walking. In this paper, we propose a new assessment method, the Perceived importance and Objective measurement of Walkability in the built Environment Rating (POWER), incorporating pedestrian preferences derived from a walking preference survey. By conducting a university campus-based case study, we developed a quantitative method, the customized analytic hierarchy process, to analyze data from the survey. The customized analytic hierarchy process was used to calculate the perceived importance of various factors. In addition, objective measurements were collected and processed from Geographic Information System (GIS) data and fieldwork. Using both perceived importance and objective measurements, we created a POWER map showing the most walkable and unwalkable places on campus. The outcome of this case study reveals the nuanced spatial variations with regard to walkability. The proposed integral measure creates an improved method for rating the walkability of the built environment. Future applications and limitations are also discussed. © The Author(s) 2019."
"10.1177/2399808319834309","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062724945&doi=10.1177%2f2399808319834309&partnerID=40&md5=0b18f251367fc715915ff2b4b31a79e0","Travel demand models are important tools used in the analysis of transportation plans, projects, and policies. Defining the level of detail (i.e. the number of roads) of the transport network in consistency with the travel demand model’s zone system is crucial for the accuracy of model results. All analyses found in the literature show that the spatial resolution of the transport network has great impact on the model results. If the zone system is fairly coarse, a network as detailed as the real road system will underestimate traffic congestion. Conversely, a network that is too coarse for a given zone system will over-represent congestion. However, there are no tools to determine how much detail is needed in a transport network to be compatible with a given zone system. This paper seeks to fill this knowledge gap by (1) providing a new methodology and algorithm to define an appropriate level of detail for a transport network under a given zone system, and (2) implementing this methodology for the Baltimore area. The results suggest that the transport network and the travel demand model’s zone system need to have a consistent level of resolution to provide reasonable model sensitivities. The evaluation of a new transportation project points out the importance of having an appropriate level of network detail for making reasonable planning decisions. © The Author(s) 2019."
"10.1177/2399808319830971","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062476425&doi=10.1177%2f2399808319830971&partnerID=40&md5=9aba12c32cfe7417aeded9c29cd6a95a","The loss of accuracy in vector-raster conversion has always been an issue for land use change models, particularly for raster based Cellular Automata models. Here we describe a vector-based cellular automata (CA) model that uses land parcels as the basic unit of analysis, and compare its results with a raster CA model. Transition rules are calibrated using an artificial neural network (ANN) and historical land use data. Using Ipswich City in Queensland, Australia as the study area, the simulation results show that the vector and raster CA models achieve 96.64% and 93.88% producer’s spatial accuracy, respectively. In addition, the vector CA model achieves a higher kappa coefficient and more consistent frequency of misclassification, while also having faster processing times. Consequently, the vector-based CA model can be applied to explore regulations of land use transformation in urban growth process, and provide a better understanding of likely urban growth to inform city planners. © The Author(s) 2019."
"10.1177/2399808319833377","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062461187&doi=10.1177%2f2399808319833377&partnerID=40&md5=b216e844d6f2bad38c5a7649c81e4dbb","Our study aims to explore whether simultaneous municipal autonomy and institutional fragmentation foster urban sprawl. We discuss the hypothesis that in such a setting, competition between municipalities to attract development leads to high degrees of urban sprawl. A comparison of the 26 Swiss cantons was carried out using fuzzy-set qualitative comparative analysis combining the aforementioned conditions with other urban sprawl drivers. As a result, four consistent, sufficient combinations of conditions were identified. These combinations show that municipal autonomy and institutional fragmentation are indeed linked to high degrees of urban sprawl. This is especially true when municipal autonomy and institutional fragmentation are combined with the economic triggers of competition between municipalities and oversized designated building zones. These results draw attention to the necessity of stronger supra-municipal coordination within the framework of Swiss land use planning policy. © The Author(s) 2019."
"10.1177/2399808319831290","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062335779&doi=10.1177%2f2399808319831290&partnerID=40&md5=fc362157a0d1653a12fa97db6ae512ce","Enhancing the role of citizens in shaping places has been a longstanding objective for governments, communities and the academy. Although a range of techniques has been developed by the state to give people an opportunity to get involved, these methods often struggle to create a meaningful way to communicate aspirations for places on citizens' terms. In this paper, we document the design, deployments and evaluation of a new technological device that enabled participants to share place views and aspirations beyond more traditional government engagement methods. The device, called JigsAudio, is an open-source device fabricated by the authors that encourages people to express themselves creatively through drawing and talking. The research contributes to our understanding of how accessible and free technologies can reduce barriers to participation, whilst encouraging creativity and expression when talking about the future of places. It goes on to discuss the potential of devices such as JigsAudio conceptually and practically within urban and regional change, and considers the balance that needs to be struck between utilising smart technology whilst creating accessible and meaningful opportunities that inspire citizens. © The Author(s) 2019."
"10.1177/2399808319830403","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061932463&doi=10.1177%2f2399808319830403&partnerID=40&md5=069153cafe372737c0eb66c7ef70ad91","Statistical analyses of urban environments have been recently improved through publicly available high resolution data and mapping technologies that have been adopted across industries. These technologies allow us to create metrics to empirically investigate urban design principles of the past half-century. Philadelphia is an interesting case study for this work, with its rapid urban development and population increase in the last decade. We outline a data analysis pipeline for exploring the association between safety and local neighborhood features such as population, economic health, and the built environment. As a particular example of our analysis pipeline, we focus on quantitative measures of the built environment that serve as proxies for vibrancy: the amount of human activity in a local area. Historically, vibrancy has been very challenging to measure empirically. Measures based on land use zoning are not an adequate description of local vibrancy and so we construct a database and set of measures of business activity in each neighborhood. We employ several matching analyses to explore the relationship between neighborhood vibrancy and safety, such as comparing high crime versus low crime locations within the same neighborhood. We find that neighborhoods with more vacancy are associated with higher crime but within neighborhoods, crimes tend not to be located near vacant properties. We also find that longer term residential ownership in a local area is associated with lower levels of crime. In addition, we find that more crimes tend to occur near business locations but businesses that are active (open) for longer periods are associated with fewer crimes. As additional sources of urban data become available, our analysis pipeline can serve as the template for further investigations into the relationships between safety, economic factors, and the built environment at the local neighborhood level. © The Author(s) 2019."
"10.1177/2399808319828730","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061672520&doi=10.1177%2f2399808319828730&partnerID=40&md5=669f0066de8acf2852d0408d6f123819","Although people may recognize urban vibrancy when they see or sense it, developing direct and comprehensive measures of urban vibrancy remains a challenge. In the context of intense global competition, there is an increased realization that urban vibrancy is vital to the social and economic sustainability of cities. Such vibrancy may be significantly shaped by the urban built environment, yet we know little about the close connections between vibrancy and urban built environments. Empowered by newly available sources of spatial big data, which provide enormous amounts of information on both human dynamics and the built environment, this paper proposes a framework for evaluating and characterizing urban vibrancy. Thus far, vibrancy measures have mostly used single-source data that hardly reflect the multifaceted manifestations of urban vibrancy. Therefore, we propose a more comprehensive measure of urban vibrancy, extracted as the common latent factor from multiple surface attributes. Using the proposed framework, we evaluated and mapped the spatial dynamics of vibrancy in Shanghai, a typical large city in post-reform China, and investigated the associations between vibrancy and various urban built environment indicators. The evidence shows that the horizontal built-up density, rather than vertical height, is the leading generator of vibrancy in Shanghai, followed by the density and mixture of urban functions, accessibility, and walkability. In this vein, we contribute to current debates and future planning practices regarding vibrant spaces in large cities. This proposed evaluation framework, equipped with spatial big data, can benefit future urban studies. © The Author(s) 2019."
"10.1177/2399808319828728","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061582858&doi=10.1177%2f2399808319828728&partnerID=40&md5=8a3738fab29ac9621dbefe1668b3f843","Is higher-quality land developed earlier? To answer this question, the paper applies comparative static analysis to the Arnott–Lewis model of the transition of land from agricultural to urban use. It is shown that (i) an increase in agricultural fertility increases structural density and delays development; (ii) a decrease in land preparation cost reduces structural density and hastens development; and (iii) both an increase in amenities and a decrease in structure construction costs normally hasten but in anomalous cases can delay development. © The Author(s) 2019."
"10.1177/2399808319828374","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061581760&doi=10.1177%2f2399808319828374&partnerID=40&md5=53713c0526278d28b9553400387d6f34","Newly emerging relationships between form and function reveal the increasingly complex nature of metropolitan regions. The present study investigates spatial diversification in settlement forms and socioeconomic functions in metropolitan Attica (the administrative region including Athens, the capital of Greece), with the aim of implementing a holistic framework assessing urban complexity in contemporary cities. Taken as key components of urban complexity, morphological and functional diversity have been analysed using multi-domain indicators that describe settlement characteristics (land-use, soil sealing, building use, vertical profile of buildings, building age, construction materials) and socioeconomic functions (economic base, working classes, education levels, population age structure, composition of non-native population by citizenship, distribution of personal incomes), thus providing a comprehensive description of local-scale diversification in urban structures. A correlation analysis was used to verify the spatial coherency between individual dimensions of urban diversification. Analysis of global Moran’s spatial autocorrelation index reveals specific gradients of urban diversification that discriminate morphological attributes from socioeconomic functions. Municipalities were profiled on the basis of Pielou’s evenness indexes for each urban dimension: a factor analysis indicates latent patterns characterizing areas with high and low diversification in metropolitan functions. Urban and rural municipalities were, respectively, characterized as the most and least diversified in the study area, with peri-urban municipalities ranking in-between, evidencing a diversification gradient correlated with the distance from downtown Athens. A multidimensional analysis of the most relevant dimensions of metropolitan complexity has proved to be a promising tool for monitoring urban gradients, polycentric development and (latent) socioeconomic transformations in contemporary cities. © The Author(s) 2019."
"10.1177/2399808319827015","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061564553&doi=10.1177%2f2399808319827015&partnerID=40&md5=177ea3d509af4fe9f72823a8a89dd75d","In architecture, city planning, visual arts, and other design areas, shapes are often made with points, or with structural representations based on point-sets. Shapes made with points can be understood more generally as finite arrangements formed with elements (i.e. points) of the algebra of shapes U i , for i = 0. This paper examines the kind of topology that is applicable to such shapes. From a mathematical standpoint, any “shape made with points” is equivalent to a finite space, so that topology on a shape made with points is no different than topology on a finite space: the study of topological structure naturally coincides with the study of preorder relations on the points of the shape. After establishing this fact, some connections between the topology of shapes made with points and the topology of “point-free” pictorial shapes (when i &gt; 0) are defined and the main differences between the two are summarized. © The Author(s) 2019."
"10.1177/2399808318825273","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060912927&doi=10.1177%2f2399808318825273&partnerID=40&md5=de3d43be2e9c4d140c30ac7a1aa7b0c6","The Victorian Government introduced the Better Apartment Design Guidelines in 2017. The introduction of these new regulations is a response to growing criticism over the quality of the large number of apartments recently constructed in Melbourne. This concern is shared in other Australian cities, but until now Victorian planning regulations have been the least prescriptive and most permissive in terms of apartment design parameters of any Australian jurisdiction. Reflecting on these concerns raises several questions in terms of the effectiveness of regulating for quality. Does regulating design in apartments improve quality or stifle innovation? Can the effect be measured, given the large number of exogenous factors involved in apartment production, and what might this tell us about the nature of ‘good-design’ and ‘quality-in-apartments’? This study explores the way in which different development control systems regulating apartment design impact the quality of internal apartment design. The two systems chosen, operating in Victoria and New South Wales, have been considered per Booth’s framework of regulatory and discretionary development control systems with the previous Victorian system seen as discretionary and the New South Wales approach a mix of regulatory-discretionary controls. Ten planning applications for high-rise residential developments were selected from Melbourne and Sydney. These were analysed against a set of good design principles defined by reviewing relevant literature and existing regulations. The results of the paper suggest the intuition of the Victorian Government that some form of intervention in the market is warranted to safeguard quality is likely to be correct. © The Author(s) 2019."
"10.1177/2399808318824108","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060768945&doi=10.1177%2f2399808318824108&partnerID=40&md5=833bf7671ed9ee7b2d18f7a5d9c7ee63","Feeder buses provide a small but important part of the public transport system by carrying people between residential areas and transport interchanges. A feeder bus to a train station planned in advance will attract new residents of a housing development to use the bus. The bus route can influence the location choice of a buyer concerned about access for children, the elderly or anyone not wishing to drive a car. Our bus route modelling starts with the bus stops – not the route – to be reached from each dwelling by the shortest possible walk. In demand terms, people locating close to bus stops are more likely to use the service than those choosing more distant locations, and the nearby residences have higher values. The stops-first application determines a feeder bus route to enhance an irregular residential plan covering an area of one square kilometre. The planned road and housing lot locations provide the data for calculating the access measure from each dwelling to each potential bus stop, the closest stop being used. A genetic algorithm tests potential bus stops to find demand maximizing locations, the propensity to use the bus being formulated as an exponential (increasing elasticity) function of walking distance. Then a ‘travelling salesman’ genetic algorithm finds the shortest route linking the stops, so that an efficient circuit route is generated for each alternative number of bus stops, ranging from 7 to 11. More stops not only give better access but also increase the route length, so that total accessibility must be assessed against route length. The distribution of walking distances shows most between 150 and 240 metres, with none more than 400 metres. The results indicate that planning policy should require prior design of a bus route to achieve good walking accessibility, so that residents become accustomed to the convenience of using the bus. This study shows that, at the planning stage, estimating a bi-objective model giving a Pareto front between accessibility and route length can reveal a policy compromise that shortens the route with little reduction in expected patronage. © The Author(s) 2019."
"10.1177/2399808317696072","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059912118&doi=10.1177%2f2399808317696072&partnerID=40&md5=387401a20e5deeffa7ee35d134f5c03b","The paper examines the growth of Chinese cities at prefecture level or above by first applying a non-parametric method. Kernel regression of the mean of growth rate conditional on city size reveals a U-shaped relationship between city growth and size, and rejects Gibrat’s law. That is to say that large cities take the form of divergent growth while small cities are convergent to each other. This U-shaped growth–size relationship holds for the registered (hukou) population in 1989–2012 as well as for the permanent population in 1999–2012. Furthermore, our results show that the growth of large cities becomes more divergent using the permanent population than using the hukou population, whereas the growth of small cities becomes less convergent. The permanent population counts a portion of floating population, so it is then concluded that rural–urban migrants move to large cities disproportionately, making large cities grow faster than small cities. Estimated results from rank–size OLS regression confirm the divergent growth of large cities, and, at the same time, reject the notion of random growth of Chinese cities (which is also supported by panel root tests). Our findings have profound policy implications. The national strategy of urbanization that stresses the growth control of mega and super-big cities has had no effect in the past and may continue to be ineffective in shaping the urbanization trajectory in China in the next couple of decades. Sustainable urbanization will depend largely on whether and how well big Chinese cities prepare themselves in accommodating fast growth. © The Author(s) 2017."
"10.1177/2399808317705880","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059911159&doi=10.1177%2f2399808317705880&partnerID=40&md5=9fedf569813380802544f2f19585d234","Air quality is affected by the interplay between emission sources and urban planning factors such as land use, built environment, development pattern, and transportation. Few empirical studies have been conducted to determine the influence of urban form characteristics on air quality in Korea. Thus, the purpose of this research is to examine the relationship between urban form and air pollution, focusing on ozone pollution in Korea. The characteristics of urban form include density, concentration, clustering, and land use mix. In this study, those characteristics were measured by population density, the Theil index, Moran’s I index, G-statistic values, and an entropy index using statistical methods and a geographic information system. We employed a spatial regression model to consider the spatial effects of ozone concentrations. We found that the degree of urban land use mix, clustering, and concentration of development are significantly associated with better air quality by using a spatial lag model, which was found to be the best fit for the data used in this study. However, an increase in population density was found to be associated with exacerbated ozone concentrations. Communities with higher daily temperatures, a large number of cars, and polluting facilities exhibited poor air quality, while those with a larger percentage of residential land use tended to have lower ozone pollution. These findings suggest that, to properly address concerns over air quality, mixed-land use and compact urban form need to be more seriously considered in sustainable urban planning. © The Author(s) 2017."
"10.1177/2399808317703381","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059910648&doi=10.1177%2f2399808317703381&partnerID=40&md5=a613eda88f94bf3ac1262f4da4efcc23","Residential relocation decision making is a complicated process, and modelling this complex course of actions requires careful scrutinisation of different aspects. The relocation decision comprises several different decisions, including the reason for the relocation, relocation timing, and attributes of the desired residence. Among these decisions needing to be taken, the reason for relocation and its timing are decided earlier than others. Depending on the variant reasons and motivations for relocating, its timing may be accelerated or decelerated. Relocation usually occurs because of a multiplicity of reasons, which necessitates using a multivariate model for relocation decision making that is jointly modelled with the timing decision. A competing accelerated failure model to jointly formulate these decisions. The housing search literature emphasizes on the importance of considering financial risk acceptance level of decision makers in residential relocation decision models. Therefore, a binary logit model is used to model whether the decision maker is financially risk averse or not. This paper used longitudinal data collected in Australia from the Household, Income, and Labour Dynamics in Australia Survey. Further, the impact of group decision making on residential relocation is captured in this paper through the information provided in Household, Income, and Labour Dynamics in Australia Survey regarding the manner in which decisions are made within households. © The Author(s) 2017."
"10.1177/2399808317705658","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059889121&doi=10.1177%2f2399808317705658&partnerID=40&md5=effd85538c4e18afe079c9be7ee4d742","This research develops an activity-based integrated land use/transport interaction model based on the concepts – activities (mainly, households and employment activities), activity location and relocation for Chinese regions. It consists of a residential and employment location sub-model, a transport sub-model and an implicit real estate rent adjustment sub-model. The model is developed to model the urban activity distribution evolution, predict urban spatial development trends and examine various planning decision implications. It spatially distributes household and employment activity change of a study area by zone based on the current activity distribution, land use policies and the accessibilities of the zones. The model is subsequently calibrated to predict the distribution of households and employment activities in Beijing metropolitan area in 2025. Model results show that the resident and employment densities are still high in central Beijing in 2025, and most zones’ resident densities are higher than their employment densities. However, there is also significant population density increase along the 6th ring road, indicating the relocation trend of the residents and businesses to the outskirts. This is consistent with the government objectives to decentralize activities within the central urban area. The paper also suggests that the model should be used mainly in examining the possible differences arising from the adoption of different policies though predicting future of a city distribution proves feasible. © The Author(s) 2017."
"10.1177/2399808317690157","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053872531&doi=10.1177%2f2399808317690157&partnerID=40&md5=9a3a410a8d27b5d76d44ad4a847260a9","Integrating a diverse set of land use types within a neighborhood is a central tenet of smart growth policy. Over a generation of urban planning research has heralded the transportation, land use, and public health benefits arising from a balanced supply of local land uses, including the improved feasibility for pedestrian travel. However, land use mixing has largely remained a transportation-land use planning goal without a conceptually valid set of environmental indicators quantifying this multifaceted spatial phenomenon. In this study, we incorporated activity-based transportation planning and landscape ecology theory within a confirmatory factor analysis framework to introduce a land use mix construct indicative of the paired landscape pattern aspects of composition and configuration. We found that our activity-related land use mix measure, and not the commonly adopted entropy-based index, predicted walk mode choice and home-based walk trip frequency when operationalized at three geographic scales. © The Author(s) 2017."
"10.1177/2399808317725318","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041863697&doi=10.1177%2f2399808317725318&partnerID=40&md5=eaaf9f373c5fd000b80b9f877772da75","This paper explores how an interpretive case-based research strategy can reveal new empirical and theoretical insights into microclimate design. Innovative fieldwork in Christchurch, New Zealand investigated the nature and social meanings of urban comfort in a city with a seasonal climate featuring microclimatic variability, and with a physical landscape undergoing rapid change following a series of major earthquakes. Ethnographic methods were combined with microclimate measurements in four Christchurch-based case study locations to identify ways in which people adjust their cultural and lifestyle values and expectations to the actual microclimatic conditions. The field investigation had to capture data relevant to the microclimatic variability and be suitable for rapidly changing urban settings. Results suggest this integrative methodology successfully adapts to challenging physical contexts, and is able to provide a coherent body of evidence. Important insights revealed through this methodology may not have become apparent if only conventional microclimate methods were used. © The Author(s) 2017."
"10.1177/2399808317724444","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041830167&doi=10.1177%2f2399808317724444&partnerID=40&md5=7499dfd8bfed93639a3401271d1b0db1","A massive amount of information as geo-referenced data is now emerging from the digitization of contemporary cities. Urban streets networks are characterized by a fairly uniform degree distribution and a low degree range. Therefore, the analysis of the graph constructed from the topology of the urban layout does not provide significant information when studying topology-based centrality. On the other hand, we have collected geo-located data about the use of various buildings and facilities within the city. This does provide a rich source of information about the importance of various areas. Despite this, we still need to consider the influence of topology, as this determines the interaction between different areas. In this paper, we propose a new model of centrality for urban networks based on the concept of Eigenvector Centrality for urban street networks which incorporates information from both topology and data residing on the nodes. So, the centrality proposed is able to measure the influence of two factors, the topology of the network and the geo-referenced data extracted from the network and associated to the nodes. We detail how to compute the centrality measure and provide the rational behind it. Some numerical examples with small networks are performed to analyse the characteristics of the model. Finally, a detailed example of a real urban street network is discussed, taking a real set of data obtained from a fieldwork, regarding the commercial activity developed in the city. © The Author(s) 2017."
"10.1177/2399808317702148","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041616706&doi=10.1177%2f2399808317702148&partnerID=40&md5=fd801ab982d8a8f8055eb5531e18747e","To measure job accessibility, person-based approaches have the advantage to capture all accessibility components: land use, transportation system, individual’s mobility and travel preference, as well as individual’s space and time constraints. This makes person-based approaches more favorable than traditional aggregated approaches in recent years. However, person-based accessibility measures require detailed individual trip data which are very difficult and expensive to acquire, especially at large scales. In addition, traveling by public transportation is a highly time sensitive activity, which can hardly be handled by traditional accessibility measures. This paper presents an agent-based model for simulating individual work trips in hoping to provide an alternative or supplementary solution to person-based accessibility study. In the model, population is simulated as three levels of agents: census tracts, households, and individual workers. And job opportunities (businesses) are simulated as employer agents. Census tract agents have the ability to generate household and worker agents based on their demographic profiles and a road network. Worker agents are the most active agents that can search jobs and find the best paths for commuting. Employer agents can estimate the number of transit-dependent employees, hire workers, and update vacancies. A case study is conducted in the Milwaukee metropolitan area in Wisconsin. Several person-based accessibility measures are computed based on simulated trips, which disclose low accessibility inner city neighborhoods well covered by a transit network. © The Author(s) 2017."
"10.1177/2399808317724445","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041588524&doi=10.1177%2f2399808317724445&partnerID=40&md5=110ea02815813affea834dbb9fd506a8","The process of urbanization is one of the most important phenomenon of our societies and it is only recently that the availability of massive amounts of geolocalized historical data allows us to address quantitatively some of its features. Here, we discuss how the number of buildings evolves with population and we show on different datasets (Chicago, 1930–2010; London, 1900–2015; New York City, 1790–2013; Paris, 1861–2011) that this ‘fundamental diagram’ evolves in a possibly universal way with three distinct phases. After an initial pre-urbanization phase, the first phase is a rapid growth of the number of buildings versus population. In a second regime, where residences are converted into another use (such as offices or stores for example), the population decreases while the number of buildings stays approximately constant. In another subsequent phase, the number of buildings and the population grow again and correspond to a re-densification of cities. We propose a stochastic model based on these simple mechanisms to reproduce the first two regimes and show that it is in excellent agreement with empirical observations. These results bring evidences for the possibility of constructing a minimal model that could serve as a tool for understanding quantitatively urbanization and the future evolution of cities. © The Author(s) 2017."
"10.1177/2399808317702897","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041582841&doi=10.1177%2f2399808317702897&partnerID=40&md5=8c16ff1827311a01418e4eb8a31fba9a","Block restructuring has been strongly emphasized in Japan for renovating cities. However, little is known about the relation between block size and building shape. Moreover, the shape of buildings designed on a block after restructuring is unclear. In this study, the relation between block size and building shape is analyzed quantitatively, and a three-dimensional building shape is estimated by a model using an urban planning GIS data set of Tokyo. Results show the quantitative relation between block size and building shape, and the building shape image on the blocks. Higher buildings and buildings with a basement tend to be built in larger blocks, leading to efficient use of the maximum volume permitted in the block. In addition, the region composed by larger blocks can be spacious, because the range of building setback will be long in larger blocks. Designation of a high floor area ratio may induce integration and enlargement of blocks. Blocks are less likely to be partitioned in zones when a high floor area ratio is designated. © The Author(s) 2017."
"10.1177/2399808317703983","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041571921&doi=10.1177%2f2399808317703983&partnerID=40&md5=811f9233eb44939817f4beeeaf59e032","The negative impacts resulting from urban sprawl are recognized as serious issues entailing environmental problems. Urban developments are moving towards a more compact form to mitigate many issues including pollution concerns, land depletion, and population growth demands. Urban compactness has been reported to be a more sustainable form of development that occurs through densification and mixed land use practices through spatial indicators that intensify the landscape. Urban modeling has been used extensively to aid in urban and regional planning as it can forecast possible scenarios of urban growth. The objective of this research is to develop and implement a spatial index for three-dimensional (3D) urban compactness to evaluate potential vertical development growth. The spatial index has two components, local and regional, and it is derived based on parameters accounting for a vertical urban growth suitability analysis, land designation, and average building height. Datasets used for this study were for the Metro Vancouver Region, Canada, a rapidly developing area with plans in place for sustainability and compact growth. The spatial index was derived for the study area for the year 2011 and projected to the year 2041 with a 10-year time interval, accounting for the spatio-temporal land use change. Results indicate concentrations of urban compactness growth near densely populated and transportation-oriented locations and also capture urban leap-frogging processes. The presented research aims to aid local governments in future planning processes related to regional sustainable development growth. © The Author(s) 2017."
"10.1177/2399808317723012","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041566320&doi=10.1177%2f2399808317723012&partnerID=40&md5=c469f5d76524280c2ce4920d1e69e607","Existing literature has examined the determinants of subjective well-being in China from the social, economic and psychological perspectives. Very few studies explore the impacts of residential environment on subjective well-being. Drawing on a large scale questionnaire survey in Beijing, this paper investigates the role of residential environment by decomposing the variations of subjective well-being at fine-grained spatial scales, i.e. district and neighbourhood levels. A bivariate response binomial multilevel model is employed to assess the relative importance of geographical contexts and individual characteristics, in particular, the household registration (hukou) status, in influencing subjective well-being. The results show significant heterogeneities in subjective well-being among districts and neighbourhoods. Neighbourhood types are significantly correlated with subjective well-being, with residents in commercial housing neighbourhoods reporting higher levels of subjective well-being than those in work-unit and affordable housing neighbourhoods. However, the impacts of neighbourhood types are not uniformly experienced by people with different hukou status. Migrants tend to express lower. © The Author(s) 2017."
"10.1177/2399808317725075","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041542568&doi=10.1177%2f2399808317725075&partnerID=40&md5=d5e1fc78b6996ff28d3f44eabd80bdae","The modern discipline of urban morphology gives us a ground for the comparative analysis of cities, which increasingly includes specific quantitative elements. In this paper, we make a further step forward towards the definition of a general method for the classification of urban form. We draw from morphometrics and taxonomy in life sciences to propose such method, which we name ‘urban morphometrics’. We then test it on a unit of the urban landscape named ‘Sanctuary Area’ (SA), explored in 45 cities whose origins span four historic time periods: HISTORIC (medieval), INDUSTRIAL (19th century), NEW TOWNS (post-WWII, high-rise) and SPRAWL (post-WWII, low-rise). We describe each SA through 207 physical dimensions and then use these to discover features that discriminate them among the four temporal groups. Nine dimensions emerge as sufficient to correctly classify 90% of the urban settings by their historic origins. These nine attributes largely identify an area’s ‘visible identity’ as reflected by three characteristics: (1) block perimeterness, or the way buildings define the street-edge; (2) building coverage, or the way buildings cover the land and (3) regular plot coverage, or the extent to which blocks are made of plots that have main access from a street. Hierarchical cluster analysis utilising only the nine key variables nearly perfectly clusters each SA according to its historic origin; moreover, the resulting dendrogram shows, just after WWII, the first ‘bifurcation’ of urban history, with the emergence of the modern city as a new ‘species’ of urban form. With ‘urban morphometrics’ we hope to extend urban morphological research and contribute to understanding the way cities evolve. © The Author(s) 2017."
"10.1177/2399808317721184","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041529025&doi=10.1177%2f2399808317721184&partnerID=40&md5=a8c2a6177e6b941a65632f2794637d36","Attaining a mixture and diversity of land use within walkable neighborhoods is an essential principle within contemporary urban planning and design. Empirical studies by New Urbanists argue that mixed land use, neo-traditional, and walkable neighborhoods yield socioeconomic benefits and generate a substantial premium in residential property prices. However, few studies apply reliable metrics to capture the connections among urban form, the spatial distribution of land use, and travel behavior and then value their combined effects on housing prices. To bridge this gap, this study calculates the metrics of spatial accessibility and centrality, combining street nodes, networks, and built density by land use type within walkable neighborhoods. We investigate empirically the extent to which residents value spatial accessibility and centrality to residential, commercial, office, and industrial space regarding housing prices in Seoul, South Korea in 2010. The multilevel hedonic price models used suggest that residents highly value urban settings that access larger volumes of commercial and residential buildings in densely spaced areas along dense street networks. However, homeowners respond negatively to higher access to industrial property and weakly to office space. This analysis identifies the value of spatial access to heterogeneous land-use density in housing prices and provides policy implications for land use, transportation, and urban design. © The Author(s) 2017."
"10.1177/2399808317700140","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041514908&doi=10.1177%2f2399808317700140&partnerID=40&md5=d66ac4c49efca55f537eba0a608fc73d","The conceptual and methodological debate on urban form has grown in the last decades to recognize that social, economic, demographic and political processes can contribute to the development of new urban forms, especially those related to urban sprawl, as well as to find alternative methodologies for measuring them. Spatial metrics derived from landscape ecology arise as principal indicators to measure urban form. This paper proposes a typology of the urban occupation of Portuguese municipalities. It uses land use/cover data from 1990 and 2006 to extract built-up areas, and it presents five spatial metrics alongside seventeen statistical indicators from 1991 to 2011 most commonly used in the literature to characterize urban occupation. It uses a self-organising map as a visual tool to identify trends and relationships among variables and to cluster municipalities. Based on the self-organising map’s visual clustering, six types of urban occupation of Portuguese municipalities are proposed. In addition, the paper discusses the added value of using indicators that describe both the patterns and the characteristics of the municipalities for making spatial planning decisions in Portugal. The observed results show that spatial metrics are particularly adequate for measuring peri-urban municipalities (urban sprawl areas). These results represent the first multidimensional and systematic analysis of Portuguese urban occupation and they can be the first step in the integration of spatial metrics as indicators that are suitable for the analysis of spatial planning, and also for comparative purposes at a broader geographical scale. © The Author(s) 2017."
"10.1177/2399808317702147","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041403343&doi=10.1177%2f2399808317702147&partnerID=40&md5=3d3dfef2059bffafceebc0447a34ff7b","This article uses a hedonic modelling approach to assess the implicit willingness to pay for the visual accessibility of voluntarily protected, privately owned, scenic lands based on single family houses. These lands are perpetually protected to preserve natural, historic, and scenic characteristics. The capitalized house premium was captured using a visual accessibility variable, which was a combined weighted measure of ‘view’ and ‘proximity,’ referred to here as the Gravity Inspired Visibility Index. Both global (adjusted R 2 = 0.52, AICc = 29,828) and geographically weighted regression models (adjusted R 2 = 0.59, AICc = 29,729) estimated the price effect but the geographically weighted regression model outperformed the global model. The results from the geographically weighted regression model indicated an average 3.4% price premium on the mean value of homes in the study area. The paper offers a useful framework for evaluating the effect of land protection for planning and real estate purposes. It also offers useful insights for conservation agencies, local governments, professional planners, and real estate professionals for prioritizing land sites with scenic views. © The Author(s) 2017."
"10.1089/big.2018.0071","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058670413&doi=10.1089%2fbig.2018.0071&partnerID=40&md5=a95fcf012be49852521dc4e68850748e","We consider the task of determining the number of chances a soccer team creates, along with the composite nature of each chance-the players involved and the locations on the pitch of the assist and the chance. We infer this information using data consisting solely of attacking events, which the authors believe to be the first approach of its kind. We propose an interpretable Bayesian inference approach and implement a Poisson model to capture chance occurrences, from which we infer team abilities. We then use a Gaussian mixture model to capture the areas on the pitch a player makes an assist/takes a chance. This approach allows the visualization of differences between players in the way they approach attacking play (making assists/taking chances). We apply the resulting scheme to the 2016/2017 English Premier League, capturing team abilities to create chances, before highlighting key areas where players have most impact. © 2018 Gavin A. Whitaker et al. Published by Mary Ann Liebert, Inc."
"10.1089/big.2018.0036","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058640160&doi=10.1089%2fbig.2018.0036&partnerID=40&md5=3769b140237e9cd3f558d51532ed36df","Football scores are an imperfect measure of a team's ability, and consequently exaggerate differences in abilities. Those teams that perform the best and the worst are not really so far from average in their ability; thus their future performances regress to the mean. Betting data indicate that gamblers do not fully account for this regression. © 2018 Mary Ann Liebert, Inc., publishers."
"10.1089/big.2018.0062","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058637442&doi=10.1089%2fbig.2018.0062&partnerID=40&md5=100e583b07bd2d28f8c01ce6a0d856a2","Coaching technology, wearables, and exergames can provide quantitative feedback based on measured activity, but there is little evidence of effective qualitative feedback to aid technique improvement. To achieve personalized qualitative feedback, we demonstrated a proof-of-concept prototype combining kinesiology and computational intelligence that could help improving tennis swing technique utilizing three-dimensional (3D) tennis motion data acquired from multicamera video. Expert data labeling relied on virtual 3D stick figure replay. Diverse assessment criteria for novice to those with intermediate skill levels and configurable coaching scenarios matched with a variety of tennis swings (22 backhands and 21 forehands), including good technique and common errors. A set of selected coaching rules (CRs) was transferred to adaptive assessment modules able to learn from data, evolve their internal structures, and produce autonomous personalized feedback, including verbal cues over virtual camera 3D replay and an end-of-session progress report. The prototype demonstrated autonomous assessment on future data based on learning from prior examples, aligned with skill level, flexible coaching scenarios, and CRs. The generated intuitive diagnostic feedback consisted of elements of safety and performance for tennis swing technique, where each swing sample was compared with the expert. For safety aspects of the relative swing width, the prototype showed improved assessment (from 81% to 91%) when taking into account occluded parts of the pelvis. This study has shown proof of concept for personalized qualitative feedback. The next generation of augmented coaching and exergaming systems will be able to help improve end user's sport discipline-specific techniques. By learning from small expert-labeled data sets, such systems will be able to adapt and provide personalized intuitive autonomous assessment and diagnostic feedback aligned with a specified coaching program and context requirements. © 2018 Mary Ann Liebert, Inc., publishers."
"10.1016/j.bdr.2018.10.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054609346&doi=10.1016%2fj.bdr.2018.10.001&partnerID=40&md5=febf72b8da4fa1b10d371028c811882d","Since the introduction of betting exchanges in 2000, there has been increased interest of ways to monetize on the new technology. Betting exchange markets are fairly similar to the financial markets in terms of their operation. Due to the lower market share and newer technology, there are very few tools available for automated trading for betting exchanges. The in-depth analysis of features available in commercial software demonstrates that there is no commercial software that natively supports machine learned strategy development. Furthermore, previously published academic software products are not publicly obtainable. Hence, this work concentrates on developing a full-stack solution from data capture, back-testing to automated Strategy Agent development for betting exchanges. Moreover, work also explores ways to forecast price movements within betting exchange using new machine learned trading strategies based on Artificial Neuron Networks (ANN) and Cartesian Genetic Programming (CGP). Automatically generated strategies can then be deployed on a server and require no human interaction. Data explored in this work were captured from 1st of January 2016 to 17th of May 2016 for all GB WIN Horse Racing markets (total of 204 GB of data processing). Best found Strategy agent shows promising 83% Return on Investment (ROI) during simulated historical validation period of one month (15th of April 2016 to 16th of May 2016). © 2018 Elsevier Inc."
"10.1016/j.bdr.2018.08.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051530518&doi=10.1016%2fj.bdr.2018.08.001&partnerID=40&md5=4a1802a533e6945fd944fbbf82ac00b5","Many modern applications and systems represent and exchange data in tree-structured form and process and produce large tree datasets. Discovering informative patterns in large tree datasets is an important research area that has many practical applications. Along the years, research has evolved from mining induced patterns to mining embedded patterns. Embedded patterns allow for discovering useful relationships hidden deeply in the datasets which cannot be captured by induced patterns. Unfortunately, previous embedded tree pattern mining approaches cannot scale satisfactorily when the size of the dataset increases. As a consequence, they focus almost exclusively on mining patterns from a collection of small trees and they are incapable of mining patterns from large data trees. However, given the ubiquitous use of tree data, this pattern mining problem needs efficient solutions. In this paper, we address the problem of mining frequent unordered embedded tree patterns from large data trees. We propose a novel approach that exploits efficient homomorphic pattern matching algorithms to compute pattern support incrementally and avoids the costly enumeration of all pattern matchings required by previous approaches. To reduce space consumption, matching information of already computed patterns is materialized as bitmaps. We further optimize our basic support computation method by designing an algorithm which incrementally generates the bitmaps of the embeddings of a new candidate pattern without first explicitly computing the embeddings of this pattern. Our extensive experimental results on real and synthetic large-tree datasets show that our approach displays orders of magnitude performance improvements over a state-of-the-art tree mining algorithm and a recent graph mining algorithm. © 2018 Elsevier Inc."
"10.1016/j.bdr.2018.05.007","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050263861&doi=10.1016%2fj.bdr.2018.05.007&partnerID=40&md5=2f670ecc6bae5eae3d61e6ce4470a7d7","In a fast growing big data era, volume and varieties of data processed in Internet applications drastically increase. Real-world search engines commonly use text classifiers with thousands of classes to improve relevance or data quality. These large scale classification problems lead to severe runtime performance challenges, so practitioners often resort to fast approximation techniques. However, the increase in classification speed comes at a cost, as approximations are lossy, mis-assigning classes relative to the original reference classification algorithm. To address this problem, we introduce a Lossless Pruned Naive Bayes (LPNB) classification algorithm tailored to real-world, big data applications with thousands of classes. LPNB achieves significant speed-ups by drawing on Information Retrieval (IR) techniques for efficient posting list traversal and pruning. We show empirically that LPNB can classify text up to eleven times faster than standard Naive Bayes on a real-world data set with 7205 classes, with larger gains extrapolated for larger taxonomies. In practice, the achieved acceleration is significant as it can greatly cut required computation time. In addition, it is lossless: the output is identical to standard Naive Bayes, in contrast to extant techniques such as hierarchical classification. The acceleration does not rely on the taxonomy structure, and it can be used for both hierarchical and flat taxonomies. © 2018 Elsevier Inc."
"10.1016/j.bdr.2018.06.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049732932&doi=10.1016%2fj.bdr.2018.06.002&partnerID=40&md5=cedb0068c4d33c6e9d99ae72273d2ac7","Gaussian Processes are widely used for regression tasks. A known limitation in the application of Gaussian Processes to regression tasks is that the computation of the solution requires performing a matrix inversion. The solution also requires the storage of a large matrix in memory. These factors restrict the application of Gaussian Process regression to small and moderate size datasets. We present an algorithm that combines estimates from models developed using subsets of the data obtained in a manner similar to the bootstrap. The sample size is a critical parameter for this algorithm. Guidelines for reasonable choices of algorithm parameters, based on a detailed experimental study, are provided. Various techniques have been proposed to scale Gaussian Processes to large-scale regression tasks. The most appropriate choice depends on the problem context. The proposed method is most appropriate for problems where an additive model works well and the response depends on a small number of features. The minimax rate of convergence for such problems is attractive and we can build effective models with a small subset of the data. The Stochastic Variational Gaussian Process and the Sparse Gaussian Process are also appropriate choices for such problems. Results from experiments conducted as part of this study indicate that the algorithm presented in this work can be as effective as these methods. Unlike these methods, the proposed algorithm requires minimal hyper-parameter tuning and is much simpler to implement. The rate of convergence is also attractive. © 2018 Elsevier Inc."
"10.1016/j.bdr.2018.04.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048522110&doi=10.1016%2fj.bdr.2018.04.002&partnerID=40&md5=2e29c72aaa06014c54711d7ec77a0f13","Recurrent neural networks are dynamical systems that provide for memory capabilities to recall past behaviour, which is necessary in the prediction of time series. In this paper, a novel neural network architecture inspired by the immune algorithm is presented and used in the forecasting of naturally occurring signals, including weather big data signals. Big Data Analysis is a major research frontier, which attracts extensive attention from academia, industry and government, particularly in the context of handling issues related to complex dynamics due to changing weather conditions. Recently, extensive deployment of IoT, sensors, and ambient intelligence systems led to an exponential growth of data in the climate domain. In this study, we concentrate on the analysis of big weather data by using the Dynamic Self Organized Neural Network Inspired by the Immune Algorithm. The learning strategy of the network focuses on the local properties of the signal using a self-organised hidden layer inspired by the immune algorithm, while the recurrent links of the network aim at recalling previously observed signal patterns. The proposed network exhibits improved performance when compared to the feedforward multilayer neural network and state-of-the-art recurrent networks, e.g., the Elman and the Jordan networks. Three non-linear and non-stationary weather signals are used in our experiments. Firstly, the signals are transformed into stationary, followed by 5-steps ahead prediction. Improvements in the prediction results are observed with respect to the mean value of the error (RMS) and the signal to noise ratio (SNR), however to the expense of additional computational complexity, due to presence of recurrent links. © 2018 Elsevier Inc."
"10.1016/j.bdr.2018.05.005","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047928365&doi=10.1016%2fj.bdr.2018.05.005&partnerID=40&md5=1fbf486e56c403539ea7107b2a757a36","Clustering is an important field in data mining that aims to reveal hidden patterns in data sets. It is widely popular in marketing or medical applications and used to identify groups of similar objects. Clustering possibly unbounded and evolving data streams is of particular interest due to the widespread deployment of large and fast data sources such as sensors. The vast majority of stream clustering algorithms employ a two-phase approach where the stream is first summarized in an online phase. Upon request, an offline phase reclusters the aggregations into the final clusters. In this setup, the online component will idle and wait for the next observation in times where the stream is slow. This paper proposes a new stream clustering algorithm called evoStream which performs evolutionary optimization in the idle times of the online phase to incrementally build and refine the final clusters. Since the online phase would idle otherwise, our approach does not reduce the processing speed while effectively removing the computational overhead of the offline phase. In extensive experiments on real data streams we show that the proposed algorithm allows to output clusters of high quality at any time within the stream without the need for additional computational resources. © 2018 Elsevier Inc."
"10.1016/j.bdr.2018.05.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047493065&doi=10.1016%2fj.bdr.2018.05.002&partnerID=40&md5=c80f88ff4a00bc3eabdfe467020a6598","With advancement of the technology, data size is increasing rapidly. For making intelligent decisions based on data, efficacious analytic methods are required. Data clustering, a prominent analytic method of data mining, is being efficiently employed in data analytics. To analyze massive data sets, the improvement in the traditional methods is the urge of todays scenario. In this paper, an efficient clustering method, MapReduce based enhanced grey wolf optimizer (MR-EGWO), is presented for clustering large-scale data sets. The proposed method introduced a novel variant of grey wolf optimizer, Enhanced grey wolf optimizer (EGWO), where the hunting strategy of grey wolf is hybridized with binomial crossover and lévy flight steps are inducted to enhance the searching capability for pray. Further, the proposed variant is used for optimizing the clustering process. The clustering efficiency of the EGWO is tested on seven UCI benchmark datasets and compared with the five existing clustering techniques namely K-Means, particle swarm optimization (PSO), gravitational search algorithm (GSA), bat algorithm (BA) and grey wolf optimizer (GWO). The convergence behavior and consistency of the EGWO has been validated through the convergence graph and boxplots. Further, the proposed EGWO is parallelized on the MapReduce model in the Hadoop framework and named MR-EGWO to handle the large-scale datasets. Moreover, the clustering quality of the MR-EGWO is also validated in terms of F-measure and compared with four MapReduce based state-of-the-art namely; parallel K-Means, parallel K-PSO, MapReduce based artificial bee colony optimization (MR-ABC), dynamic frequency based parallel k-bat algorithm (DFBPKBA). Experimental results affirm that the proposed technique is promising and powerful alternative for the efficient and large-scale data clustering. © 2018 Elsevier Inc."
"10.1016/j.bdr.2018.04.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045852537&doi=10.1016%2fj.bdr.2018.04.001&partnerID=40&md5=c29b8965d03623e9da383060a1f13b1d","We propose a memetic approach to find bottlenecks in complex networks based on searching for a graph partitioning with minimum conductance. Finding the optimum of this problem, also known in statistical mechanics as the Cheeger constant, is one of the most interesting NP-hard network optimisation problems. The existence of low conductance minima indicates bottlenecks in complex networks. However, the problem has not yet been explored in depth in the context of applied discrete optimisation and evolutionary approaches to solve it. In this paper, the use of a memetic framework is explored to solve the minimum conductance problem. The approach combines a hybrid method of initial population generation based on bridge identification and local optima sampling with a steady-state evolutionary process with two local search subroutines. These two local search subroutines have complementary qualities. Efficiency of three crossover operators is explored, namely one-point crossover, uniform crossover, and our own partition crossover. Experimental results are presented for both artificial and real-world complex networks. Results for Barabási–Albert model of scale-free networks are presented, as well as results for samples of social networks and protein–protein interaction networks. These indicate that both well-informed initial population generation and the use of a crossover seem beneficial in solving the problem in large-scale. © 2018 Elsevier Inc."
"10.1016/j.bdr.2018.03.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045195892&doi=10.1016%2fj.bdr.2018.03.001&partnerID=40&md5=fdd0e7ddf40e6083a249332e86c34fb4","This paper proposes a new Direction Aware Particle Swarm Optimization algorithm with Sensitive Swarm Leader (DAPSO-SSL). DAPSO-SSL maps the basic human nature of awareness, maturity, leader and followers relationship and leadership qualities to the popular PSO algorithm. It assigns these qualities to swarm leader and individual particles. In practical life, it is the moral responsibility of the leader to improve the status, quality or direction of the life of his followers. Thus, he influences the decision making of the group members through his policies and actions. A great leader is one which continuously keeps track of his followers’ performance and accordingly adapts to various situations. If their performance is degrading because of him, he can either change his policies or can groom a new leader to take his position. Hence this leader can be Sensitive to the needs of group members. It also incorporates the concept of iterative directional awareness among the swarm particles in PSO. The particles over successive iterations become more conscious about their direction of motion by taking account of their performances. DAPSO-SSL thus tries to prevent stagnation while improving convergence rate. The algorithm is tested on twenty-four benchmark functions on COCO framework and its performance is compared with other state-of-the-art algorithms. Further, in order to check the effectiveness of the proposed algorithm, DAPSO-SSL is applied to community detection problem of big data networks. The comparative analysis of the results with other state-of-the-art algorithms has indicated the competitiveness of the proposed algorithm. © 2018 Elsevier Inc."
"10.1140/epjds/s13688-018-0178-0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058932808&doi=10.1140%2fepjds%2fs13688-018-0178-0&partnerID=40&md5=56ea7f2bc1cf668a30cb1d444a62821b","Social media data is widely analyzed in computational social science. Twitter, one of the largest social media platforms, is used for research, journalism, business, and government to analyze human behavior at scale. Twitter offers data via three different Application Programming Interfaces (APIs). One of which, Twitter’s Sample API, provides a freely available 1% and a costly 10% sample of all Tweets. These data are supposedly random samples of all platform activity. However, we demonstrate that, due to the nature of Twitter’s sampling mechanism, it is possible to deliberately influence these samples, the extent and content of any topic, and consequently to manipulate the analyses of researchers, journalists, as well as market and political analysts trusting these data sources. Our analysis also reveals that technical artifacts can accidentally skew Twitter’s samples. Samples should therefore not be regarded as random. Our findings illustrate the critical limitations and general issues of big data sampling, especially in the context of proprietary data and undisclosed details about data handling. © 2018, The Author(s)."
"10.1140/epjds/s13688-018-0177-1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057877483&doi=10.1140%2fepjds%2fs13688-018-0177-1&partnerID=40&md5=7242c4804475e54bfb0e7ab009f93200","Cities are growing at a fast rate, and transportation networks need to adapt accordingly. To design, plan, and manage transportation networks, domain experts need data that reflect how people move from one place to another, at what times, for what purpose, and in what mode(s) of transportation. However, traditional data collection methods are not cost-effective or timely. For instance, travel surveys are very expensive, collected every ten years, a period of time that does not cope with quick city changes, and using a relatively small sample of people. In this paper, we propose an algorithmic pipeline to infer the distribution of mode of transportation usage in a city, using mobile phone network data. Our pipeline is based on a Topic-Supervised Non-Negative Matrix Factorization model, using a Weak-Labeling strategy on user trajectories with data obtained from open datasets, such as GTFS and OpenStreetMap. As a case study, we show results for the city of Santiago, Chile, which has a sophisticated intermodal public transportation system. Importantly, our pipeline delivers coherent results that are explainable, with interpretable parameters at each step. Finally, we discuss the potential applications and implications of such a system in transportation and urban planning. © 2018, The Author(s)."
"10.1140/epjds/s13688-018-0175-3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056603378&doi=10.1140%2fepjds%2fs13688-018-0175-3&partnerID=40&md5=e26f61caa4b2062cb92ab7f94c9b7182","Advancing our understanding of human behavior hinges on the ability of theories to unveil the mechanisms underlying such behaviors. Measuring the ability of theories and models to predict unobserved behaviors provides a principled method to evaluate their merit and, thus, to help establish which mechanisms are most plausible. Here, we propose models and develop rigorous inference approaches to predict strategic decisions in dyadic social dilemmas. In particular, we use bipartite stochastic block models that incorporate information about the dilemmas faced by individuals. We show, combining these models with empirical data on strategic decisions in dyadic social dilemmas, that individual strategic decisions are to a large extent predictable, despite not being “rational.” The analysis of these models also allows us to conclude that: (i) individuals do not perceive games according their game-theoretical structure; (ii) individuals make decisions using combinations of multiple simple strategies, which our approach reveals naturally. © 2018, The Author(s)."
"10.1140/epjds/s13688-018-0176-2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056363195&doi=10.1140%2fepjds%2fs13688-018-0176-2&partnerID=40&md5=096b4247c95c7f316aa5c293fc1e68e1","Online activity leaves digital traces of human behavior. In this paper we investigate if online interest can be used as a proxy of housing demand, a key yet so far mostly unobserved feature of housing markets. We analyze data from an Italian website of housing sales advertisements (ads). For each ad, we know the timings at which website users clicked on the ad or used the corresponding contact form. We show that low online interest—a small number of clicks/contacts on the ad relative to other ads in the same neighborhood—predicts longer time on market and higher chance of downward price revisions, and that aggregate online interest is a leading indicator of housing market liquidity and prices. As online interest affects time on market, liquidity and prices in the same way as actual demand, we deduce that it is a good proxy. We then turn to a standard econometric problem: what difference in demand is caused by a difference in price? We use machine learning to identify pairs of duplicate ads, i.e. ads that refer to the same housing unit. Under some caveats, differences in demand between the two ads can only be caused by differences in price. We find that a 1% higher price causes a 0.66% lower number of clicks. © 2018, The Author(s)."
"10.1140/epjds/s13688-018-0174-4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055571611&doi=10.1140%2fepjds%2fs13688-018-0174-4&partnerID=40&md5=0be69c9e69283ab54b3c8ed86dc8df6c","Human activity follows an approximately 24-hour day-night cycle, but there is significant individual variation in awake and sleep times. Individuals with circadian rhythms at the extremes can be categorized into two chronotypes: “larks”, those who wake up and go to sleep early, and “owls”, those who stay up and wake up late. It is well established that a person’s chronotype can affect their activities and health. However, less is known about the effects of chronotypes on social behavior, even though many social interactions require coordinated timings. To study how chronotypes relate to social behavior, we use data collected with a smartphone app on a population of more than seven hundred volunteer students to simultaneously determine their chronotypes and social network structure. We find that owls maintain larger personal networks, albeit with less time spent per contact. On average, owls are more central in the social network of students than larks, frequently occupying the dense core of the network. These results point out that there is a strong connection between the chronotypes of people and the structure of social networks that they form. © 2018, The Author(s)."
"10.1140/epjds/s13688-018-0173-5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055547169&doi=10.1140%2fepjds%2fs13688-018-0173-5&partnerID=40&md5=7fb375a09ee850f21b2dc31b04578bc3","In this research, we exploit repeated parts in daily trajectories in people’s movements, which we refer to as mobility patterns, to train models to identify and predict a person’s lifestyles. We use cellular data of a group (“society”) of people and represent a person’s daily trajectory using semantic labels (e.g., “home”, “work”, and “gym”) given to the main places of interest (POI) he has visited during the day, as determined collectively based on interviewing all people of the group. First, in an unsupervised manner using a neural network (NN), we embed POI-based daily trajectories that always appear together with others in consecutive weeks and identify the result of this embedding with social lifestyles. Second, using these lifestyles as labels for lifestyle prediction, user POI-based daily trajectories are used to train a convolutional NN to extract mobility patterns in the trajectories and a dynamic NN with flexible memory to assemble these patterns to predict a lifestyle for a trajectory never-seen-before. The two-stage algorithm shows model accuracy and generalizability in lifestyle identification and prediction (both for a novel trajectory and a novel user) that are superior to those shown by state-of-the-art algorithms. The code for the algorithm and data sets used in our experiments are available online. © 2018, The Author(s)."
"10.1140/epjds/s13688-018-0168-2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055335795&doi=10.1140%2fepjds%2fs13688-018-0168-2&partnerID=40&md5=3cd1f5eec77296c56477eeff1ecc2bf9","Tourist flows in historical cities are continuously growing in a globalized world and adequate governance processes, politics and tools are necessary in order to reduce impacts on the urban livability and to guarantee the preservation of cultural heritage. The ICTs offer the possibility of collecting large amount of data that can point out and quantify some statistical and dynamic properties of human mobility emerging from the individual behavior and referring to a whole road network. In this paper we analyze a new dataset that has been collected by the Italian mobile phone company TIM, which contains the GPS positions of a relevant sample of mobile devices when they actively connected to the cell phone network. Our aim is to propose innovative tools allowing to study properties of pedestrian mobility on the whole road network. Venice is a paradigmatic example for the impact of tourist flows on the resident life quality and on the preservation of cultural heritage. The GPS data provide anonymized georeferenced information on the displacements of the devices. After a filtering procedure, we develop specific algorithms able to reconstruct the daily mobility paths on the whole Venice road network. The statistical analysis of the mobility paths suggests the existence of a travel time budget for the mobility and points out the role of the rest times in the empirical relation between the mobility time and the corresponding path length. We succeed to highlight two connected mobility subnetworks extracted from the whole road network, that are able to explain the majority of the observed mobility. Our approach shows the existence of characteristic mobility paths in Venice for the tourists and for the residents. Moreover the data analysis highlights the different mobility features of the considered case studies and it allows to detect the mobility paths associated to different points of interest. Finally we have disaggregated the Italian and foreigner categories to study their different mobility behaviors. © 2018, The Author(s)."
"10.1140/epjds/s13688-018-0169-1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055262017&doi=10.1140%2fepjds%2fs13688-018-0169-1&partnerID=40&md5=fbef1b19c99790a6b397ecc9ea7735db","Fast-food outlets play a significant role in the nutrition of British children who get more food from such shops than the school canteen. To reduce young people’s access to fast-food meals during the school day, many British cities are implementing zoning policies. For instance, cities can create buffers around schools, and some have used 200 meters buffers while others used 400 meters. But how close is too close? Using the road network is needed to precisely computing the distance between fast-food outlets (for policies limiting the concentration), or fast-food outlets and the closest school (for policies using buffers). This estimates how much of the fast-food landscape could be affected by a policy, and complementary analyses of food utilization can later translate the estimate into changes on childhood nutrition and obesity. Network analyses of retail and urban forms are typically limited to the scale of a city. However, to design national zoning policies, we need to perform this analysis at a national scale. Our study is the first to perform a nation-wide analysis, by linking large datasets (e.g., all roads, fast-food outlets and schools) and performing the analysis over a high performance computing cluster. We found a strong spatial clustering of fast-food outlets (with 80% of outlets being within 120 of another outlet), but much less clustering for schools. Results depend on whether we use the road network on the Euclidean distance (i.e. ‘as the crow flies’): for instance, half of the fast-food outlets are found within 240 m of a school using an Euclidean distance, but only one-third at the same distance with the road network. Our findings are consistent across levels of deprivation, which is important to set equitable national policies. In line with previous studies (at the city scale rather than national scale), we also examined the relation between centrality and outlets, as a potential target for policies, but we found no correlation when using closeness or betweenness centrality with either the Spearman or Pearson correlation methods. © 2018, The Author(s)."
"10.1140/epjds/s13688-018-0166-4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055193087&doi=10.1140%2fepjds%2fs13688-018-0166-4&partnerID=40&md5=70da8bc63b17ce0bdb89975c9551ab68","The spread of ideas in the scientific community is often viewed as a competition, in which good ideas spread further because of greater intrinsic fitness, and publication venue and citation counts correlate with importance and impact. However, relatively little is known about how structural factors influence the spread of ideas, and specifically how where an idea originates might influence how it spreads. Here, we investigate the role of faculty hiring networks, which embody the set of researcher transitions from doctoral to faculty institutions, in shaping the spread of ideas in computer science, and the importance of where in the network an idea originates. We consider comprehensive data on the hiring events of 5032 faculty at all 205 Ph.D.-granting departments of computer science in the U.S. and Canada, and on the timing and titles of 200,476 associated publications. Analyzing five popular research topics, we show empirically that faculty hiring can and does facilitate the spread of ideas in science. Having established such a mechanism, we then analyze its potential consequences using epidemic models to simulate the generic spread of research ideas and quantify the impact of where an idea originates on its longterm diffusion across the network. We find that research from prestigious institutions spreads more quickly and completely than work of similar quality originating from less prestigious institutions. Our analyses establish the theoretical trade-offs between university prestige and the quality of ideas necessary for efficient circulation. Our results establish faculty hiring as an underlying mechanism that drives the persistent epistemic advantage observed for elite institutions, and provide a theoretical lower bound for the impact of structural inequality in shaping the spread of ideas in science. © 2018, The Author(s)."
"10.1140/epjds/s13688-018-0172-6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055144687&doi=10.1140%2fepjds%2fs13688-018-0172-6&partnerID=40&md5=cb5be8a005ebc1a6bd8b7a5f6a73ea4d","It is well reported that long commutes have a large detrimental effect on people’s health and on the economy of cities. Interestingly, despite the strong impact on our daily lives, a simple way to measure the quality of urban transportation is still unknown. We performed data analysis on the transportation network of two large cities (Fortaleza and Dublin). By dividing each bus trajectory into equal pieces of space, we determine the distribution of time intervals for each trip, and we propose that the heterogeneity of the time distribution can be used to characterize the quality of that trip. Inspired by the use of the Gini coefficient to quantify the inequality level of income distribution, we used the Gini in order to characterize the heterogeneity level of the time distribution. We demonstrated that Gini coefficients are strongly correlated with peak usage of the mobility system, as well as the schedule delays in the system. Finally, our method can be used to find highly heterogeneous trips which have a large negative effect on the urban mobility and can help find new directions for new public planning strategies. © 2018, The Author(s)."
"10.1140/epjds/s13688-018-0165-5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055138402&doi=10.1140%2fepjds%2fs13688-018-0165-5&partnerID=40&md5=5fdc67ed13172cebe8bbb22b069f6b18","Customer retention is crucial in a variety of businesses as acquiring new customers is often more costly than keeping the current ones. As a consequence, churn prediction has attracted great attention from both the business and academic worlds. Traditional efforts in the financial domain mainly focus on domain specific variables such as product ownership or service usage aggregation, however, without considering dynamic behavioral patterns of customers’ financial transactions. In this paper, we attempt to fill in this gap by investigating the spatio-temporal patterns and entropy of choices underlying the customers’ financial decisions, and their relations to customer churning activities. Inspired by previous works in the emerging field of computational social science, we built a prediction model based on spatio-temporal and choice behavioral traits using individual transaction records. Our results show that proposed dynamic behavioral models could predict churn decisions significantly better than traditionally considered factors such as demographic-based features, and that this effect remains consistent across multiple data sets and various churn definitions. We further study the relative importance of the various behavioral features in churn prediction, and how the predictive power varies across different demographic groups. More generally, the proposed features can also be applied to churn prediction in other domains where spatio-temporal behavioral data are available. © 2018, The Author(s)."
"10.1140/epjds/s13688-018-0170-8","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055136427&doi=10.1140%2fepjds%2fs13688-018-0170-8&partnerID=40&md5=189ea964ef4e0cebb4a5605030fce8ff","As the first decentralized digital currency introduced in 2009 together with the blockchain, Bitcoin offers new opportunities both for developed and developing countries. Bitcoin peer-to-peer transactions are independent of the banking system, facilitating foreign exchanges with low transaction fees, such as remittances, and offering a high degree of anonymity. These opportunities together with other key factors led the Bitcoin to become extremely popular and caused its price to skyrocket during 2017 (Henry et al. in J Digit Bank 2(4):311–337, 2018). However, while the Bitcoin blockchain attracts a lot of attention, it remains difficult to investigate where this attention comes from, due to the pseudo-anonymity of the system, and consequently to appreciate its social impact. Here we make an attempt to characterize the adoption of the Bitcoin blockchain by country. In the first part of the work we show that information about the number of Bitcoin software client downloads, the IP addresses that act as relays for the transactions, and the Internet searches about Bitcoin provide together a coherent picture of the system evolution in different countries. Using these quantities as a proxy for user adoption, we identify several socio-economic indexes such as the GDP per capita, freedom of trade and the Internet penetration as key variables correlated with the degree of user adoption. In the second part of the work, we build a network of Bitcoin transactions between countries using the IP addresses of nodes relaying transactions and we develop an augmented version of the gravity model of trade in order to identify socio-economic factors linked to the flow of Bitcoin between countries. In a nutshell our study provides a new insight on Bitcoin adoption by country and on the potential socio-economic drivers of the international Bitcoin flow. © 2018, The Author(s)."
"10.1140/epjds/s13688-018-0171-7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055089000&doi=10.1140%2fepjds%2fs13688-018-0171-7&partnerID=40&md5=70b4a760580adf358af5ef8e0785e267","Nowadays, Location-Based Social Networks (LBSN) collect a vast range of information which can help us to understand the regional dynamics (i.e. human mobility) across an entire city. LBSN provides unprecedented opportunities to tackle various social problems. In this work, we explore dynamic features derived from Foursquare check-in data in short-term crime event prediction with fine spatio-temporal granularity. While crime event prediction has been investigated widely due to its social importance, its success rate is far from satisfactory. The existing studies rely on relatively static features such as regional characteristics, demographic information and the topics obtained from tweets but very few studies focus on exploring human mobility through social media. In this study, we identify a number of dynamic features based on the research findings in Criminology, and report their correlations with different types of crime events. In particular, we observe that some types of crime events are more highly correlated to the dynamic features, e.g., Theft, Drug Offence, Fraud, Unlawful Entry and Assault than others e.g. Traffic Related Offence. A key challenge of the research is that the dynamic information is very sparse compared to the relatively static information. To address this issue, we develop a matrix factorization based approach to estimate the missing dynamic features across the city. Interestingly, the estimated dynamic features still maintain the correlation with crime occurrence across different types. We evaluate the proposed methods in different time intervals. The results verify that the crime prediction performance can be significantly improved with the inclusion of dynamic features across different types of crime events. © 2018, The Author(s)."
"10.1140/epjds/s13688-018-0167-3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054685042&doi=10.1140%2fepjds%2fs13688-018-0167-3&partnerID=40&md5=b9aa9743bebecb5cf8da848773e0500c","A simple model, named the flow and jump model (FJM) is used for describing commuter fluxes at different distances. The model is based on a master equation which allows a local net probability flow and non-local jumps. FJM is in principle a one-parameter model, however it is found that by fixing this parameter we get a parameter free model, similar with the radiation model. We find that FJM offers an improved description for commuting data from USA, Italy and Hungary. For a special choice of the model parameter FJM leads to the radiation model. © 2018, The Author(s)."
"10.1140/epjds/s13688-018-0158-4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054097221&doi=10.1140%2fepjds%2fs13688-018-0158-4&partnerID=40&md5=6bcd3591de20a66ca7857718551891bd","In the last decades, the notion that cities are in a state of equilibrium with a centralised organisation has given place to the viewpoint of cities in disequilibrium and organised from bottom to up. In this perspective, cities are evolving systems that exhibit emergent phenomena built from local decisions. While urban evolution promotes the emergence of positive social phenomena such as the formation of innovation hubs and the increase in cultural diversity, it also yields negative phenomena such as increases in criminal activity. Yet, we are still far from understanding the driving mechanisms of these phenomena. In particular, approaches to analyse urban phenomena are limited in scope by neglecting both temporal non-stationarity and spatial heterogeneity. In the case of criminal activity, we know for more than one century that crime peaks during specific times of the year, but the literature still fails to characterise the mobility of crime. Here we develop an approach to describe the spatial, temporal, and periodic variations in urban quantities. With crime data from 12 cities, we characterise how the periodicity of crime varies spatially across the city over time. We confirm one-year criminal cycles and show that this periodicity occurs unevenly across the city. These ‘waves of crime’ keep travelling across the city: while cities have a stable number of regions with a circannual period, the regions exhibit non-stationary series. Our findings support the concept of cities in a constant change, influencing urban phenomena—in agreement with the notion of cities not in equilibrium. © 2018, The Author(s)."
"10.1140/epjds/s13688-018-0156-6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053707598&doi=10.1140%2fepjds%2fs13688-018-0156-6&partnerID=40&md5=0a43c4159980a96434ee475015b24c0e","In the hospitality industry, the room and apartment sharing platform of Airbnb has been accused of unfair competition. Detractors have pointed out the chronic lack of proper legislation. Unfortunately, there is little quantitative evidence about Airbnb’s spatial penetration upon which to base such a legislation. In this study, we analyze Airbnb’s spatial distribution in eight U.S. urban areas, in relation to both geographic, socio-demographic, and economic information. We find that, despite being very different in terms of population composition, size, and wealth, all eight cities exhibit the same pattern: that is, areas of high Airbnb presence are those occupied by the “talented and creative” classes, and those that are close to city centers. This result is consistent so much so that the accuracy of predicting Airbnb’s spatial penetration is as high as 0.725. © 2018, The Author(s)."
"10.1140/epjds/s13688-018-0160-x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053441451&doi=10.1140%2fepjds%2fs13688-018-0160-x&partnerID=40&md5=eae46818cd5e17ffb40676d077c281d9","Railways are a key infrastructure for any modern country. The reliability and resilience of this peculiar transportation system may be challenged by different shocks such as disruptions, strikes and adverse weather conditions. These events compromise the correct functioning of the system and trigger the spreading of delays into the railway network on a daily basis. Despite their importance, a general theoretical understanding of the underlying causes of these disruptions is still lacking. In this work, we analyse the Italian and German railway networks by leveraging on the train schedules and actual delay data retrieved during the year 2015. We use these data to infer simple statistical laws ruling the emergence of localized delays in different areas of the network and we model the spreading of these delays throughout the network by exploiting a framework inspired by epidemic spreading models. Our model offers a fast and easy tool for the preliminary assessment of the effectiveness of traffic handling policies, and of the railway network criticalities. © 2018, The Author(s)."
"10.1140/epjds/s13688-018-0161-9","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053307421&doi=10.1140%2fepjds%2fs13688-018-0161-9&partnerID=40&md5=30e2f1feafe82d33fa2d6de6ae3a15d8","Comprehensive and quantitative investigations of social theories and phenomena increasingly benefit from the vast breadth of data describing human social relations that is now available within the realm of computational social science. Such data are, however, typically proxies for one of the many interaction layers composing social networks, which can be defined in many ways and are composed of communication of various types (e.g., phone calls, face-to-face communication, etc.). As a result, many studies focus on one single layer, corresponding to the data at hand. Several studies have however shown that these layers are not interchangeable, despite the presence of a certain level of correlation between them. Here, we investigate whether different layers of interactions among individuals lead to similar conclusions with respect to the presence of homophily patterns in a population—homophily represents one of the widest studied phenomenon in social networks. To this aim, we consider a data set describing interactions and links of various nature in a population of Asian students with diverse nationalities, first language and gender. We study homophily patterns, as well as their temporal evolution in each layer of the social network. To facilitate our analysis, we put forward a general method to assess whether the homophily patterns observed in one layer inform us about patterns in another layer. For instance, our study reveals that three network layers—cell phone communications, questionnaires about friendship, and trust relations—lead to similar and consistent results despite some minor discrepancies. The homophily patterns of the co-presence network layer, however, does not yield any meaningful information about other network layers. © 2018, The Author(s)."
"10.1140/epjds/s13688-018-0163-7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053305070&doi=10.1140%2fepjds%2fs13688-018-0163-7&partnerID=40&md5=eb2f91239e723271b224ce58f10e2ff1","The prevalence of online media has attracted researchers from various domains to explore human behavior and make interesting predictions. In this research, we leverage heterogeneous data collected from various online platforms to predict Taiwan’s 2016 general election. In contrast to most existing research, we take a “signal” view of heterogeneous information and adopt the Kalman filter to fuse multiple signals into daily vote predictions for the candidates. We also consider events that influenced the election in a quantitative manner based on the so-called event study model that originated in the field of financial research. We obtained the following interesting findings. First, public opinions in online media dominate traditional polls in Taiwan election prediction in terms of both predictive power and timeliness. But offline polls can still function on alleviating the sample bias of online opinions. Second, although online signals converge as election day approaches, the simple Facebook “Like” is consistently the strongest indicator of the election result. Third, most influential events have a strong connection to cross-strait relations, and the Chou Tzu-yu flag incident followed by the apology video one day before the election increased the vote share of Tsai Ing-Wen by 3.66%. This research justifies the predictive power of online media in politics and the advantages of information fusion. The combined use of the Kalman filter and the event study method contributes to the data-driven political analytics paradigm for both prediction and attribution purposes. © 2018, The Author(s)."
"10.1140/epjds/s13688-018-0162-8","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053235594&doi=10.1140%2fepjds%2fs13688-018-0162-8&partnerID=40&md5=d2a4a26217018f63f7e15cf89070eff4","Understanding the importance of links in transmitting information in a network can provide ways to hinder or postpone ongoing dynamical phenomena like the spreading of epidemic or the diffusion of information. In this work, we propose a new measure based on stochastic diffusion processes, the transmission centrality, that captures the importance of links by estimating the average number of nodes to whom they transfer information during a global spreading diffusion process. We propose a simple algorithmic solution to compute transmission centrality and to approximate it in very large networks at low computational cost. Finally we apply transmission centrality in the identification of weak ties in three large empirical social networks, showing that this metric outperforms other centrality measures in identifying links that drive spreading processes in a social network. © 2018, The Author(s)."
"10.1140/epjds/s13688-018-0159-3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052878091&doi=10.1140%2fepjds%2fs13688-018-0159-3&partnerID=40&md5=1d0e2cad2a5fe0e3b1ab1eaa2efd73b2","We investigate Bitcoin network observing transactions broadcasted into the network during a week from 04/05/2016 and then monitoring their inclusion into the blockchain during the following seven months.We unveil that 42% of the transactions are still not included in the Blockchain after 1 h from their appearance and 20% of the transactions are still not included in the Blockchain after 30 days, therefore revealing a great inefficiency in the Bitcoin system. However, we observe that most of these “forgotten” transactions have low values and in terms of transferred value the system is less inefficient with 93% of the transactions value being included into the Blockchain within 3 h and 98.8% within a day. The fact that a sizeable fraction of transactions is not processed timely casts serious doubts on the usability of the Bitcoin Blockchain for reliable time-stamping purposes. It also calls for a debate about the right systems of incentives which a peer-to-peer unintermediated system should introduce to promote efficient transaction recording. © 2018, The Author(s)."
"10.1140/epjds/s13688-018-0157-5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052568686&doi=10.1140%2fepjds%2fs13688-018-0157-5&partnerID=40&md5=829626218dda7297ebba64edcfa31bab","In Latin America, shopping malls seem to offer an open, safe and democratic version of the public space. However, it is often difficult to quantitatively measure whether they indeed foster, hinder, or are neutral with respect to social inclusion. In this work, we investigate if, and by how much, people from different social classes are attracted by the same malls. Using a dataset of mobile phone network records from 387,152 devices identified as customers of 16 malls in Santiago de Chile, we performed several analyses to study whether malls with higher social mixing attract more people. Our pipeline, which starts with the socio-economic characterization of mall visitors, includes the estimation of social mixing and diversity of malls, the application of the gravity model of mobility, and the definition of a co-visitation model. Results showed that people tend to choose a profile of malls more in line with their own socio-economic status and the distance from their home to the mall, and that higher mixing does positively contribute to the process of choosing a mall. We conclude that (a) there is social mixing in malls, and (b) that social mixing is a factor at the time of choosing which mall to go to. Thus, the potential for social mixing in malls could be capitalized by designing public policies regarding transportation and mobility to make some malls strong social inclusion hubs. © 2018, The Author(s)."
"10.1140/epjds/s13688-018-0155-7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051285233&doi=10.1140%2fepjds%2fs13688-018-0155-7&partnerID=40&md5=12018e851822ff2e51c3b4c48dd32169","Very low birth weight (VLBW) infants require specialized care in neonatal intensive care units. In the United States (U.S.), such infants frequently are transferred between hospitals. Although these neonatal transfer networks are important, both economically and for infant morbidity and mortality, the national level pattern of neonatal transfers is largely unknown. Using data from Vermont Oxford Network on 44,753 births, 2122 hospitals, and 9722 interhospital infant transfers from 2015, we performed the largest analysis to date on the interhospital transfer network for VLBW infants in the U.S. We find that transfers are organized around regional communities, but that despite being largely within state boundaries, most communities often contain at least two hospitals in different states. To classify the structural variation in transfer pattern amongst these communities, we applied a spectral measure for regionalization and found an association between a community’s degree of regionalization and their infant transfer rate, which was not utilized in detecting communities. We also demonstrate that the established measures of network centrality and hierarchy, e.g., the community-wide entropy in PageRank or betweenness centrality and number of distinct “layers,” within a community, correlate weakly with our regionalization index and were not significantly associated with metrics on infant transfer rate. Our results suggest that the regionalization index captures novel information about the structural properties of VLBW infant transfer networks, have the practical implication of characterizing neonatal care in the U.S., and may apply more broadly to the role of centralizing forces in organizing complex adaptive systems. © 2018, The Author(s)."
"10.1140/epjds/s13688-018-0150-z","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050384983&doi=10.1140%2fepjds%2fs13688-018-0150-z&partnerID=40&md5=48849e0113490f9ae3e0876c6a1e196e","Traditional crime prediction models based on census data are limited, as they fail to capture the complexity and dynamics of human activity. With the rise of ubiquitous computing, there is the opportunity to improve such models with data that make for better proxies of human presence in cities. In this paper, we leverage large human mobility data to craft an extensive set of features for crime prediction, as informed by theories in criminology and urban studies. We employ averaging and boosting ensemble techniques from machine learning, to investigate their power in predicting yearly counts for different types of crimes occurring in New York City at census tract level. Our study shows that spatial and spatio-temporal features derived from Foursquare venues and checkins, subway rides, and taxi rides, improve the baseline models relying on census and POI data. The proposed models achieve absolute R2 metrics of up to 65% (on a geographical out-of-sample test set) and up to 89% (on a temporal out-of-sample test set). This proves that, next to the residential population of an area, the ambient population there is strongly predictive of the area’s crime levels. We deep-dive into the main crime categories, and find that the predictive gain of the human dynamics features varies across crime types: such features bring the biggest boost in case of grand larcenies, whereas assaults are already well predicted by the census features. Furthermore, we identify and discuss top predictive features for the main crime categories. These results offer valuable insights for those responsible for urban policy or law enforcement. © 2018, The Author(s)."
"10.1140/epjds/s13688-018-0152-x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050123431&doi=10.1140%2fepjds%2fs13688-018-0152-x&partnerID=40&md5=81f3a1114c383155cf7a6f6e88a3fcd3","Social capital has been studied in economics, sociology and political science as one of the key elements that promote the development of modern societies. It can be defined as the source of capital that facilitates cooperation through shared social norms. In this work, we investigate whether and to what extent synchronization aspects of mobile communication patterns are associated with social capital metrics. Interestingly, our results show that our synchronization-based approach well correlates with existing social capital metrics (i.e., Referendum turnout, Blood donations, and Association density), being also able to characterize the different role played by high synchronization within a close proximity-based community and high synchronization among different communities. Hence, the proposed approach can provide timely, effective analysis at a limited cost over a large territory. © 2018, The Author(s)."
"10.1140/epjds/s13688-018-0153-9","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049945668&doi=10.1140%2fepjds%2fs13688-018-0153-9&partnerID=40&md5=4daffda5808cd22b9ac98eaefaedd387","Human mobility always had a great influence on the spreading of cultural, social and technological ideas. Developing realistic models that allow for a better understanding, prediction and control of such coupled processes has gained a lot of attention in recent years. However, the modeling of spreading processes that happened in ancient times faces the additional challenge that available knowledge and data is often limited and sparse. In this paper, we present a new agent-based model for the spreading of innovations in the ancient world that is governed by human movements. Our model considers the diffusion of innovations on a spatial network that is changing in time, as the agents are changing their positions. Additionally, we propose a novel stochastic simulation approach to produce spatio-temporal realizations of the spreading process that are instructive for studying its dynamical properties and exploring how different influences affect its speed and spatial evolution. © 2018, The Author(s)."
"10.1140/epjds/s13688-018-0148-6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049855821&doi=10.1140%2fepjds%2fs13688-018-0148-6&partnerID=40&md5=b06736711daca770bdabe204e4ddcc1e","Subway and bus networks work as an integrated multiplex transportation system and play an indispensable role in modern big cities. Even though a variety of works have investigated the coupling dynamics of multiplex transportation networks, empirical data that validates the determinant coupling factors are still lacking. In this paper, we employ smartcard data of 2.4 million subway and bus passengers in Shenzhen, China to study the coupling dynamics of subway and bus networks. Surprisingly, the coupling of subway and bus networks is not notably influenced by the time-varying speed ratio of the two network layers but is jointly determined by the distribution of travel demands and transportation facilities. Our findings highlight the important role of real travel demand data in analyzing the coupling dynamics of multiplex transportation networks. They also suggest that the speed ratio of different network layers, which was regarded as a key factor in determining coupling strength, has a negligible effect on travelers’ route selections, and thus the coupling dynamics of multiplex transportation networks. © 2018, The Author(s)."
"10.1140/epjds/s13688-018-0154-8","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049836097&doi=10.1140%2fepjds%2fs13688-018-0154-8&partnerID=40&md5=4f12b7776ca822f0b247ee58fbaaf83d","In this paper, we follow the short-ranged Syrian refugees’ migration to Lebanon as documented by the UNHCR. We propose a model inspired by the Debye–Hückel theory and show that it properly predicts the refugees’ mobility while the gravity model fails. We claim that the interaction between origin cities attenuates and/or extenuates the flux to destinations, and consequently, in analogy with the effective charges of interacting particles in a plasma, these source cities are characterized by effective populations determined by their pairwise remoteness/closeness and defined by areas of control between the fighting parties. © 2018, The Author(s)."
"10.1140/epjds/s13688-018-0149-5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049626449&doi=10.1140%2fepjds%2fs13688-018-0149-5&partnerID=40&md5=77175f325ed1f678c212f414a036422a","Popularity prediction has been studied in diverse online contexts with demonstrable practical, sociological and technical benefit. Here, we add to the popularity prediction literature by studying the popularity of recipes on two large and well visited online recipe portals (Allrecipes.com, USA and Kochbar.de, Germany). Our analyses show differences between the platforms in terms of how the recipes are interacted with and categorized, as well as in the content of the food and its nutritional properties. For both datasets, we were able to show correlations between recipe features and proxies for popularity, which allow popularity of dishes to be predicted with some accuracy. The trends were more prominent in the Kochbar.de dataset, which was mirrored in the results of the prediction task experiments. © 2018, The Author(s)."
"10.1140/epjds/s13688-018-0151-y","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049563234&doi=10.1140%2fepjds%2fs13688-018-0151-y&partnerID=40&md5=2a6e61fce0a7e001c91f40eb556fe658","As cities grow, certain neighborhoods experience a particularly high demand for housing, resulting in escalating rents. Despite far-reaching socioeconomic consequences, it remains difficult to predict when and where urban neighborhoods will face such changes. To tackle this challenge, we adapt the concept of ‘bioindicators’, borrowed from ecology, to the urban context. The objective is to use an ‘indicator group’ of people to assess the quality of a complex environment and its changes over time. Specifically, we analyze 92 million geolocated Twitter records across five US cities, allowing us to derive socio-economic user profiles based on individual movement patterns. As a proof-of-concept, we define users with a ‘high-income-profile’ as an indicator group and show that their visitation patterns are a suitable indicator for expected future rent increases in different neighborhoods. The concept of indicator groups highlights the potential of closely monitoring only a specific subset of the population, rather than the population as a whole. If the indicator group is defined appropriately for the phenomenon of interest, this approach can yield early predictions while simultaneously reducing the amount of data that needs to be collected and analyzed. © 2018, The Author(s)."
"10.1140/epjds/s13688-018-0146-8","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049528018&doi=10.1140%2fepjds%2fs13688-018-0146-8&partnerID=40&md5=97b8c92c782ea03dabff4618f2a48a24","As dominant factors driving human actions, personalities can be excellent indicators to predict the offline and online behavior of individuals. However, because of the great expense and inevitable subjectivity in questionnaires and surveys, it is challenging for conventional studies to explore the connection between personality and behavior and to gain insight in the context of a large number of individuals. Considering the increasingly important role of online social media in daily communications, we argue that the footprints of massive numbers of individuals, such as tweets on Weibo, can be used as a proxy to infer personality and further understand its function in shaping online human behavior. In this study, a map from self-reports of personalities to online profiles of 293 active users on Weibo is established to train a competent machine learning model, which then successfully identifies more than 7000 users as extroverts or introverts. Systematic comparison from the perspectives of tempo-spatial patterns, online activities, emotional expressions and attitudes to virtual honors show that extroverts indeed behave differently from introverts on Weibo. Our findings provide solid evidence to justify the methodology of employing machine learning to objectively study the personalities of a massive number of individuals and shed light on applications of probing personalities and corresponding behaviors solely through online profiles. © 2018, The Author(s)."
"10.1140/epjds/s13688-018-0147-7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049466074&doi=10.1140%2fepjds%2fs13688-018-0147-7&partnerID=40&md5=a66d12deb9bc097c356fa3e4b7565c17","As users of mobile devices make phone calls, browse the web, or use an app, large volumes of data are routinely generated that are a potentially useful source for investigating human behavior in space. However, as such data are usually collected only as a by-product, they often lack stringent experimental design and ground truth, which makes interpretation and derivation of valid behavioral conclusions challenging. Here, we propose an unsupervised, data-driven approach to identify different user types based on high-resolution human movement data collected from a smartphone navigation app, in the absence of ground truth. We capture spatio-temporal footprints of users, characterized by meaningful summary statistics, which are then used in an unsupervised step to identify user types. Based on an extensive dataset of users of the mobile navigation app Sygic in Australia, we show how the proposed methodology allows to identify two distinct groups of users: ‘travelers’, visiting different areas with distinct, salient characteristics, and ‘locals’, covering shorter distances and revisiting many of their locations. We verify our approach by relating user types to space use: we find that travelers and locals prefer to visit distinct, different locations in the Australian cities Sydney and Melbourne, as suggested independently by other studies. Although we use high-resolution GPS data, the proposed methodology is potentially transferable to low-resolution movement data (e.g. Call Detail Records), since we rely only on summary statistics. © 2018, The Author(s)."
"10.1140/epjds/s13688-018-0145-9","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048894901&doi=10.1140%2fepjds%2fs13688-018-0145-9&partnerID=40&md5=b0a4a8147406d6cbed0b72cb3d8432c1","Epidemic outbreaks are an important healthcare challenge, especially in developing countries where they represent one of the major causes of mortality. Approaches that can rapidly target subpopulations for surveillance and control are critical for enhancing containment and mitigation processes during epidemics. Using a real-world dataset from Ivory Coast, this work presents an attempt to unveil the socio-geographical heterogeneity of disease transmission dynamics. By employing a spatially explicit meta-population epidemic model derived from mobile phone Call Detail Records (CDRs), we investigate how the differences in mobility patterns may affect the course of a hypothetical infectious disease outbreak. We consider different existing measures of the spatial dimension of human mobility and interactions, and we analyse their relevance in identifying the highest risk sub-population of individuals, as the best candidates for isolation countermeasures. The approaches presented in this paper provide further evidence that mobile phone data can be effectively exploited to facilitate our understanding of individuals’ spatial behaviour and its relationship with the risk of infectious diseases’ contagion. In particular, we show that CDRs-based indicators of individuals’ spatial activities and interactions hold promise for gaining insight of contagion heterogeneity and thus for developing mitigation strategies to support decision-making during country-level epidemics. © 2018, The Author(s)."
"10.1140/epjds/s13688-018-0144-x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048315618&doi=10.1140%2fepjds%2fs13688-018-0144-x&partnerID=40&md5=88b3af475a26421b90f86736e82a978b","Billions of users of mobile phones, social media platforms, and other technologies generate an increasingly large volume of data that has the potential to be leveraged towards solving public health challenges. These and other big data resources tend to be most successful in epidemiological applications when utilized within an appropriate conceptual framework. Here, we demonstrate the importance of assumptions about host mobility in a framework for dynamic modeling of infectious disease spread among districts within a large urban area. Our analysis focused on spatial and temporal variation in the transmission of dengue virus (DENV) during a series of large seasonal epidemics in Lahore, Pakistan during 2011–2014. Similar to many directly transmitted diseases, DENV transmission occurs primarily where people spend time during daytime hours, given that DENV is transmitted by a day-biting mosquito. We inferred spatiotemporal variation in DENV transmission under five different assumptions about mobility patterns among ten districts of Lahore: no movement among districts, movement following patterns of geo-located tweets, movement proportional to district population size, and movement following the commonly used gravity and radiation models. Overall, we found that inferences about spatiotemporal variation in DENV transmission were highly sensitive to this range of assumptions about intra-urban human mobility patterns, although the three assumptions that allowed for a modest degree of intra-urban mobility all performed similarly in key respects. Differing inferences about transmission patterns based on our analysis are significant from an epidemiological perspective, as they have different implications for where control efforts should be targeted and whether conditions for transmission became more or less favorable over time. © 2018, The Author(s)."
"10.1140/epjds/s13688-018-0143-y","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048277405&doi=10.1140%2fepjds%2fs13688-018-0143-y&partnerID=40&md5=915a56a09c64fb06a177019afe75b451","Recurrent interactions between agents play an essential role in the organization of a dynamic complex system. While intensive researches have been done on social systems formed by human interactions, dynamical rules are not well understood in economic systems. Here we study the evolution of financial networks and show that repeated interactions between financial institutions taking place at the daily scale are characterized by social communication patterns of humans emerging at higher time scales. The “social” dynamics of financial interactions are highly stable and little affected by external shocks such as the occurrence of the global financial crisis. A dynamic network model based on random pairwise matching accurately explains the observed daily dynamical patterns. The observed similarity between social and financial interactions gives us previously unknown stylized facts about a financial system, which could lead to a deeper understanding of the fundamental source of systemic risk. © 2018, The Author(s)."
"10.1140/epjds/s13688-018-0139-7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047868644&doi=10.1140%2fepjds%2fs13688-018-0139-7&partnerID=40&md5=032237ce24e4e5e8e08bde67bb8c0ee7","A multi-modal transportation system of a city can be modeled as a multiplex network with different layers corresponding to different transportation modes. These layers include, but are not limited to, bus network, metro network, and road network. Formally, a multiplex network is a multilayer graph in which the same set of nodes are connected by different types of relationships. Intra-layer relationships denote the road segments connecting stations of the same transportation mode, whereas inter-layer relationships represent connections between different transportation modes within the same station. Given a multi-modal transportation system of a city, we are interested in assessing its quality or efficiency by estimating the coverage i.e., a portion of the city that can be covered by a random walker who navigates through it within a given time budget, or steps. We are also interested in the robustness of the whole transportation system which denotes the degree to which the system is able to withstand a random or targeted failure affecting one or more parts of it. Previous approaches proposed a mathematical framework to numerically compute the coverage in multiplex networks. However solutions are usually based on eigenvalue decomposition, known to be time consuming and hard to obtain in the case of large systems. In this work, we propose MUME, an efficient algorithm for Multi-modal Urban Mobility Estimation, that takes advantage of the special structure of the supra-Laplacian matrix of the transportation multiplex, to compute the coverage of the system. We conduct a comprehensive series of experiments to demonstrate the effectiveness and efficiency of MUME on both synthetic and real transportation networks of various cities such as Paris, London, New York and Chicago. A future goal is to use this experience to make projections for a fast growing city like Doha. © 2018, The Author(s)."
"10.1140/epjds/s13688-018-0142-z","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047271125&doi=10.1140%2fepjds%2fs13688-018-0142-z&partnerID=40&md5=f8aa4d54af01205de620d00fde9a1e05","Estimating revenue and business demand of a newly opened venue is paramount as these early stages often involve critical decisions such as first rounds of staffing and resource allocation. Traditionally, this estimation has been performed through coarse-grained measures such as observing numbers in local venues or venues at similar places (e.g., coffee shops around another station in the same city). The advent of crowdsourced data from devices and services carried by individuals on a daily basis has opened up the possibility of performing better predictions of temporal visitation patterns for locations and venues. In this paper, using mobility data from Foursquare, a location-centric platform, we treat venue categories as proxies for urban activities and analyze how they become popular over time. The main contribution of this work is a prediction framework able to use characteristic temporal signatures of places together with k-nearest neighbor metrics capturing similarities among urban regions, to forecast weekly popularity dynamics of a new venue establishment in a city neighborhood. We further show how we are able to forecast the popularity of the new venue after one month following its opening by using locality and temporal similarity as features. For the evaluation of our approach we focus on London. We show that temporally similar areas of the city can be successfully used as inputs of predictions of the visit patterns of new venues, with an improvement of 41% compared to a random selection of wards as a training set for the prediction task. We apply these concepts of temporally similar areas and locality to the real-time predictions related to new venues and show that these features can effectively be used to predict the future trends of a venue. Our findings have the potential to impact the design of location-based technologies and decisions made by new business owners. © 2018, The Author(s)."
"10.1140/epjds/s13688-018-0140-1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046692352&doi=10.1140%2fepjds%2fs13688-018-0140-1&partnerID=40&md5=51c5488b34b1b34f24b97d0667dd93b5","Technological advances have led to a strong increase in the number of data collection efforts aimed at measuring co-presence of individuals at different spatial resolutions. It is however unclear how much co-presence data can inform us on actual face-to-face contacts, of particular interest to study the structure of a population in social groups or for use in data-driven models of information or epidemic spreading processes. Here, we address this issue by leveraging data sets containing high resolution face-to-face contacts as well as a coarser spatial localisation of individuals, both temporally resolved, in various contexts. The co-presence and the face-to-face contact temporal networks share a number of structural and statistical features, but the former is (by definition) much denser than the latter. We thus consider several down-sampling methods that generate surrogate contact networks from the co-presence signal and compare them with the real face-to-face data. We show that these surrogate networks reproduce some features of the real data but are only partially able to identify the most central nodes of the face-to-face network. We then address the issue of using such down-sampled co-presence data in data-driven simulations of epidemic processes, and in identifying efficient containment strategies. We show that the performance of the various sampling methods strongly varies depending on context. We discuss the consequences of our results with respect to data collection strategies and methodologies. © 2018, The Author(s)."
"10.1140/epjds/s13688-018-0141-0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046676792&doi=10.1140%2fepjds%2fs13688-018-0141-0&partnerID=40&md5=e0641899d5b4cd508f9173809f1f93cf","International aid is a complex system: it involves different issues, countries, and donors. In this paper, we use web crawling to collect information about the activities of international aid organizations on different health-related topics and network analysis to depict this complex system of relationships among organizations. By systematically collecting co-occurrences of issues, countries, and organization names from more than a hundred websites, we are able to construct multilayer networks describing, for instance, which issues are related to each other according to which organizations. Our results show that there is a surprising amount of homophily among organizations: organizations of the same type (multilateral, bilateral, private donors, etc.) tend to be co-cited in groups. We also create a taxonomy of issues that are generally mentioned together. Finally, we perform simulations, showing that messages originating from different organizations in the international aid community can have a different reach. © 2018, The Author(s)."
"10.1140/epjds/s13688-018-0137-9","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045763588&doi=10.1140%2fepjds%2fs13688-018-0137-9&partnerID=40&md5=7b05a36e238662955785e2c53f9d5c95","Using a large dataset with individual-level demographic information of almost 60,000 families in contemporary Finland, we analyse the regional variation and cultural assortativity by studying the network between families and the network between kins. For the network of families the largest connected component is found to consist of around 1000 families mostly originated from one single region in Western Finland. We characterize the networks in terms of the basic structural properties. In particular, we focus on the k-cores and the presence of transitive triangles. Clustering in the networks is found to result from homophily by language and religious affiliations. The large network fragments appear to be small-worlds. We also compare the fragments in the kin network with respect to the average coefficient of relationship. The measures of assortativity are able to distinguish the families in terms of their regions of origin. Overall, we distinguish between two patterns of regional effects, the ‘metropolitan’ and the ‘cultural’ pattern. © 2018, The Author(s)."
"10.1140/epjds/s13688-018-0136-x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045402942&doi=10.1140%2fepjds%2fs13688-018-0136-x&partnerID=40&md5=638d3865cc6fc0d437676560c1558936","Societal unrest and similar events are important for societies, but it is often difficult to quantify their effects on individuals, hindering a timely and effective policy-making in emergencies and in particular localized social shocks such as protests. Traditionally, effects are assessed through economic indicators or surveys with relatively low temporal and spatial resolutions. In this work, we compute two behavioral indexes, based on the use of credit card transaction data, for measuring the economic effects of a series of protests on consumer actions and personal consumption. Using data from a metropolitan area in an OECD country, we show that protests affect consumers’ shopping frequency and spending, but in noticeably different ways. The effects show strong temporal and spatial patterns, vary between neighborhoods and customers of different socio-demographical characteristics as well as between merchants of different categories, and suggest interesting subtleties in purchase behavior such as displaced or delayed shopping activities. Our method can generally serve for the real-time monitoring of the effects of major social shocks or events on urban economy and consumer sentiment, providing high-resolution and cost-effective measurement tools to complement traditional economic indicators. © 2018, The Author(s)."
"10.1140/epjds/s13688-018-0135-y","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045125233&doi=10.1140%2fepjds%2fs13688-018-0135-y&partnerID=40&md5=954a8ade898c496f56017b8f79dd0042","Reading remains the preferred leisure activity for most individuals, continuing to offer a unique path to knowledge and learning. As such, books remain an important cultural product, consumed widely. Yet, while over 3 million books are published each year, very few are read widely and less than 500 make it to the New York Times bestseller lists. And once there, only a handful of authors can command the lists for more than a few weeks. Here we bring a big data approach to book success by investigating the properties and sales trajectories of bestsellers. We find that there are seasonal patterns to book sales with more books being sold during holidays, and even among bestsellers, fiction books sell more copies than nonfiction books. General fiction and biographies make the list more often than any other genre books, and the higher a book’s initial place in the rankings, the longer the book stays on the list as well. Looking at patterns characterizing authors, we find that fiction writers are more productive than nonfiction writers, commonly achieving bestseller status with multiple books. Additionally, there is no gender disparity among bestselling fiction authors but nonfiction, most bestsellers are written by male authors. Finally we find that there is a universal pattern to book sales. Using this universality we introduce a statistical model to explain the time evolution of sales. This model not only reproduces the entire sales trajectory of a book but also predicts the total number of copies it will sell in its lifetime, based on its early sales numbers. The analysis of the bestseller characteristics and the discovery of the universal nature of sales patterns with its driving forces are crucial for our understanding of the book industry, and more generally, of how we as a society interact with cultural products. © 2018, The Author(s)."
"10.1140/epjds/s13688-018-0133-0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85043387254&doi=10.1140%2fepjds%2fs13688-018-0133-0&partnerID=40&md5=795657967d36b32db30b9a20dd4da61c","In this paper we investigate the regularities characterizing the temporal purchasing behavior of the customers of a retail market chain. Most of the literature studying purchasing behavior focuses on what customers buy while giving few importance to the temporal dimension. As a consequence, the state of the art does not allow capturing which are the temporal purchasing patterns of each customers. These patterns should describe the customer’s temporal habits highlighting when she typically makes a purchase in correlation with information about the amount of expenditure, number of purchased items and other similar aggregates. This knowledge could be exploited for different scopes: set temporal discounts for making the purchases of customers more regular with respect the time, set personalized discounts in the day and time window preferred by the customer, provide recommendations for shopping time schedule, etc. To this aim, we introduce a framework for extracting from personal retail data a temporal purchasing profile able to summarize whether and when a customer makes her distinctive purchases. The individual profile describes a set of regular and characterizing shopping behavioral patterns, and the sequences in which these patterns take place. We show how to compare different customers by providing a collective perspective to their individual profiles, and how to group the customers with respect to these comparable profiles. By analyzing real datasets containing millions of shopping sessions we found that there is a limited number of patterns summarizing the temporal purchasing behavior of all the customers, and that they are sequentially followed in a finite number of ways. Moreover, we recognized regular customers characterized by a small number of temporal purchasing behaviors, and changing customers characterized by various types of temporal purchasing behaviors. Finally, we discuss on how the profiles can be exploited both by customers to enable personalized services, and by the retail market chain for providing tailored discounts based on temporal purchasing regularity. © 2018, The Author(s)."
"10.1140/epjds/s13688-018-0134-z","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85043313645&doi=10.1140%2fepjds%2fs13688-018-0134-z&partnerID=40&md5=f16de7c18b412827b15e065908d9aeaa","The features of collaboration patterns are often considered to be different from discipline to discipline. Meanwhile, collaborating among disciplines is an obvious feature emerged in modern scientific research, which incubates several interdisciplines. The features of collaborations in and among the disciplines of biological, physical and social sciences are analyzed based on 52,803 papers published in a multidisciplinary journal PNAS during 1999 to 2013. From those data, we found similar transitivity and assortativity of collaboration patterns as well as the identical distribution type of collaborators per author and that of papers per author, namely a mixture of generalized Poisson and power-law distributions. In addition, we found that interdisciplinary research is undertaken by a considerable fraction of authors, not just those with many collaborators or those with many papers. This case study provides a window for understanding aspects of multidisciplinary and interdisciplinary collaboration patterns. © 2018, The Author(s)."
"10.1140/epjds/s13688-018-0132-1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042261871&doi=10.1140%2fepjds%2fs13688-018-0132-1&partnerID=40&md5=64c14c347c6067003fdc421b073a7504","The world is undergoing a process of fast and unprecedented urbanisation. It is reported that by 2050 66% of the entire world population will live in cities. Although this phenomenon is generally considered beneficial, it is also causing housing crises and more inequality worldwide. In the past, the relationship between design features of cities and socio-economic levels of their residents has been investigated using both qualitative and quantitative methods. However, both sets of works had significant limitations as the former lacked generalizability and replicability, while the latter had a too narrow focus, since they tended to analyse single aspects of the urban environment rather than a more complex set of metrics. This might have been caused by the lack of data availability. Nowadays, though, larger and freely accessible repositories of data can be used for this purpose. In this paper, we propose a scalable method that delves deeper into the relationship between features of cities and socio-economics. The method uses openly accessible datasets to extract multiple metrics of urban form and then models the relationship between urban form and socio-economic levels through spatial regression analysis. We applied this method to the six major conurbations (i.e., London, Manchester, Birmingham, Liverpool, Leeds, and Newcastle) of the United Kingdom (UK) and found that urban form could explain up to 70% of the variance of the English official socio-economic index, the Index of Multiple Deprivation (IMD). In particular, results suggest that more deprived UK neighbourhoods are characterised by higher population density, larger portions of unbuilt land, more dead-end roads, and a more regular street pattern. © 2018, The Author(s)."
"10.1140/epjds/s13688-017-0129-1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042101304&doi=10.1140%2fepjds%2fs13688-017-0129-1&partnerID=40&md5=17b07ffe88974101d1eddce827f33565","Predictive models for human mobility have important applications in many fields including traffic control, ubiquitous computing, and contextual advertisement. The predictive performance of models in literature varies quite broadly, from over 90% to under 40%. In this work we study which underlying factors - in terms of modeling approaches and spatio-temporal characteristics of the data sources - have resulted in this remarkably broad span of performance reported in the literature. Specifically we investigate which factors influence the accuracy of next-place prediction, using a high-precision location dataset of more than 400 users observed for periods between 3 months and one year. We show that it is much easier to achieve high accuracy when predicting the time-bin location than when predicting the next place. Moreover, we demonstrate how the temporal and spatial resolution of the data have strong influence on the accuracy of prediction. Finally we reveal that the exploration of new locations is an important factor in human mobility, and we measure that on average 20-25% of transitions are to new places, and approx. 70% of locations are visited only once. We discuss how these mechanisms are important factors limiting our ability to predict human mobility. © 2018, The Author(s)."
"10.1140/epjds/s13688-018-0130-3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040745624&doi=10.1140%2fepjds%2fs13688-018-0130-3&partnerID=40&md5=23330fb690f08bd5fc1ffa37f73409d1","Preserving individual control over private information is one of the rising concerns in our digital society. Online social networks exist in application ecosystems that allow them to access data from other services, for example gathering contact lists through mobile phone applications. Such data access might allow social networking sites to create shadow profiles with information about non-users that has been inferred from information shared by the users of the social network. This possibility motivates the shadow profile hypothesis: the data shared by the users of an online service predicts personal information of non-users of the service. We test this hypothesis for the first time on Twitter, constructing a dataset of users that includes profile biographical text, location information, and bidirectional friendship links. We evaluate the predictability of the location of a user by using only information given by friends of the user that joined Twitter before the user did. This way, we audit the historical prediction power of Twitter data for users that had not joined Twitter yet. Our results indicate that information shared by users in Twitter can be predictive of the location of individuals outside Twitter. Furthermore, we observe that the quality of this prediction increases with the tendency of Twitter users to share their mobile phone contacts and is more accurate for individuals with more contacts inside Twitter. We further explore the predictability of biographical information of non-users, finding evidence in line with our results for locations. These findings illustrate that individuals are not in full control of their online privacy and that sharing personal data with a social networking site is a decision that is collectively mediated by the decisions of others. © 2018, The Author(s)."
"10.1186/s40537-018-0158-z","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059353684&doi=10.1186%2fs40537-018-0158-z&partnerID=40&md5=7dee52a9ce550967854e913be7d7e73b","Using the theory of affordance from perceptual psychology and through discussion of literature within visual data mining and immersive analytics, a position for the multi-sensory representation of big data using virtual reality (VR) is developed. While it would seem counter intuitive, information-dense virtual environments are theoretically easier to process than simplified graphic encoding—if there is alignment with human ecological perception of natural environments. Potentially, VR affords insight into patterns and anomalies through dynamic experience of data representations within interactive, kinaesthetic audio-visual virtual environments. To this end we articulate principles that can inform the development of VR applications for immersive analytics: a mimetic approach to data mapping that aligns spatial, aural and kinaesthetic attributes with abstractions of natural environments; layered with constructed features that complement natural structures; the use of cross-modal sensory mapping; a focus on intermediate levels of contrast; and the adaptation of naturally occurring distribution patterns for the granularity and distribution of data. While it appears problematic to directly translate visual data mining techniques to VR, the ecological approach to human perception discussed in this article provides a new framework for big data visualization researchers to consider. © 2018, The Author(s)."
"10.1186/s40537-018-0163-2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059346732&doi=10.1186%2fs40537-018-0163-2&partnerID=40&md5=c6d1b992bc4a0b9e479838214881cbd5","Measuring pairwise document similarity is an essential operation in various text mining tasks. Most of the similarity measures judge the similarity between two documents based on the term weights and the information content that two documents share in common. However, they are insufficient when there exist several documents with an identical degree of similarity to a particular document. This paper introduces a novel text document similarity measure based on the term weights and the number of terms appeared in at least one of the two documents. The effectiveness of our measure is evaluated on two real-world document collections for a variety of text mining tasks, such as text document classification, clustering, and near-duplicates detection. The performance of our measure is compared with that of some popular measures. The experimental results showed that our proposed similarity measure yields more accurate results. © 2018, The Author(s)."
"10.1186/s40537-018-0161-4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059041921&doi=10.1186%2fs40537-018-0161-4&partnerID=40&md5=a3bf9b4229ab71d4e6d0b648521b4e0e","Query processing based on labeling dynamic XML documents has gained more attention in the past several years. An efficient labeling scheme should provide small size labels keeping the simplicity of the exploited algorithm in order to avoid complex computations as well as retaining the readability of structural relationships between nodes. Moreover, for dynamic XML data, relabeling the nodes in XML updates should be avoided. However, the existing schemes lack the capability of supporting all of these requirements. In this paper, we propose a new labeling scheme which assigns variable-length labels to nodes in dynamic XML documents. Our method employs the FibLSS encoding scheme that exploits the properties of the Fibonacci sequence to provide variable-length node labels of appropriate size. In XML updating process, we add a new section only in the new node’s label without relabeling the existing nodes while keeping the order of nodes as well as preserving the structural relationships. Our labeling method is scalable as it is not subject to overflow, and as the number of nodes to be labeled increases exponentially, the size of labels grows linearly, which makes it suitable for big datasets. It also has the best performance in computational processing costs compared to existing approaches. The results of the experiments confirm the advantages of our proposed method in comparison to state-of-the-art techniques. © 2018, The Author(s)."
"10.1186/s40537-018-0160-5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058861651&doi=10.1186%2fs40537-018-0160-5&partnerID=40&md5=5c152e0eee4bdb72601a2a9ddc39ad83","This paper intends to contribute to the field of trend forecasting by proposing a new forecasting approach for stock market prices and geopolitical time series data of economic, financial and geopolitical importance. Designing models which account for every possible exogenous variable of relevance to a time series in question can often be an onerous and impractical task. Instead, this paper explores a new method which uses periods of decreased significance in the variable of foremost importance as a window of opportunity to observe the possible effects other variables may be having in a general way for the purpose of trend forecasting. When the latter variables are too unquantifiable to be accounted for in a model, having the ability to nonetheless discern their overall influence can be useful for anticipating trend changes. The proposed method was used in conjunction with the existing method of exponential smoothing to generate forecasts. It was also applied alone and contrasted with the results of exponential smoothing when used separately. This paper specifically addresses the ability of the newly proposed method to forecast the upwards/downwards extrapolation of the weekly trend for 9 weeks on stock closing prices for five companies of interest (Apple Inc, Amazon.com Inc, General Electric Company, Intel Corporation, and Alcoa Corporation). It was also applied to forecasting the annual trend for 9 years of Afghan asylum seeker data. These differing areas were chosen in order to demonstrate applications in finance as well as international relations. The empirical results and 95% confidence intervals indicate a clear advantage when the newly proposed method is used both in conjunction with exponential smoothing and on its own. © 2018, The Author(s)."
"10.1186/s40537-018-0164-1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058839014&doi=10.1186%2fs40537-018-0164-1&partnerID=40&md5=667ac1f1957d2b838a4addcf534078bb","Big data encompasses social networking websites including Twitter as popular micro-blogging social media platform for a political campaign. The explosive Twitter data as a respond of the political campaign can be used to predict the Presidential election as has been conducted to predict the political election in several countries such as US, UK, Spain, and French. The authors use tweets from President Candidates of Indonesia (Jokowi and Prabowo), and tweets from relevant hashtags for sentiment analysis gathered from March to July 2018 to predict Indonesian Presidential election result. The authors make an algorithm and method to count important data, top words and train the model and predict the polarity of the sentiment. The experimental result is produced by using R language and show that Jokowi leads the current election prediction. This prediction result is corresponding to four survey institutes in Indonesia that proved our method had produced reliable prediction results. © 2018, The Author(s)."
"10.1186/s40537-018-0159-y","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058472583&doi=10.1186%2fs40537-018-0159-y&partnerID=40&md5=f12f19f85fa71ce4149b333b9924ae13","Regression trees (RTs) are simple, but powerful models, which have been widely used in the last decades in different scopes. Fuzzy RTs (FRTs) add fuzziness to RTs with the aim of dealing with uncertain environments. Most of the FRT learning approaches proposed in the literature aim to improve the accuracy, measured in terms of mean squared error, and often neglect to consider the computation time and/or the memory requirements. In today’s application domains, which require the management of huge amounts of data, this carelessness can strongly limit their use. In this paper, we propose a distributed FRT (DFRT) learning scheme for generating binary RTs from big datasets, that is based on the MapReduce paradigm. We have designed and implemented the scheme on the Apache Spark framework. We have used eight real-world and four synthetic datasets for evaluating its performance, in terms of mean squared error, computation time and scalability. As a baseline, we have compared the results with the distributed RT (DRT) and the Distributed Random Forest (DRF) available in the Spark MLlib library. Results show that our DFRT scales similarly to DRT and better than DRF. Regarding the performance, DFRT generalizes much better than DRT and similarly to DRF. © 2018, The Author(s)."
"10.1186/s40537-018-0157-0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058100572&doi=10.1186%2fs40537-018-0157-0&partnerID=40&md5=03d3d29a70f9b180ba7386c9d401dac3","Short term traffic forecasting is one of the important fields of study in the transportation domain. Short term traffic forecasting is very useful to develop a more advanced transportation system to control traffic signals and avoid congestions. Several studies have made efforts for short term traffic flow forecasting for divided and undivided highways across the world. However, all these studies relied on the dataset which are greatly varied between countries due to the technology used for transportation data collection. India is a developing country in which efforts are being done to improve the transportation system to avoid congestion and travel time. Two-lane undivided highways with mixed traffic constitute a large portion of Indian road network. This study is an attempt to develop a short term traffic forecasting model using back propagation artificial neural network for two lane undivided highway with mixed traffic conditions in India. The results were compared with random forest, support vector machine, k-nearest neighbor classifier, regression tree and multiple regression models. It was found that back-propagation neural network performs better than other approaches and achieved an R 2 value 0.9962, which is a good score. © 2018, The Author(s)."
"10.1186/s40537-018-0156-1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058048573&doi=10.1186%2fs40537-018-0156-1&partnerID=40&md5=f24c133db333ae085763516b6656d578","New generation databases also called NoSQL (Not only SQL) databases are highly scalable, flexible, and low-latent. These types of databases emerge as a result of the rigidity shown by traditional databases to handle today’s data which is voluminous, highly diversified and generated at a very high rate. With NoSQL, problems such as database expansion difficulties, low query performance and low storage capacity are addressed. However, the inherent complexity of contemporary datasets coupled with programmers’ low NoSQL modeling competence are increasingly making database modeling and design vastly challenging, especially when parameters like consistency, availability and scalability are to be balanced in accordance with system requirements. As such, a schema suggestion model for NoSQL databases is posed to address this balancing issue. The proposed model aims to abstractly suggest schemas at the initial stage of system development based on user defined system requirements and CRUD (Create, Read, Update and Delete) operations among others. This is achieved through the adaptation of exploratory and experimental approaches of research. Also, few mathematical formulas are introduced to calculate clusters availability during entity mappings. A comparison was conducted between the schema produced using the proposed model and the one without. Results obtained shows substantial improvement in the areas of security and read–write query performance. © 2018, The Author(s)."
"10.1186/s40537-018-0155-2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057112468&doi=10.1186%2fs40537-018-0155-2&partnerID=40&md5=932c891f537313c97c3db5fdab4d9e4a","In manufacturing, the technology to capture and store large volumes of data developed earlier and faster than corresponding capabilities to analyze, interpret, and apply it. The result for many manufacturers is a collection of unanalyzed data and uncertainty with respect to where to begin. This paper examines big data as both an enabler and a challenge for the connected manufacturing enterprise and presents a framework that sequentially tests and selects independent variables for training applied machine learning models. Unsuitable features are discarded, and each remaining feature receives a crisp numeric output and a linguistic label, both of which are measures of the feature’s suitability. The framework is tested using three datasets employing time series, binary, and continuous input data. Results of filtered models are compared to results obtained by base, unfiltered sets of features using a proposed metric of performance-size ratio. Framework results outperform base feature sets in all tested cases, and the proposed future research will be to implement it in a case study in the electronic assembly manufacture. © 2018, The Author(s)."
"10.1186/s40537-018-0152-5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056585031&doi=10.1186%2fs40537-018-0152-5&partnerID=40&md5=93504ac258c73d18803055d4a286c47c","Sentiment classification or sentiment analysis has been acknowledged as an open research domain. In recent years, an enormous research work is being performed in these fields by applying numerous methodologies. Feature generation and selection are consequent for text mining as the high dimensional feature set can affect the performance of sentiment analysis. This paper investigates the inability of the widely used feature selection method (IG, Chi Square, Gini Index) individually as well as their combined approach on four machine learning classification algorithm. The proposed methods are evaluated on three standard datasets viz. IMDb movie review, electronics and kitchen product review dataset. Initially, select the feature subsets from three different feature selection methods. Thereafter, statistical method UNION, INTERSECTION and revised UNION method are applied to merge these different feature subsets to obtain all top ranked including common selected features. Finally, train the classifier SMO, MNB, RF, and LR (logistic regression) with this feature vector for classification of the review data set. The performance of the algorithm is measured by evaluation methods such as precision, recall, F-measure and ROC curve. Experimental results show that the combined method achieved best accuracy of 92.31 with classifier SMO, which is encouraging and comparable to the related research. © 2018, The Author(s)."
"10.1186/s40537-018-0154-3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056396615&doi=10.1186%2fs40537-018-0154-3&partnerID=40&md5=161dc06a012861a77a7152f0bc267f51","Introduction: Mass appraisals in the rental housing market are far less common than those in the sales market. However, there is evidence for substantial growth in the rental market and this lack of insight hampers commercial organisations and local and national governments in understanding this market. Case description: This case study uses data that are supplied from a property listings web site and are unique in their scale, with over 1.2 million rental property listings available over a 2 year period. The data is analysed in a large data institute using generalised linear regression, machine learning and a pseudo practitioner based approach. Discussion and evaluation: The study should be seen as a practical guide for property professionals and academics wishing to undertake such appraisals and looking for guidance on the best methods to use. It also provides insight into the property characteristics which most influence rental listing price. Conclusions: From the regression analysis, attributes that increase the rental listing price are: the number of rooms in the property, proximity to central London and to railway stations, being located in more affluent neighbourhoods and being close to local amenities and better performing schools. Of the machine learning algorithms used, the two tree based approaches were seen to outperform the regression based approaches. In terms of a simple measure of the median appraisal error, a practitioner based approach is seen to outperform the modelling approaches. A practical finding is that the application of sophisticated machine learning algorithms to big data is still a challenge for modern desktop PCs. © 2018, The Author(s)."
"10.1186/s40537-018-0151-6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056084218&doi=10.1186%2fs40537-018-0151-6&partnerID=40&md5=b9a320bfc6451e2a04ec19e2c0d8335a","In a majority–minority classification problem, class imbalance in the dataset(s) can dramatically skew the performance of classifiers, introducing a prediction bias for the majority class. Assuming the positive (minority) class is the group of interest and the given application domain dictates that a false negative is much costlier than a false positive, a negative (majority) class prediction bias could have adverse consequences. With big data, the mitigation of class imbalance poses an even greater challenge because of the varied and complex structure of the relatively much larger datasets. This paper provides a large survey of published studies within the last 8 years, focusing on high-class imbalance (i.e., a majority-to-minority class ratio between 100:1 and 10,000:1) in big data in order to assess the state-of-the-art in addressing adverse effects due to class imbalance. In this paper, two techniques are covered which include Data-Level (e.g., data sampling) and Algorithm-Level (e.g., cost-sensitive and hybrid/ensemble) Methods. Data sampling methods are popular in addressing class imbalance, with Random Over-Sampling methods generally showing better overall results. At the Algorithm-Level, there are some outstanding performers. Yet, in the published studies, there are inconsistent and conflicting results, coupled with a limited scope in evaluated techniques, indicating the need for more comprehensive, comparative studies. © 2018, The Author(s)."
"10.1186/s40537-018-0150-7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055696595&doi=10.1186%2fs40537-018-0150-7&partnerID=40&md5=f9a23d23fa9cdfbf7080890a463dbbf1","In the era of Big Data, with the increasing use of large-scale data-driven applications, visualization of very large high-resolution images and extracting useful information (searching for specific targets or rare signal events) from these images can pose challenges to the current display wall technologies. At Bellarmine University, we have set up an Advanced Visualization and Computational Lab using a state-of-the-art next generation display wall technology, called Hiperwall (Highly Interactive Parallelized Display Wall). The 16 ft × 4.5 ft Hiperwall visualization system has a total resolution of 16.5 Megapixels (MP) which consists of eight display-tiles that are arranged in a 4 × 2 tile configuration. Using Hiperwall, we can perform interactive visual data analytics of large images by conducting comparative views of multiple large images in Astronomy and multiple event displays in experimental High Energy Physics. Users can display a single large image across all the display-tiles, or view many different images simultaneously on multiple display-tiles. Hiperwall enables simultaneous visualization of multiple high resolution images and its contents on the entire display wall without loss of clarity and resolution. Hiperwall’s middleware also allows researchers in geographically diverse locations to collaborate on large scientific experiments. In this paper we will provide a description of a new generation of display wall setup at Bellarmine University that is based on the Hiperwall technology, which is a robust visualization system for Big Data research. © 2018, The Author(s)."
"10.1186/s40537-018-0144-5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055621865&doi=10.1186%2fs40537-018-0144-5&partnerID=40&md5=5a130a66295fb5b1707643480ae6e579","Currently, immense quantities of data cannot be managed by traditional database management systems. Instead, they must be managed by big data solutions using shared nothing architectures. Data warehouse systems are systems that address very large amounts of information. The most prominent data warehouse model is star schema, which consists of a fact table and some number of dimension tables. It is necessary to join the facts and dimensions for query executions on the data warehouse. In shared nothing architecture, all of the required information is not placed on a single node so it is necessary to retrieve information from other nodes, which causes network congestion and low speeds of query execution. To avoid this problem and achieve maximum parallelism, dimensions can be replicated over nodes if they are not too large. However, if there are dimensions with data volumes greater than the capacity of a node or dimensions where the data volume summation exceeds node capacity, the query execution is confronted with serious problems. In big data problems, the amount of data is immense, and thus replicating immense data cannot be considered an appropriate method. In this paper, we propose a method called Chabok, which uses two-phased Map-Reduce to solve the data warehouse problem. In this method, aggregation is performed completely on Mappers, and intermediate results are sent to the Reducer. Chabok does not need data replication for join omission. The proposed method was implemented on Hadoop, and TPC-DS queries were executed for benchmarking. The query execution time on Chabok surpassed prominent big data products for data warehousing. © 2018, The Author(s)."
"10.1186/s40537-018-0147-2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055094958&doi=10.1186%2fs40537-018-0147-2&partnerID=40&md5=976e35ad51de65bf9353c18745c98edb","This paper presents a framework for discovering similar users on Twitter that can be used in profiling users for social, recruitment and security reasons. The framework contains a novel formula that calculates the similarity between users on Twitter by using seven different signals (features). The signals are followings and followers, mention, retweet, favorite, common hashtag, common interests, and profile similarity. The proposed framework is scalable and can handle big data because it is implemented using the MapReduce paradigm. It is also adjustable since the weight and contribution of each signal in calculating the final similarity score is determined by the user based on their needs. The accuracy of the system was evaluated through human judges and by comparing the system’s results against Twitter’s Who To Follow service. The results show moderately accurate results. © 2018, The Author(s)."
"10.1186/s40537-018-0148-1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054923233&doi=10.1186%2fs40537-018-0148-1&partnerID=40&md5=49111f0b6bdbbbb87319a8d7d8aeb3c5","Objective: To predict the next-year status in patients with rheumatoid arthritis using big data. Methods: Joint index (JI) of upper/large (UL), upper/small (US), lower/large (LL), and lower/small (LS) was calculated as the sum of tender and swollen joint counts divided by the number of evaluable joints in each region of interest. Joint index vector V (x, y, z) was defined as x = JIUL + JIUS, y = JILL + JILS, and z = JIUL + JILL − JIUS − JILS. Low disease activity was defined as |Vxy| (= √x2 + y2) ≤ 0.1. Patients with |Vxy| &gt; 0.1 were further classified into three groups: evenly affected (EVN): |z| ≤ 0.2, small joint dominant (SML): z &lt; − 0.2, and large joint dominant (LAR): z &gt; 0.2. To predict the next-year V (x, y, z) of each patient, a transformation matrix was computed from the mean vectors of the EVN, SML, and LAR groups and their translation vectors. Results: |Vxy| was correlated with Simplified Disease Activity Index (SDAI) (r = 0.82). Z of mean vector increased as the disability index of the Health Assessment Questionnaire (HAQ-DI) and the Steinbrocker class worsened. The LAR group had the worst HAQ-DI and the second highest SDAI after those in the SML group. Positive predictive value and likelihood ratio in predicting the LAR group were 58.7% and 5.9, respectively. Likelihood ratio was greater with treatment, at 7.2, 7.4, and 8.6 when targeted patients were treated with methotrexate, biologics, and both drugs, respectively. Conclusions: Patients with high disease activity and poor functional state were predicted with high probability using joint index vectors. © 2018, The Author(s)."
"10.1186/s40537-018-0149-0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054678301&doi=10.1186%2fs40537-018-0149-0&partnerID=40&md5=8cf567743d8d3fed3b1751d168dc7438","One of the biggest concerns of big data and analytics is privacy. We believe the forthcoming frameworks and theories will establish several solutions for the privacy protection. One of the known solutions is the k-anonymity that was introduced for traditional data. Recently, two major frameworks leveraged big data processing and applications; these are MapReduce and Spark. Spark data processing has been attracting more attention due to its crucial impacts on a wide range of big data applications. One of the predominant big data applications is data analytics and anonymization. We previously proposed an anonymization method for implementing k-anonymity in MapReduce processing framework. In this paper, we investigate Spark performance in processing data anonymization. Spark is a fast processing framework that was implemented in several applications such as: SQL, multimedia, and data stream. Our focus is the SQL Spark, which is adequate for big data anonymization. Since Spark operates in-memory, we need to observe its limitations, speed, and fault tolerance on data size increase, and to compare MapReduce to Spark in processing anonymity. Spark introduces an abstraction called resilient distributed datasets, which reads and serializes a collection of objects partitioned across a set of machines. Developers claim that Spark can outperform MapReduce by 10 times in iterative machine learning jobs. Our experiments in this paper compare between MapReduce and Spark. The overall results show a better performance for Spark’s processing time in anonymity operations. However, in some limited cases, we prefer to implement the old MapReduce framework, when the cluster resources are limited and the network is non-congested. © 2018, The Author(s)."
"10.1186/s40537-018-0146-3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054581701&doi=10.1186%2fs40537-018-0146-3&partnerID=40&md5=daa9d2ec4637639ce883a2e27d7337f7","MapReduce (MR) is a criterion of Big Data processing model with parallel and distributed large datasets. This model knows difficult problems related to low-level and batch nature of MR that gives rise to an abstraction layer on the top of MR. Therefore; several High-Level MapReduce Query Languages built on the top of MR provide more abstract query languages and extend the MR programming model. These High-Level MapReduce Query Languages remove the burden of MR programming away from the developers and make a soft migration of existing competences with SQL skills to Big Data. This paper investigates the very used—common High-Level MapReduce Query Languages built directly on the top of MR that translate queries into executable native MR jobs. It evaluates the performance of the four presented High-Level MapReduce Query Languages: JAQL, Hive, Big SQL and Pig, with regards to their insightful perspectives and ease of programming. The baseline metrics reported are increasing input size, scale-out number of nodes and controlling number of reducers. The experimental results study the technical advantages and limitations of each High-Level MapReduce Query Languages. Finally, the paper provides a summary for developers to choose the High-Level MapReduce Query Languages which fulfill their needs and interests. © 2018, The Author(s)."
"10.1186/s40537-018-0142-7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054154073&doi=10.1186%2fs40537-018-0142-7&partnerID=40&md5=d5fe854ced245d98fbcc29f7c1dfd22d","One of the challenges in diagnosing stroke disease is the lack of useful analysis tool to identify critical stroke data that contains hidden relationships and trends from a vast amount of data. In order to address this problem, we proposed Intuitionistic Fuzzy Based Decision Tree in order to diagnosis the different types of stroke disease. The approach is implemented by mapping observation data into Intuitionistic Fuzzy Set. These results lead to a compound of a membership function, non-membership function, and a hesitation degree for each record. The result of Intuitionistic Fuzzy is calculated using Hamming Distance as main requirement for Intuitionistic Fuzzy Entropy. The Hamming Distance calculate the difference between values on the same variable. Main advantage of this approach is that we can find out variables effected on the stroke disease using information gain derived from Intutionistics Entropy. Furthermore, the Intuitionistic Fuzzy based Decision Tree are able to provide plenty of information to stakeholders regarding the hidden facts of established rules and utilize linguistic terms to accommodate unclearness, ambiguity, and hesitation in human perception. The results of Intuitionistic Fuzzy Entropy determine the root and node in the formation of the decision tree model based on the information gain of variables in the data. In this study, simulation results show that the approach successfully determine 20 variables that directly influence stroke. These variables are used to classify the types of stroke. Furthermore, results show that the approach has resulted in 90.59% in classifying stroke disease. Results of the study also demonstrates that the approach produces the best diagnosis performance compared to the other two models according to the accuracy of classification from the type of stroke disease. © 2018, The Author(s)."
"10.1186/s40537-018-0145-4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053860878&doi=10.1186%2fs40537-018-0145-4&partnerID=40&md5=d91f001524819ac4446868165b180591","Recently, the huge amounts of data and its incremental increase have changed the importance of information security and data analysis systems for Big Data. Intrusion detection system (IDS) is a system that monitors and analyzes data to detect any intrusion in the system or network. High volume, variety and high speed of data generated in the network have made the data analysis process to detect attacks by traditional techniques very difficult. Big Data techniques are used in IDS to deal with Big Data for accurate and efficient data analysis process. This paper introduced Spark-Chi-SVM model for intrusion detection. In this model, we have used ChiSqSelector for feature selection, and built an intrusion detection model by using support vector machine (SVM) classifier on Apache Spark Big Data platform. We used KDD99 to train and test the model. In the experiment, we introduced a comparison between Chi-SVM classifier and Chi-Logistic Regression classifier. The results of the experiment showed that Spark-Chi-SVM model has high performance, reduces the training time and is efficient for Big Data. © 2018, The Author(s)."
"10.1186/s40537-018-0141-8","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053705049&doi=10.1186%2fs40537-018-0141-8&partnerID=40&md5=5032ea35afab87316a0d865f2d90ece5","Incredible amounts of data is being generated by various organizations like hospitals, banks, e-commerce, retail and supply chain, etc. by virtue of digital technology. Not only humans but machines also contribute to data in the form of closed circuit television streaming, web site logs, etc. Tons of data is generated every minute by social media and smart phones. The voluminous data generated from the various sources can be processed and analyzed to support decision making. However data analytics is prone to privacy violations. One of the applications of data analytics is recommendation systems which is widely used by ecommerce sites like Amazon, Flip kart for suggesting products to customers based on their buying habits leading to inference attacks. Although data analytics is useful in decision making, it will lead to serious privacy concerns. Hence privacy preserving data analytics became very important. This paper examines various privacy threats, privacy preservation techniques and models with their limitations, also proposes a data lake based modernistic privacy preservation technique to handle privacy preservation in unstructured data. © 2018, The Author(s)."
"10.1186/s40537-018-0143-6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053235652&doi=10.1186%2fs40537-018-0143-6&partnerID=40&md5=452c66f0e3723fc592205ebff9d3105e","Background: Stepwise regression is a popular data-mining tool that uses statistical significance to select the explanatory variables to be used in a multiple-regression model. Findings: A fundamental problem with stepwise regression is that some real explanatory variables that have causal effects on the dependent variable may happen to not be statistically significant, while nuisance variables may be coincidentally significant. As a result, the model may fit the data well in-sample, but do poorly out-of-sample. Conclusion: Many Big-Data researchers believe that, the larger the number of possible explanatory variables, the more useful is stepwise regression for selecting explanatory variables. The reality is that stepwise regression is less effective the larger the number of potential explanatory variables. Stepwise regression does not solve the Big-Data problem of too many explanatory variables. Big Data exacerbates the failings of stepwise regression. © 2018, The Author(s)."
"10.1186/s40537-018-0139-2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053203338&doi=10.1186%2fs40537-018-0139-2&partnerID=40&md5=89413e933af696fea7059dd7c79c5d5e","This paper presents an ontology based deep learning approach for extracting disease names from Twitter messages. The approach relies on simple features obtained via conceptual representations of messages to obtain results that out-perform those from word level models. The significance of this development is that it can potentially reduce the cost of generating named entity recognition models by reducing the cost of annotating training data since ontology creation is a one-time cost as the conceptual level the ontology is meant to be fairly static and reusable. This is of great importance when it comes to social media text like Twitter messages where you have a large, unbounded lexicon with spatial and temporal variations and other inherent biases that make it logistically untenable to annotate a representative amount of text for general purpose models for live applications. © 2018, The Author(s)."
"10.1186/s40537-018-0140-9","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052882960&doi=10.1186%2fs40537-018-0140-9&partnerID=40&md5=66ba704627a436ecd3e61b7e4d4695f8","Big Data Analytics have become an integral part of Health Informatics over the past years, with the analysis of Internet data being all the more popular in health assessment in various topics. In this study, we first examine the geographical distribution of the online behavioral variations towards Chlamydia, Gonorrhea, Syphilis, Tuberculosis, and Hepatitis in the United States by year from 2004 to 2017. Next, we examine the correlations between Google Trends data and official health data from the ‘Centers for Disease Control and Prevention’ (CDC) on said diseases, followed by estimating linear regressions for the respective relationships. The results show that Infoveillance can assist with exploring public awareness and accurately measure the behavioral changes towards said diseases. The correlations between Google Trends data and CDC data on Chlamydia cases are statistically significant at a national level and in most of the states, while the forecasting exhibits good performing results in many states. For Hepatitis, significant correlations are observed for several US States, while forecasting also exhibits promising results. On the contrary, several factors can affect the applicability of this forecasting method, as in the cases of Gonorrhea, Syphilis, and Tuberculosis, where the correlations are statistically significant in fewer states. Thus this study highlights that the analysis of Google Trends data should be done with caution in order for the results to be robust. In addition, we suggest that the applicability of this method is not that trivial or universal, and that several factors need to be taken into account when using online data in this line of research. However, this study also supports previous findings suggesting that the analysis of real-time online data is important in health assessment, as it tackles the long procedure of data collection and analysis in traditional survey methods, and provides us with information that could not be accessible otherwise. © 2018, The Author(s)."
"10.1186/s40537-018-0138-3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052861491&doi=10.1186%2fs40537-018-0138-3&partnerID=40&md5=381215a7c1b5d3ae1a5fe38bbf367221","In the United States, advances in technology and medical sciences continue to improve the general well-being of the population. With this continued progress, programs such as Medicare are needed to help manage the high costs associated with quality healthcare. Unfortunately, there are individuals who commit fraud for nefarious reasons and personal gain, limiting Medicare’s ability to effectively provide for the healthcare needs of the elderly and other qualifying people. To minimize fraudulent activities, the Centers for Medicare and Medicaid Services (CMS) released a number of “Big Data” datasets for different parts of the Medicare program. In this paper, we focus on the detection of Medicare fraud using the following CMS datasets: (1) Medicare Provider Utilization and Payment Data: Physician and Other Supplier (Part B), (2) Medicare Provider Utilization and Payment Data: Part D Prescriber (Part D), and (3) Medicare Provider Utilization and Payment Data: Referring Durable Medical Equipment, Prosthetics, Orthotics and Supplies (DMEPOS). Additionally, we create a fourth dataset which is a combination of the three primary datasets. We discuss data processing for all four datasets and the mapping of real-world provider fraud labels using the List of Excluded Individuals and Entities (LEIE) from the Office of the Inspector General. Our exploratory analysis on Medicare fraud detection involves building and assessing three learners on each dataset. Based on the Area under the Receiver Operating Characteristic (ROC) Curve performance metric, our results show that the Combined dataset with the Logistic Regression (LR) learner yielded the best overall score at 0.816, closely followed by the Part B dataset with LR at 0.805. Overall, the Combined and Part B datasets produced the best fraud detection performance with no statistical difference between these datasets, over all the learners. Therefore, based on our results and the assumption that there is no way to know within which part of Medicare a physician will commit fraud, we suggest using the Combined dataset for detecting fraudulent behavior when a physician has submitted payments through any or all Medicare parts evaluated in our study. © 2018, The Author(s)."
"10.1186/s40537-018-0137-4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052316308&doi=10.1186%2fs40537-018-0137-4&partnerID=40&md5=d7d8b75d04d845b3845d9003313e1201","Influence maximization in the social network becomes increasingly important due to its various benefit and application in diverse areas. In this paper, we propose DERND D-hops that adapt the radius-neighborhood degree to a directed graph which is an improvement of our previous algorithm RND d-hops. Then, we propose UERND D-hops algorithm for the undirected graph which is based on radius-neighborhood degree metric for selection of top-K influential users by improving the selection process of our previous algorithm RND d-hops. We set up in the two algorithms a selection threshold value that depends on structural properties of each graph data and thus improves significantly the selection process of seed set, and use a multi-hops distance to select most influential users with a distinct range of influence. We then, determine a multi-hops distance in which each consecutive seed set should be chosen. Thus, we measure the influence spread of selected seed set performed by our algorithms and existing approaches on two diffusion models. We, therefore, propose an analysis of time complexity of the proposed algorithms and show its worst time complexity. Experimental results on large scale data of our proposed algorithms demonstrate its performance against existing algorithms in term of influence spread within a less time compared with our previous algorithm RND d-hops thanks to a selection threshold value. © 2018, The Author(s)."
"10.1186/s40537-018-0135-6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050589027&doi=10.1186%2fs40537-018-0135-6&partnerID=40&md5=fb35c7327e90604d35922bf89c608d8b","With the increasing of using workflow management systems workflow improvement becomes a new emerging problem. Many issues must be considered to handle all aspects of the workflow improvement. Workflows might become quite complex, especially when we move to Web3 (ubiquitous computing web). Workflows from different domains (e.g., scientific or business) have similarities and, more important, differences between themselves. Some concepts and solutions developed in one domain may be readily applicable to the other. In ubiquitous computing, multi-domain workflow data analysis might cause Big Data challenge. This paper investigates the problem of workflow improvement having an observed behavior (i.e., event logs). It proposes a cross-domain concept extraction by similarity assessment to solve some aspects of workflow improvement problem, and it has a new research effort at the intersection of workflow domains. Besides, the proposed technique is evaluated with the benefit of using Deep learning and Transfer learning. One of the greatest assets to use these both learning methods is analyzing a massive amount of data. Our results show that our proposed technique is effectively applicable for analyzing real-life huge data in workflow improvement. © 2018, The Author(s)."
"10.1186/s40537-018-0129-4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050144620&doi=10.1186%2fs40537-018-0129-4&partnerID=40&md5=d32ae9f968ccbf215f9bd4ce85145123","The immense amount of data generated on a daily basis by various devices and systems necessitates a change in data analysis methods. As an important part of analytics, data mining methods require a paradigm shift to solve problems because the old methods cannot manage massive data. Association rule mining is a data mining algorithm used to solve various domain problems. Because of the immense volume of data, one-node solutions are no longer useful, and it is necessary to solve problems by using a distributed and shared-nothing architecture such as Map-Reduce. However, when association rule mining is transferred to these architectures, new problems appear. The main problems are lack of data locality and iteration support and process skewness. In this paper, a method is proposed that solves these problems. Kavosh converts data into a unified format that helps nodes perform their tasks independently without the need to exchange data with other nodes. In addition, the proposed method compresses input data to facilitate data management. Another advantage is the lack of process skewness because it is possible to allocate a predefined amount of data to each node. Kavosh omits iterations required for finding frequent itemsets by changing the Map-Reduce architecture. The proposed method is implemented using Hadoop, and the results are compared with open-source products in terms of three aspects: execution time, load balancing and data compression. The results show that Kavosh outperforms other methods in these aspects. © 2018, The Author(s)."
"10.1186/s40537-018-0134-7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049982912&doi=10.1186%2fs40537-018-0134-7&partnerID=40&md5=4b36803da16cba6079cb9d5943d603dd","Drones are increasingly being used to perform risky and labor intensive aerial tasks cheaply and safely. To ensure operating costs are low and flights autonomous, their flight plans must be pre-built. In existing techniques drone flight paths are not automatically pre-calculated based on drone capabilities and terrain information. Instead, they focus on adaptive shortest paths, manually determined paths, navigation through camera, images and/or GPS for guidance and genetic or geometric algorithms to guide the drone during flight, all of which makes flight navigation complex and risky. In this paper we present details of an automated flight plan builder DIMPL that pre-builds flight plans for drones tasked with surveying a large area to take photographs of electric poles to identify ones with hazardous vegetation overgrowth. The flight plans are built for subregions allowing the drones to navigate autonomously. DIMPL employs a distributed in-memory paradigm to process subregions in parallel and build flight paths in a highly efficient manner. Experiments performed with network and elevation datasets validated the efficiency of DIMPL in building optimal flight plans for a fleet of different types of drones and demonstrated the tremendous performance improvements possible using the distributed in-memory paradigm. © 2018, The Author(s)."
"10.1186/s40537-018-0133-8","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049777296&doi=10.1186%2fs40537-018-0133-8&partnerID=40&md5=3b7966348a8a968b25e341f20344b1c1","In this paper, we propose a framework for processing and analysing large-scale spatio-temporal data that uses a battery of machine learning methods based on a meta-data representation of point patterns. Existing spatio-temporal analysis methods do not include a specific mechanism for analysing meta-data (point pattern information). In this work, we extend a spatial point pattern analysis method (the Morisita index) with meta-data analysis, which includes anomaly behaviour detection and unsupervised learning to support spatio-temporal data analysis and demonstrate its practical use. The resulting framework is robust and has the capability to detect anomalies among large-scale spatio-temporal data using meta-data based on point pattern analysis. It returns visualized reports to end users. © 2018, The Author(s)."
"10.1186/s40537-018-0131-x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049742257&doi=10.1186%2fs40537-018-0131-x&partnerID=40&md5=a8ea44e0ddf66b3e99fd239d85082ba8","Currently, the number of surveillance cameras is rapidly increasing responding to security issues. But constructing an intelligent detection system is not easy because it needs high computing performance. This study aims to construct a real-world video surveillance system that can effectively detect moving person using limited resources. To this end, we propose a simple framework to detect and recognize moving objects using outdoor CCTV video footages by combining background subtraction and Convolutional Neural Networks (CNNs). A background subtraction algorithm is first applied to each video frame to find the regions of interest (ROIs). A CNN classification is then carried out to classify the obtained ROIs into one of the predefined classes. Our approach much reduces the computation complexity in comparison to other object detection algorithms. For the experiments, new datasets are constructed by filming alleys and playgrounds, places where crimes are likely to occur. Different image sizes and experimental settings are tested to construct the best classifier for detecting people. The best classification accuracy of 0.85 was obtained for a test set from the same camera with training set and 0.82 with different cameras. © 2018, The Author(s)."
"10.1186/s40537-018-0130-y","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049602291&doi=10.1186%2fs40537-018-0130-y&partnerID=40&md5=04a5f2267ec8198adb1f40e945f9bdcd","Privacy preserving data publication is the main concern in present days, because the data being published through internet has been increasing day by day. This huge amount of data was named as Big Data by its size. This project deals with the privacy preservation in context of big data using a data warehousing solution called hive. We implemented nearest similarity based clustering (NSB) with Bottom-up generalization to achieve (v,l)-anonymity which deals with the sensitivity vulnerabilities and ensures the individual privacy. We also calculate the sensitivity levels by simple comparison method using the index values, by classifying the different levels of sensitivity. The experiments were carried out on the hive environment to verify the efficiency of algorithms with big data. This framework also supports the execution of existing algorithms without any changes. The model in the article outperforms than existing models. © 2018, The Author(s)."
"10.1186/s40537-018-0132-9","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049589167&doi=10.1186%2fs40537-018-0132-9&partnerID=40&md5=8bfc5fd5227f47211d954b1ba3723759","This paper proposes a theoretical foundation for Big Data. More precisely, it explains how “functors”, a concept coming from Category Theory, can serve to model the various data structures commonly used to represent (large) data sets, and how “natural transformations” can formalize relations between these structures. Algorithms, such as querying a precise information, mainly depend on the data structure considered, and thus natural transformations can serve to optimize these algorithms and get a result in a shorter time. The paper details four functors modeling tabular data, graph structures (e.g. triple stores), cached and split data. Next, the paper explains how, by considering a functional programming language, the concepts can be implemented without effort to propose new tools (e.g. efficient information servers and query languages). And, as a complement to the mathematical models proposed, the paper also presents a optimized data server and a specific query language (based on “unification” to facilitates the search of information). Finally, the paper gives a comparison study and shows that this tool is more efficient than most of the standards available in the market: the functional server appears to be 10+ times faster than relational or document oriented databases (Mysql and MongoDB), and 100+ times faster than a graph database (Neo4j). © 2018, The Author(s)."
"10.1186/s40537-018-0128-5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048003557&doi=10.1186%2fs40537-018-0128-5&partnerID=40&md5=35cad14691918a5cc02e45f13373421b","This paper identifies a criterion for choosing an optimum set of selected features, or rejected null hypotheses, in high-dimensional data analysis. The method is designed for dimension reduction with multiple hypothesis testing used in filtering process of big data, and in exploratory research, to identify significant associations among many predictor variables and few outcomes. The novelty of the proposed method is that the selected p-value threshold will be insensitive to dependency within features, and between features and outcome. The method neither requires predetermined thresholds for level of significance, nor uses presumed thresholds for false discovery rate. Using the presented method, the optimum p-value for powerful yet parsimonious model is chosen, then for every set of rejected hypotheses, the researcher can also report traditional measures of statistical accuracy such as the expected number of false positives, and false discovery rate. The upper limit for number of rejected hypotheses (or selected features) is determined by finding the maximum difference between expected true hypotheses and expected false hypotheses among all possible sets of rejected hypotheses. Then, many methods of choosing an optimum number of selected features such as piecewise regression are used to form a parsimonious model. The paper reports the results of implementation of proposed methods in a novel example of non-parametric analysis of high-dimensional ordinal survey data. © 2018, The Author(s)."
"10.1186/s40537-018-0127-6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047527887&doi=10.1186%2fs40537-018-0127-6&partnerID=40&md5=3c1bc1b7457dcc85763911951c6ebc95","The aim of this article is to analyze search and retrieval of workflows. It represents workflows relatedness based on transfer learning. Workflows from different domains (e.g. scientific or business) have similarities and, more important, differences between themselves. Some concepts and solutions developed in one domain may be readily applicable to the other. This paper proposes a cross-domain concept extraction by similarity measurement and has a new research effort at the intersection of workflow domains. It deals with the huge amount of structured and unstructured data (Big Data) that is a demanding task when working on real-life event logs. The proposed method in this paper gives a general solution in the sense that it can be coupled to any Process Aware Information System. © 2018, The Author(s)."
"10.1186/s40537-018-0126-7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047276389&doi=10.1186%2fs40537-018-0126-7&partnerID=40&md5=9e4f8511b595226743a524f62ee188a3","Over the past decade and with the increasing use of the Internet, the assessment of health issues using online search traffic data has become an integral part of Health Informatics. Internet data in general and from Google Trends in particular have been shown to be valid and valuable in predictions, forecastings, and nowcastings; and in detecting, tracking, and monitoring diseases’ outbreaks and epidemics. Empirical relationships have been shown to exist between Google Trends’ data and official data in several health topics, with the science of infodemiology using the vast amount of information available online for the assessment of public health and policy matters. The aim of this study is to provide a method of forecasting AIDS prevalence in the US using online search traffic data from Google Trends on AIDS related terms. The results at first show that significant correlations between Google Trends’ data and official health data on AIDS prevalence (2004–2015) exist in several States, while the estimated forecasting models for AIDS prevalence show that official health data and Google Trends data on AIDS follow a logarithmic relationship. Overall, the results of this study support previous work on the subject suggesting that Google data are valid and valuable for the analysis and forecasting of human behavior towards health topics, and could further assist with Health Assessment in the US and in other countries and regions with valid available official health data. © 2018, The Author(s)."
"10.1186/s40537-018-0125-8","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046533283&doi=10.1186%2fs40537-018-0125-8&partnerID=40&md5=cb4b4049afdffc5d72e7ddd4dbdd1a89","This paper deals with an efficient parallel and distributed framework for intensive computation with A* algorithm based on MapReduce concept. The A* algorithm is one of the most popular graph traversal algorithm used in route guidance. It requires exponential time computation and very costly hardware to compute the shortest path on large-scale networks. Thus, it is necessary to reduce the time complexity while exploiting a low cost commodity hardwares. To cope with this situation, we propose a novel approach that reduces the A* algorithm into a set of Map and Reduce tasks for running the path computation on Hadoop MapReduce framework. An application on real road networks illustrates the feasibility and reliability of the proposed framework. The experiments performed on a 6-node Hadoop cluster proves that the proposed approach outperforms A* algorithm and achieves significant gain in terms of computation time. © 2018, The Author(s)."
"10.1186/s40537-018-0124-9","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045386866&doi=10.1186%2fs40537-018-0124-9&partnerID=40&md5=95efd0bb1c78a264aa1ca7bfb83d995a","Data is being produced in large amounts and in rapid pace which is diverse in quality, hence, the term big data used. Now, big data has started to influence modern day life in almost every sphere, be it business, education or healthcare. Data being a part and parcel of everyday life, privacy has become a topic requiring emphasis. Privacy can be defined as the capacity of a person or group to seclude themselves or information about themselves, and thereby express them selectively. Privacy in big data can be achieved through various means but here the focus is on differential privacy. Differential privacy is one such field with one of the strongest mathematical guarantee and with a large scope of future development. Along these lines, in this paper, the fundamental ideas of sensitivity and privacy budget in differential privacy, the noise mechanisms utilized as a part of differential privacy, the composition properties, the ways through which it can be achieved and the developments in this field till date has been presented. The research gap and future directions have also been mentioned as part of this paper. © 2018, The Author(s)."
"10.1186/s40537-018-0122-y","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044288140&doi=10.1186%2fs40537-018-0122-y&partnerID=40&md5=09183bbfe9a29a7dd80bb3726948b877","Random swap algorithm aims at solving clustering by a sequence of prototype swaps, and by fine-tuning their exact location by k-means. This randomized search strategy is simple to implement and efficient. It reaches good quality clustering relatively fast, and if iterated longer, it finds the correct clustering with high probability. In this paper, we analyze the expected number of iterations needed to find the correct clustering. Using this result, we derive the expected time complexity of the random swap algorithm. The main results are that the expected time complexity has (1) linear dependency on the number of data vectors, (2) quadratic dependency on the number of clusters, and (3) inverse dependency on the size of neighborhood. Experiments also show that the algorithm is clearly more efficient than k-means and almost never get stuck in inferior local minimum. © 2018, The Author(s)."
"10.1186/s40537-018-0120-0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85043321256&doi=10.1186%2fs40537-018-0120-0&partnerID=40&md5=ebf59df1794f6f341a92c738d0f5405c","Gathering public opinion by analyzing big social data has attracted wide attention due to its interactive and real time nature. For this, recent studies have relied on both social media and sentiment analysis in order to accompany big events by tracking people’s behavior. In this paper, we propose an adaptable sentiment analysis approach that analyzes social media posts and extracts user’s opinion in real-time. The proposed approach consists of first constructing a dynamic dictionary of words’ polarity based on a selected set of hashtags related to a given topic, then, classifying the tweets under several classes by introducing new features that strongly fine-tune the polarity degree of a post. To validate our approach, we classified the tweets related to the 2016 US election. The results of prototype tests have performed a good accuracy in detecting positive and negative classes and their sub-classes. © 2018, The Author(s)."
"10.1186/s40537-018-0119-6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042945857&doi=10.1186%2fs40537-018-0119-6&partnerID=40&md5=f881fc8fcde4b21933ed618ed3ef8511","The present article describes a concept for the creation and application of energy forecasting models in a distributed environment. Additionally, a benchmark comparing the time required for the training and application of data-driven forecasting models on a single computer and a computing cluster is presented. This comparison is based on a simulated dataset and both R and Apache Spark are used. Furthermore, the obtained results show certain points in which the utilization of distributed computing based on Spark may be advantageous. © 2018, The Author(s)."
"10.1186/s40537-018-0121-z","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042871835&doi=10.1186%2fs40537-018-0121-z&partnerID=40&md5=43c3522ae7f0806a59e8e1e8399e81f9","Massive graphs are ubiquitous and at the heart of many real-world problems and applications ranging from the World Wide Web to social networks. As a result, techniques for compressing graphs have become increasingly important and remains a challenging and unsolved problem. In this work, we propose a graph compression and encoding framework called GraphZIP based on the observation that real-world graphs often form many cliques of a large size. Using this as a foundation, the proposed technique decomposes a graph into a set of large cliques, which is then used to compress and represent the graph succinctly. In particular, disk-resident and in-memory graph encodings are proposed and shown to be effective with three important benefits. First, it reduces the space needed to store the graph on disk (or other permanent storage device) and in-memory. Second, GraphZIP reduces IO traffic involved in using the graph. Third, it reduces the amount of work involved in running an algorithm on the graph. The experiments demonstrate the scalability, flexibility, and effectiveness of the clique-based compression techniques using a collection of networks from various domains. © 2018, The Author(s)."
"10.1186/s40537-018-0114-y","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042770146&doi=10.1186%2fs40537-018-0114-y&partnerID=40&md5=80c330080e1bb79cf6b73fefefa6a54d","Next-Generation Sequencing technologies are generating a huge amount of genetic data that need to be mapped and analyzed. Single machine sequence alignment tools are becoming incapable or inefficient in keeping track of the same. Therefore, distributed computing platforms based on MapReduce paradigm, which uses thousands of commodity machines to process and analyze huge datasets, are emerging as the best solution for growing genomics data. A lot of MapReduce-based sequence alignment tools like CloudBurst, CloudAligner, Halvade, and SparkBWA are proposed by various researchers in recent few years. These sequence aligners are very fast and efficient. These sequence aligners are capable of aligning billions of reads (stored as fasta or fastq files) on reference genome in few minutes. In the current era of fastly growing technology, analyzing huge genome data fast is not enough. We need to analyze data in real time to automate alignment process. Therefore, we propose a MapReduce-based sequence alignment tool StreamAligner which is implemented on Spark streaming engine. StreamAligner can align stream of reads on reference genome in real time. Therefore, it can be used to automate sequencing and alignment process. It uses suffix array index for read alignment which is generated using distributed index generation algorithm. Due to distributed index generation algorithm, index generation time is very less. It needs to upload index only once when StreamAligner is launched. After that index stays in Spark memory and can be used for an unlimited times without reloading. Whereas, current state-of-the-art sequence aligner either generate (hash index based) or load (sorted index based) index for every task. Hence, StreamAligner reduces time to generate or load index for every task. A working and tested implementation of streamAligner is available on GitHub for download and use. We tested the effectiveness, efficiency, and scalability of our aligner for various standard and real-life datasets. © 2018, The Author(s)."
"10.1186/s40537-018-0118-7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042666514&doi=10.1186%2fs40537-018-0118-7&partnerID=40&md5=6e6133af46da744f6dad908c811183b7","In the last decade, significant advances have been made in sensing and communication technologies. Such progress led to a considerable growth in the development and use of intelligent transportation systems. Characterizing driving styles of drivers using in-vehicle sensor data is an interesting research problem and an essential real-world requirement for automotive industries. A good representation of driving features can be extremely valuable for anti-theft, auto insurance, autonomous driving, and many other application scenarios. This paper addresses the problem of driver identification using real driving datasets consisting of measurements taken from in-vehicle sensors. The paper investigates the minimum learning and classification times that are required to achieve a desired identification performance. Further, feature selection is carried out to extract the most relevant features for driver identification. Finally, in addition to driving pattern related features, driver related features (e.g., heart-rate) are shown to further improve the identification performance. © 2018, The Author(s)."
"10.1186/s40537-018-0117-8","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042488050&doi=10.1186%2fs40537-018-0117-8&partnerID=40&md5=7e61fdc55f91768c16529edb0c4a15e2","Background: Complications of pregnancy and childbirth are a leading cause of maternal morbidities and mortalities in developing countries. World Health Organization (WHO) estimates that over 500,000 women and girls die each year from the complications. Despite proven interventions that could prevent death or disability during pregnancy and childbirth, maternal mortality remains a major burden in many developing countries, including Ethiopia. This study aimed to assess the status of antenatal care utilization and modeling Bayesian Count Regression model for the determinants of utilization of antenatal care services visits among pregnant women in Amhara regional state. Methods: It was a community based analytical cross-sectional study, conducted in Amhara region among women in the reproductive age group (age 15–49). The analysis was based on data from women who had at least one birth during the 5 years preceding the survey. The source of data was the 2014 Ethiopia Demographic and Health Survey which was accessed from Central Statistical Agency. Bayesian analytic approach was applied to model the mixture data structure inherent in zero-inflated count data by using the zero-inflated Poisson model. Results: About 37% (95% CI 0.32, 0.42) of the pregnant mothers were not received antenatal care services during their pregnancy and about 23% of them were visited at least four times. From Bayesian zero inflated Poisson regression it was found that rural pregnant women (OR = 1.13; HPD CI 1.12, 1.44), women who can read and write (OR = 0.54; HPD CI 0.40, 0.72), middle Wealth index (OR = 0.60; HPD CI 0.46, 0.78) and media exposures (OR = 0.72; HPD: 0.56, 0.92) were statistically associated with no ANC visits. Conclusions: About three-fourth pregnant mothers were not receive adequate number of visits recommended by the World Health Organization. Mother’s education, media exposure, residence and wealth index were significant predictors of ANC service utilization. This research suggests that to reduce the inadequate number of ANC visits in Amhara region, attention should be given to women with low educational status and rural women. © 2018, The Author(s)."
"10.1186/s40537-018-0112-0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042391082&doi=10.1186%2fs40537-018-0112-0&partnerID=40&md5=c32f3c9c620b4e9241e6d67286e7aad2","Extraction of valuable data from extensive datasets is a standout amongst the most vital exploration issues. Association rule mining is one of the highly used methods for this purpose. Finding possible associations between items in large transaction based datasets (finding frequent itemsets) is most crucial part of the association rule mining task. Many single-machine based association rule mining algorithms exist but the massive amount of data available these days is above the capacity of a single machine based algorithm. Therefore, to meet the demands of this ever-growing enormous data, there is a need for distributed association rule mining algorithm which can run on multiple machines. For these types of parallel/distributed applications, MapReduce is one of the best fault-tolerant frameworks. Hadoop is one of the most popular open-source software frameworks with MapReduce based approach for distributed storage and processing of large datasets using standalone clusters built from commodity hardware. But heavy disk I/O operation at each iteration of a highly iterative algorithm like Apriori makes Hadoop inefficient. A number of MapReduce based platforms are being developed for parallel computing in recent years. Among them, a platform, namely, Spark have attracted a lot of attention because of its inbuilt support to distributed computations. Therefore, we implemented a distributed association rule mining algorithm on Spark named as Adaptive-Miner which uses adaptive approach for finding frequent patterns with higher accuracy and efficiency. Adaptive-Miner uses an adaptive strategy based on the partial processing of datasets. Adaptive-Miner makes execution plans before every iteration and goes with the best suitable plan to minimize time and space complexity. Adpative-Miner is a dynamic association rule mining algorithm which change its approach based on the nature of dataset. Therefore, it is different and better than state-of-the-art static association rule mining algorithms. We conduct in-depth experiments to gain insight into the effectiveness, efficiency, and scalability of the Adaptive-Miner algorithm on Spark. Available: https://github.com/sanjaysinghrathi/Adaptive-Miner. © 2018, The Author(s)."
"10.1186/s40537-018-0116-9","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041927679&doi=10.1186%2fs40537-018-0116-9&partnerID=40&md5=8dbe0a8412775f552ae5b8d569f20c05","A prevalent type of big data is in the form of space–time measurements. Cyclostationary empirical orthogonal function (CSEOF) analysis is introduced as an efficient and valuable technique to interpret space–time structure of variability in a big dataset. CSEOF analysis is demonstrated to be a powerful tool in understanding the space–time structure of variability, when data exhibits periodic statistics in time. As an example, CSEOF analysis is applied to the hourly passenger traffic on Subway Line #2 of Seoul, South Korea during the period of 2010–2017. The first mode represents the weekly cycle of subway passengers and captures the majority (~ 97%) of the total variability. The corresponding loading vector exhibits a typical weekly pattern of subway passengers as a function of time and the locations of subway stations. The associated principal component time series shows that there are two occasions of significant reduction in the amplitude of the weekly activity in each year; these reductions are associated with two major holidays—lunar New Year and Fall Festival (called Chuseok in Korea). The second and third modes represent daily contrasts in a week and are associated with taking extra days off before or after holidays. The fourth mode exhibits an interesting upward trend, which represents a general decrease in the number of subway passengers during weekdays except for Wednesday and an increase over the weekends. © 2018, The Author(s)."
"10.1186/s40537-018-0115-x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041496832&doi=10.1186%2fs40537-018-0115-x&partnerID=40&md5=8f33f6d72062d4ea27a28fc8dd14c99a","Efficient management and analysis of large volumes of data is a demanding task of increasing scientific and industrial importance, as the ubiquitous generation of information governs more and more aspects of human life. In this article, we introduce FML-kNN, a novel distributed processing framework for Big Data that performs probabilistic classification and regression, implemented in Apache Flink. The framework’s core is consisted of a k-nearest neighbor joins algorithm which, contrary to similar approaches, is executed in a single distributed session and is able to operate on very large volumes of data of variable granularity and dimensionality. We assess FML-kNN’s performance and scalability in a detailed experimental evaluation, in which it is compared to similar methods implemented in Apache Hadoop, Spark, and Flink distributed processing engines. The results indicate an overall superiority of our framework in all the performed comparisons. Further, we apply FML-kNN in two motivating uses cases for water demand management, against real-world domestic water consumption data. In particular, we focus on forecasting water consumption using 1-h smart meter data, and extracting consumer characteristics from water use data in the shower. We further discuss on the obtained results, demonstrating the framework’s potential in useful knowledge extraction. © 2018, The Author(s)."
"10.1186/s40537-017-0110-7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041361340&doi=10.1186%2fs40537-017-0110-7&partnerID=40&md5=35fc29f2ac7304088777972c0baadc15","Big data has fundamentally changed the way organizations manage, analyze and leverage data in any industry. One of the most promising fields where big data can be applied to make a change is healthcare. Big healthcare data has considerable potential to improve patient outcomes, predict outbreaks of epidemics, gain valuable insights, avoid preventable diseases, reduce the cost of healthcare delivery and improve the quality of life in general. However, deciding on the allowable uses of data while preserving security and patient’s right to privacy is a difficult task. Big data, no matter how useful for the advancement of medical science and vital to the success of all healthcare organizations, can only be used if security and privacy issues are addressed. To ensure a secure and trustworthy big data environment, it is essential to identify the limitations of existing solutions and envision directions for future research. In this paper, we have surveyed the state-of-the-art security and privacy challenges in big data as applied to healthcare industry, assessed how security and privacy issues occur in case of big healthcare data and discussed ways in which they may be addressed. We mainly focused on the recently proposed methods based on anonymization and encryption, compared their strengths and limitations, and envisioned future research directions. © 2018, The Author(s)."
"10.1186/s40537-018-0113-z","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041010070&doi=10.1186%2fs40537-018-0113-z&partnerID=40&md5=953b0b46b0983e146131da3907698261","Despite the urban heat islands phenomenon has long been recognized as a major urban environmental problem, it was not until recently that this urban phenomenon gained attention from the discipline of urban planning. To integrate the findings of the urban heat islands research into the planning practice, the relationship between land surface temperatures and urban physical and socioeconomic characteristics should be addressed at the planning relevant spatial scale, a land parcel. Using a parcel as a unit of analysis, this study proposed to use a machine learning approach to identify important variables in the formation of urban heat islands in Indianapolis, Indiana. Applying random forest method to planning zones, this study identified planning zone specific urban physical and socioeconomic characteristics that are important for the interpretation of urban heat islands phenomenon of Indianapolis, Indiana. The main contribution of this study is twofold: to integrate urban physical and socioeconomic characteristics into a land parcel for the better interpretation of the result of urban heat islands study into planning practice and to apply machine learning approach to identify highly determinant variables in the formation of urban heat islands. © 2018, The Author(s)."
"10.1186/s40537-017-0111-6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040990671&doi=10.1186%2fs40537-017-0111-6&partnerID=40&md5=781d986169a4f5d536dd745922745c6b","Deep Learning and Big Data analytics are two focal points of data science. Deep Learning models have achieved remarkable results in speech recognition and computer vision in recent years. Big Data is important for organizations that need to collect a huge amount of data like a social network and one of the greatest assets to use Deep Learning is analyzing a massive amount of data (Big Data). This advantage makes Deep Learning as a valuable tool for Big Data. Deep Learning can be used to extract incredible information that buried in a Big Data. The modern stock market is an example of these social networks. They are a popular place to increase wealth and generate income, but the fundamental problem of when to buy or sell shares, or which stocks to buy has not been solved. It is very common among investors to have professional financial advisors, but what is the best resource to support the decisions these people make? Investment banks such as Goldman Sachs, Lehman Brothers, and Salomon Brothers dominated the world of financial advice for more than a decade. However, via the popularity of the Internet and financial social networks such as StockTwits and SeekingAlpha, investors around the world have new opportunity to gather and share their experiences. Individual experts can predict the movement of the stock market in financial social networks with the reasonable accuracy, but what is the sentiment of a mass group of these expert authors towards various stocks? In this paper, we seek to determine if Deep Learning models can be adapted to improve the performance of sentiment analysis for StockTwits. We applied several neural network models such as long short-term memory, doc2vec, and convolutional neural networks, to stock market opinions posted in StockTwits. Our results show that Deep Learning model can be used effectively for financial sentiment analysis and a convolutional neural network is the best model to predict sentiment of authors in StockTwits dataset. © 2018, The Author(s)."
"10.1007/s41019-018-0078-0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062734627&doi=10.1007%2fs41019-018-0078-0&partnerID=40&md5=5253183b715fe39071288066f0dfe1b0","Computing k- cores is a fundamental and important graph problem, which can be applied in many areas, such as community detection, network visualization, and network topology analysis. Due to the complex relationship between different entities, dual graph widely exists in the applications. A dual graph contains a physical graph and a conceptual graph, both of which have the same vertex set. Given that there exist no previous studies on the k- core in dual graphs, we formulate a k-connected core (k- CCO) model in dual graphs. A k- CCO is a k- core in the conceptual graph, and also connected in the physical graph. Given a dual graph and an integer k, we propose a polynomial time algorithm for computing all k- CCOs. We also propose three algorithms for computing all maximum-connected cores (MCCO), which are the existing k- CCOs such that a (k+ 1) -CCO does not exist. We further study a subgraph search problem, which is computing a k- CCO that contains a set of query vertices. We propose an index-based approach to efficiently answer the query for any given parameter k. We conduct extensive experiments on six real-world datasets and four synthetic datasets. The experimental results demonstrate the effectiveness and efficiency of our proposed algorithms. © 2018, The Author(s)."
"10.1007/s41019-018-0081-5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062723241&doi=10.1007%2fs41019-018-0081-5&partnerID=40&md5=850a3b0525d486eb200334460164c4c6","Prior solutions for securely handling SQL range predicates in outsourced Cloud-resident databases have primarily focused on passive attacks in the Honest-but-Curious adversarial model, where the server is only permitted to observe the encrypted query processing. We consider here a significantly more powerful adversary, wherein the server can launch an active attack by clandestinely issuing specific range queries via collusion with a few compromised clients. The security requirement in this environment is that data values from a plaintext domain of size N should not be leaked to within an interval of size H. Unfortunately, all prior encryption schemes for range predicate evaluation are easily breached with only O(log 2 ψ) range queries, where ψ= N/ H. To address this lacuna, we present SPLIT, a new encryption scheme where the adversary requires exponentially more—O(ψ) —range queries to breach the interval constraint and can therefore be easily detected by standard auditing mechanisms. The novel aspect of SPLIT is that each value appearing in a range-sensitive column is first segmented into two parts. These segmented parts are then independently encrypted using a layered composition of a secure block cipher with the order-preserving encryption and prefix-preserving encryption schemes, and the resulting ciphertexts are stored in separate tables. At query processing time, range predicates are rewritten into an equivalent set of table-specific sub-range predicates, and the disjoint union of their results forms the query answer. A detailed evaluation of SPLIT on benchmark database queries indicates that its execution times are well within a factor of two of the corresponding plaintext times, testifying its efficiency in resisting active adversaries. © 2018, The Author(s)."
"10.1007/s41019-018-0082-4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062704916&doi=10.1007%2fs41019-018-0082-4&partnerID=40&md5=3144f0f1c8336f6a625ac841a1501c89","Given a knowledge graph and a fact (a triple statement), fact checking is to decide whether the fact belongs to the missing part of the graph. Facts in real-world knowledge bases are typically interpreted by both topological and semantic context that is not fully exploited by existing methods. This paper introduces a novel fact checking method that explicitly exploits discriminant subgraph structures. Our method discovers discriminant subgraphs associated with a set of training facts, characterized by a class of graph fact checking rules. These rules incorporate expressive subgraph patterns to jointly describe both topological and ontological constraints. (1) We extend graph fact checking rules (GFCs) to a class of ontological graph fact checking rules (OGFCs). OGFCs generalize GFCs by incorporating both topological constraints and ontological closeness to best distinguish between true and false fact statements. We provide quality measures to characterize useful patterns that are both discriminant and diversified. (2) Despite the increased expressiveness, we show that it is feasible to discover OGFCs in large graphs with ontologies, by developing a supervised pattern discovery algorithm. To find useful OGFCs as early as possible, it generates subgraph patterns relevant to training facts and dynamically selects patterns from a pattern stream with a small update cost per pattern. We verify that OGFCs can be used as rules and provide useful features for other statistical learning-based fact checking models. Using real-world knowledge bases, we experimentally verify the efficiency and the effectiveness of OGFC-based techniques for fact checking. © 2018, The Author(s)."
"10.1007/s41019-018-0080-6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062703555&doi=10.1007%2fs41019-018-0080-6&partnerID=40&md5=c3b2a886f2357faf8bb271e91ebb1610","We introduce a novel interactive framework to handle both instance-level and temporal smoothness constraints for clustering large longitudinal data and for tracking the cluster evolutions over time. It consists of a constrained clustering algorithm, called CVQE+, which optimizes the clustering quality, constraint violation and the historical cost between consecutive data snapshots. At the center of our framework is a simple yet effective active learning technique, named Border, for iteratively selecting the most informative pairs of objects to query users about, and updating the clustering with new constraints. Those constraints are then propagated inside each data snapshot and between snapshots via two schemes, called constraint inheritance and constraint propagation, to further enhance the results. Moreover, a historical constraint is enforced between consecutive snapshots to ensure the consistency of results among them. Experiments show better or comparable clustering results than state-of-the-art techniques as well as high scalability for large datasets. Finally, we apply our algorithm for clustering phenotypes in patients with Obstructive Sleep Apnea as well as for tracking how these clusters evolve over time. © 2018, The Author(s)."
"10.1007/s41019-018-0079-z","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062701977&doi=10.1007%2fs41019-018-0079-z&partnerID=40&md5=cf5d3279a5d38cc4d521af43355dfc4c","In our daily life, how to match clothing well is always a troublesome problem especially when we are shopping online to select a pair of matched pieces of clothing from tens of thousands available selections. To help common customers overcome selection issues, recent studies in the recommender system area have started to infer the fashion matching results automatically. The traditional fashion recommendation is normally achieved by considering visual similarity of clothing items or/and item co-purchase history from existing shopping transactions. Due to the high complexity of visual features and the lack of historical item purchase records, most of the existing work is unlikely to make an efficient and accurate recommendation. To address the problem, in this paper, we propose a new model called Discrete Supervised Fashion Coordinates Hashing. Its main objective is to learn meaningful yet compact high-level features of clothing items, which are represented as binary hash codes. In detail, this learning process is supervised by a clothing matching matrix, which is initially constructed based on limited known matching pairs and subsequently on the self-augmented ones. The proposed model jointly learns the intrinsic matching patterns from the matching matrix and the binary representations from the clothing items’ images, where the visual feature of each clothing item is discretized into a fixed-length binary vector. The binary representation learning significantly reduces the memory cost and accelerates the recommendation speed. The experiments compared with several state-of-the-art approaches have evidenced the superior performance of the proposed approach on efficient fashion recommendation. © 2018, The Author(s)."
"10.1007/s41019-018-0074-4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062147891&doi=10.1007%2fs41019-018-0074-4&partnerID=40&md5=594cf9775c26deaba6cb0952cbc2f47b","Online analytical processing (OLAP) is a core functionality in database systems. The performance of OLAP is crucial to make online decisions in many applications. However, it is rather costly to support OLAP on large datasets, especially big data, and the methods that compute exact answers cannot meet the high-performance requirement. To alleviate this problem, approximate query processing (AQP) has been proposed, which aims to find an approximate answer as close as to the exact answer efficiently. Existing AQP techniques can be broadly categorized into two categories. (1) Online aggregation: select samples online and use these samples to answer OLAP queries. (2) Offline synopses generation: generate synopses offline based on a-priori knowledge (e.g., data statistics or query workload) and use these synopses to answer OLAP queries. We discuss the research challenges in AQP and summarize existing techniques to address these challenges. In addition, we review how to use AQP to support other complex data types, e.g., spatial data and trajectory data, and support other applications, e.g., data visualization and data cleaning. We also introduce existing AQP systems and summarize their advantages and limitations. Lastly, we provide research challenges and opportunities of AQP. We believe that the survey can help the partitioners to understand existing AQP techniques and select appropriate methods in their applications. © 2018, The Author(s)."
"10.1057/s41270-018-0042-x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055469839&doi=10.1057%2fs41270-018-0042-x&partnerID=40&md5=78d6db1daa172315fd2bbccb06673d86","Over the last decade, many software companies have moved away from selling traditional packaged goods and toward providing “freemium” services. Under this business model, companies give away their basic product and generate revenue by repeatedly selling additional features and content. With this movement, away from one-off transactions and toward longer-term, repeated customer relationships, businesses are increasingly using analytics to understand their customers’ behavior, with retention—a measure of the likelihood that a customer will return—being of particular importance. However, despite the value of measuring customer retention, there is no consensus on its definition. Using a unique dataset of freemium mobile games, this study compares and contrasts four commonly used retention measures and finds that a measure that takes into account when a user interacts with an application provides the highest correlation with future monetization. This study also provides empirical evidence of a positive relationship between customer retention and future monetization. © 2018, Springer Nature Limited."
"10.1057/s41270-018-0043-9","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055468299&doi=10.1057%2fs41270-018-0043-9&partnerID=40&md5=12cef87066563d1a85fc10e37616dc0c","Several authors developed predictive analytic models that link the value that represents customers to a firm (i.e., customer lifetime value) to several outcome variables, such as customer profitability, in relationship marketing. However, similar models that link the value that customers perceive and firm outcomes or customer responses are uncommon. To reduce this gap, we construct an analytic model that links customer-perceived value and a company’s competitive strategy, achieved through a multi-attribute model, analytic hierarchy processing, and a conceptualization of value that considers disparity between benefits and sacrifices. Operationalized in a context of industrial enterprise, the model predicts and orients a company’s competitive strategy and extends generic competitive strategies introduced in Bowman’s strategy clock mode by identifying when two strategies create competitive advantages. © 2018, Springer Nature Limited."
"10.1057/s41270-018-0038-6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049120523&doi=10.1057%2fs41270-018-0038-6&partnerID=40&md5=14d9ae0dd038673b9ba54b5f9390c597","Creating a “unique” or differentiated product is every company’s goal. But, how does the company sustain its product’s uniqueness? While sustainability is popular, it remains an elusive goal for entrepreneurs of unique products. It is elusive because the concept of sustainability is difficult to market to those who are resistant to change, who question the need for sustainability concepts, or who are reluctant to purchase differentiated products. Can marketing analytics help? This paper presents a case study of where analytics did support an effort to market a product that was considered unique and difficult to sustain without analytics in a competitive market. The case is Domino’s AnyWare™. © 2018, Macmillan Publishers Ltd., part of Springer Nature."
"10.1057/s41270-018-0039-5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049071569&doi=10.1057%2fs41270-018-0039-5&partnerID=40&md5=5bb4e3213d61815c37602329c8a0b1bc","This paper presents a methodology for placement of the items of surveys to obtain optimal reliability. A mathematical model is developed based on some specific structured assumptions for reliability and consistency. The problem is transformed into a mathematical optimization problem. We present solution methodology for the problem and also properties of the algorithm. An example is presented to illustrate our methodology. © 2018, Macmillan Publishers Ltd., part of Springer Nature."
"10.1177/2399808318801958","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056736831&doi=10.1177%2f2399808318801958&partnerID=40&md5=0ee3d638c77cb3fc16dc2ce0314a651c","What are the social bases of neighborhood formation in urban areas, and at what spatial scale are they most distinct from other neighborhoods? We address these questions in the case of St. Louis, Missouri, in 1930, where we can take advantage of unique geocoded census microdata on the whole population of the city that identifies who, with what background characteristics, lived where. Our analyses show that homophily by race and ethnicity was by far the strongest factor linking characteristics of persons to the composition of their neighbors. Measures of social class also were quite important, while the person’s nativity and family status were statistically significant but minor predictors. Yet while this hierarchy of social factors held for the population as a whole, their relative importance varied greatly across racial/ethnic groups. Similarity in social class to neighbors was most important for native whites, nativity counted as much or more than class for recently arriving immigrant groups including Russians, Italians, and Poles, and race/ethnicity was by far the key predictor for these groups and blacks. We also found that these patterns of homophily were clearest at the scale of individual street segment and first-order combinations of segments. They were similar but less distinct at a larger spatial scale. © The Author(s) 2018."
"10.1177/2399808318782703","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049836348&doi=10.1177%2f2399808318782703&partnerID=40&md5=0ae8165cb29cfb85eaa373b8a51f18ac","Traditional studies of residential segregation use a descriptive index approach with predefined spatial units to report the degree of neighbourhood differentiation. We develop a model-based approach which explicitly includes spatial effects at multiple scales, recognising the complexity of the urban environment while simultaneously distinguishing segregation at each scale net of all other scales. Moreover, this model distinguishes segregation as unevenness and as spatial clustering in the presence of stochastic variation. The modelling approach, unlike traditional index approaches, allows hypothesis evaluation concerning alternative scales and zonation through an accompanying badness-of-fit measure. Ultimately, this permits the identification of the scale and zonation regime where the spatial patterns come into focus thereby directly tackling the modifiable areal unit problem. The model is applied to Indian ethnicity in Leicester, UK, finding segregation as unevenness and as spatial clustering at multiple scales. © The Author(s) 2018."
"10.1177/2399808318783748","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049807459&doi=10.1177%2f2399808318783748&partnerID=40&md5=fbc11b1a62df6f965e23a380430309cc","In this paper, we analyse the spatial dimension of changing ethnic diversity at the neighbourhood level. Drawing from recent work on income convergence, we characterise the evolution of population diversity in the Netherlands over space. Our analysis is structured over three dimensions, which allow us to find clear spatial patterns in how cultural diversity changes at the neighbourhood level. Globally, we use directional statistics to visualise techniques of exploratory data analysis, finding a clear trend towards ‘spatially integrated change’: a situation where the trajectory of ethnic change in a neighbourhood is closely related to that in adjacent neighbourhoods. When we zoom into the local level, a visualisation of recent measures of local concordance allows us to document a high degree of spatial heterogeneity in how the overall change is distributed over space. Finally, to further explore the nature and characteristics of neighbourhoods that experience the largest amount of change, we develop a spatial, multilevel model. Our results show that the largest cities, as well as those at the boundaries with Belgium and Germany, with the most diverse neighbourhoods, have large clusters of stable neighbourhood diversity over time, while concentrations of high dynamic areas are nearby these largest cities. The analysis shows that neighbourhood diversity spatially ‘spills over’, gradually expanding outside traditionally diverse areas. © The Author(s) 2018."
"10.1177/2399808318760572","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044971205&doi=10.1177%2f2399808318760572&partnerID=40&md5=8907798674ab2bcedc4f74daec9329b3","There has been extensive use of segregation indices for measuring residential segregation since 1950s, with continuous progress made in the field. Recent developments include the propositions of spatial global and local versions of traditionally used segregation indices, which have opened avenues for representing and analysing segregation as a multiscale and spatially varying phenomenon. Much less explored has been the issue of how important research design choices, such as the extent of geographical boundaries, grouping systems and scales of analysis, can influence the measurement of segregation. This paper contributes in this direction by investigating the impact of such decisions in the outcomes of the indices of generalized dissimilarity (D) and information theory (H) using a set of sensitivity analysis. Using a comparative study between London and São Paulo as basis, results obtained with different geographical boundaries, grouping systems and scales for the two indices are analysed visually and quantitatively. Results suggest that although D and H depict the same spatial dimension of segregation (unevenness/clustering), they present different sensitivity to geographical boundaries and grouping systems. The study also revealed how the two indices unfold different aspects of the segregation, which impact on their interpretation and applicability. The study concludes with a discussion of the considerations on research design choices concerning the interpretation of the results, which indicate the two indices should not be used interchangeably. © The Author(s) 2018."
"10.1177/2399808318766067","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044722474&doi=10.1177%2f2399808318766067&partnerID=40&md5=fac8b5cbb4b63d8105d63fc5f7ad4bbd","“Egocentric” segregation profiles allow researchers to avoid a reliance on a priori definitions of local neighborhoods that contribute an unknown amount of error to measures of segregation. To date, however, such profiles have used distance-decay techniques that rely on “as the crow flies” measures of space. Yet we know that major roads, railroads, and other physical attributes of space mean that such techniques may introduce error into the measurement and visualization of residential segregation. Here, I use a variation on standard smoothing techniques that allows the smoothing function to vary based on a second variable of interest, in this case, the location of major roads, railroads, and nonresidential land use. Using Philadelphia as a case study due to access to detailed land-use data, I show that barriers do not affect observed values of city-level racial and ethnic dissimilarity. Visualizing the impact of barriers on local neighborhoods, however, shows that while barriers may not affect city-wide indexes of segregation, they continue to powerfully shape local experiences of the city, including protecting new immigrant ethnic enclaves, wealthy white neighborhoods, and also isolating high-poverty, predominantly black neighborhoods in different parts of the city. © The Author(s) 2018."
"10.1177/2399808318760858","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044063979&doi=10.1177%2f2399808318760858&partnerID=40&md5=31571097859d08796d8c4497877c828a","A well-known limitation of commonly used segregation measures is their inability to describe patterns at multiple scales. Multi-level modeling approaches can describe how different levels of geography contribute to segregation, but may be difficult to interpret for non-technical audiences and have rarely been applied in the US context. This paper provides a readily interpretable description of multi-scale Black–non-Black segregation in the United States using a multi-level modeling approach and the most recent Census data available. We fit a three-level random intercept multi-level logistic regression model predicting the proportion of the population that is Black (Hispanic and non-Hispanic) at the block group level, with block groups nested in tracts and tracts nested in Metropolitan Statistical Areas (MSAs). For the 102 largest MSAs in the United States, we then estimated the extent to which micro- versus meso-level variability drives overall racial residential patterning within the MSA. Finally, we created a typology of racial residential patterning within MSAs based on the total proportion of the MSA population that is Black and the relative contribution of block groups (micro) versus tracts (meso) in driving variation. We find that nearly 80% of the national variation in the geographic concentration of Black residents is driven by within-MSA, tract-level processes. However, the relative contribution of small versus larger scales to within-MSA segregation varies substantially across metropolitan areas. We detect five meaningfully different types of metropolitan segregation across the largest MSAs. Multi-level descriptions of segregation may help planners and policymakers understand how and why segregated residential patterns are evolving in different places and could provide important insights into interventions that could improve integration at multiple scales. © The Author(s) 2018."
"10.1177/2399808318760570","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042443939&doi=10.1177%2f2399808318760570&partnerID=40&md5=8596cf44276bc88196b316138c0a7ca4","Multiscale segregation measures have the potential to increase understanding of residential context and ultimately a wide range of social and spatial processes. By examining segregation at multiple scales, we have the opportunity to study it as more than the outcome of a single process or a measure describing a single contextual effect. Multiscale segregation encourages us to look for sorting processes and contextual effects operating at different scales and potentially even with different meanings. However, the complexity of multiscale measures introduces significant uncertainty about the role of underlying data and assumptions in producing observed outcomes, particularly at fine geographic scales. While traditional measures of segregation have been exposed to decades of scrutiny, multiscale measures are still relatively novel and less well understood. The theoretical contribution of this paper is to consider the implications of segregation as both an outcome and signifier of sorting processes at multiple scales. The empirical contribution is to consider how zoning and the degree of spatial association shape outcomes expressed as multiscale segregation measures. I examine the effects of different allocation strategies for measuring population at small scales by comparing four delineation methods. I find that the method chosen for allocating population to small areas matters, but that by the time observation units reach about 700 m2 most of the difference between methods has washed out. I also test the effect of changing the degree of assumed spatial association in generating multiscale segregation measures. I find that, as suggested by Reardon and O'Sullivan in their original exposition of their spatial segregation measure, this assumption has a relatively small effect on outcomes and is unlikely to shape substantive findings. © The Author(s) 2018."
"10.1177/2399808318756642","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042068857&doi=10.1177%2f2399808318756642&partnerID=40&md5=825cd6ff7eef8f7bf0e22fb2a02929b7","Studies of segregation continue to explore analytic tools to engage with patterns of separation within cities. In recent work, scale has emerged as an important dimension of understanding segregation – simply put, separation is strongly affected by the scale which is used in the measurement process. Levels of segregation are also influenced by the time in which the analysis takes place. We outline an approach to separation which has four dimensions – (1) using bespoke neighborhoods – who do you meet at varying scales, (2) measuring the size of the change in separation over time, (3) estimating the rate of change in separation across space and time and (4) visualizing the change, mapping changing levels of contact. The themes are explored using data from the diverse, multi ethnic neighborhoods in Californian metropolitan areas. The result of a bespoke neighborhood approach to segregation provides a more complete demonstration of the pattern of ethnic segregation. We know that there are declining overall levels of segregation, but while levels are decreasing for Whites they are increasing for Hispanics and Asians but at different rates depending on local contexts. Viewing assimilation in a multi-scalar visual context expands our understanding of segregation and assimilation. © The Author(s) 2018."
"10.1177/2399808317748328","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041847702&doi=10.1177%2f2399808317748328&partnerID=40&md5=8762e96d99a1789e4f9315e3cc2eda6e","This paper introduces the Multilevel Index of Dissimilarity package, which provides tools and functions to fit a Multilevel Index of Dissimilarity in the open source software, R. It extends the conventional Index of Dissimilarity to measure both the amount and geographic scale of segregation, thereby capturing the two principal dimensions of segregation, unevenness and clustering. The statistical basis for the multilevel approach is discussed, making connections to other work in the field and looking especially at the relationships between the Index of Dissimilarity, variance as a measure of segregation, and the partitioning of the variance to identify scale effects. A brief tutorial for the package is provided followed by a case study of the scales of residential segregation for various ethnic groups in England and Wales. Comparing 2001 with 2011 Census data, we find that patterns of segregation are emerging at less localised geographical scales but the Index of Dissimilarity is falling. This is consistent with a process whereby minority groups have spread out into more ethnically mixed neighbourhoods. © The Author(s) 2017."
"10.1177/2399808317744558","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041600844&doi=10.1177%2f2399808317744558&partnerID=40&md5=9b2d3c1b36939d7c52d89fdb1789ce49","Neighborhoods in US metropolitan areas experienced dramatic changes in racial composition during the 1990s and again during the 2000s. We ask to what extent does the recent period of neighborhood racial change reflect an extension of the local processes operative in the 1990s, processes characteristic of large metropolitan areas or the nation more generally, or reflect new dynamics. After classifying neighborhoods in US metropolitan areas into different types based on their racial composition and having harmonized a set of tracts to consistent boundaries, we use metropolitan-scale tract transition matrices from the 1990s to predict changes in neighborhood racial mix between 2000 and 2010. To capture scale effects, we repeat this using a set of pooled metropolitan-scale tract transition matrices and again using a national tract transition matrix. We show that the main dynamic at work across the metropolitan system is the underprediction of moderately diverse white majority tracts: i.e., in the 2000s, the rate of increase in the racial diversity of white majority tracts that transitioned from being predominantly white to moderately diverse was much higher than expected based on 1990s trends. In some metropolitan areas, shares of moderately diverse white tracts in 2010 are anticipated by their 1990s neighborhood dynamics, suggesting temporal stability and a locational specificity in these processes. Others experience a temporal rupture in these dynamics, and their moderately diverse white tract share is better anticipated by pooling transition information. The study also invites us to think about the nature of residential change currently taking place that we can capture in 2020 census data. © The Author(s) 2017."
"10.5334/dsj-2018-028","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055752487&doi=10.5334%2fdsj-2018-028&partnerID=40&md5=8cc9587b1c90b36bd22d94e5a989ff28","EnviDat is the environmental data portal developed by the Swiss Federal Institute for Forest, Snow and Landscape Research WSL. The strategic initiative EnviDat highlights the importance WSL lays on Research Data Management (RDM) at the institutional level and demonstrates the commitment to accessible research data in order to advance environmental science. EnviDat focuses on registering and publishing environmental data sets and provides unified and efficient access to the WSL’s comprehensive reservoir of environmental monitoring and research data. Research data management is organized in a decentralized manner where the responsibility to curate research data remains with the experts and the original data providers. EnviDat supports data producers and data users in registration, documentation, storage, publication, search and retrieval of a wide range of heterogeneous data sets from the environmental domain. Innovative features include (i) a flexible, three-layer metadata schema, (ii) an additive data discovery model that considers spatial data and (iii) a DataCRediT mechanism designed for specifying data authorship. In addition, the overall user-friendly appearance in EnviDat provides an important opportunity for showcasing WSL research activities and results. The EnviDat portal builds on a conceptual system consisting of a core system, a set of guiding principles and a number of key services. Its development closely follows the conceptual framework, being guided by principles towards the ultimate goal of providing useful services for researchers. © 2018 The Author(s)."
"10.5334/dsj-2018-027","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055746219&doi=10.5334%2fdsj-2018-027&partnerID=40&md5=4df7b47419c8fa8350f6a0a94e639966","In certain cases, the only evidence to identify terrorists, who are seen in digital images or videos is their hands’ shapes, particularly, the victory sign as performed by many of them when they intentionally hide their faces, and/or distort their voices. This paper proposes new methods to identify those persons for the first time from their victory sign. These methods are based on features extracted from the fingers areas using shape moments in addition to other features related to fingers contours. To evaluate the proposed methods and to show the feasibility of this study we have created a victory sign database for 400 volunteers using a mobile phone camera. The experimental results using different classifiers show encouraging identification results; as the best precision/recall were achieved by merging normalized features from both methods using linear discriminate analysis classifier with 96.6% precision and 96.3 recall. Such a high performance achieved by the proposed methods shows their great potential to be applied for terrorists’ identification from their victory sign. © 2018 The Author(s)."
"10.5334/dsj-2018-026","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055743042&doi=10.5334%2fdsj-2018-026&partnerID=40&md5=84381e69679c34fb67b5de5600fed728","Research data is acquired, interpreted, published, reused, and sometimes eventually discarded. Understanding this life cycle better will help the development of appropriate infrastructural services, ones which make it easier for researchers to preserve, share, and find data. Structural biology is a discipline within the life sciences, one that investigates the molecular basis of life by discovering and interpreting the shapes and motions of macromolecules. Structural biology has a strong tradition of data sharing, expressed by the founding of the Protein Data Bank (PDB) in 1971. The culture of structural biology is therefore already in line with the perspective that data from publicly funded research projects are public data. This review is based on the data life cycle as defined by the UK Data Archive. It identifies six stages: creating data, processing data, analysing data, preserving data, giving access to data, and re-using data. For clarity, ʻpreserving dataʼ and ʻgiving access to dataʼ are discussed together. A final stage to the life cycle, ʻdiscarding dataʼ, is also discussed. The review concludes with recommendations for future improvements to the IT infrastructure for structural biology. © 2018 The Author(s)."
"10.5334/dsj-2018-025","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055730674&doi=10.5334%2fdsj-2018-025&partnerID=40&md5=1a56e7c7290d389f59cce4e7523cc1b6","This paper gives an overview of activities regarding RDM in Germany including the national political context as well as initiatives on federal state level. The knowledge about Germany’s federal system, which also entails the autonomy of the federal states regarding the higher education system, is fundamental to understand the different approaches towards RDM in Germany. The state initiatives of Thuringia, Baden-Wuerttemberg and Hesse are described to compare them to the state initiative (Landesinitiative NFDI) of Germany’s most populous state of North-Rhine Westphalia (NRW). The aim of the initiative in NRW is to initiate the collaboration between institutions, to link current RDM activities in NRW and to prepare the local institutions for the participation in a National Research Data Infrastructure (Nationale Forschungsdateninfrastruktur, NFDI). © 2018 The Author(s)."
"10.5334/dsj-2018-024","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055752940&doi=10.5334%2fdsj-2018-024&partnerID=40&md5=f2949b6845410bf81f22ef258e4fa3c1","As the mobile phone data (CDR data) has gained an increasing interest in research, such as social science, transportation, urban informatics, and big data, this study aims at examining the representativeness of the CDR data in terms of resemblance of the actual population density distribution from three perspectives; operator’s market share, urban-rural user population ratio, and user gender ratio. The results reveal that the representativeness of the data does not scale at the same rate with the operator’s market share, the urban-rural user population ratio of 80:20 can best represent the population density distribution, and an equal mixture of male and female user population can best resemble the population density distribution. This study is the first investigation into the representativeness of the CDR data. The findings provide useful information, which can serve an insightful guideline when dealing with the CDR data. © 2018 The Author(s)."
"10.1080/23270012.2018.1530619","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057140279&doi=10.1080%2f23270012.2018.1530619&partnerID=40&md5=3fb0960a19a9bfd1de73781545b6cb7f","This study deals an integrated manufacturer–buyer supply chain system for imperfect production under stochastic lead time demand. Here, defective rate has been followed as a function of production rate. Also, the produced units have been inspected in order to screen the defective units but screening rate is less than the production rate and greater than the demand rate. Buyer purchases the products from the manufacturer. Also, we assume that shortage during the lead time is permitted and demand during the shortage period is fully back-ordered. The objective is to derive the optimal production rate, ordering quantity and to maximize joint total profit. Basically, two different models for different probability distribution functions of stochastic lead time demand have been developed. Some numerical examples are provided to show the applicability of the proposed models comparing the optimum average profits. Finally, sensitive analysis, conclusion and future researches are presented. © 2018, © 2018 Antai College of Economics and Management, Shanghai Jiao Tong University."
"10.1080/23270012.2018.1508377","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057109206&doi=10.1080%2f23270012.2018.1508377&partnerID=40&md5=a7f6e4b761f7b251802e400a593bee0d",[No abstract available]
"10.1080/23270012.2018.1512059","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057102133&doi=10.1080%2f23270012.2018.1512059&partnerID=40&md5=3c403dbd7fdbc8ed61b358e628c6ffce","In the context of popularized healthcare, cloud computing centers are used to collect medical data from the cloud and diagnose illnesses. This means a technical framework that can be applied to the medical diagnostic process in popularized healthcare is needed in order to provide technical support. Based on the evidence fusion theory, this study established a multi-modality image evidence fusion method, which can simulate the doctor’s diagnostic process and use multiple modalities of medical images to diagnose illnesses. This study used the evidence fusion method to fuse two different modalities of medical images. The accuracy of the diagnosis after fusion was higher than that of diagnosis through two modalities separately. This fusion method has achieved great results in the process of multi-modality image fusion. © 2018, © 2018 Antai College of Economics and Management, Shanghai Jiao Tong University."
"10.1080/23270012.2018.1474390","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057089212&doi=10.1080%2f23270012.2018.1474390&partnerID=40&md5=69a520b4c30cecd7a14fc3d2cb9eb1f6","Vehicle replacement time depends mainly on the rate of deterioration. Overloading has been found to be a significant factor of deterioration and cost accumulation. We examined the impact of overloading on vehicle deterioration and its total costs and hence the replacement time of vehicle fleets in Nigeria. The research design was a mixture of field survey and a longitudinal study of vehicle fleets of three transport companies in Benin City, Nigeria. The population of the study consisted of all interstate transport companies in Benin City. Multi-stage sampling was used to select the three transport companies studied. An overloading factor was developed and used to estimate the cost of operating overloaded vehicles. Research data were analysed using discrete dynamic programming, which was implemented using computer software developed through Pascal Programming. Results indicate that overloading significantly precipitates vehicle deterioration, increases operating and total cost and thus affects replacement time of vehicles. © 2018, © 2018 Antai College of Economics and Management, Shanghai Jiao Tong University."
"10.1080/23270012.2018.1478329","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057073517&doi=10.1080%2f23270012.2018.1478329&partnerID=40&md5=24447800b83dafbcc88eeb97f4b42f5b","Based on joint-innovation patent data from 2000 to 2016 in the Beijing–Tianjin–Hebei region of China, the purpose of this paper is to analyze how technological proximity affects university–industry collaborative innovation in the Beijing–Tianjin–Hebei region. We adopt a 1:1 matching design to conduct an empirical study. The results show that the effect of technological proximity on the formation of collaborative innovation displays an inverted U-shape, and geographical proximity and institutional proximity play a positive role of forming a tie. Geographical proximity and institutional proximity as a coordination mechanism, have negatively influenced the relationship between technological proximity and the formation of university–industry collaborative innovation. Furthermore, university strength improves the possibility of collaborative innovation. These findings contributed to the understanding of the relationship between technological proximity and collaborative innovation. © 2018, © 2018 Antai College of Economics and Management, Shanghai Jiao Tong University."
"10.1080/23270012.2018.1490211","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053452209&doi=10.1080%2f23270012.2018.1490211&partnerID=40&md5=a6a82e14164155a6a1fc61ec425728f7","Regional medical unions are practical approaches to deal with the cases that patients crowd in Grade 3 Class A general hospitals in metropolitan cities such as Shanghai, in China. However, electronic medical data analysis exists challenges when patients are referred among different hospitals in the regional medical unions during treatment procedures. In smart cities, demands for medical services provided by smart devises, complicate the environment of medical data analysis. In order to tackle the above problems, in this paper, a healthcare data analysis system for regional medical union is designed to support doctors from different hospitals to assess health conditions of patients in an overall data view. Behaviour patterns are mined from physiological index values. Tags are generated from social networks data to find the hot topics concerned by people living in common region. Experiments are given to illustrate the feasibility of the system in supporting healthcare data analysis. © 2018, © 2018 Antai College of Economics and Management, Shanghai Jiao Tong University."
"10.4018/IJBAN.2018100101","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052713215&doi=10.4018%2fIJBAN.2018100101&partnerID=40&md5=53e781fab05e8b294edb11c475f426e8","Cloud computing is one of emerging computing models that has many advantages. The IT industry is keenly aware of the need for Green Cloud computing solutions that save energy for the environment as well as reduce operational costs. This article presents a new green Cloud Computing framework based on multi agent systems for optimizing resource allocation in data centers (DCs). Our framework based on a new cloud computing architecture that benefits from the combination of the Cloud and agent technologies. DCs hosting Cloud applications need energy-Aware resource allocation mechanisms that minimize energy costs and other operational costs. This article offers a logical solution to manage physical and virtual resources in smarter data center. Copyright © 2018, IGI Global. Copying."
"10.4018/IJBAN.2018100102","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052697353&doi=10.4018%2fIJBAN.2018100102&partnerID=40&md5=bc7475f38d4054e16c34fa83fcaf8b82","This article presents a restricted maximum likelihood-based algorithm to estimate who influences whose opinions and to what degree when agents share their opinions over large online social networks such as Twitter. The proposed algorithm uses multi-core processing and distributed computing to provide a scalable solution as the optimization problems are large in scale; a network with 10,000 agents and average connectivity of 100 requires estimates of about 1 million parameters. A computational study is then used to show that the estimates are efficient and robust when the full rank conditions for the covariance matrix are met. The results also highlight the importance of the quantity of the information being shared over the social network for the inference of the influence structure. Copyright © 2018, IGI Global. Copying."
"10.4018/IJBAN.2018100103","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052694991&doi=10.4018%2fIJBAN.2018100103&partnerID=40&md5=e7a5db060e1ebd1fe87c9b5d6709bde3","As more print media move to online, news and media websites have evolved with increasing complexity in content, design, and monetization strategies. In this article, the authors examined and reported the web design patterns of 150 leading news and media websites in six different categories: TV news, online newspapers, online magazines, and technology news, sports news, and business news, using 28 analytics metrics in four dimensions: content structure, multimedia, social sharing, and advertising placements. Copyright © 2018, IGI Global. Copying."
"10.4018/IJBAN.2018100105","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052680100&doi=10.4018%2fIJBAN.2018100105&partnerID=40&md5=05e8c4a71a43d0a5965a833d60eafb89","Nowadays, one of the most significant concerns is regarding to air pollution. Carbon dioxide, a greenhouse gas, is the main pollutant that is warming Earth. Industrialized countries have worked to reduce levels of sulfur dioxide, smog, and smoke in order to improve people's health. The problem of air pollution in the metropolises of Iran has been increased. The identification and development of clean fuels and also alternative fuels can play an important role in solving pollution problem. In this article, to find best fuel in terms of economic, social and environmental aspects which are part of sustainability. So, the multi-criteria decision-making techniques like fuzzy hierarchy analysis method and PROMETHEE used to rank and determine the preferred option for alternative fuels used in high-speed buses in Tehran. The results show that the biodiesel was selected as the best fuel to prevent the air pollution emission among the available options. Copyright © 2018, IGI Global."
"10.4018/IJBAN.2018100104","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052675168&doi=10.4018%2fIJBAN.2018100104&partnerID=40&md5=995b92447b09fdd975be619cad3426ef","This article evaluates online grocery shopping web sites catering to customers primarily in India. The process of evaluation has been carried out in 3 parts using Rapidminer. In part A, the authors have studied the similarity in content that resides on the grocery shopping web sites. Using unstructured data from homepage of grocery shopping websites and the keywords specified for the web sites, the authors have made an effort to establish a cosine similarity index amongst them. In part B, the authors have analysed the customer reviews from the web sites. Studying the resulting association rules, authors have attempted to identify the attributes that drive customer happiness. In part C, the authors have documented the web traffic metric parameters (attributes) measured by search engine optimization (SEO) tool web sites. Hence, the created a correlation matrix to determine the parameters that are significantly impacting per day revenue for the web sites. Copyright © 2018, IGI Global. Copying."
"10.5334/dsj-2018-023","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055735711&doi=10.5334%2fdsj-2018-023&partnerID=40&md5=84905eb177cc7a5b1dc863c533e47cad","The domain ontology, which plays a significant role in knowledge-based systems, still needs the manual work of domain experts to be constructed currently. The main motivation of this paper is to provide a semi-automatic platform which can construct fairly comprehensive domain ontology from unstructured data. Firstly, a brief QA process is proposed to simplify the interaction with the domain experts. A novel algorithm MPVW, which extends from the classical algorithm TF-IDF, is proposed to extract the terminologies from domain documents. MPVW balanced more parameters and factors to evaluate the feature of terminologies. The 3-layers taxonomy and terminology hyponymy height provide sufficient guide and prompt for domain experts to construct ontology from terminologies. According to our approach we have developed ROCP, a rapid ontology construction platform which has been applied in the space debris mitigation domain. The experimental data indicates that ROCP has sufficient accuracy to extract terminologies. Meanwhile, it is effective to relieve the labor of domain experts to construct domain ontology. © 2018 The Author(s)."
"10.5334/dsj-2018-021","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055745991&doi=10.5334%2fdsj-2018-021&partnerID=40&md5=3d2c38cd1a3dcc7f70fd41af14f0e603","Interpreting observational data is a fundamental task in the sciences, specifically in earth and environmental science where observational data are increasingly acquired, curated, and published systematically by environmental research infrastructures. Typically subject to substantial processing, observational data are used by research communities, their research groups and individual scientists, who interpret such primary data for their meaning in the context of research investigations. The result of interpretation is information—meaningful secondary or derived data—about the observed environment. Research infrastructures and research communities are thus essential to evolving uninterpreted observational data to information. In digital form, the classical bearer of information are the commonly known “(elaborated) data products,” for instance maps. In such form, meaning is generally implicit e.g., in map colour coding, and thus largely inaccessible to machines. The systematic acquisition, curation, possible publishing and further processing of information gained in observational data interpretation—as machine readable data and their machine readable meaning—is not common practice among environmental research infrastructures. For a use case in aerosol science, we elucidate these problems and present a Jupyter based prototype infrastructure that exploits a machine learning approach to interpretation and could support a research community in interpreting observational data and, more importantly, in curating and further using resulting information about a studied natural phenomenon. © 2018 The Author(s)."
"10.5334/dsj-2018-022","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055731302&doi=10.5334%2fdsj-2018-022&partnerID=40&md5=43c3cc701fe46ae59fe1aeff662f4a52","Environmental research data repositories provide much needed services for data preservation and data dissemination to diverse communities with domain specific or programmatic data needs and standards. Due to independent development these repositories serve their communities well, but were developed with different technologies, data models and using different ontologies. Hence, the effectiveness and efficiency of these services can be vastly improved if repositories work together adhering to a shared community platform that focuses on the implementation of agreed upon standards and best practices for curation and dissemination of data. Such a community platform drives forward the convergence of technologies and practices that will advance cross-domain interoperability. It will also facilitate contributions from investigators through standardized and streamlined workflows and provide increased visibility for the role of data managers and the curation services provided by data repositories, beyond preservation infrastructure. Ten specific suggestions for such standardizations are outlined without any suggestions for priority or technical implementation. Although the recommendations are for repositories to implement, they have been chosen specifically with the data provider/data curator and synthesis scientist in mind. © 2018 The Author(s)."
"10.5334/dsj-2018-020","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055732194&doi=10.5334%2fdsj-2018-020&partnerID=40&md5=4bf2aefe64bc6b9caadccdbbec8030f5","Accurate, up-to-date maps of and georeferenced data about human population distribution are essential for meeting the United Nations Sustainable Development Goals progress measures, for supporting real-time crisis mapping and response efforts, and for performing many demographic and economic analyses. In December 2014, Esri published the initial version of the World Population Estimate (WPE) image service to ArcGIS Online. The service represents a dasymetric footprint of human settlement at 250-meter resolution. It is global and contains an estimate of the 2013 population for each populated cell. In 2016 Esri published an additional image service representing the earth’s population in 2015 at 162-meter resolution. Esri’s WPE is produced by combining classified land cover data indicating predominantly built-up or agricultural locations with Landsat8 Panchromatic imagery, road intersections, and known populated places. The model detects where settlement is likely to exist beyond the areas classified as predominantly built up. The result is a global dasymetric raster surface of the footprint of settlement with a score of the likelihood of human settlement for each cell of the footprint. Population data are apportioned to this settlement likelihood surface by overlaying population counts in polygons representing census enumeration units or political units representing population surveys. This paper presents the method developed at Esri for producing the estimate of settlement likelihood. © 2018 The Author(s)."
"10.1089/big.2018.0092","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053795247&doi=10.1089%2fbig.2018.0092&partnerID=40&md5=9039150f3be24fe40ff4e1d67d908f9e","We develop a number of data-driven investment strategies that demonstrate how machine learning and data analytics can be used to guide investments in peer-to-peer loans. We detail the process starting with the acquisition of (real) data from a peer-to-peer lending platform all the way to the development and evaluation of investment strategies based on a variety of approaches. We focus heavily on how to apply and evaluate the data science methods, and resulting strategies, in a real-world business setting. The material presented in this article can be used by instructors who teach data science courses, at the undergraduate or graduate levels. Importantly, we go beyond just evaluating predictive performance of models, to assess how well the strategies would actually perform, using real, publicly available data. Our treatment is comprehensive and ranges from qualitative to technical, but is also modular - which gives instructors the flexibility to focus on specific parts of the case, depending on the topics they want to cover. The learning concepts include the following: data cleaning and ingestion, classification/probability estimation modeling, regression modeling, analytical engineering, calibration curves, data leakage, evaluation of model performance, basic portfolio optimization, evaluation of investment strategies, and using Python for data science. © Maxime C. Cohen et al., 2018."
"10.1089/big.2018.0002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053756655&doi=10.1089%2fbig.2018.0002&partnerID=40&md5=99713a07b2ea34ca12b66d8f61468a36","Existing methods of screening for substance abuse (standardized questionnaires or clinician's simply asking) have proven difficult to initiate and maintain in primary care settings. This article reports on how predictive modeling can be used to screen for substance abuse using extant data in electronic health records (EHRs). We relied on data available through Veterans Affairs Informatics and Computing Infrastructure (VINCI) for the years 2006 through 2016. We focused on 4,681,809 veterans who had at least two primary care visits; 829,827 of whom had a hospitalization. Data included 699 million outpatient and 17 million inpatient records. The dependent variable was substance abuse as identified from 89 diagnostic codes using the Agency for Healthcare Quality and Research classification of diseases. In addition, we included the diagnostic codes used for identification of prescription abuse. The independent variables were 10,292 inpatient and 13,512 outpatient diagnoses, plus 71 dummy variables measuring age at different years between 20 and 90 years. A modified naive Bayes model was used to aggregate the risk across predictors. The accuracy of the predictions was examined using area under the receiver operating characteristic (AROC) curve in 20% of data, randomly set aside for the evaluation. Many physical/mental illnesses were associated with substance abuse. These associations supported findings reported in the literature regarding the impact of substance abuse on various diseases and vice versa. In randomly set-aside validation data, the model accurately predicted substance abuse for inpatient (AROC = 0.884), outpatient (AROC = 0.825), and combined inpatient and outpatient (AROC = 0.840) data. If one excludes information available after substance abuse is known, the cross-validated AROC remained high, 0.822 for inpatient and 0.817 for outpatient data. Data within EHRs can be used to detect existing or predict potential future substance abuse. © Farrokh Alemi et al., 2018."
"10.1089/big.2018.0064","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053750634&doi=10.1089%2fbig.2018.0064&partnerID=40&md5=09894e9b0ebaa1700fdec0e0e370df40","The advances in information technology of both hardware and software have allowed big data to emerge recently, classification of such data is extremely slow, particularly when using K-nearest neighbors (KNN) classifier. In this article, we propose a new approach that creates a binary search tree (BST) to be used later by the KNN to speed up the big data classification. This approach is based on finding the furthest-pair of points (diameter) in a data set, and then, it uses this pair of points to sort the examples of the training data set into a BST. At each node of the BST, the furthest-pair is found and the examples located at that particular node are further sorted based on their distances to these local furthest points. The created BST is then searched for a test example to the leaf; the examples found in that particular leaf are used to classify the test example using the KNN classifier. The experimental results on some well-known machine learning data sets show the efficiency of the proposed method, in terms of speed and accuracy compared with the state-of-the-art methods reviewed. With some optimization, the proposed method has a great potential to be used for big data classification and can be generalized for other applications, particularly when classification speed is the main concern. © Copyright 2018, Mary Ann Liebert, Inc."
"10.1016/j.bdr.2018.06.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049558621&doi=10.1016%2fj.bdr.2018.06.001&partnerID=40&md5=a4bf72e47224ab31a8c20f42fab1caea","The value of knowledge assets generated by analytics processes using Data Science techniques tends to decay over time, as a consequence of changes in the elements the process depends on: external data sources, libraries, and system dependencies. For large-scale problems, refreshing those outcomes through greedy re-computation is both expensive and inefficient, as some changes have limited impact. In this paper we address the problem of refreshing past process outcomes selectively, that is, by trying to identify the subset of outcomes that will have been affected by a change, and by only re-executing fragments of the original process. We propose a technical approach to address the selective re-computation problem by combining multiple techniques, and present an extensive experimental study in Genomics, namely variant calling and their clinical interpretation, to show its effectiveness. In this case study, we are able to decrease the number of required re-computations on a cohort of individuals from 495 (blind) down to 71, and that we can reduce runtime by at least 60% relative to the naïve blind approach, and in some cases by 90%. Starting from this experience, we then propose a blueprint for a generic re-computation meta-process that makes use of process history metadata to make informed decisions about selective re-computations in reaction to a variety of changes in the data. © 2018 Elsevier Inc."
"10.1016/j.bdr.2018.05.004","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048925853&doi=10.1016%2fj.bdr.2018.05.004&partnerID=40&md5=f71ed6c6888a6f5b99801103eb03440b","Feature selection for predictive analytics continues to be a major challenge in the healthcare industry, particularly as it relates to readmission prediction. Several research works in mining healthcare data have focused on structured data for readmission prediction. Even within those works that are based on unstructured data, significant gaps exist in addressing class imbalance, context specific noise removal which thus necessitates new approaches readmission prediction using unstructured data. In this work, a novel approach is proposed for feature selection and domain related stop words removal from unstructured with class imbalance in discharge summary notes. The proposed predictive model uses these features along with other relevant structured data. Five iterations of predictions were performed to tune and improve the models, results of which are presented and analyzed in this paper. The authors suggest future directions in implementing the proposed approach in hospitals or clinics aimed at leveraging structured and unstructured discharge summary notes. © 2018 Elsevier Inc."
"10.1016/j.bdr.2018.05.003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048150207&doi=10.1016%2fj.bdr.2018.05.003&partnerID=40&md5=cb89b4f2087af4adf5935dab2d9518a1","The majority of the clinical observation data stored in large-scale Electronic Health Record (EHR) research data networks are unlabeled. Unsupervised clustering can provide invaluable tools for studying patient sub-groups in these data. Many of the popular unsupervised clustering algorithms are dependent on identifying the number of clusters. Multiple statistical methods are available to approximate the number of clusters in a dataset. However, available methods are computationally inefficient when applied to large amounts of data. Scalable analytical procedures are needed to extract knowledge from large clinical datasets. Using both simulated, clinical, and public data, we developed and tested the kluster procedure for approximating the number of clusters in a large clinical dataset. The kluster procedure iteratively applies four statistical cluster number approximation methods to small subsets of data that were drawn randomly with replacements and recommends the most frequent and mean number of clusters resulted from the iterations as the potential optimum number of clusters. Our results showed that the kluster's most frequent product that iteratively applies a model-based clustering strategy using Bayesian Information Criterion (BIC) to samples of 200–500 data points, through 100 iterations, offers a reliable and scalable solution for approximating the number of clusters in unsupervised clustering. We provide the kluster procedure as an R package. © 2018 Elsevier Inc."
"10.1016/j.bdr.2018.03.003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047445987&doi=10.1016%2fj.bdr.2018.03.003&partnerID=40&md5=4db57db2b3a43ed391e19a1beaf6af3a","Predicting risk of adverse events (AEs) following surgical procedure is of significant interest, as that may guide in better resource utilization and an improved quality of care. Currently available comorbidity indices are largely inaccurate to predict adverse events other than death, as well as off-the-shelf machine learning models do not typically account for the temporal sequence of events to enable predictive analytics. We propose a study to improve the current techniques for assessing and predicting the risk of adverse events (AEs) associated with multiple chronic conditions by designing machine learning models that account for and incorporate the temporal sequence and timing of conditions. We formalize the task as a binary classification problem. Our technical contributions include devising novel sequence based feature discovery techniques to augment existing supervised classification algorithms, as well as formalizing the classification task as a Markov Chain Model (MCM) that captures the temporal sequence of prior chronic conditions/events. Finally, we design a hybrid or multi-classifier that combines prediction from the aforementioned classification models to finally predict AE. Our experimental results, conducted using the Truven Health MarketScan Research Databases with more than 27 million of claim records on two different surgery types, discover interesting insights that can guide patient-centered decision-making and can direct healthcare teams to adjust techniques and interventions. We also extensively compare the performance of our solutions to appropriate baselines. © 2018 Elsevier Inc."
"10.1016/j.bdr.2018.05.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047072435&doi=10.1016%2fj.bdr.2018.05.001&partnerID=40&md5=8165afa8ab69c43fdb8d26fce15cc155","Patient similarity analysis is a precondition to apply machine learning technology on medical data. In this sense, patient similarity analysis harnesses the information wealth of electronic medical records (EMRs) to support medical decision making. A pairwise similarity computation can be used as the basis for personalized health prediction. With n patients the amount of (n2) similarity calculations is required. Thus, analyzing patient similarity leads to data explosion when exploiting big data. By increasing the data size the computational burden of this analysis increases. A real-life medical application may exceed the limits of current hardware in a fairly short amount of time. Finding ways to optimize patient similarity analysis and handling this data explosion is the topic of this paper. Current implementations for patient similarity analysis require their users to have knowledge of complex data analysis tools. Moreover, data pre-processing and analysis are performed in synthetic conditions: the data are extracted from the EMR database and then the data preparation and analysis are processed in external tools. After all of this effort the users might not experience a superior performance of the patient similarity analysis. We propose methods to optimize the patient similarity analysis in order to make it scalable to big data. Our method was tested against two real datasets and a low execution time was accomplished. Our result hence benefits a comprehensive medical decision support system. Moreover, our implementation comprises a balance between performance and applicability: the majority of the workload is processed within a database management system to enable a direct implementation on an EMR database. © 2018 Elsevier Inc."
"10.1016/j.bdr.2018.03.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046872300&doi=10.1016%2fj.bdr.2018.03.002&partnerID=40&md5=c1969c4f58430fecaab5468a17483c96","Health insurance companies in Brazil have their data about claims organized having the view only for service providers. In this way, they lose the view of physicians’ activity and how physicians share patients. Partnership between physicians can be seen as fruitful, when they team up to help a patient, but could represent an issue as well, when a recommendation to visit another physician occurs only because they work in same clinic. This work took place during a short-term project involving a partnership between our lab and a large health insurance company in Brazil. The goal of the project was to provide insights (with business impact) about physicians’ activity from the analysis of the claims database. This work presents one of the outcomes of the project, i.e., a way of modeling the underlying referrals in the social network of physicians resulting from health insurance claims data. The approach considers the flow of patients through the physician–physician network, highlighting connections where referrals between physicians potentially occurred. We present the results from the analysis of a claims database (detailing 18 months of activity) from the health insurance company we partnered with. The main contribution presented in this paper is the model to reveal mutual referrals between physicians. Results show the proposed model reveals underlying characteristics of physicians’ activity from real health insurance claims data with multiple business applications. © 2018 Elsevier Inc."
"10.1016/j.bdr.2018.02.005","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044256269&doi=10.1016%2fj.bdr.2018.02.005&partnerID=40&md5=acdcdda7dc718dd56b0687ab8034efb3","DNA methylation is a well-studied genetic modification crucial to regulate the functioning of the genome. Its alterations play an important role in tumorigenesis and tumor-suppression. Thus, studying DNA methylation data may help biomarker discovery in cancer. Since public data on DNA methylation become abundant – and considering the high number of methylated sites (features) present in the genome – it is important to have a method for efficiently processing such large datasets. Relying on big data technologies, we propose BIGBIOCL an algorithm that can apply supervised classification methods to datasets with hundreds of thousands of features. It is designed for the extraction of alternative and equivalent classification models through iterative deletion of selected features. We run experiments on DNA methylation datasets extracted from The Cancer Genome Atlas, focusing on three tumor types: breast, kidney, and thyroid carcinomas. We perform classifications extracting several methylated sites and their associated genes with accurate performance (accuracy >97%). Results suggest that BIGBIOCL can perform hundreds of classification iterations on hundreds of thousands of features in few hours. Moreover, we compare the performance of our method with other state-of-the-art classifiers and with a wide-spread DNA methylation analysis method based on network analysis. Finally, we are able to efficiently compute multiple alternative classification models and extract – from DNA-methylation large datasets – a set of candidate genes to be further investigated to determine their active role in cancer. BIGBIOCL, results of experiments, and a guide to carry on new experiments are freely available on GitHub at https://github.com/fcproj/BIGBIOCL. © 2018 Elsevier Inc."
"10.1016/j.bdr.2018.02.006","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85043374398&doi=10.1016%2fj.bdr.2018.02.006&partnerID=40&md5=c1ebc34779e7c153cb817fbb2f3a4e07","Healthcare reimbursement has had a tremendous impact on healthcare institutions and the economy. The healthcare reimbursement process consists of coding, billing, and payment based on the care provided to the patient. The rapid development of new medical treatments and procedures and changes in regulations and policies have been increasing the complexity of the reimbursement process, resulting in financial, operational, and care delivery issues for healthcare institutions. Therefore, methods of process analysis, such as process mining, have been used as a basic strategy to improve the organizational effectiveness of healthcare institutions. In this context, the main objective of this study is to propose an approach to investigate the factors that may cause delays in the reimbursement process using a combination of process mining and data mining techniques to extract information from the process data and support decision-making. To accomplish this analysis, process mining is applied to map the reimbursement process from the event log and to determine possible bottlenecks. In contrast, data mining is used to identify frequent patterns and interesting associations in the process data. Finally, by applying the proposed approach to a real case of a healthcare institution in Brazil, we extracted valuable insights regarding process execution and confirmed the effectiveness and potential of combining process mining and the association rules mining techniques. © 2018 Elsevier Inc."
"10.1007/s41019-018-0070-8","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062710273&doi=10.1007%2fs41019-018-0070-8&partnerID=40&md5=c14df90685d74deea637c68da21ab1bd","Analyzing job hopping behavior is important for understanding job preference and career progression of working individuals. When analyzed at the workforce population level, job hop analysis helps to gain insights of talent flow among different jobs and organizations. Traditionally, surveys are conducted on job seekers and employers to study job hop behavior. Beyond surveys, job hop behavior can also be studied in a highly scalable and timely manner using a data-driven approach in response to fast-changing job landscape. Fortunately, the advent of online professional networks (OPNs) has made it possible to perform a large-scale analysis of talent flow. In this paper, we present a new data analytics framework to analyze the talent flow patterns of close to 1 million working professionals from three different countries/regions using their publicly accessible profiles in an established OPN. As OPN data are originally generated for professional networking applications, our proposed framework repurposes the same data for a different analytics task. Prior to performing job hop analysis, we devise a job title normalization procedure to mitigate the amount of noise in the OPN data. We then devise several metrics to measure the amount of work experience required to take up a job, to determine that the duration of a job’s existence (also known as the job age), and the correlation between the above metric and propensity of hopping. We also study how job hop behavior is related to job promotion/demotion. Lastly, we perform connectivity analysis at job and organization levels to derive insights on talent flow as well as job and organizational competitiveness. © 2018, The Author(s)."
"10.1007/s41019-018-0076-2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062699494&doi=10.1007%2fs41019-018-0076-2&partnerID=40&md5=d745868e36487ab1f7a09161efd793e0","Continuous training is crucial for creating and maintaining the right skill-profile for the industrial organization’s workforce. There is a tremendous variety in the available trainings within an organization: technical, project management, quality, leadership, domain-specific, soft-skills, etc. Hence it is important to assist the employee in choosing the best trainings, which perfectly suits her background, project needs and career goals. In this paper, we focus on algorithms for training recommendation in an industrial setting. We formalize the problem of next training recommendation, taking into account the employee’s training and work history. We present several new unsupervised sequence mining algorithms to mine the past trainings data from the organization for arriving at personalized next training recommendation. Using the real-life data about trainings of 118,587 employees over 5019 distinct trainings from a large multi-national IT organization, we show that these algorithms outperform several standard recommendation engine algorithms as well as those based on standard sequence mining algorithms. © 2018, The Author(s)."
"10.1007/s41019-018-0072-6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062695716&doi=10.1007%2fs41019-018-0072-6&partnerID=40&md5=b70d52a2629b9774965bb97ca4bc1e54","Hardware techniques and environments underwent significant transformations in the field of information technology, represented by high-performance processors and hardware accelerators characterized by abundant heterogeneous parallelism, nonvolatile memory with hybrid storage hierarchies, and RDMA-enabled high-speed network. Recent hardware trends in these areas deeply affect data management and analysis applications. In this paper, we first introduce the development trend of the new hardware in computation, storage, and network dimensions. Then, the related research techniques which affect the upper data management system design are reviewed. Finally, challenges and opportunities are addressed for the key technologies of data management and analysis in new hardware environments. © 2018, The Author(s)."
"10.1007/s41019-018-0073-5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062685946&doi=10.1007%2fs41019-018-0073-5&partnerID=40&md5=9b393a4f430c2a9600f87258b075a5c2","Recommender system is one of the most important components for many companies and social networks such as Facebook and YouTube. A recommendation system consists of algorithms which allow to predict and recommend friends or products. This paper studies to facilitate finding like-minded people with same interests in social networks. In our research, we used real data from the most popular social network in Russia, VK (Vkontakte). The study is motivated on the assumption that similarity breeds connection. We evaluate well-known similarity measures in the field on our collected VK datasets and find limited performance results. The result shows that majority of users in VK tend not to add possible users with whom they have common acquaintances. We also propose a topology-based similarity measure to predict future friends. Then, we compare our results with the results of other well-known methods and discuss differences. © 2018, The Author(s)."
"10.1007/s41019-018-0075-3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062638818&doi=10.1007%2fs41019-018-0075-3&partnerID=40&md5=226865e8336429f6af66c047c5958089","This paper proposes an approach to estimating fungibility between skills given multiple information sources of those skills. An estimate of skill adjacency or fungibility or substitutability is critical for effective capacity planning, analytics and optimization in the face of changing skill requirements of an organization. The proposed approach is based on computing a similarity measure between skills, using each available data source, and combining these similarities into a measure of fungibility. We present both supervised and unsupervised integration methods and demonstrate that these produce improved outcomes, compared to using any single skill similarity source alone, using data from a large IT organization. The skills’ fungibility matrix created using this approach has been deployed by the organization for demand forecasting across groups of skills. We discuss how the fungibility matrix is deployed to generate skill clusters and present a forecasting algorithm that additionally incorporates past/future engagements and a mechanism to quantify uncertainty in the forecast. A possible extension of this work is to use the fungibility measure to cluster skills and develop a skill-centric representation of an organization to enable strategic assessments and planning. © 2018, The Author(s)."
"10.1007/s41019-018-0071-7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051520166&doi=10.1007%2fs41019-018-0071-7&partnerID=40&md5=b2da99c03ad54cee5b78b6abb4ff33f3","In the recruitment domain, knowing the employer industry of jobs is important to get an insight about the demand in each industry. The existing system at CareerBuilder uses an employer name normalization system and an employer knowledge base (KB) to infer the employer industry of a job. However, errors may occur during the computation of the job employer and in the construction of the employer KB with the industry attributes. Since the KB is huge, it is not possible to manually detect the errors. Therefore, in this paper we use machine learning techniques to automatically detect the errors. With the observation that the main jobs posted by an employer often relate to the employer industry, e.g., truck driver jobs often correspond to employers in the transportation industry, we develop a system that classifies the industry of an employer using job posting data. We aggregate job postings from an employer and derive features from employer names, employer descriptions, job titles, and job descriptions to predict the industry of the employer. Two models are used for classification: (1) support vector machine and (2) random forest. Our experiments show that random forest is more effective than SVM in identifying the errors in the existing industry classification system, which achieves precision 0.69, recall 0.78, and f-score 0.73. It especially better handles mixed feature vectors when normalization errors occur. We also observe that generally our models perform better in detecting errors for industries that have higher error rates. © 2018, The Author(s)."
,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059198426&partnerID=40&md5=08f10e5e09103c37d07a8e5d1a30a027","This is the age of data-driven marketing. Marketers utilise data to target better, to improve communication and to learn more about their customers. And yet, marketers seldom utilise data to be more creative and engage better with the person behind the data. As this has been the cornerstone of advertising for decades, it is time to start looking at data in a different way. The full resonance framework outlined in this paper helps analytics professionals to identify the actionable insights that resonate with the target audience on an emotional and motivational level and which form the foundation of outstanding creative advertising. © Henry Stewart Publications."
"10.1007/978-3-319-10912-1_126","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059189110&doi=10.1007%2f978-3-319-10912-1_126&partnerID=40&md5=001e6dbe3914021e650fe17ee189f608","This paper discusses the issue of low stability associated with free-choice brand-attribute associations where the second interview with a sample of respondents does not yield the same positive answers as the first interview. This paper recommends a novel two-step approach, using unrestricted attribute-elicitation procedure mapping (UAM) in conjunction with longitudinal generalised Procrustes analysis (GPA) to resolve the low stability problem. The paper offers a general discussion of the subject along with specific propositions. © Henry Stewart Publications."
,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059174658&partnerID=40&md5=6ef06d7ec9361ce10023248d1aee03dd","Compared with the digital experience, the shopper in-store journey has traditionally been under-measured. Stores know how many shoppers they get (door-counting) and what they have sold (point-of-sale). What happens in between, however, has long been a mystery. Thanks to new technologies such as camera-tracking, Wi-Fi geolocation and mobile app location, however, the in-store shopper journey is becoming increasingly trackable. This paper looks at the basics of shopper measurement: what gets tracked and what the data look like. It then reviews the core collection technologies with attention to their strengths and weaknesses, relative to specific business use cases. This is followed by an overview of the various metrics that can be inferred from the data: everything from draws to lingers to exits. Metrics provide a language for describing the data in intelligible terms, and the in-store journey requires a new language — although it is one that has been heavily adapted from digital analytics. Finally, some key use cases for such data are considered, allowing the analyst to go from the basics of collection to data aggregation and translation, and finally to real analysis. © Henry Stewart Publications."
,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059150637&partnerID=40&md5=b8d4381f313d20cd3615de8bc068c1d5","This paper demonstrates that various psychological patterns and a lack of knowledge of basic statistical principles can lead to erroneous interpretations of A/B tests. It explains the statistics behind testing with emphasis on the pitfalls to be avoided. Readers will gain an understanding of the importance of significance in A/B testing, but also learn that a significant test is no guarantee of a universally valid outcome. For reliable results, experimenters must also select a representative time frame. Finally, the paper discusses methods to create a healthy testing culture. © Henry Stewart Publications."
,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059120730&partnerID=40&md5=f95c60d8f435a1ba63dd18cfda97d0c2","The need for true incremental marketing measurement is well established but the ability to do this varies by channel. This paper describes matched market lift (MML), a methodology designed to bring accountability and objectivity to channels where user-level targeting and randomised control trials are impossible. The paper provides step-by-step guidance to matching geographical markets, and advice on how to apply this methodology to online and offline marketing measurement. © Henry Stewart Publications."
,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059119910&partnerID=40&md5=54820022e2faaa6bbdf93cf9d76e546e","The biggest influence on the customer experience, particularly in digital, is the experience customers have as they seek to complete their top tasks. Digital tends to be a very functional, utilitarian environment. Even in social spaces, people are very active. People may not mind wasting time on Facebook with their friends. However, Facebook would not survive long if people felt they were wasting time uploading pictures or trying to figure out how to tag those pictures. Time is a brutal determiner of success, and digital leaders measure customer time in milliseconds, not seconds. Digital is an explosion of possibilities — an almost unlimited world of content and tools. Yet, what truly matters to people when making a decision remains very small and concise. ‘Top tasks’ are those things that matter most to someone when they are deciding to buy a car, choose a university, or select another product or service. By contrast, ‘tiny tasks’ are those kinds of organisation-centric tasks that explode with content, often severely disrupting customers’ top tasks journey. Thus, an important step in delivering excellent customer experience is to remove or mitigate the influence of tiny tasks. This paper aims to explain how the Top Tasks methods work and how you can apply them to develop a better understanding of what matters most to your customers. © Henry Stewart Publications."
,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059118212&partnerID=40&md5=4c18f9412389724d58ec1e2846feafef","User experience is critical to the performance of an organisation’s website and its key performance indicators (KPIs). However, one aspect of the customer experience that is often overlooked is page speed. Indeed, when organisations compete to create cutting-edge websites that are heavy with media assets and graphics, resulting in a noticeable impact on page speed, the customer experience is seldom taken into consideration. Even if they are aware of the potential impact on page speed, there is no clear way for organisations to measure this impact and connect it to such KPIs as revenue, transactions or lead completions. This paper provides a step-by-step approach to obtaining quantitative data on how page speed impacts primary KPIs. Further, this method can also be utilised in conjunction with the testing of future changes to the site, allowing for a deeper level of analysis, and a better understanding of true impact to the business. © Henry Stewart Publications."
,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059118031&partnerID=40&md5=b6ab506314b3a58d462a2a174166eba0","In today’s digital economy, marketing is increasingly driven by accurate data, thoughtfully constructed technology stacks (ie MarTech), Agile processes and constantly updated skill sets. Applications of artificial intelligence (AI) and machine learning — led by the rapid adoption of cloud computing — permeate most, if not all, marketing activities undertaken by small and large brands today. A data management platform (DMP) is at the heart of modern marketing: it combines data, technology, collaboration and multi-channel targeting/personalisation in ways that were unthinkable just a few years ago. Data privacy and consumer choices about how they would like their data to be used for better content play a very important role in this space. This paper covers key aspects of building a robust data management practice for medium and large firms that either already own a DMP or are looking to bring it in-house in the near future. The topics covered will also be very helpful for marketers wanting to make optimal use of DMPs owned by their agency partners. As always, no matter how good the data, it is invariably the correct composition of people, processes and technology that yields the best results. These critical aspects are highlighted herein along with tips on setting up experiments to evaluate success and to improve over time by adopting a ‘test and learn’ approach. © FedEx Services, 2018."
"10.1057/s41270-018-0037-7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048525741&doi=10.1057%2fs41270-018-0037-7&partnerID=40&md5=6a0e36e7b4e7250df80ded02eec13006","Brand positioning is frequently facilitated by the use of perceptual maps. Several approaches exist for deriving such maps. This research uses the variability inherent in customer data to build confidence regions around brands and attributes in perceptual maps. Doing so generalizes the typical descriptive approach to a truer, statistical inferential approach to mapping. The resulting visualizations clarify the interpretations regarding which brands are similar, with overlapping confidence regions, and which brands are distinct, given non-overlapping confidence ellipses. The modeling is first demonstrated on a small, synthetic dataset and then on real consumer data. The model extension is shown to be useful, and it is relatively straightforward in implementation. It is hoped that this extension to this frequently used market mapping approach should enhance interpretive precision, and therefore, lead to more accurate and successful strategic positioning decisions. © 2018, Macmillan Publishers Ltd., part of Springer Nature."
"10.1057/s41270-018-0036-8","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048094014&doi=10.1057%2fs41270-018-0036-8&partnerID=40&md5=ae6db4c03335dff4e78de927cb2ad0a6","Marketing analytics (MA) is a relatively new but increasingly prominent field in which data tools are applied to quantify and monitor marketing performance and customer information to optimize investments in marketing programs and maximize customer interaction. MA is a sub-discipline of broader analytics and includes the people, processes, and technology to generate insights that improve marketing performance. In this B2B study, the authors established an initial set of factors, with the help of deep subject matter experts in the field, which help determine the degree to which a firm’s marketing function is analytically driven. The research builds on extant theories of market orientation and lays the foundation for the development of a new construct known as marketing analytics orientation through qualitative research methods. © 2018, The Author(s)."
"10.1057/s41270-018-0035-9","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047248054&doi=10.1057%2fs41270-018-0035-9&partnerID=40&md5=ffcf17317a1b0a9b2cc86355ce3240fc","The current paper proposes a novel method for detecting careless respondents, namely, floodlight detection of careless respondents. This novel method consists of two steps: (1) creating a nonsense regression model and (2) testing a moderator role of response time on the nonsense regression model. An illustration of the floodlight detection of careless respondents method was performed with online survey data collected from 341 Turkish participants. The floodlight detection of careless respondents method is the first systematic approach to calculate a cut-off value for response time, which distinguishes careless respondents from careful respondents. According to the results of floodlight detection of careless respondents, the percentage of careless respondents was 59.8%, which is little higher than the percentage of careless respondents calculated through Instructional Manipulation Check (40.7%) and bogus item (44.2%). The floodlight detection of careless respondents method is described herein, and implications are provided for future research. © 2018, Macmillan Publishers Ltd., part of Springer Nature."
"10.1057/s41270-018-0032-z","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046343490&doi=10.1057%2fs41270-018-0032-z&partnerID=40&md5=93a863cd67da2f346ad1596b277e50ec","Online relationship marketing enables organisations to maintain and develop relationships with new and existing customers. In this study, we investigate online relationship marketing and the resulting loyalty for banks and their customers in an economy where face-to-face interactions are the norm. We introduce a model comprising relationship interaction and relationship quality and validate this for Jordanian banks and their SME customers. Increases in relational interaction were found to have a positive effect on the relationship quality dimensions of trust, satisfaction, and commitment. These then led to an increase in customer loyalty. Despite the preference in the economy for social presence in retail interactions, banks were able to establish effective online retail relationships with their SME customers. © 2018, Macmillan Publishers Ltd., part of Springer Nature."
"10.1177/2399808317690150","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052889683&doi=10.1177%2f2399808317690150&partnerID=40&md5=0928d7f359385916ff3dbfea2b6ce85c","Which urban form factor most affects household electricity consumption? This study investigated the relationships between urban density, community layout, and land use factors and household electricity consumption simultaneously, along with building characteristics and demographic indicators. The study site involved 231 communities located in the former provincial area of Tainan City, Taiwan. Due to the area’s subtropical climate, air conditioning accounts for approximately 40% of the total yearly household electricity consumption. Of the urban form factors examined, greater population density was most strongly associated with lower household electricity consumption, followed respectively by greater urban canyon narrowness, or higher height to width ratios, and greater percentages of vacant space and building land use. Notably, both urban canyons and building land use percentages were associated with decreased consumption only after increasing past threshold levels, specifically a 1.5 height to width ratio and 40.7%, respectively. In addition, building characteristics, namely smaller household living areas and greater building age, were most strongly connected with lower household electricity consumption. In contrast, larger household living areas were linked with decreased household electricity consumption/floor area, revealing the importance of lower energy intensities of sizable scales. Of the demographic indicators studied, higher percentages of older adults were associated with lower household electricity consumption. Concerning urban form, the findings suggest that to reduce residential energy usage in a subtropical climate, buildings should be clustered to maximize the inter-building shadows resulting from narrower urban canyons, while simultaneously increasing non-built land use percentages in the adjacent areas. © The Author(s) 2017."
"10.1177/2399808317690156","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041813356&doi=10.1177%2f2399808317690156&partnerID=40&md5=4ac5c8c3ea404ef298e77cf440d3bf39","The objective is to automate the design of residential layouts as an aid for planners dealing with complex situations. The algorithm COmputational Urban Layout Design, applied to sites with various shapes, is guided by the goal of many mutually accessible residences and can be set to generate orthogonal or irregular road layouts. Using biological principles of genomic equivalence, conditional differentiation and induction, it grows from an embryonic ‘adaptive cell’ into a plan. Cells are ‘genetically identical’ with full development potential and can simultaneously lay roads and residential lots, using the gene set to change cell expression and adapt to local contexts. Cells can be seen as self-propagating agents that sort out their dependencies through local interactions. When COmputational Urban Layout Design is set to grow a non-orthogonal layout, the plan has winding roads and irregular residential lots. Such a plan achieves the objective of relatively high residential density and accessibility, leading to walkable and coherent communities. © The Author(s) 2017."
"10.1177/0265813516688688","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041550024&doi=10.1177%2f0265813516688688&partnerID=40&md5=8a7c425f43cd4c4bbd78f3dbb1955f58","This paper discusses a project on the completion of a database of socio-economic indicators across the European Union for the years from 1990 onward at various spatial scales. Thus the database consists of various time series with a spatial component. As a substantial amount of the data was missing a method of imputation was required to complete the database. A Markov Chain Monte Carlo approach was opted for. We describe the Markov Chain Monte Carlo method in detail. Furthermore, we explain how we achieved spatial coherence between different time series and their observed and estimated data points. © The Author(s) 2017."
"10.1177/2399808317690149","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041492490&doi=10.1177%2f2399808317690149&partnerID=40&md5=711a61607bf2548f841949edc60c3fe6","In today's developed world, the ability of a city to generate good experiences for its residents and visitors is a main aspect of its attractiveness. A good city is considered to be one in which people feel secure, relaxed, and happy. This article explores the factors that influence the subjective momentary experiences of individuals in the city, while focusing on the impact of spatial variables on these experiences; 91 students living in Jerusalem, Israel, were asked to repeatedly self-report four dimensions of episodic experience, namely, sense of security, happiness, annoyance, and sense of comfort. Reports were sent in real time using a smartphone application during an eight-month period. The results, based on over 5000 experience samples, indicate that subjective momentary experiences, particularly sense of comfort and sense of security, are highly influenced by situational variables and environmental characteristics including type of activity and environment, place characteristics, and company. Surprisingly, personality variables which are considered to be a main determinant of wellbeing and general life satisfaction were found to be non-significant in the multilevel models that were implemented. This finding further supports the notion that momentary experiences greatly differ from general evaluations of subjective wellbeing. © The Author(s) 2017."
"10.1177/0265813516686972","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041397246&doi=10.1177%2f0265813516686972&partnerID=40&md5=02d403793c978898111fdb073ccfb568","Mitigating carbon emission efforts in urban planning and design phase have become increasingly popular due to climate change. However, it is difficult to verify whether the carbon mitigation target could be achieved for a new city in the absence of quantitative analysis methods. About 100 new cities have emerged every year in the past decades, yet few of them employed low carbon strategies within proper prediction methods. In response, this paper offers an integrated analysis method of assessment and mitigation for urban carbon dioxide (CO2) of new cities. Building sector, transportation sector, and green land sector are considered as urban CO2 sources and sink. Life cycle analysis was employed in building sector to estimate its emissions. Based on the current and predicted emission data, a mitigation goal was then set and allocated efficiently through different sectors. To elaborate on this process, a case study of Shanghai Lingang New City was presented. The urban low carbon roadmap was planned and a variety of recommendations concerning policy were offered to assist the local government and policy makers in order to achieve the low carbon development goal as well. © The Author(s) 2017."
"10.1177/2399808317690148","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029159218&doi=10.1177%2f2399808317690148&partnerID=40&md5=ef831186b326f8b5713cfe40ba2b7598","Urban sprawl is now a common and threatening phenomenon in Europe, severely affecting environmental and economic sustainability. An analytical characterization and measurement of urban sprawl are required to gain a better understanding of the phenomenon and to propose the possible solutions. Traditional factor analysis techniques, especially Principal Component Analysis and Factor Analysis, have been commonly used. In this paper, we additionally test Independent Component Analysis with the aim to obtain a multidimensional characterization of the sprawl phenomenon. We also use Bayesian Factor Analysis to obtain a single (unidimensional) measuring index of sprawl, which also allows us to obtain the uncertainty of the inferred index, in contrast to traditional approaches. All these techniques have been applied to study the phenomenon of urban sprawl at the municipality level in Valencia, Spain using a wide set of variables related to the characteristics and patterns of urban land use. © The Author(s) 2017."
"10.1177/0265813516688687","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027363725&doi=10.1177%2f0265813516688687&partnerID=40&md5=24095cd721be855f9eca7b7360ecabba","Behaviorally, regret-based choice models implicitly assume that individuals anticipate the amount of attribute-level regret by comparing the attribute levels of a considered choice alternative against the attribute levels of the best or all other choice alternatives. Arguing that the amount of effort depends on attribute variation and number of paired comparisons, we suggest a way of incorporating the effects of these factors into two regret-based choice models. The cognitive effort involved in anticipating the amount of regret in paired comparisons of choice alternatives is incorporated into the scale of the regret function of each alternative. Because more cognitive effort causes higher randomness in the assessment of the amount of regret (i.e. higher variance of error terms), the cognitive effort is expressed as a flexible heteroscedastic scale factor, which is a decreasing function of attribute variation and number of paired comparisons. The models are applied to two different data sets, and compared with a heteroscedastic multinomial logit model. Estimation results of the suggested flexible heteroscedastic random regret models show a significant improvement in predictive performance over their homoscedastic formulations. A similar but smaller improvement is obtained for multinomial logit models. These results imply that the conventional assumption of identically distributed error terms underlying random regret models may not sufficiently reflect the process of anticipating the amount of regret. © The Author(s) 2017."
"10.1177/2399808317690155","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85014825170&doi=10.1177%2f2399808317690155&partnerID=40&md5=9aff2b6bb7ba4efe147e9f338e87c0c2","This article reports a multi-scale analysis of polycentric urban development in 22 Chinese city-regions. Using fine-grained population data, our analysis contrasts polycentric development patterns at multiple geographical scales. We present a typology of Chinese city-regions based on both (1) their inter-city polycentricity and (2) the intra-city polycentricity of the individual cities that comprise these urban regions. Overall, we find only limited levels of association between inter-city and intra-city polycentricity. The Pearl River and Yangtze River Deltas have high levels of inter-city and intra-city polycentricity. Most city-regions in Central and Western China are characterized by a primate urban system and low levels of inter-city polycentricity. We hypothesize the major economic, political, and geographical processes underlying observed patterns. © The Author(s) 2017."
"10.5334/dsj-2018-019","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055754656&doi=10.5334%2fdsj-2018-019&partnerID=40&md5=f51ec74c05dcfbb1b99cd77fb376b7b1","In the 21st century, digital data drive innovation and decision-making in nearly every field. However, little is known about the total size, characteristics, and sustainability of these data. In the scholarly sphere, it is widely suspected that there is a gap between the amount of valuable digital data that is produced and the amount that is effectively stewarded and made accessible. The Stewardship Gap Project (http://bit.ly/stewardshipgap) investigates characteristics of, and measures, the stewardship gap for sponsored scholarly activity in the United States. This paper presents a preliminary definition of the stewardship gap based on a review of relevant literature and investigates areas of the stewardship gap for which metrics have been developed and measurements made, and where work to measure the stewardship gap is yet to be done. The main findings presented are 1) there is not one stewardship gap but rather multiple “gaps” that contribute to whether data is responsibly stewarded; 2) there are relationships between the gaps that can be used to guide strategies for addressing the various stewardship gaps; and 3) there are imbalances in the types and depths of studies that have been conducted to measure the stewardship gap. © 2018 The Author(s)."
"10.5334/dsj-2018-018","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050126160&doi=10.5334%2fdsj-2018-018&partnerID=40&md5=4fc1c90f8981395ca0aa633cf824fe78","Since 2006 the education authorities in Switzerland have been obliged by the Constitution to harmonize important benchmarks in the educational system throughout Switzerland. With the development of national educational objectives in four disciplines an important basis for the implementation of this constitutional mandate was created. In 2013 the Swiss National Core Skills Assessment Program (in German: ÜGK – Überprüfung der Grundkompetenzen) was initiated to investigate the skills of students, starting with three of four domains: mathematics, language of teaching and first foreign language in grades 2, 6 and 9. ÜGK uses a computer-based test and a sample size of 25.000 students per year. A huge challenge for computer-based educational assessment is the research data management process. Data from several different systems and tools existing in different formats has to be merged to obtain data products researchers can utilize. The long term preservation has to be adapted as well. In this paper, we describe our current processes and data sources as well as our ideas for enhancing the data management. © 2018, Ubiquity Press Ltd. All rights reserved."
"10.5334/dsj-2018-017","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050125722&doi=10.5334%2fdsj-2018-017&partnerID=40&md5=c5fb287a69af0d88640ff74131c9485a","The sustainable management of anthropogenically-impacted ecosystems will require ongoing monitoring and advocacy by people across the globe. To this end, automatic methods are developed herein for acquiring several types of such political-ecological data. On the political side, a method is developed for gathering news articles about human actions that affect the ecosystem along with a method for identifying themes in social media that concern the consumption of an ecosystem’s products. On the ecosystem side, a method is derived for estimating wildlife abundance from purchasable high-resolution satellite images. A simple website architecture is described for holding this data and enabling its use in developing sustainable conservation policies. A rhino conservation website illustrates this architecture. A fundamental contradiction between the desire for open data on the locations of endangered flora and fauna versus the need to hide these locations from poachers is addressed through a new security protocol that enables the secure distribution of sensitive ecosystem data to trusted data consumers. © 2018, Ubiquity Press Ltd. All rights reserved."
"10.1080/23270012.2018.1462112","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057201890&doi=10.1080%2f23270012.2018.1462112&partnerID=40&md5=e9d190edced2e21c56695608edddd9b4","This paper proposes a two-period model to examine which bankruptcy procedure is better for both the debtor and creditor when insolvency happens. Our theoretical model depicts how firm size, industry characteristics, equity structure, and debt structure determine firms’ bankruptcy resolutions. Using a comprehensive sample of bankrupt firms in the United States, we verify the predictions of a theoretical model. In our study, we deploy Probit and Logit models to address the predictions in the theoretical framework and conduct a series of robustness checks with econometric methods like Propensity Score Matching to confirm the empirical results. This paper finds that firms with larger size have more chance to file for Chapter 11 reorganization when insolvency happens. We also find that firms in asset-heavy (asset-light) industries are more likely to be reorganized (liquidated) under U.S. bankruptcy code. © 2018, © 2018 Antai College of Economics and Management, Shanghai Jiao Tong University."
"10.1080/23270012.2018.1490212","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057143194&doi=10.1080%2f23270012.2018.1490212&partnerID=40&md5=dd2c11acfe6e9b91f8f5c9ba3a891c7c","A big data arena (BDA) is proposed to offer an effective trial-and-error environment for analysis algorithm design and application exploration through the provision of computational resources and intensive data. It is expected to innovate the theories and technologies of, e.g. management analytics, when a huge volume of data are shared in the BDA. How to provide an effective data protection mechanism, however, is one of the most serious challenges that the BDA faces. Motivated to resolve the above issues, this paper analyzes potential security threats that the data in the BDA face at four different processing stages, and puts forward effective access control mechanisms with their design and implementation, to resist the threats from different stages. © 2018, © 2018 Antai College of Economics and Management, Shanghai Jiao Tong University."
"10.1080/23270012.2018.1462111","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052902639&doi=10.1080%2f23270012.2018.1462111&partnerID=40&md5=de9ef5ad3663e4c00542a24129d388c6","One of the major concerns for the technology market is the demand volatility and its impact on inventory policies. Demand volatility in the technology sector may arise due to many factors namely customer choices, competition, growing market size, and so on. Often companies use rented warehouses to absorb any fluctuations in demand. Unfortunately, warehouse and inventory researches ignore the phenomenon of growing market size to formulate policy decisions. In this paper, we proposed a two-warehouse inventory model with deterioration for technology products with linearly increasing market size where demand follows innovation diffusion criterion. The model is based on the assumption that the holding costs in the rented warehouse are more than the own warehouse. A simple solution procedure also discussed to solve nonlinear cost function. Numerical example and sensitivity analysis are also used to describe the utility of the model. © 2018, © 2018 Antai College of Economics and Management, Shanghai Jiao Tong University."
"10.1016/j.bdr.2018.02.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047457133&doi=10.1016%2fj.bdr.2018.02.002&partnerID=40&md5=9832cc65ed8c3f7428665fcfbd1cc884","The growth of big data is transforming many economic sectors, including the medical and healthcare sector. Despite this, research into the practical application of data analytics to the development of health policy is still limited. In this study we examine how data science and machine learning methods can be applied to a variety of open health datasets, including GP prescribing data, disease prevalence data and economic deprivation data. This paper discusses the context of mental health and antidepressant prescribing in Northern Ireland and highlights its importance as a public policy issue. A hypothesis is proposed, suggesting that the link between antidepressant usage and economic deprivation is mediated by depression prevalence. An analysis of various heterogeneous open datasets is used to test this hypothesis. A description of the methodology is provided, including the open health datasets under investigation and an explanation of the data processing pipeline. Correlations between key variables and several different clustering analyses are presented. Evidence is provided which suggests that the depression prevalence hypothesis is flawed. Clusters of GP practices based on prescribing behaviour and disease prevalence are described and key characteristics are identified and discussed. Possible policy implications are explored and opportunities for future research are identified. © 2018"
"10.1016/j.bdr.2018.02.007","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046768097&doi=10.1016%2fj.bdr.2018.02.007&partnerID=40&md5=8fef5b4abf00b8b52eda982ad75d86bf","This study focuses on feature subset selection from high dimensionality databases and presents modification to the existing Random Subset Feature Selection (RSFS) algorithm for the random selection of feature subsets and for improving stability. A standard k-nearest-neighbor (kNN) classifier is used for classification. The RSFS algorithm is used for reducing the dimensionality of a data set by selecting useful novel features. It is based on the random forest algorithm. The current implementation suffers from poor dimensionality reduction and low stability when the database is very large. In this study, an attempt is made to improve the existing algorithm's performance for dimensionality reduction and increase its stability. The proposed algorithm was applied to scientific data to test its performance. With 10 fold cross-validation and modifying the algorithm classification accuracy is improved. The applications of the improved algorithm are presented and discussed in detail. From the results it is concluded that the improved algorithm is superior in reducing the dimensionality and improving the classification accuracy when used with a simple kNN classifier. The data sets are selected from public repository. The datasets are scientific in nature and mostly used in cancer detection. From the results it is concluded that the algorithm is highly recommended for dimensionality reduction while extracting relevant data from scientific datasets. © 2018 Elsevier Inc."
"10.1016/j.bdr.2018.02.003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045043936&doi=10.1016%2fj.bdr.2018.02.003&partnerID=40&md5=5231cd3ace8e9fbf217460a42fc3fdec","The efficient automatic detection of cardiac arrhythmia using a hybrid technique from ECG big data has been proposed with novel feature extraction technique using Multiresolution Discrete Wavelet Transform (MRDWT) and Multilayer Probabilistic Neural Network (MPNN) classifier. Big Data of ECG signals have been selected from MIT–BIH arrhythmia database for detection of two types of arrhythmias LBBB (Left Bundle Branch Block) and RBBB (Right Bundle Branch Block). The proposed technique can accurately detect and classify LBBB and RBBB along with normal heartbeat. A novel and hybrid method of detection of cardiac arrhythmia have four main stages: denoising of raw ECG, baseline wander removal, proposed feature extraction, and detection of abnormal heartbeats using MPNN neural classifier. 8600 ECG beats were selected, including 4200 normal and 4400 abnormal beats (2200 LBBB and 2200 RBBB) were utilized for testing the proposed technique. The detection outcome using MPNN was compared with other two neural classifiers: Feed Forward Neural Network (FFNN) and Back Propagation Neural Network (BPNN) classifiers. The accuracy and efficiency of classifiers performance were attained in terms of CER (Classification Error Rate), SP (Specificity), Se (Sensitivity), Pr (Precision), PPr (Positive Predictivity) and F-Score. The system performance is achieved with 96.22%, 97.15% and 99.07% overall accuracy using FFNN, BPNN and MPNN. The average percentage of classification error rate (CER) using MPNN classifier is lowest 0.62% whereas FFNN and BPNN show 2.2% and 1. 90% average CER. © 2018 Elsevier Inc."
"10.1016/j.bdr.2017.11.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044927890&doi=10.1016%2fj.bdr.2017.11.001&partnerID=40&md5=481d513b01d52e4e49d0e5e0593ef82e","The definition of data might at first glance seem prosaic, but formulating a definitive and useful definition is surprisingly difficult. This question is important because of the protection given to data in law and ethics. Healthcare data are universally considered sensitive (and confidential), so it might seem that the categorisation of less sensitive data is relatively unimportant for medical data research. This paper will explore the arguments that this is not necessarily the case and the relevance of recognizing this. The categorization of data and information requires re-evaluation in the age of Big Data in order to ensure that the appropriate protections are given to different types of data. The aggregation of large amounts of data requires an assessment of the harms and benefits that pertain to large datasets linked together, rather than simply assessing each datum or dataset in isolation. Big Data produce new data via inferences, and this must be recognized in ethical assessments. We propose a schema for a granular assessment of data categories. The use of schemata such as this will assist decision-making by providing research ethics committees and information governance bodies with guidance about the relative sensitivities of data. This will ensure that appropriate and proportionate safeguards are provided for data research subjects and reduce inconsistency in decision making. © 2017"
"10.1016/j.bdr.2018.02.004","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85043390972&doi=10.1016%2fj.bdr.2018.02.004&partnerID=40&md5=5537fcdcb08fc269c4b101d2c831dcf9","We show how the analysis of very large amounts of drug prescription data make it possible to detect, on the day of hospital admission, patients at risk of developing complications during their hospital stay. We explore, for the first time, to which extent volume and variety of big prescription data help in constructing predictive models for the automatic detection of at-risk profiles. Our methodology is designed to validate our claims that: (1) drug prescription data on the day of admission contain rich information about the patient's situation and perspectives of evolution, and (2) the various perspectives of big medical data (such as veracity, volume, variety) help in extracting this information. We build binary classification models to identify at-risk patient profiles. We use a distributed architecture to ensure scalability of model construction with large volumes of medical records and clinical data. We report on practical experiments with real data of millions of patients and hundreds of hospitals. We demonstrate how the fine-grained analysis of such big data can improve the detection of at-risk patients, making it possible to construct more accurate predictive models that significantly benefit from volume and variety, while satisfying important criteria to be deployed in hospitals. © 2018 Elsevier Inc."
"10.1016/j.bdr.2018.02.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041953033&doi=10.1016%2fj.bdr.2018.02.001&partnerID=40&md5=21270c34b66b8fbc96cf6b47a7020720","In the healthcare sector, information is the most important aspect, and the human body in particular is the major source of data production: as a result, the new challenge for world healthcare is to take advantage of these huge amounts of data de-structured among themselves. In order to benefit from this advantage, technology offers a solution called Big Data Analysis that allows the management of large amounts of data of a different nature and coming from different sources of a “computerized” healthcare, as there are considerable changes made by the input of digital technology in all major health areas. Clinical intelligence consists of all the analytical methods made possible through the use of computer tools, in all the processes and disciplines of extraction and transformation of crude clinical data into significant insights, new purposes and knowledge that provide greater clinical efficacy and best health pronouncements about past performance, current operations and future events. It can therefore be stated that clinical intelligence, through patient data analysis, will become a standard operating procedure that will address all aspects of care delivery. The purpose of this paper is to present clinical intelligence approaches through Data Mining and Process Mining, showing the differences between these two methodologies applied to perform “real process” extraction to be compared with the procedures in the corporate compliance template (the so called “Model 231”) by “conformance checking”. © 2018 Elsevier Inc."
"10.4018/IJBIR.2018070102","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060637696&doi=10.4018%2fIJBIR.2018070102&partnerID=40&md5=6b88caa9bca1ba1c26ac298467573248","This article focuses on critical success factors during the implementation of a business intelligence system. The existing literature was reviewed, and critical success factors were extracted. Subsequently, the critical success factors that occur in practice were collected through qualitative expert interviews that are analysed through a qualitative content analysis. The critical success factors found in literature are afterwards compared with those that have been collected during the expert interviews. It was found that many of the critical success factors were mentioned in the literature and in the expert interviews as well, such as a strong management support, a light-weight approach, user acceptance, the project team and data quality. In addition, the performance of the business intelligence system, the definition of standards, terminology and key performance indicators as well as an institutionalization and integration of business intelligence were mentioned in the expert interviews. © 2018, IGI Global."
"10.4018/IJBIR.2018070103","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060590211&doi=10.4018%2fIJBIR.2018070103&partnerID=40&md5=e8b67a830c9dfa66a177f366d932280f","Business intelligence (BI) has proliferated due to its growing application for business decision support. Research on organizational factors may offer significant use in BI implementation. However, a limited number of studies focus on organizational factors for revealing adverse impacts on effective decision support. The aim of this theoretical study is to conduct a literature analysis to identify organizational factors relevant to BI implementation. Through a systematic literature review, a qualitative content analysis on 49 relevant sample articles for generating themes inductively is adopted to reveal organizational factors. Findings suggest two contexts: Information management that integrates factors such as technological capability and personnel capability and organizational context that integrates factors such as organizational capability, managerial decision, and organizational culture for facilitating embedding information management capability for BI implementation in businesses. It is hoped that these contextual understanding can be useful for further BI implementations. © 2018, IGI Global."
"10.4018/IJBIR.2018070104","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060579626&doi=10.4018%2fIJBIR.2018070104&partnerID=40&md5=388a6979e549f214dee7238336eba538","Prediction of app usage and location of smartphone users is an interesting problem and active area of research. Several smartphone sensors such as GPS, accelerometer, gyroscope, microphone, camera and Bluetooth make it easier to capture user behavior data and use it for appropriate analysis. However, differences in user behavior and increasing number of apps have made such prediction a challenging problem. In this article, a prediction approach that takes smartphone user behavior into consideration is proposed. The proposed approach is illustrated using data from over 30000 users from a leading IT company in China by first converting data in to recency, frequency, and monetary variables and then performing cluster analysis to capture user behavior. Prediction models are then developed for each cluster using a training dataset and their performance is assessed using a test dataset. The study involves ten different categories of apps and four different regions in Beijing. The proposed app usage prediction and next location prediction approach has provided interesting results. © 2018, IGI Global."
"10.4018/IJBIR.2018070101","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060569575&doi=10.4018%2fIJBIR.2018070101&partnerID=40&md5=374aa42929811361da2c1a416a97c87f","The past few years have seen an explosion in the business use of analytics. Corporations around the world are using analytical tools to gain a better understanding of their customer's needs and wants. Predictive analytics has become an increasingly hot topic in analytics landscape as more companies realize that predictive analytics enables them to reduce risks, make intelligent decisions, and create differentiated customer experiences. As a result, predictive analytics deployments are gaining momentum. Yet, the adoption rate is slow, and organizations are only beginning to scratch the surface in regards to the potential applications of this technology. Implemented properly, the business benefits can be substantial. However, there are strategic pitfalls to consider. The key objective of this article is to propose a conceptual model for successful implementation of predictive analytics in organizations. This article also explores the changing dimensions of analytics, highlights the importance of predictive analytics, identifies determinants of implementation success, and covers some of the potential benefits of this technology. Furthermore, this study reviews key attributes of a successful predictive analytics platform and illustrates how to overcome some of the strategic pitfalls of incorporating this technology in business. Finally, this study highlights successful implementation of analytics solutions in manufacturing and service industry. © 2018, IGI Global."
"10.4018/IJBAN.2018070103","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047780052&doi=10.4018%2fIJBAN.2018070103&partnerID=40&md5=d98127047f8112957d44fc165518bb0a","This article describes how in order to prioritize the risks for a large number of assets, various criteria are used for ranking the asset risk. Since the criteria used for developing the risk rank is a mixture of quantitative and qualitative, a method was required to handle both the quantitative and qualitative criteria with varying scales that can be used for the electrical industry's assets. This article proposes a hybrid multi-criteria decision model (MCDM) that combines both weighted sum model (WSM) and analytical hierarchy process (AHP). The hybrid model is then applied to the strategic asset management plan for the electric power industry for ranking the assets risk. In this application, a large number of criteria reflecting asset conditions with their numerical values are available for which WSM method worked quite efficiently. The AHP method was applied to the criteria where qualitative criteria were available. Both methods were then synthesized, and the proposed hybrid method was formulated which resulted in a computationally efficient outcome with robust mathematical framework. The results show that the proposed method exhibited optimal results for the electric industry's asset where qualitative criteria are for AHP method was limited to 3 to 5. In the case of WSM, a larger number of quantitative criteria could be accommodated although for the application only six criteria were utilized. Copyright © 2018, IGI Global."
"10.4018/IJBAN.2018070101","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047765206&doi=10.4018%2fIJBAN.2018070101&partnerID=40&md5=30ce9ed06e748d034c039859e86e5e46","This article presents the relationship between a firm's advertisement spending and sales in a duopoly when information about the competitors' advertisement spending is unavailable. The competitive interaction between the firms has been modeled as imperfect information Cournot and Stackelberg games and the conditions for subgame perfect Bayesian Nash equilibrium are presented. The results suggest that when the firms are similar in size and advertisement effectiveness, both firms are better off sharing their advertising plans with each other. On the other hand, when one of the firms is a market leader, the follower may profit from the leader's advertisement spending and so is better off keeping the leader guessing. A practical approach to estimate the optimum advertisement budget based on the expected values of the competitors' historic advertising spending is presented as well. Copyright © 2018, IGI Global."
"10.4018/IJBAN.2018070102","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047748134&doi=10.4018%2fIJBAN.2018070102&partnerID=40&md5=931b136c7b164e509803887d611a498d","This article describes how magnetic resonance imaging (MRI) systems play a crucial role in radiology, specifically in the diagnosis of diseases and management of patient treatment. The objective of this article is to present MRI technicians' perspective on the relative importance of the required factors when selecting and purchasing an MRI system. Analytic Hierarchy Process (AHP) methodology was used to determine the relative priorities for different criteria along with the consistency of responses. A set of criteria for MRI system were identified based on the literature and interviews with experts (i.e., MRI technicians), and organized into a rational hierarchical framework consisting of the five main criteria and nineteen sub-criteria. An online survey including demographic questions was conducted to identify the relative weights of these criteria. Survey responses from 87 technicians indicate that brand is found to be the most important criteria, followed by patient comfort, usability, technical issues, and performance. Among the sub-criteria, the highest weights are assessed for country of origin, user-friendly independent workstation, reputation, software support. The findings demonstrate the factors that can be critical discriminators between different MRI systems. Copyright © 2018, IGI Global."
"10.4018/IJBAN.2018070105","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047736611&doi=10.4018%2fIJBAN.2018070105&partnerID=40&md5=649e5c9e4010c1b22a0cb68b4ee10966","This article describes how thanks to the technological development, social media has propagated in recent years. The latter describes a range of Web-based platforms that enable people to socially interact with one another online. Several types of social media appeared. In this context, the author focuses on scientific social network which connects the researchers and allow them to communicate and collaborate online. In this paper, we, particularly, aim to detect the scientific leaders through firstly detect communities in social network then identify the leader of each group. To do this, the author introduces a new hierarchical semi-supervised clustering method based on ordinal density. The results of carried out experiments on real scientific warehouse have shown significant profits in terms of accuracy and performance. Copyright © 2018, IGI Global."
"10.4018/IJBAN.2018070104","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047723109&doi=10.4018%2fIJBAN.2018070104&partnerID=40&md5=780194b17a87561c5748813bf5d19ae4","This chapter describes how in today's competitive world, the survival of businesses is highly dependent on satisfying the customers and responding to their demands. The furniture industry is one of the markets that consists high level of competition and customers in different groups. The industry has a rapid growth by improving the economic conditions and quality of life in the world. However, the capability and competitiveness of suppliers and retailers for domestic and imported products are a big challenge. This brings the importance of the strategic analysis approach to the industry. The aim of this article is to implement strategic group maps to understand the dynamics of the furniture market in a selected case study. The used method is the first in the furniture industry and the market analysis is based on the both suppliers/retailers and customers' viewpoints. The outputs show the furniture industry can be categorized in the five groups with specific required demand and strategy in each group. Copyright © 2018, IGI Global."
"10.1177/0265813516684827","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048500269&doi=10.1177%2f0265813516684827&partnerID=40&md5=fa3ba2c4937357377c2f4eee7749ec77","Urban expansion determines socioeconomic and environmental changes with unpredictable impacts on peri-urban land, especially in ecologically fragile areas. The present study assesses the impact of dense and, respectively, discontinuous urban expansion on high-quality land consumption in 76 metropolitan regions of Southern Europe. Land quality indicators and land-use maps were considered together with the aim to analyze urban growth and land take processes in Portugal, Spain, Southern France, Italy and Greece. Differences in the rate of selective land take (high- vs. low-quality soils) were observed at the metropolitan scale depending on the size of urban regions, the average level of land quality and the percentage of built-up areas and cropland in the total landscape. Discontinuous residential settlements were more frequently developed on high-quality soils in respect to both dense and mixed residential settlements and service settlements. Urbanization – especially discontinuous urban expansion – consumed high-quality land mainly in Spain and Greece. The approach presented in this paper may inform joint policies for urban containment and the preservation of high-quality soils in peri-urban areas. © 2016, The Author(s) 2016."
"10.1177/0265813516683188","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048494090&doi=10.1177%2f0265813516683188&partnerID=40&md5=ae54569e1085a4f5898fb3f68763decd","Climate change is certain, given the sufficient evidence for it. Thus, it needs to be integrated into the process of spatial and community planning to empower communities so they can adapt to it. This study addresses the critical gap in the literature on community planning and climate change adaptations by creating planning methodologies based on the participatory method, and simulating application of the process to the Saebat Maeul community in Busan, Korea. In the Saebat Maeul community, the study area, people have struggled with urban flooding, which is related to the poor sewer system and heat waves. Green infrastructures were used as the main tools to mitigate the negative impacts of climate change. The suggested community plan includes physical and socioeconomic measures, such as new parking structures with green roofs and pervious pavement, drainage facilities that reduce flooding, repairs to homes with cool roofs, and installation of rainwater harvesting facilities to lower the impact of heat waves. If planners have proper knowledge about climate change and if they guide the participatory planning process (thus reflecting local knowledge and suggestions), we concluded that it is possible to consider climate change adaptations within the context of spatial and community planning. © 2016, The Author(s) 2016."
"10.1177/0265813516676489","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048488635&doi=10.1177%2f0265813516676489&partnerID=40&md5=bf4ea6470e17195d7295e030760510cf","John Portman’s work attracts significant commentary, although the focus is typically on the commercial and social aspects of his work as opposed to the actual designs and their related architectural implications. The obvious place to start unpacking his contribution is in his widely recognized and published commercial portfolio, yet he maintains that his design principles are found in his personal domestic work. Here, his 1964 residence Entelechy I is analyzed to inform the development of a parametric shape grammar that generates the original design as well as a series of variations. The goal of this research is to engage Portman’s architectural philosophy and constructively assess his claims of its implicit relationship to his work to date. Key rules suggesting his principles and anticipating his ongoing architectural contribution are outlined. The structure provided by shape computation, involving both shape rules and rule schemas, is positioned as the theoretical basis for an ongoing study of transformations within Portman’s language. © 2016, The Author(s) 2016."
"10.1177/0265813516686971","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048481002&doi=10.1177%2f0265813516686971&partnerID=40&md5=d21f417c99e8fb654e05bd0f2b6e0734","This article presents an exploratory framework to predict ratings of subjectively perceived urban stress in open public spaces by analysing properties of the built environment with GIS and Space Syntax. The authors report on the findings of an empirical study in which the environmental properties of a sample of open public spaces in the city of Darmstadt, Germany were constructed and paired to users’ ratings. The data are analysed using different types of multivariate analyses with the aim to predict the ratings of perceived urban stress with a high explained variance and significance. The study finds that open public space typologies (park, square, courtyard, streets) are the best predictors for perceived urban stress, followed by isovist characteristics, street network characteristics and building density. Specifically, the isovist visibility, vertices number and perimeter, previously related to arousal and complexity in indoor spaces, show significant relation to perceived urban stress in open public spaces, but with different direction of effects. A model is presented that achieves a predictive power of R2= 54.6%. It extends existing models that focused on green spaces and streetscapes with a first exploratory attempt to predict more complex reactions such as perceived urban stress. © 2017, The Author(s) 2017."
"10.1177/0265813516677291","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048473430&doi=10.1177%2f0265813516677291&partnerID=40&md5=4c493edc6c0be14ea79eded7c8af97ec","The purpose of this paper is to determine the existence of a possible expected bridge frequency (number of bridges per unit of measurement) in an urban context. To achieve this, a method was developed to calculate the real frequency of crossing points already built over water bodies in inner cities. The method was based on collecting different measurable data of different cities using images from Google Earth. Furthermore, this method was applied to evaluate bridges built in several European cities that have a river. The results show that there is a clear link between the frequency of urban bridges and the width of the river, expressed by a power law function which defines bridge frequency as an inverse function of river width. Also, there is no direct link between the size of the city and the number of crossing points built in the city. Additionally, two new urban development projects on rivers were evaluated by using the defined power law function. Thus, it was concluded that future urban development close to rivers could use this function to decide the number of new crossing points. Furthermore, the ideal distance between consecutive urban bridges has been determined. © 2016, The Author(s) 2016."
"10.1177/0265813516676488","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048333284&doi=10.1177%2f0265813516676488&partnerID=40&md5=1a6ae3e185b25d00541361cac1f14e07","Developing a scientific understanding of cities in a fast urbanizing world is essential for planning sustainable urban systems. Recently, it was shown that income and wealth creation follow increasing returns, scaling superlinearly with city size. We study scaling of per capita incomes for separate census defined income categories against population size for the whole of Australia. Across several urban area definitions, we find that lowest incomes grow just linearly or sublinearly (β = 0.94 to 1.00), whereas highest incomes grow superlinearly (β = 1.00 to 1.21), with total income just superlinear (β = 1.03 to 1.05). These findings show that as long as total or aggregate income scaling is considered, the earlier finding is supported: the bigger the city, the richer the city, although the scaling exponents for Australia are lower than those previously reported for other countries. But, we find an emergent scaling behavior with regard to variation in income distribution that sheds light on socio-economic inequality: the larger the population size and densities of a city, while lower incomes grow proportionately or less than proportionately, higher incomes grow more quickly, suggesting a disproportionate agglomeration of incomes in the highest income categories in big cities. Because there are many more people on lower incomes that scale sublinearly as compared to the highest that scale superlinearly, these findings suggest an empirical observation on inequality: the larger the population, the greater the income agglomeration in the highest income categories. The implications of these findings are qualitatively discussed for various income categories, with respect to living costs and access to opportunities and services that big cities provide. © 2016, The Author(s) 2016."
"10.1177/0265813516686566","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041634113&doi=10.1177%2f0265813516686566&partnerID=40&md5=f791ab4ab4d447b65db92faf74e73205","To generate alternatives and variations of specific architectural models, shape grammars can be used by applying a set of geometric rules step by step. With the development in human life, advances in store design, design concept, commercial buildings’ architectural and spatial fiction, the magazine of the interior, and facade design cause rising competition between stores and also between designers. For this reason, in this paper we study the evaluation of store plan alternatives produced with shape grammar using two of multi-criteria decision-making (MCDM) techniques with fuzzy numbers, namely fuzzy analytic hierarchy process and fuzzy analytic network process. The main contribution of this paper is to prioritize plan alternatives using numerical methods with experts’ view. To the authors’ knowledge, this will be the first interdisciplinary study which uses MCDM techniques for evaluating shape grammar outputs in architectural design. © 2017, The Author(s) 2017."
"10.1177/0265813516686070","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041590187&doi=10.1177%2f0265813516686070&partnerID=40&md5=0e7d63baec31c9c998c2a3eb810b02df","Statistics about citizen satisfaction regarding urban facilities and services are required for governing urban areas. Such statistics are often unavailable or outdated. At times, existing statistics are irrelevant to the major problems of most citizens. In this article, we propose a cost-effective method for estimating citizen satisfaction regarding urban facilities and services using crowdsourced place data. Two indicators are proposed based on place data derived from the Foursquare social media application. Both indicators are based on the hypothesis that the higher the number of places is that belong to a facility or service type on social media, then the higher the satisfaction of citizens regarding this facility or service type will be. This hypothesis was tested by using the Eurobarometer survey data as reference. The accuracy assessment revealed strong and statistically significant linear relationships (R2 ˃ 0.6) between the reference percentage of very satisfied citizens and some categories (i.e. “Sport facilities”, “Cultural Categories,” and “Streets &amp; buildings”). Other categories showed moderate and statistically significant linear relationships (i.e. “Public spaces” and “Green spaces”). Therefore, the proposed indicators provide estimates about citizen satisfaction with regard to these five categories. The new indicators can be used to better design public opinion surveys by making them more relevant to the public in terms of topics, space, and time. © 2016, The Author(s) 2016."
"10.1177/0265813516685566","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041517759&doi=10.1177%2f0265813516685566&partnerID=40&md5=575bc3445148be5984b2fba22fe92da8","On several occasions, the poet Pablo Neruda referred to Isla Negra as his favorite house and, for many years, he worked to transform it into his primary home. The building of this house, an incremental process over time that saw Neruda’s direct participation in its planning, constitutes a perfect case study for understanding the poet’s intention in the construction of his particular environment: the Nerudian space. Much has been written on Neruda’s houses, but the novelty of the approach presented here lies in the configurational perspective that privileges both the totality (instead of the parts) and the historical process behind its construction (instead of the final product). This article presents a morphological study of the Isla Negra house, using the Space Syntax methodology and analyzing the parts and stages of growth of the house in relation to one another and among themselves until Neruda’s last intervention in 1973. © 2017, The Author(s) 2017."
"10.1177/0265813516686970","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041422410&doi=10.1177%2f0265813516686970&partnerID=40&md5=ebbb67f880775fd80e3ad598e023eb85","This paper aims to investigate the impact of typomorphological changes of residential environments on residents’ sense of place’. Seven housing developments representing different types introduced in Ankara, Turkey since the late 19th-century are selected as case studies. Their morphological characters at the building, street and neighbourhood scales are examined, and typological transformations among the cases in terms of the degrees of continuity are identified. The paper proposes a conceptual model consisting of ten indicators to assess sense of place at the building, street and neighbourhood scales of the residents of the seven cases. The scores of sense of place are generated through structured interviews with the residents and analysed in SPSS. The results show that sense of place is negatively affected by typomorphological changes over time, particularly when mutational changes occur. Continuity in typomorphological transformation helps to maintain sense of place at a desirable level. Furthermore, physical changes at the street and neighbourhood scales have larger impact on sense of place than that at the building scale. The research thus suggests that planning and design should be responsive to traditional types in residential development, particularly at the street and neighbourhood scales to maintain residents’ sense of place. © 2017, The Author(s) 2017."
"10.5334/dsj-2018-016","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050143397&doi=10.5334%2fdsj-2018-016&partnerID=40&md5=5a061bbece84c48ed6f439bd636caa16","Digital research objects are packets of information that scientists can use to organize and store their data. There are currently many different methods in use for optimizing digital objects for research purposes. These methods have been applied to many scientific disciplines but differ in architecture and approach. The goals of this joint digital research object (DRO) conference were to discuss the challenge of characterizing DROs at scale in volume and over time and possible organizing principles that might connect current DRO architectures. One of the primary challenges concerns convincing scientists that these tools and practices will actually make the research process easier and more fruitful. This conference included work from CENDI, the National Federal STI Managers Group, the National Federation of Advanced Information Services (NFAIS), the Research Data Alliance (RDA), and the National Academy of Science (NAS). © 2018, Ubiquity Press Ltd. All rights reserved."
"10.5334/dsj-2018-015","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050088342&doi=10.5334%2fdsj-2018-015&partnerID=40&md5=7e120e5a43c1581215f043ebc7a360c7","Scientific data stewardship is an important part of long-term preservation and the use/reuse of digital research data. It is critical for ensuring trustworthiness of data, products, and services, which is important for decision-making. Recent U.S. federal government directives and scientific organization guidelines have levied specific requirements, increasing the need for a more formal approach to ensuring that stewardship activities support compliance verification and reporting. However, many science data centers lack an integrated, systematic, and holistic framework to support such efforts. The current business-and process-oriented stewardship frameworks are too costly and lengthy for most data centers to implement. They often do not explicitly address the federal stewardship requirements and/or the uniqueness of geospatial data. This work proposes a data-centric conceptual enterprise framework for managing stewardship activities, based on the philosophy behind the Plan-Do-Check-Act (PDCA) cycle, a proven industrial concept. This framework, which includes the application of maturity assessment models, allows for quantitative evaluation of how organizations manage their stewardship activities and supports informed decision-making for continual improvement towards full compliance with federal, agency, and user requirements. © 2018, Ubiquity Press Ltd. All rights reserved."
"10.5334/dsj-2018-014","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050160869&doi=10.5334%2fdsj-2018-014&partnerID=40&md5=5014318fcac2e15101c488339edbe358","This paper describes a Virtual Research Environment (VRE) based on a web GIS platform ‘Climate+’, which provides an access to analytic instruments processing 19 collections of meteorological and climate data of several international organizations. This environment provides systematization of spatial data and related climate information and allows a user getting analysis results using geoinformation technologies. The ontology approach to this systematization is described, making it possible to match semantics of meteorological and climate parameters presented in different collections and used in solving various applied problems. © 2018, Ubiquity Press Ltd. All rights reserved."
"10.5334/dsj-2018-013","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050144441&doi=10.5334%2fdsj-2018-013&partnerID=40&md5=48f47610429ae30ee8c5033a57d7d2b2","Data tracking analysis is an important mechanism for increasing data analysis capacity and eliminating interference from observational data. In this study, the technique was applied to the geomagnetic fixed-station network to improve the efficiency and accuracy of analysis to extract useful information. This paper introduces the scope, workflow, analysis platform, abnormal variation status, and results of the geomagnetic data tracking analysis. We present some typical examples of abnormal variations in addition to our proposals for future work. © 2018, Ubiquity Press Ltd. All rights reserved."
"10.5334/dsj-2018-012","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050133538&doi=10.5334%2fdsj-2018-012&partnerID=40&md5=cae7be7070f3d581a4df3cf1beb13d86","Data Compression has been one of the enabling technologies for the on-going digital multimedia revolution for decades which resulted in renowned algorithms like Huffman Encoding, LZ77, Gzip, RLE and JPEG etc. Researchers have looked into the character/word based approaches to Text and Image Compression missing out the larger aspect of pattern mining from large databases. The central theme of our compression research focuses on the Compression perspective of Data Mining as suggested by Naren Ramakrishnan et al. wherein efficient versions of seminal algorithms of Text/Image compression are developed using various Frequent Pattern Mining(FPM)/Clustering techniques. This paper proposes a cluster of novel and hybrid efficient text and image compression algorithms employing efficient data structures like Hash and Graphs. We have retrieved optimal set of patterns through pruning which is efficient in terms of database scan/storage space by reducing the code table size. Moreover, a detailed analysis of time and space complexity is performed for some of our approaches and various text structures are proposed. Simulation results over various spare/dense benchmark text corpora indicate 18% to 751% improvement in compression ratio over other state of the art techniques. In Image compression, our results showed up to 45% improvement in compression ratio and up to 40% in image quality efficiency. © 2018, Ubiquity Press Ltd. All rights reserved."
"10.1089/big.2018.0007","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048963598&doi=10.1089%2fbig.2018.0007&partnerID=40&md5=8f90761d1748b4523251170578aaaccd","The proliferation of new data sources, stemmed from the adoption of open-data schemes, in combination with an increasing computing capacity causes the inception of new type of analytics that process Internet of things with low-cost engines to speed up data processing using parallel computing. In this context, the article presents an initiative, called BIG-Boletín Oficial del Estado (BOE), designed to process the Spanish official government gazette (BOE) with state-of-the-art processing engines, to reduce computation time and to offer additional speed up for big data analysts. The goal of including a big data infrastructure is to be able to process different BOE documents in parallel with specific analytics, to search for several issues in different documents. The application infrastructure processing engine is described from an architectural perspective and from performance, showing evidence on how this type of infrastructure improves the performance of different types of simple analytics as several machines cooperate. © Copyright 2018, Mary Ann Liebert, Inc."
"10.1089/big.2018.0023","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048963189&doi=10.1089%2fbig.2018.0023&partnerID=40&md5=1c1ddc1ab23652c507c654b2dd8848b7","In this article, the application of the deep learning method based on Gaussian-Bernoulli type restricted Boltzmann machine (RBM) to the detection of denial of service (DoS) attacks is considered. To increase the DoS attack detection accuracy, seven additional layers are added between the visible and the hidden layers of the RBM. Accurate results in DoS attack detection are obtained by optimization of the hyperparameters of the proposed deep RBM model. The form of the RBM that allows application of the continuous data is used. In this type of RBM, the probability distribution of the visible layer is replaced by a Gaussian distribution. Comparative analysis of the accuracy of the proposed method with Bernoulli-Bernoulli RBM, Gaussian-Bernoulli RBM, deep belief network type deep learning methods on DoS attack detection is provided. Detection accuracy of the methods is verified on the NSL-KDD data set. Higher accuracy from the proposed multilayer deep Gaussian-Bernoulli type RBM is obtained. © Copyright 2018, Mary Ann Liebert, Inc."
"10.1089/big.2018.0017","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048952521&doi=10.1089%2fbig.2018.0017&partnerID=40&md5=18091208bee9c4f7641fcf5d659cf3d3","In this article we develop a novel online framework to visualize news data over a time horizon. First, we perform a Natural Language Processing analysis, wherein the words are extracted, and their attributes, namely the importance and the relatedness, are calculated. Second, we present a Mathematical Optimization model for the visualization problem and a numerical optimization approach. The model represents the words using circles, the time-varying area of which displays the importance of the words in each time period. Word location in the visualization region is guided by three criteria, namely, the accurate representation of semantic relatedness, the spread of the words in the visualization region to improve the quality of the visualization, and the visual stability over the time horizon. Our approach is flexible, allowing the user to interact with the display, as well as incremental and scalable. We show results for three case studies using data from Danish news sources. © Copyright 2018, Mary Ann Liebert, Inc."
"10.1089/big.2017.0041","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048950691&doi=10.1089%2fbig.2017.0041&partnerID=40&md5=f6a9ec590edb9117b27120bae4414b07","Computational propaganda deploys social or political bots to try to shape, steer, and manipulate online public discussions and influence decisions. Collective behavior of populations of social bots has not been yet widely studied, although understanding of collective patterns arising from interactions between bots would aid social bot detection. In this study, we show that there are significant differences in collective behavior between population of bots and population of humans as detected from their Twitter activity. Using a large dataset of tweets we have collected during the UK-EU referendum campaign, we separated users into population of bots and population of humans based on the length of sequences of their high-frequency tweeting activity. We show that, while pairwise correlations between users are weak, they co-exist with collective correlated states; however the statistics of correlations and co-spiking probability differ in both populations. Our results demonstrate that populations of social bots and human users in social media exhibit collective properties similar to the ones found in social and biological systems placed near a critical point. © Copyright 2018, Mary Ann Liebert, Inc."
"10.1089/big.2017.0054","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048946164&doi=10.1089%2fbig.2017.0054&partnerID=40&md5=915094a3b7ca541c399781903239592e","This article proposes a novel approach, called data snapshots, to generate real-time probabilities of winning for National Basketball Association (NBA) teams while games are being played. The approach takes a snapshot from a live game, identifies historical games that have the same snapshot, and uses the outcomes of these games to calculate the winning probabilities of the teams in this game as the game is underway. Using data obtained from 20 seasons worth of NBA games, we build three models and compare their accuracies to a baseline accuracy. In Model 1, each snapshot includes the point difference between the home and away teams at a given second of the game. In Model 2, each snapshot includes the net team strength in addition to the point difference at a given second. In Model 3, each snapshot includes the rate of score change in addition to the point difference at a given second. The results show that all models perform better than the baseline accuracy, with Model 1 being the best model. © Copyright 2018, Mary Ann Liebert, Inc."
"10.1007/s41019-018-0067-3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062731815&doi=10.1007%2fs41019-018-0067-3&partnerID=40&md5=e727ed7fb8fdc72d10855e50ba3804b9","Extracting web content is to obtain the required data embedded in web pages, usually including structured records, such as product information, and text content, such as news. Web pages use a large number of HTML tags to organize and to present various information. Both knowing little about the structures of web pages and mixing kinds of information in web pages are making the extraction process very challenging to guarantee extraction performance and extraction adaptability. This study proposes a unified web content extraction framework that can be applied in various web environments to extract both structured records and text content. First, we construct a characteristic container to hold kinds of characteristics related with extraction objectives, including visual text information, content semantics(instead of HTML tag semantics), web page structures, etc. Second, the above characteristics are integrated into an extraction framework for extraction decisions on different web sites. Especially, we put forward different strategies, path aggregation for extracting text content and HMM model for structured records, to locate the extraction area by exploiting both those extraction characteristics. Comparative experiments on multiple web sites with popular extraction methods, including CETR, CETD and CNBE, show that our proposed extraction method can provide better extraction precision and extraction adaptability. © 2018, The Author(s)."
"10.1007/s41019-018-0064-6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062729154&doi=10.1007%2fs41019-018-0064-6&partnerID=40&md5=db96b62bac29db4cbcc7a633766756b8","In the context of RDF document matching/integration, the datatype information, which is related to literal objects, is an important aspect to be analyzed in order to better determine similar RDF documents. In this paper, we present an RDF Datatype in Ferring Framework, called RDF-F, which provides two independent datatype inference processes: 1) a four-step process consisting of (i) a predicate information analysis (i.e., deduce the datatype from existing range property), (ii) an analysis of the object value itself by a pattern-matching process (i.e., recognize the object lexical space), (iii) a semantic analysis of the predicate name and its context, and (iv) generalization of Numeric and Binary datatypes to ensure the integration; and 2) a non-ambiguous lexical-space-matching process, where literal values are inferred by the modification of their representation, following new lexical spaces. We evaluated the performance and the accuracy of both processes with datasets from DBpedia. Results show that the execution time of both indicators is linear and their accuracy can increase up to 97.10 and 99.30%, respectively. © 2018, The Author(s)."
"10.1007/s41019-018-0068-2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062716277&doi=10.1007%2fs41019-018-0068-2&partnerID=40&md5=a76d87c615b112a04f3c5b9a466983c4","Discovering dense subgraphs in a graph is a fundamental graph mining task, which has a wide range of applications in social networks, biology and visualization to name a few. Even the problem of computing most cohesive subgraphs is NP-hard (like clique, quasi-clique, k-densest subgraph), there exists a polynomial time algorithm for computing the k-core and k-truss. In this paper, we propose a novel dense subgraph model, k-core-truss, which leverages on a new type of important edges based on the basis of k-core and k-truss. We investigate the structural properties of the k-core-truss model. Compared to k-core and k-truss, k-core-truss can significantly discover the interesting and important structural information out the scope of k-core and k-truss. We study two useful problems of k-core-truss decomposition and k-core-truss search. In particular, we develop a k-core-truss decomposition algorithm to find all k-core-truss in a graph G by iteratively removing edges with the smallest degree-support. In addition, we offer a k-core-truss search algorithm to identifying a particular k-core-truss containing a given query node such that the core number k is the largest. Extensive experiments on several web-scale real-world datasets show the effectiveness and efficiency of k-core-truss model and proposed algorithms. © 2018, The Author(s)."
"10.1007/s41019-018-0066-4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062713744&doi=10.1007%2fs41019-018-0066-4&partnerID=40&md5=303aa5b35f4236e8a9f5c56cb70cc45f","Twitter is a social network that provides a powerful source of data. The analysis of those data offers many challenges among those stands out the opportunity to find reputation of a product, a person or any other entity of interest. Several approaches for sentiment analysis have been proposed in the literature to assess the general opinion expressed in tweets on an entity. Nevertheless, these methods aggregate sentiment scores retrieved from tweets, which is a static view to evaluate the overall reputation of an entity. The reputation of an entity is not static; entities collaborate with each other, and they get involved in different events over time. A simple aggregation of sentiment scores is then not sufficient to represent this dynamism. In this paper, we present a new approach to determine the reputation of an entity on the basis of the set of events in which it is involved. To achieve this, we propose a new sampling method driven by a tweet weighting measure to give a better quality and summary of the target entity. We introduce the concept of Frequent Named Entities to determine the events involving the target entity. Our evaluation achieved for different entities shows that 90% of the reputation of an entity originates from the events it is involved in and the breakdown into events allows interpreting the reputation in a transparent and self-explanatory way. © 2018, The Author(s)."
"10.1007/s41019-018-0063-7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062710886&doi=10.1007%2fs41019-018-0063-7&partnerID=40&md5=b2602d3a8e73ce461f4589b2a4fe9525","Learning effective feature descriptors that bridge the semantic gap between low-level visual features directly extracted from image pixels and the corresponding high-level semantics perceived by humans is a challenging task in image retrieval. This paper proposes a hybrid deep learning architecture (HDLA) that generates sparse latent topic-based representation with the objective of minimizing the semantic gap problem in image retrieval. In fact, HDLA has a deep network structure with a constrained replicated Softmax Model in the lower layer and constrained restricted Boltzmann machines in the upper layers. The advantage of HDLA is that there exist nonnegativity restrictions on the model weights together with ℓ 1 -sparsity enforced over the activations of the hidden layer nodes of the network. This, in turn, enhances the modeling power of the network and leads to sparse, parts-based latent topic representation of images. Experimental results on various benchmark datasets show that the proposed model exhibits better generalization ability and the resulting high-level abstraction yields better retrieval performance as compared to state-of-the-art latent topic-based image representation schemes. © 2018, The Author(s)."
"10.1007/s41019-018-0065-5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062686095&doi=10.1007%2fs41019-018-0065-5&partnerID=40&md5=1ce5aad2b7b56ae572209cadc2e28ea2","Graph classification is a difficult task because finding a good feature representation for graphs is challenging. Existing methods use topological metrics or local subgraphs as features, but the time complexity for finding discriminatory subgraphs or computing some of the crucial topological metrics (such as diameter and shortest path) is high, so existing methods do not scale well when the graphs to be classified are large. Another issue of graph classification is that the number of distinct graphs for each class that are available for training a classification model is generally limited. Such scarcity of graph data resources yields models that have much fewer instances than the model parameters, which leads to poor classification performance. In this work, we propose a novel approach for solving graph classification by using two alternative graph representations: the bag of vertices and the bag of partitions. For the first representation, we use representation learning-based node features and for the second, we use traditional metric-based features. Our experiments with 43 real-life graphs from seven different domains show that the bag representation of a graph improves the performance of graph classification significantly. We have shown 4–75% improvement on the vertex-based and 4–36% improvement on partition-based approach over the existing best methods. Besides, our vertex and partition multi-instance methods are on average 75 and 11 times faster in feature construction time than the current best, respectively. © 2018, The Author(s)."
"10.1057/s41270-018-0030-1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046353528&doi=10.1057%2fs41270-018-0030-1&partnerID=40&md5=2ce53adcffafe07316f685bbabf978c9","Internet marketing is a business imperative due to the irrevocable and unstoppable trend of Internet. In this paper, we propose the methodology of unsupervised learning algorithm to apply on the survey data of Internet marketing. Hidden relationships among critical factors in the model are examined using PC-algorithm. To analyze the ecology of Internet marketing in small-and-medium enterprises, similar commercial organizations are grouped into segments by K-means clustering for studying their characteristics. The last methodology is to perform two levels of clustering using self-organizing map and K-means algorithm to save computational cost for massive data instances. The analytical result describes the overview of the Internet usage under stiff-competition business environment which is beneficial to management level to make a more appropriate decision to upload the existing marketing activities online. © 2018 Macmillan Publishers Ltd., part of Springer Nature."
"10.1057/s41270-018-0033-y","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045475441&doi=10.1057%2fs41270-018-0033-y&partnerID=40&md5=e7f84b1a3a705cba0be637451386ab67","In this research, we address an important gap in the literature as to the search behavior of new car buyers. While the effect of the Internet on this process is known, the literature still lacks a comprehensive study which (1) concurrently covers time periods before and after the launch of the Internet, and (2) compares trends of consumer search across those combined years. Our unique survey dataset, which spans 22 years and includes consumer search information for new cars from both the pre-and post-Internet eras, enables us to investigate this important gap. Using a latent class model, we classify respondents according to variables that measure consumer search for new automobiles. We unveil changes in characteristics of the six latent segments of car shoppers. Our main findings show that, over the years since the advent of the Internet, the segment of car buyers who mainly search through car dealers/stores has been shrinking drastically. We also find evidence that, over time, the heavy Internet user segment has become less likely to have decided on the manufacturer/dealer prior to searching. Our findings benefit researchers, practitioners, car manufacturers, dealers, and buyers. © 2018 Macmillan Publishers Ltd., part of Springer Nature."
"10.1057/s41270-018-0031-0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044480546&doi=10.1057%2fs41270-018-0031-0&partnerID=40&md5=b3ad2308dac0282483e83bb8b5e46989","The traditional approach to build incremental conversion prediction model has to rely on A/B test results. However, under certain intense business environment, A/B testing can be limited by technical support, platform, or budget, and become not practically available. In this paper, we propose an algorithm to rank users by incremental conversions resulted from advertising effects, which is based on user's conversion history and the output from a conversion prediction model. By appropriately defining who an active user is, this algorithm is proven to work well with real data. In case where an A/B test is not available and incremental conversion-based user targeting is desired, this algorithm offers a practical solution. © 2018 Macmillan Publishers Ltd., part of Springer Nature."
"10.5334/dsj-2018-011","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050165958&doi=10.5334%2fdsj-2018-011&partnerID=40&md5=42d0808f484d21a3a9db20c73d0f5e56","In this paper we introduce about the marine data archived at Indian National Centre for Ocean Information Services (INCOIS), Ministry of Earth Sciences, India. Heterogeneous data from in situ, remote sensing and ocean models are archived. In-situ ocean observations includes data from Lagrangian as well Eulerian platforms like Argo floats, moored buoys etc, while remote sensing include data from NOAA satellite series, OceanScat etc. The data generated is translated into ocean information services through analysis and modelling. Data is disseminated to users using variety of means like web with GIS features, ERDDAP, Live Access server with facilities to search, visualize and download. © 2018, Ubiquity Press Ltd. All rights reserved."
"10.5334/dsj-2018-010","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050092811&doi=10.5334%2fdsj-2018-010&partnerID=40&md5=e2970d0b3809e95e26a57de49956e43d","Both ontology builders and users need a way to evaluate ontologies in terms of usability, but existing ontology evaluation approaches do not fit this purpose. We propose the Ontology Usability Scale (OUS), a ten-item Likert scale derived from statements prepared according to a semiotic framework and an online poll in the Semantic Web community to provide a practical way of ontology usability evaluation. Case studies were conducted to bookkeep current usability evaluation results for ontologies expecting revisions in the future, and discussions of the poll results are presented to help proper use and customization of the OUS. © 2018, Ubiquity Press Ltd. All rights reserved."
"10.1177/2399808317710023","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046715746&doi=10.1177%2f2399808317710023&partnerID=40&md5=f839ddf2765922477f3ed5004b13ed35","The increasing availability of urban trajectory data from the GPS-enabled devices has provided scholars with opportunities to study urban dynamics at a finer spatiotemporal scale. Yet given the multi-dimensionality of urban trajectory dynamics, current research faces challenges of systematically uncovering spatiotemporal and societal implications of human movement patterns. Particularly, a data-driven policy-making process may need to use data from various sources with varying resolutions, analyze data at different levels, and compare the results with different scenarios. As such, a synthesis of varying spatiotemporal and network methods is needed to provide researchers and planning specialists a foundation for studying complex social and spatial processes. In this paper, we propose a framework that combines various spatiotemporal and network analysis units. By customizing the combination of analysis units, the researcher can employ trajectory data to evaluate urban built environment dynamically and comparatively. Two case studies of Chinese cities are carried out to evaluate the usefulness of proposed conceptual framework. Our results suggest that the proposed framework can comprehensively quantify the variation of urban trajectory across various scales and dimensions. © 2017, © The Author(s) 2017."
"10.1177/2399808317721930","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046706041&doi=10.1177%2f2399808317721930&partnerID=40&md5=dc77544069ac60a3a69ca52dada29f2f","We introduce a version of the Huff retail expenditure model, where retail demand depends on households’ access to retail centers. Household-level survey data suggest that total retail visits in a system of retail centers depends on the relative location pattern of stores and customers. This dependence opens up an important question—could overall visits to retail centers be increased with a more efficient spatial configuration of centers in planned new towns? To answer this question, we implement the model as an Urban Network Analysis tool in Rhinoceros 3D, where facility patronage can be analyzed along spatial networks and apply it in the context of the Punggol New Town in Singapore. Using fixed household locations, we first test how estimated store visits are affected by the assumption of whether shoppers come from homes or visit shops en route to local public transit stations. We then explore how adjusting both the locations and sizes of commercial centers can maximize overall visits, using automated simulations to test a large number of scenarios. The results show that location and size adjustments to already planned retail centers in a town can yield a 10% increase in estimated store visits. The methodology and tools developed for this analysis can be extended to other context for planning and right-sizing retail developments and other public facilities so as to maximize both user access and facilities usage. © 2017, © The Author(s) 2017."
"10.1177/2399808317690158","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041609200&doi=10.1177%2f2399808317690158&partnerID=40&md5=2d1bc82a00a196a6e15625bab8129275","In this study, we examine how different features of the built environment—density, diversity of land uses, and design—have consequences for personal networks. We also consider whether different features of the built environment have consequences for the spatial location of persons to whom one is tied by considering their distribution in local area, broader city region, and a more macro spatial scale. We test these ideas with a large sample of the Western United States for three different types of ties. Our findings suggest that the built environment is crucial for personal network structure, both in the number of social ties and where they are located. © 2017, © The Author(s) 2017."
"10.1177/2399808317710658","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041554478&doi=10.1177%2f2399808317710658&partnerID=40&md5=3c9bd3471a80a2671969bc312eade8ff","In this paper, we demonstrate an integrated spatio-social network analysis to measure the degree of segregation within a Chinese urban neighborhood in terms of the everyday activities by rural migrants and local urban residents at such routine venues as restaurants, grocery stores, barber shops, etc. Our data were collected in May 2014, through an integrated geographical and social survey conducted within an inner-city neighborhood around Kecun in south China’s Guangzhou Municipality. Although Kecun features a highly condensed and mixed dwelling pattern between the rural migrants and indigenous urban population, we find that within our sample of 110 local and 132 migrant residents, the former tend to socialize more inwardly with their peer locals and visit neighborhood amenities more often such as pubs, stadia, and public kindergartens. In contrast, the migrants tend to attend local roadside food stalls, outdoor recreation facilities, small clinic shops, grocery malls, and private (minban) kindergartens more often. Overall, only a modest degree of social interaction between the locals and migrants appears to exist in Kecun. On top of the methodological implications of our study, we argue that urban segregation in China is both socially and spatially different from its Anglo-American counterpart. More empirical research is needed to understand and assess social segregation underlying the everyday urban life in Chinese cities. © 2017, © The Author(s) 2017."
"10.1177/2399808317696073","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041492264&doi=10.1177%2f2399808317696073&partnerID=40&md5=5a0132883b62f3307599e3dbfe455ec3","Development of urban networks of cities and towns has received attention including discussions of tensions between population concentrations and overlaps with environmentally sensitive and disaster-prone areas. Moreover, certain development in broad regions of China, such as its deltas, has become a subject of debate. Contrary to some assumptions, this development within places like the Changjiang Delta (also known as the Yangtze River Delta) has proceeded in a relatively incremental manner. However, at this juncture, controlled development of larger cities, like Shanghai, has shifted to more conventional urbanization pathways forward involving larger city expansions. Nevertheless, further urban growth management appears to depend on development and maintenance of a well-balanced network of large, medium, and small-scaled cities and towns. An important aspect of this development involves definition of the Changjiang Delta region itself, and in particular, alongside its likely further economic performance. To these ends, a scenario-based Cellular Automata model of spatial distribution is deployed, reflecting separate thematic projections. A baseline for economic performance is developed, incorporating measures of fixed-asset investment in urban service, revenue from urban maintenance, and Gross Domestic Product. Revelation of a well-performing network involves spatial distribution of development at various scales, and in various concentrations within the region, moreover, location of this development, largely perpendicular to well-travelled corridors, appears as a preferable outcome, contrary to earlier depictions along the major transportation corridors. © 2017, © The Author(s) 2017."
"10.1177/2399808317690154","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041426442&doi=10.1177%2f2399808317690154&partnerID=40&md5=f24d151c5e261179aba93d452213f1d9","The rapid development of information and communication technology has led to the Internet and social media becoming a vital platform for public participation in China. The present research sought to understand the complexity of participation in the network society by taking the cancellation of the number 55 bus route in Shanghai as a case study. Both qualitative and quantitative research methods were used to analyze data from a leading social networking site in China. An analysis of participation patterns led to an understanding of the main characteristics of public participation in the network society, and a statistical analysis of the network revealed the features of elite participants in the planning adjustment. A qualitative approach was also used to explore the communication process, which was influenced by Chinese social capital—guanxi. The case study revealed an uneven pattern of public participation in the network society, and suggestions are provided to enhance fairness and effectiveness in this process. © 2017, © The Author(s) 2017."
"10.1177/2399808317740354","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041425823&doi=10.1177%2f2399808317740354&partnerID=40&md5=b750c1beb9c3cb8d90f20b93a46e27fa","While urban planners and transportation geographers have long emphasized the importance of social influences on individual travel behavior, many challenges remain to bridge the gap between complex conceptual frameworks and operational behavioral models. Improving the ability of models to forecast activity-travel behavior can provide greater insights into urban planning issues. This paper proposes a new model framework by evaluating how individual travel behavior is influenced by inter- and intra-household interactions. The built environment, land-use mix, and social interactions influence household member choices among different transport modes. We propose a spatial multivariate Tobit specification that allows each individual to face a set of potential destinations and transport modes and takes into consideration the travel behavior of other household members and nearby neighbors. Using the Greater Cincinnati Household Travel Survey, we analyzed more than 37,000 trips made by 1968 individuals located in Hamilton County in Cincinnati, Ohio. Results reveal that social influences and the built environment have a strong impact on the willingness to walk and to cycle. © 2017, © The Author(s) 2017."
"10.1177/2399808317728289","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041414367&doi=10.1177%2f2399808317728289&partnerID=40&md5=9113c088689684102ae50a1bc688b77e","Gentrification, the rise of affluent socioeconomic populations in economically depressed urban neighborhoods, has been accused of disrupting community in these neighborhoods. Social media networks meanwhile have been recognized not only to create new communities in neighborhoods, but are also associated with gentrification. What relation then does gentrification and social media networks have to urban communities? To explore this question, this study uses social media networks found on Twitter to identify communities in Washington, DC. With space-time analysis of 821,095 geo-tagged tweets generated by 77,528 users captured from 15 October 2015 to 18 July 2016, we create a location-based interaction measure of tweets which overlays the social networks of the comprising users based on their followers and followees. We identify gentrifying neighborhoods with the 2000 Census and the 2010–2014 American Community Survey at the block group level. We then compare the density of location-based interactions between gentrifying and nongentrifying neighborhoods. We find that gentrification is significantly related to these location-based interactions. This suggests that gentrification indeed is associated with some communities in neighborhoods, though questions remain as to who has access. Making novel use of big data, these results demonstrate the important role built environment has on social connections forged “online.”. © 2017, © The Author(s) 2017."
"10.1177/2399808317707967","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041407711&doi=10.1177%2f2399808317707967&partnerID=40&md5=778cb3715dea15b13c4bf6b4d40cd4d3","Studies on the jobs–housing balance and self-containment of employment are mainly focused on observed journey-to-work trips using travel survey data. This study examines the relationship between the jobs–housing balance and the self-containment of employment through the use of mobile phone location data in Shenzhen, a megacity in southern China. Individual-level journey-to-work trips are explored based on mobile phone location big data. Self-containment of employment in the suburban districts is higher than that in the central districts. The effect of the jobs–housing balance on self-containment of employment is examined at a 2 km grid level. Jobs–housing balance policies positively affect the self-containment of employment in the suburban districts, but its effect is limited in the central districts. Two extreme commuting spectrum measures are used to analyze self-containment of employment in different journey-to-work scenarios with the same jobs–housing distribution. Workers are disaggregated into secondary and tertiary sector workers according to job types. The self-containment of employment is found to be mainly affected by the local jobs–housing balance for secondary-sector workers and the regional city level job distribution for tertiary-sector workers. The extreme scenarios of commuting behavior using the commuting spectrum method can provide benchmarks that can help to understand the observed self-containment of employment better. © 2017, © The Author(s) 2017."
"10.1177/0265813516687302","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030622405&doi=10.1177%2f0265813516687302&partnerID=40&md5=a232448547e576cb978c1701423310b4","For centuries, philosophers, policy-makers and urban planners have debated whether aesthetically pleasing surroundings can improve our wellbeing. To date, quantifying how scenic an area is has proved challenging, due to the difficulty of gathering large-scale measurements of scenicness. In this study we ask whether images uploaded to the website Flickr, combined with crowdsourced geographic data from OpenStreetMap, can help us estimate how scenic people consider an area to be. We validate our findings using crowdsourced data from Scenic-Or-Not, a website where users rate the scenicness of photos from all around Great Britain. We find that models including crowdsourced data from Flickr and OpenStreetMap can generate more accurate estimates of scenicness than models that consider only basic census measurements such as population density or whether an area is urban or rural. Our results provide evidence that by exploiting the vast quantity of data generated on the Internet, scientists and policy-makers may be able to develop a better understanding of people's subjective experience of the environment in which they live. © 2017, © The Author(s) 2017."
"10.5334/dsj-2018-009","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050110151&doi=10.5334%2fdsj-2018-009&partnerID=40&md5=854fbac4b34037d0d4c4edab11fbe428","This paper undertakes a detailed examination of the availability and quality of data on public expenditures in agriculture in Africa. We consider the case of Mozambique, a country charac-terised by low income and low administrative capacity, but also by a policy environment that has turned a focused lens on public funding to agriculture. We explore the extent to which domestic analysts may be able to access and use such data to reliably quantify public resource allocation to the sector, and to unpack the ‘black box’ of what goes into country-level public expenditure statistics. We find that data are, surprisingly, freely available in great abundance. This has encouraging aspects but also pitfalls: On the one hand, data that are often out of public sight are openly accessible for Mozambican researchers to draw upon. But the drawback of high abundance emanates from its manifestation in the form of a proliferation of multiple classification systems used to create a fine disaggregation of public funds data; given Mozambique’s limited public sector capacity, this has meant that each classification system leaves a lot to be desired, making it hard to use any single one to accurately and fully reliably reconstruct the amount of public resources going to agriculture. Making the hard choice to eliminate some of the classification systems, and dedicate this freed-up capacity to be more thorough on the retained ones, would better serve domestic users of such data, as well as the government, which is both a consumer and producer of these data. © 2018, Ubiquity Press Ltd. All rights reserved."
"10.1080/23270012.2018.1436988","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057003805&doi=10.1080%2f23270012.2018.1436988&partnerID=40&md5=f46e826e107e7c4ec1cd84ec46004bb4","Before mass production of individual components industries may assess its capability to produce it according to specifications. This capability assessment is a common requirement in the automotive sector. This work shows a case study, providing an in-depth analysis, on a critical component and finds evidence of process degradation over two years of production, quantified through capability analysis. At pre-production, it was concluded that the process was capable and, thus, no statistical process control was done during its production. Over the months no defective units were detected but then its level began to increase and it was apparent that process variability had increased and the process was no longer capable. Process improvement activities were developed using known quality tools and methodologies. This work shows how the control plan initially defined became obsolete and discusses the need to periodically review quality control mechanisms. © 2018, © 2018 Antai College of Economics and Management, Shanghai Jiao Tong University."
"10.1080/23270012.2018.1443405","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055715893&doi=10.1080%2f23270012.2018.1443405&partnerID=40&md5=54ab911a6d31db540651a64080d5027c",[No abstract available]
"10.1080/23270012.2018.1434425","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055697693&doi=10.1080%2f23270012.2018.1434425&partnerID=40&md5=36c6f6f992e96e1f690d3048b5357adf","Decision Support Systems are considered as a robust technology able to provide an advantage to several manufacturing companies. As part of the Z-Fact0r EU project Early Stage-Decision Support System, a framework for the inspection of a printed circuit boards (PCB) and the inference of faults, regarding the excess or insufficient glue, is proposed. For the inspection of the PCB, a pixel-based vector of the regions of interest is utilized and several very popular in research community machine learning algorithms are tested on their performance on fault recognition. In order to determine the most efficient and effective classifier, a schema of Monte Carlo simulations for each classification algorithm and set of hyper-parameters was performed. Simulation results show a superiority of the support vector machine (SVM) classifier with polynomial and radial basis function kernels, compared to the rest. The best overall classifier was the SVM polynomial (accuracy: 81.39%, f-measure: 78.72%). © 2018, © 2018 Antai College of Economics and Management, Shanghai Jiao Tong University."
"10.1080/23270012.2018.1424571","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055680200&doi=10.1080%2f23270012.2018.1424571&partnerID=40&md5=4e065a4fb75605fe255005c8f2a4e817","This paper presents a new optimisation approach for variance within a supply chain management process. The approach is presented by the variance cube of purchasing (VCP) that involves a lean method for variance optimisation, namely the cost and variance driver analysis. The approach focuses on the optimisation and the control of existing process variance within the supply chain. The application of the cube is presented by a case study involving a globally acting Tier 1 supplier, who produces steering systems for passenger cars and commercial vehicles. In this case, the sourcing process of this Tier 1 supplier will be analysed, evaluated and optimised regarding variance. The variance is presented in the form of the number of suppliers who are involved in the sourcing process. Unnecessary existing process variance, like an unnecessary huge number of suppliers within the sourcing process, is a type of waste. Time, money, quality and technology can be saved through a greater understanding of the optimal number of suppliers within a sourcing process. The results of the case study led to a generalised method to optimise the existing process variance, present cost improvements as well as optimising the key performance indicator to manage the number of suppliers in the sourcing process. The general approach can be used for other company departments like logistics and for different industries other than automotive. The insights of this article support the operative user and the strategic company management in order to reduce and improve unnecessary variance in different sections. The structured analysis of supply chain process variance via the VCP and the key performance indicator “optimal supplier number per sourcing process” are new to company management. © 2018, © 2018 Antai College of Economics and Management, Shanghai Jiao Tong University."
"10.1080/23270012.2018.1436989","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053554608&doi=10.1080%2f23270012.2018.1436989&partnerID=40&md5=5d84752ac4e3919182f4c04cd0cd2731","Spare parts management is a function of maintenance management that aims to support maintenance activities, giving real-time information on the available quantities of each spare part and adopting the inventory policies that ensure their availability when required, minimizing costs. The classification of spare parts is crucial to control the vast number of parts that have different characteristics and specificities. Spare parts management involves mainly two areas, maintenance and logistics. Therefore, the integration of both input information is recommended to make decisions. This paper presents a multi-criteria classification methodology combining maintenance and logistics perspectives that intends to differentiate and group spare parts to, subsequently, define the most appropriate stock management policy for each group. The methodology was developed based on a case study carried out in a multi-national manufacturing company and is intended to be included in its computerized maintenance management system to support decision-making. © 2018, © 2018 Antai College of Economics and Management, Shanghai Jiao Tong University."
"10.5334/dsj-2018-008","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050156837&doi=10.5334%2fdsj-2018-008&partnerID=40&md5=1082085795445f39ea0b689fdcbbf3fe","This paper assesses concordance and inconsistency among three small area estimation methods that are currently providing county-level health indicators in the United States. The three methods are multi-level logistic regression, spatial logistic regression, and spatial Poison regression, all proposed since 2010. Diabetes prevalence is estimated for each county in the continental United States from the 2012 sample of Behavioral Risk Factor Surveillance System. The mapping results show that all three methods displayed elevated diabetes prevalence in the South. While the Pearson correlation coefficients among three model-based estimates were all above 0.60, the highest one was 0.80 between the multilevel and spatial logistic methods. While point estimates are apparently different among the three small area estimate methods, their top and bottom of quintile distributions are fairly consistent based on Bangdiwala’s B-statistic, suggesting that outputs from each method would support consistent policy making in terms of identifying top and bottom percent counties. © 2018, Ubiquity Press Ltd. All rights reserved."
"10.4018/IJBAN.2018040105","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046299187&doi=10.4018%2fIJBAN.2018040105&partnerID=40&md5=ec29a89fe6d0e814b2be5b0818a7ee89","Much of the research in data mining and knowledge discovery has focused on the development of efficient data mining algorithms. Researchers and practitioners have developed data mining techniques to solve diverse real-world data mining problems. But there is no single source that identifies which techniques solve what problems and how, the advantages and limitations, and real-life use-cases. Lately, identifying data mining techniques and corresponding problems that they solve has drawn significant attention. In this paper, the author describes the progress made in developing data mining techniques and then classify them in terms of data mining problems taxonomy to help assist practitioners in using appropriate data mining techniques that solve business problems. This will allow researchers to expand the body of knowledge in this discipline. This article proposes a data mining problems taxonomy based on data mining techniques being used. Prominent data mining problems include classification, optimization, prediction, partitioning, relationship, pattern matching, recommendation, ranking, sequential patterns and anomaly detection. The data mining techniques that are used to solve these data mining problems in general fall under top 10 data mining algorithms. Copyright © 2018, IGI Global."
"10.4018/IJBAN.2018040103","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046294065&doi=10.4018%2fIJBAN.2018040103&partnerID=40&md5=3e9549a55eb8f629a0b9fb85e0bb2c45","The purpose of this article is to develop a new stochastic multi objective optimization model to mitigate disruption risks while simultaneously addressing operational risks as well. Indeed, this model considers five objective functions for selecting a set of suppliers considering disruption risk and stochastic demand, quality, and delay time. The authors use two types of risk evaluation models: value-at-risk (VaR) and conditional value-at-risk (CVaR). Two examples are given to illustrate our model and two solution methods are compared and tested. Copyright © 2018, IGI Global."
"10.4018/IJBAN.2018040102","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046289986&doi=10.4018%2fIJBAN.2018040102&partnerID=40&md5=8f501cd7c863850e0fb20b6f102947f5","This article presents and experimentally tests a new method for measuring student risk preferences where monetary outcomes are not directly involved. The authors call this new method the Lazy Professor Risk Task (LPRT). This article compares the LPRT’s results to popular conventional methods where monetary outcomes are involved. The results show that the new method is capable of producing consistent responses at approximately the same rate as comparable conventional methods. In addition, the method produced responses that were no noisier than conventional methods. It is hoped that future research can perfect this new method and use it to compare classroom risk taking to risk taking in other domains. Copyright © 2018, IGI Global."
"10.4018/IJBAN.2018040101","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046279942&doi=10.4018%2fIJBAN.2018040101&partnerID=40&md5=ba83a8fe80ca89138c42c8b49c51d8e0","This article contends that vendors are now pushing into the small to mid-size organizational markets with simplified, less-risk, less-reward systems while consultants have project experience and have thorough contracts isolating their role. However, SMEs that adopted ERP on-premise solutions before are now facing a dilemma: continuing with ERP on-premise upgrades or switch to on-demand solutions. This article contains data surveyed from Chief Information Officers (CIO) of SMEs with respect to indicators of ERP adoptions. Cost, reduced demand for own IT resources, outage/accessibility and performance were found to be the most critical and important factors to assess ERP adoptions for SMEs. Copyright © 2018, IGI Global."
"10.4018/IJBAN.2018040104","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046256430&doi=10.4018%2fIJBAN.2018040104&partnerID=40&md5=db425cf691b26388a17033fde52ea35b","Leveraging a popular operations management simulation, we examine how team interaction, students’ previous software experience, simulations’ ease of use, team leadership, and instructors’ guidance affects students’ comprehension. Respondents completed a 27-item survey designed to assess individual comprehension. To illustrate the various affects, we use structural equation modeling to compare first-generation and continuing-generation undergraduate populations. The results indicate software’s ease of use and instructors’ guidance affects both groups, while team interaction to comprehension is present for only first-generation students. We also find that the strength of the various relationships depends on the population of students being studied. As simulations continue to grow in popularity, educators need to recognize how these pedagogical tools affect different student populations. Copyright © 2018, IGI Global."
"10.1089/big.2017.0083","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044386792&doi=10.1089%2fbig.2017.0083&partnerID=40&md5=fff6a08a83ac4b6280ff536c736fa5ba","Nonstandard insurers suffer from a peculiar variant of fraud wherein an overwhelming majority of claims have the semblance of fraud. We show that state-of-the-art fraud detection performs poorly when deployed at underwriting. Our proposed framework ""FraudBuster"" represents a new paradigm in predicting segments of fraud at underwriting in an interpretable and regulation compliant manner. We show that the most actionable and generalizable profile of fraud is represented by market segments with high confidence of fraud and high loss ratio. We show how these segments can be reported in terms of their constituent policy traits, expected loss ratios, support, and confidence of fraud. Overall, our predictive models successfully identify fraud with an area under the precision-recall curve of 0.63 and an f-1 score of 0.769. © Copyright 2018, Mary Ann Liebert, Inc. 2018."
"10.1089/big.2017.0085","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044384627&doi=10.1089%2fbig.2017.0085&partnerID=40&md5=36bbf6c193a807f492497156ec6056c9","Studies have shown that certain features from geography, demography, trade area, and environment can play a vital role in retail site selection, largely due to the impact they asserted on retail performance. Although the relevant features could be elicited by domain experts, determining the optimal feature set can be intractable and labor-intensive exercise. The challenges center around (1) how to determine features that are important to a particular retail business and (2) how to estimate retail sales performance given a new location? The challenges become apparent when the features vary across time. In this light, this study proposed a nonintervening approach by employing feature selection algorithms and subsequently sales prediction through similarity-based methods. The results of prediction were validated by domain experts. In this study, data sets from different sources were transformed and aggregated before an analytics data set that is ready for analysis purpose could be obtained. The data sets included data about feature location, population count, property type, education status, and monthly sales from 96 branches of a telecommunication company in Malaysia. The finding suggested that (1) optimal retail performance can only be achieved through fulfillment of specific location features together with the surrounding trade area characteristics and (2) similarity-based method can provide solution to retail sales prediction. © Copyright 2018, Mary Ann Liebert, Inc. 2018."
"10.1089/big.2018.0015","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044353343&doi=10.1089%2fbig.2018.0015&partnerID=40&md5=75d70d9c94b9b3df8eedaa65f37594d4","The goal of customer retention campaigns, by design, is to add value and enhance the operational efficiency of businesses. For organizations that strive to retain their customers in saturated, and sometimes fast moving, markets such as the telecommunication and banking industries, implementing customer churn prediction models that perform well and in accordance with the business goals is vital. The expected maximum profit (EMP) measure is tailored toward this problem by taking into account the costs and benefits of a retention campaign and estimating its worth for the organization. Unfortunately, the measure assumes fixed and equal customer lifetime value (CLV) for all customers, which has been shown to not correspond well with reality. In this article, we extend the EMP measure to take into account the variability in the lifetime values of customers, thereby basing it on individual characteristics. We demonstrate how to incorporate the heterogeneity of CLVs when CLVs are known, when their prior distribution is known, and when neither is known. By taking into account individual CLVs, our proposed approach of measuring model performance gives novel insights when deciding on a customer retention campaign. The method is dependent on the characteristics of the customer base as is compliant with modern business analytics and accommodates the data-driven culture that has manifested itself within organizations. © Copyright 2018, Mary Ann Liebert, Inc. 2018."
"10.1016/j.bdr.2017.09.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85034415335&doi=10.1016%2fj.bdr.2017.09.001&partnerID=40&md5=da8b9ccb7e22e8bcdd9608789862d465","A popular testbed for change-detection algorithms consists in detecting changes that have been synthetically injected in real-world datastreams. Unfortunately, most of experimental practices in the literature lead to injecting changes whose magnitude is unknown and can not be controlled. As a consequence, results are difficult to interpret, reproduce, and compare with. Most importantly, controlling the change magnitude is a primary requirement to investigate the change-detection performance when data dimension scales, which is an issue to be typically addressed in big data scenarios. Here we present a best practice to inject changes in multivariate/high-dimensional datastreams: “Controlling Change Magnitude” (CCM) is a rigorous method to generate datastreams affected by a change having a desired magnitude at a known location. In CCM, changes are introduced by directly applying a roto-translation to the data, and the change magnitude is measured by the symmetric Kullback–Leibler divergence between the pre- and post-change data distributions. The roto-translation parameters yielding the desired change magnitude are identified by two iterative algorithms whose convergence is here proven. Our experiments show that CCM can effectively control the change magnitude in real-world datastreams, while traditional experimental practices might not be appropriate for assessing the performance of change-detection algorithms in high-dimensional data. © 2017 Elsevier Inc."
"10.1016/j.bdr.2017.09.003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032196908&doi=10.1016%2fj.bdr.2017.09.003&partnerID=40&md5=72b2b80dac84e435935528670c894913","Sensors are present in various forms all around the world such as mobile phones, surveillance cameras, smart televisions, intelligent refrigerators and blood pressure monitors. Usually, most of the sensors are a part of some other system with similar sensors that compose a network. One of such networks is composed of millions of sensors connected to the Internet which is called Internet of Things (IoT). With the advances in wireless communication technologies, multimedia sensors and their networks are expected to be major components in IoT. Many studies have already been done on wireless multimedia sensor networks in diverse domains like fire detection, city surveillance, early warning systems, etc. All those applications position sensor nodes and collect their data for a long time period with real-time data flow, which is considered as big data. Big data may be structured or unstructured and needs to be stored for further processing and analyzing. Analyzing multimedia big data is a challenging task requiring a high-level modeling to efficiently extract valuable information/knowledge from data. In this study, we propose a big database model based on graph database model for handling data generated by wireless multimedia sensor networks. We introduce a simulator to generate synthetic data and store and query big data using graph model as a big database. For this purpose, we evaluate the well-known graph-based NoSQL databases, Neo4j and OrientDB, and a relational database, MySQL. We have run a number of query experiments on our implemented simulator to show that which database system(s) for surveillance in wireless multimedia sensor networks is efficient and scalable. © 2017 Elsevier Inc."
"10.1016/j.bdr.2017.09.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032175326&doi=10.1016%2fj.bdr.2017.09.002&partnerID=40&md5=8678b5b38142a89d28c65482f8b0ad60","Clustering algorithms are recently regaining attention with the availability of large datasets and the rise of parallelized computing architectures. However, most clustering algorithms suffer from two drawbacks: they do not scale well with increasing dataset sizes and often require proper parametrization which is usually difficult to provide. A very important example is the cluster count, a parameter that in many situations is next to impossible to assess. In this paper we present A-BIRCH, an approach for automatic threshold estimation for the BIRCH clustering algorithm. This approach computes the optimal threshold parameter of BIRCH from the data, such that BIRCH does proper clustering even without the global clustering phase that is usually the final step of BIRCH. This is possible if the data satisfies certain constraints. If those constraints are not satisfied, A-BIRCH will issue a pertinent warning before presenting the results. This approach renders the final global clustering step of BIRCH unnecessary in many situations, which results in two advantages. First, we do not need to know the expected number of clusters beforehand. Second, without the computationally expensive final clustering, the fast BIRCH algorithm will become even faster. For very large data sets, we introduce another variation of BIRCH, which we call MBD-BIRCH, which is of particular advantage in conjunction with A-BIRCH but is independent from it and also of general benefit. © 2017 The Authors"
"10.1016/j.bdr.2017.06.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025460499&doi=10.1016%2fj.bdr.2017.06.002&partnerID=40&md5=48354b438026bf95eae4952a27c22ddf","Face detection constitutes a key visual information analysis task in Machine Learning. The rise of Big Data has resulted in the accumulation of a massive volume of visual data which requires proper and fast analysis. Deep Learning methods are powerful approaches towards this task as training with large amounts of data exhibiting high variability has been shown to significantly enhance their effectiveness, but often requires expensive computations and leads to models of high complexity. When the objective is to analyze visual content in massive datasets, the complexity of the model becomes crucial to the success of the model. In this paper, a lightweight deep Convolutional Neural Network (CNN) is introduced for the purpose of face detection, designed with a view to minimize training and testing time, and outperforms previously published deep convolutional networks in this task, in terms of both effectiveness and efficiency. To train this lightweight deep network without compromising its efficiency, a new training method of progressive positive and hard negative sample mining is introduced and shown to drastically improve training speed and accuracy. Additionally, a separate deep network was trained to detect individual facial features and a model that combines the outputs of the two networks was created and evaluated. Both methods are capable of detecting faces under severe occlusion and unconstrained pose variation and meet the difficulties of large scale real-world, real-time face detection, and are suitable for deployment even in mobile environments such as Unmanned Aerial Vehicles (UAVs). © 2017 Elsevier Inc."
"10.1016/j.bdr.2017.06.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025140606&doi=10.1016%2fj.bdr.2017.06.001&partnerID=40&md5=1d7a0de0a1ec60a9163fd3370abb745b","The advent of Solid State Drives (SSDs) stimulated a lot of research to investigate and exploit to the extent possible the potentials of the new drive. The focus of this work is on the investigation of the relative performance and benefits of SSDs versus hard disk drives (HDDs) when they are used as underlying storage for Hadoop's MapReduce. In particular, we depart from all earlier relevant works in that we do not use their workloads, but examine MapReduce tasks and data suitable for performing analysis of complex networks which present different execution patterns. Despite the plethora of algorithms and implementations for complex network analysis, we carefully selected our “benchmarking methods” so that they include methods that perform both local and network-wide operations in a complex network, and also they are generic enough in the sense that they can be used as primitives for more sophisticated network processing applications. We evaluated the performance of SSDs and HDDs by executing these algorithms on real social network data and excluding the effects of network bandwidth which can severely bias the results. The obtained results confirmed in part earlier studies which showed that SSDs are beneficial to Hadoop. However, we also provided solid evidence that the processing pattern of the running application has a significant role, and thus future studies must not blindly add SSDs to Hadoop, but they should build components for assessing the type of processing pattern of the application and then direct the data to the appropriate storage medium. © 2017 Elsevier Inc."
"10.1016/j.bdr.2017.05.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020215215&doi=10.1016%2fj.bdr.2017.05.002&partnerID=40&md5=64fcb84c6ba69f20bd5c05ced10280eb","Current train delay prediction systems do not take advantage of state-of-the-art tools and techniques for handling and extracting useful and actionable information from the large amount of historical train movements data collected by the railway information systems. Instead, they rely on static rules built by experts of the railway infrastructure based on classical univariate statistic. The purpose of this paper is to build a data-driven Train Delay Prediction System (TDPS) for large-scale railway networks which exploits the most recent big data technologies, learning algorithms, and statistical tools. In particular, we propose a fast learning algorithm for Shallow and Deep Extreme Learning Machines that fully exploits the recent in-memory large-scale data processing technologies for predicting train delays. Proposal has been compared with the current state-of-the-art TDPSs. Results on real world data coming from the Italian railway network show that our proposal is able to improve over the current state-of-the-art TDPSs. © 2017 Elsevier Inc."
"10.1016/j.bdr.2017.05.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019635733&doi=10.1016%2fj.bdr.2017.05.001&partnerID=40&md5=c3ed6eea86c707969205610ffee436b4","Spark has been established as an attractive platform for big data analysis, since it manages to hide most of the complexities related to parallelism, fault tolerance and cluster setting from developers. However, this comes at the expense of having over 150 configurable parameters, the impact of which cannot be exhaustively examined due to the exponential amount of their combinations. The default values allow developers to quickly deploy their applications but leave the question as to whether performance can be improved open. In this work, we investigate the impact of the most important tunable Spark parameters with regards to shuffling, compression and serialization on the application performance through extensive experimentation using the Spark-enabled Marenostrum III (MN3) computing infrastructure of the Barcelona Supercomputing Center. The overarching aim is to guide developers on how to proceed to changes to the default values. We build upon our previous work, where we mapped our experience to a trial-and-error iterative improvement methodology for tuning parameters in arbitrary applications based on evidence from a very small number of experimental runs. The main contribution of this work is that we propose an alternative systematic methodology for parameter tuning, which can be easily applied onto any computing infrastructure and is shown to yield comparable if not better results than the initial one when applied to MN3; observed speedups in our validating test case studies start from 20%. In addition, the new methodology can rely on runs using samples instead of runs on the complete datasets, which render it significantly more practical. © 2017 Elsevier Inc."
"10.5334/dsj-2018-006","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050130834&doi=10.5334%2fdsj-2018-006&partnerID=40&md5=60b76bd797a4b9e7ebf372b527822871","Shapelet classification algorithms are an accurate classification method for time series data. Existing shapelet classifying processes are relatively inefficient and slow due to the large amount of necessary complex distance computations. This paper therefore introduces piecewise aggregate approximation(PAA) representation and an efficient subsequence matching algorithm for shapelet classification algorithms; the paper also proposes shapelet transformation classification algorithm based on efficient series matching. First, the proposed algorithm took the PAA representation for appropriate dimension reduction, and then used a subsequence matching algorithm to simplify the data classification process. The research experimented on 14 public time series datasets taken from UCI and UCR, used the original and new algorithm for classification, and compared the efficiency and accuracy of the two methods. Experimental results showed that the efficient subsequence matching algorithm could be combined with the shapelet classification algorithm; the new algorithm could ensure relatively high classification accuracy, effectively simplified the algorithm calculation process, and improved classification efficiency. © 2018, Ubiquity Press Ltd. All rights reserved."
"10.1007/s41019-018-0062-8","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062689057&doi=10.1007%2fs41019-018-0062-8&partnerID=40&md5=f125a888bff4436cf3012ad84ce3105a","With widely available large-scale network data, one hot topic is how to adopt traditional classification algorithms to predict the most probable labels of nodes in a partially labeled network. In this article, we propose a new algorithm called identifier-based relational neighbor classifier (IDRN) to solve the within-network multi-label classification problem. We use the node identifiers in the egocentric networks as features and propose a within-network classification model by incorporating community structure information to predict the most probable classes for unlabeled nodes. We demonstrate the effectiveness of our approach on several publicly available datasets. First, taking a semi-supervised approach, IDRN without any community prior is applied in community detection experiments, and it outperforms most existing unsupervised community detection algorithms. After that, in large-scale graph-based multi-label classification tasks, our approaches perform well in both fully labeled and partially labeled networks in most cases. To evaluate the scalability of our algorithm, we also show a scalability test to evaluate the running time of our algorithm in different networks. The experiment results show that our approach is quite efficient and suitable for large-scale real-world classification tasks. © 2018, The Author(s)."
"10.1007/s41019-017-0042-4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062114899&doi=10.1007%2fs41019-017-0042-4&partnerID=40&md5=d5141fbdd48045dd6f2498bf25cca554","Exascale computing promises quantities of data too large to efficiently store and transfer across networks in order to be able to analyze and visualize the results. We investigate compressed sensing (CS) as an in situ method to reduce the size of the data as it is being generated during a large-scale simulation. CS works by sampling the data on the computational cluster within an alternative function space such as wavelet bases and then reconstructing back to the original space on visualization platforms. While much work has gone into exploring CS on structured datasets, such as image data, we investigate its usefulness for point clouds such as unstructured mesh datasets often found in finite element simulations. We sample using a technique that exhibits low coherence with tree wavelets found to be suitable for point clouds. We reconstruct using the stagewise orthogonal matching pursuit algorithm that we improved to facilitate automated use in batch jobs. We analyze the achievable compression ratios and the quality and accuracy of reconstructed results at each compression ratio. In the considered case studies, we are able to achieve compression ratios up to two orders of magnitude with reasonable reconstruction accuracy and minimal visual deterioration in the data. Our results suggest that, compared to other compression techniques, CS is attractive in cases where the compression overhead has to be minimized and where the reconstruction cost is not a significant concern. © 2017, The Author(s)."
"10.1007/s41019-018-0060-x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060120346&doi=10.1007%2fs41019-018-0060-x&partnerID=40&md5=20254e962f5d60054f9e2d0daf2c6a7a","Window functions have been a part of the SQL standard since 2003 and have been studied extensively during the past decade. They are widely used in data analysis; almost all the current mainstream commercial databases support window functions. However, in recent years the size of datasets is growing steeply; the existing window function implementations are not efficient enough. Recently, some sampling-based algorithms (e.g., online aggregation) are proposed to deal with large and complex data in relational databases, which offer us a flexible trade-off between accuracy and efficiency. However, few sampling techniques has been considered for window functions in databases. In this paper, we extend our previous work (Song et al. in Asia-Pacific web and web-age information management joint conference on web and big data, Springer, pp 229–244, 2017) and proposed two new algorithms: range-based global sampling algorithm and row-labeled sampling algorithm. The proposed algorithms use global sampling rather than local sampling and are more efficient than other existing algorithms. And we find our proposed algorithms out performed the baseline method over the TPC-H benchmark dataset. © 2018, The Author(s)."
"10.1007/s41019-018-0061-9","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051539694&doi=10.1007%2fs41019-018-0061-9&partnerID=40&md5=134ec4ad381c9033ac9973a3b1368fd4","Increasing the on-time rate of bus service can prompt the people’s willingness to travel by bus, which is an effective measure to mitigate the city traffic congestion. Performing queries on the bus arrival can be used to identify and analyze various kinds of non-on-time events that happened during the bus journey, which is helpful for detecting the factors of delaying events, and providing decision support for optimizing the bus schedules. We propose a data management model, called Bus-OLAP, for querying bus journey data, considering the characteristics of bus running and the scenarios of non-on-time analysis. While fulfilling typical requirements of bus journey data queries, Bus-OLAP not only provides a flexible way to manage the data and to implement multiple granularity data query and update, but it also supports distributed queries and computation. The experiments on real-world bus journey data verify that Bus-OLAP is effective and efficient. © 2018, The Author(s)."
,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059194721&partnerID=40&md5=6b201b62073f221aad7862e4524342a7","This paper gives an overview of the use of survival analysis techniques for measuring and predicting customer life cycles. The key concepts of these techniques are presented, together with case studies using descriptive and predictive survival analyses. The main applications for customer management are identified. Issues are discussed, including which products and services are most suitable for survival analysis, the distinction between market-wide and brand churn, and the practical questions that commonly arise when using these methods. © Henry Stewart Publications 2054-7544 (2018)."
,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059177398&partnerID=40&md5=c0a2369b7846f7d7619dd5117e4b4420","Today’s marketing strategies must embrace deep data analysis to obtain actionable insights and make proper decisions. This paper explains how a conversion rate optimisation (CRO) data framework can have a huge impact on conversion rate. The case history shows how to exploit the drivers, avoid the barriers and implement the hooks that lead users to convert. Data outcomes are thus the result of analysing quantitative, qualitative and visual insights. At the core of CRO is a mix of different tools and techniques, adapted to the specific product/industry/timeframe. This paper provides readers with a guide to creating their own data-driven CRO framework and how to apply it to their projects. © Henry Stewart Publications 2054-7544 (2018)."
,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059172935&partnerID=40&md5=1befe97a815d5209164c2ed636e8b5a2","While brands, advertisements and social media feeds are important routes to attracting and converting customers, packaging can contribute to the overall customer experience. From making a lasting first impression to inspiring positive reviews based on the ‘unboxing experience’, great packaging has the ability to wow customers and set a company apart from its competition. This paper highlights the important role packaging plays in decisions where customers make proportion judgments using packaging containers as references. The authors first introduce a system of psychophysical models to describe how evaluability conditions influence judgments of proportion and pouring volume. The paper proceeds to examine these conditions using laboratory experiments with real adult customers. They show that the elongation of a packaging container causes customers to pour more to reach target proportions, an effect that is maximised when the proportion of content to container is 1/e (or approximately 36.79 per cent). The final section of the paper focuses on the managerial implications. The recommendations will help companies achieve a better understanding of customer visual processing biases and the strategic benefits that container packaging design can offer to a business. © Henry Stewart Publications 2054-7544 (2018)."
,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059171693&partnerID=40&md5=169d069592ce6b7c49c6bd32ce8c8b20","Due to the availability of massive amounts of text data, both from online (Twitter, Facebook, online forums, etc) and offline open-ended survey questions, text analytics is growing in marketing research and analytics. Most companies are now using open-ended survey questions to solicit customer opinions on any number of topics (eg ‘how can we improve our service?’). With large sample sizes, however, the task of collating this information manually is practically impossible. This paper describes an end-to-end process to extract insight from text survey data via topic modelling. A case study from a Fortune 500 firm is used to illustrate the process. © Henry Stewart Publications 2054-7544 (2018)."
,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059157999&partnerID=40&md5=35086910f0aed7cb083778bc8f1ad924","Because dashboards and associated metrics are difficult to set up and maintain for internal usage and external partners, business-to-business (B2B) companies often struggle to measure the success of global campaigns. Common analytics interfaces such as Google Analytics or Adobe Analytics are designed to support business-to-consumer business cases. Using the example of W.L. Gore, this paper discusses the implementation of dynamic dashboards to measure the progress and success of global campaigns for both associates and agencies. Examples are given to demonstrate the implementation of a technical infrastructure and the development cycle for designing and creating dashboards for a global B2B company. The paper specifies a robust multi-channel measurement framework that focuses on conversion and engagement. © Henry Stewart Publications 2054-7544 (2018)."
,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059144481&partnerID=40&md5=52b2a54a177e49da2a77d62bc5ebd64f","Conversational interfaces allow people to direct devices and programs through natural dialogue. Chatbots (which integrate with messaging apps like Facebook Messenger) and virtual assistants (including Apple’s Siri and Amazon’s Alexa) are examples of conversational interfaces that have emerged in recent years. As adoption of conversational interfaces increases, so will opportunities for marketers to utilise them to better meet consumer needs. This paper highlights the trends driving the rise of conversational interfaces, identifies the types of conversational ‘jobs to be done’ for customers in the context of the marketing funnel, and discusses the implications of conversational data for marketers. © Henry Stewart Publications 2054-7544 (2018)."
,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059128809&partnerID=40&md5=2eeb5e3799874cd6ba8641b5a4af0473","This paper provides effective solutions for removing silos between commercial teams in order to improve in-market performance. The proposed steps are: (1) Single-sourcing the reporting commercial structure into the marketing officer or commercial officer; this consolidates the commercial responsibilities within the organisation while establishing a common set of performance metrics across the different commercial teams. (2) Shifting focus from ‘path to purchase’ to ‘path to market’; this addresses the incomplete knowledge about the path the product takes from the vendor to the customer. (3) ‘Sense– Make Sense–Respond’; this step demonstrates the need for a common construct that allows the organisation to access information quickly, understand that information, and take actions to maximise its performance in the marketplace. (4) Disrupting inertia; this final step requires the change agent to be patient with company and/or individual willingness to disrupt the inertia inherent in most mature organisations. Having awareness about what it takes to remove silos yet choosing to do nothing will only cause a company to underperform in the marketplace. Ignorance is only bliss when it is truly a lack of knowledge — but this will never lead to improvements within the organisation. © Henry Stewart Publications 2054-7544 (2018)."
,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059128256&partnerID=40&md5=2fa30ac4dd4f7580363d7c5b0db101b4","Marketing analytics can move mountains when everyone in the organisation understands what the analytics mean, how to use them, and trusts in their veracity. The ideal end-state for organisations is for marketing analytics to form the basis of a language that enables people to communicate with one another to accomplish a common goal. To reach this level of organisational marketing analytics maturity requires a plan backed by a strategy. This paper describes the characteristics of a high-performing marketing analytics culture, with practical examples and experiential insights to help marketers establish a common language for marketing analytics in their organisation from top to bottom. © Henry Stewart Publications 2054-7544 (2018)."
"10.1057/s41270-018-0028-8","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042180365&doi=10.1057%2fs41270-018-0028-8&partnerID=40&md5=d678191a4bc4dab780fdb8bebf38af28","Can a local celebrity endorser event influence the endorsed global brand's stock value? Employing cultural perspective, signal theory and attribution theory, we hypothesized that, in an emerging market such as China, the endorsed global brands could gain and brands endorsed by a low-blameworthy celebrity could gain more from a local celebrity endorser scandal event. Using event study method, our results suggest that the entire endorsed brand portfolio exhibits statistically significant gains on the global stock market shortly after the focal scandal event, and brand portfolio endorsed by low-blameworthiness celebrity accrued more gains than brand portfolio endorsed by high-blameworthiness celebrity. The discussion of managerial and policy implications is presented in the concluding section. © 2018 Macmillan Publishers Ltd., part of Springer Nature."
"10.1057/s41270-017-0027-1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040696480&doi=10.1057%2fs41270-017-0027-1&partnerID=40&md5=fdc1321c87ee079c5be916579df1ac11","Predictive algorithms are increasingly used to support decision making. Understanding the costs and benefits of a predictive model is an important aspect for businesses. However, algorithms are abstract, and their impact oftentimes remains vague. We present a case study, where a machine-learning algorithm is used for bid qualification. We show how to apply classification matrices for business value assessment and propose guidelines and metrics for interpreting the impact in practical solutions. © 2018 Macmillan Publishers Ltd., part of Springer Nature."
"10.1057/s41270-017-0026-2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040331317&doi=10.1057%2fs41270-017-0026-2&partnerID=40&md5=78f97988f5359b26a281896d4076fc06","Customer satisfaction level is one of key performance indicators in the service industry. The various factors affecting this are studied to maintain an excellent relationship with customers. Self-service technology (SST) is widely implemented by companies in service sector. This paper proposes to apply the customer satisfaction survey to investigate factors influencing the customer satisfaction. The relationships among the factors are discovered using PC-algorithm. The critical factors are identified to be inputs in regression tree and ANN to estimate the customer satisfaction level. By means of comparison of models, importance of selected inputs is quantified and discussed. The results show that customer satisfaction has strong connectivity relationship with personal service attributes as well as affective and temporal commitment by running PC-algorithm. ANN validated by 10-fold cross validation is the best among the models. The most important factor influencing the satisfaction level to the companies is the customer's desire of continuing a relationship. The key benefit of the proposed approach is to avoid making subjective decisions, for instance, building a plausible initial path models in the analysis. The analytical results facilitate the decision-making process and better resource allocation in the airline and state its future development of self-service technology. © 2018 Macmillan Publishers Ltd., part of Springer Nature."
"10.1177/0265813516676486","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044163096&doi=10.1177%2f0265813516676486&partnerID=40&md5=d5324c0bf5806af9de29dfe94e8d22d8","The ability to predict the human perception of space in dense urban environments would have a vast impact on planning and design processes. Many analytical models, methods and tools have been developed to describe and predict human perception and behaviour in the urban environment, and academic papers have addressed the issue of the view in urban environments as a significant variant influencing perception and quality of life. In the present paper, we introduce the integration of weighted views (the relative impact of a view on a viewer) in a 3D Line of Sight visibility analysis as a predictive tool for perceptions of space focusing on ‘perceived density’. The integration of subjective qualitative information with objective measurements of the volume of visible space may bring evaluations closer to human perceptions of space. The research background consists of state-of-the-art visibility analysis and research concerned with the impact of the view on perceptions of space. An experiment in a visualization laboratory explored the relative impact of various views on the ‘perceived density’ and ‘visual privacy’ of 100 participants. The relative weight of each view presented to the subjects was based on the statistical results of the experiment. The weights were integrated into the 3D Line of Sight visibility analysis and the model is applied in a case study. The integrated 3D Line of Sight visibility analysis currently runs with off-the-shelf software available to all practicing architects and planners. It is expected that in the near future, the tool developed in this work will become an important aid to all practitioners, the method becoming a valuable evaluation tool in planning and design processes. Considering design alternatives, it may become a stepping stone for design principles and regulations. © 2016, © The Author(s) 2016."
"10.1177/0265813516673060","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044144392&doi=10.1177%2f0265813516673060&partnerID=40&md5=9dc989929f008bfac81f5e69fbd675d8","Children with consistent exposure to air pollution have increased asthma, chronic respiratory problems, and neurobehavioral dysfunction. However, many schools are located in close proximity to highways and industrial facilities which are key sources of air pollution to children. The goal of this study is to explore the association between the proximity from schools to highways and industrial facilities, and children’s school performance and health hazards. We measured the distances from 3,660 Michigan public schools to highways and industrial facilities, and linked these to the Michigan Educational Assessment Program test performance rate and the National Air Toxics Assessment’s respiratory and neurological hazards. We found that schools located closer to highways and industrial facilities had higher risks of respiratory and neurological diseases than those located farther away. We also found that schools located closer to major highways had a higher percentage of students failing to meet the state standards than the latter after controlling for the location of schools, student expenditure, school size, student–teacher ratio, and free lunch enrollment. In addition, a larger percentage of black, Hispanic, or economically disadvantaged children attended schools nearest to pollution emissions than white students. © 2016, © The Author(s) 2016."
"10.1177/0265813516671311","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044143455&doi=10.1177%2f0265813516671311&partnerID=40&md5=11de8f19bcf8ead0bfa1c372e9dbc76c","Urban models serve as laboratories, providing researchers with the opportunity to assess the impact of a wide range of social and economic processes on the development of a built environment. Novelty and unexpected changes play an essential role in this development, but these are difficult to formalize and imitate. Typically, urban models simulate innovation by introducing stochastic fluctuations of the pre-established development rules. This research offers a methodology for assessing innovation in a developing city and examining its impact on urban development. The methodology is implemented in the Israeli city of Netanya, where urban development is analyzed at a resolution of single buildings over a period of three decades. We recognize two types of innovation: spatial innovation, manifested by leapfrogging residential clusters that establish new development areas; and contextual innovation, manifested by residential clusters that include buildings that are substantially higher than their surroundings. We demonstrate the impact of few innovative residential clusters on urban development in the following decades and highlight the diffusion of innovation in the city. © 2016, © The Author(s) 2016."
"10.1177/0265813516672454","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044130473&doi=10.1177%2f0265813516672454&partnerID=40&md5=e2d0f63891456fb7b413a498e7e15828","The purpose of this research is to examine relationships between occurrences of snatch-and-run offences and hourly population estimated from mobile phone users’ locations, with particular focus on differences between daytime and nighttime. Using an hourly population dataset allows us to count the so-called ‘ambient population’ by hour of day to accurately quantify the influence of such population as capable guardians and suitable targets in a framework of routine activity theory. Our major findings based on logistic regression models are that (1) the effects of ambient population and (2) its temporal change are large, and the effects differ between daytime and nighttime. During the daytime, snatch-and-run offences are less likely to occur in areas where hourly population density is expect to increase, possibly because offenders are highly sensitive to the risk of being detected by other people. On the other hand, offences at night occur even in relatively crowded areas, and they are only weakly related to population change. In addition, our study found that (3) snatch-and-run offences are more likely to occur in or near local town centres and (4) socially vulnerable neighbourhoods are only targeted at night. We attempted to explain this in terms of offenders’ characteristics and motivations depending on time of day. © 2016, © The Author(s) 2016."
"10.1177/0265813516669139","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044119071&doi=10.1177%2f0265813516669139&partnerID=40&md5=3f9cdfbff87cf7ddcaaa4ba6421770b3","This paper presents the early results of a study aimed at experimenting an innovative approach to the design and the evaluation of complex urban transformation processes, based on the combined use of different design strategies and tools. The purpose of the paper is to illustrate, by means of a case study, a multi-level decision aiding process, able to support strategic urban design, with specific reference to regeneration processes for abandoned industrial sites in urban areas. The case study presented in the paper concerns different alternative proposals for the requalification of the former Shougang/Er-Tong mechanical factory in Beijing, China. The choice of a Chinese case study as a field test for an experimentation about mixed methods research approaches in the domain of urban transformation is related to the peculiar emerging conditions of that context, in which huge economic potentials are speeding up the transformation but a substantial lack of cultural and methodological instruments to manage a so fast modification exists. During the design process, three methods in particular have been combined according to a multi-phase design: (i) Stakeholders Analysis, (ii) Multicriteria Analysis, and (iii) Discounted Cash Flow Analysis. Each one of them has been applied in parallel to the evolution of the different design scenarios. The results of the performed study show that mixed methods approaches are a promising line of research in the field of environmental evaluation and urban design. Insights and guidelines for the replication of the proposed methodological approach in other territorial contexts are also proposed. © 2016, © The Author(s) 2016."
"10.1177/0265813516670901","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044105796&doi=10.1177%2f0265813516670901&partnerID=40&md5=a33e0be4f1ce5624f9092981190c4b74","Landmarks are ideal wayfinding tools to guide a person from A to B as they allow fast reasoning and efficient communication. However, very few path-finding algorithms start from the availability of landmarks to generate a path. In this paper, which focuses on indoor wayfinding, a landmark-based path-finding algorithm is presented in which the endpoint partition is proposed as spatial model of the environment. In this model, the indoor environment is divided into convex sub-shapes, called e-spaces, that are stable with respect to the visual information provided by a person’s surroundings (e.g. walls, landmarks). The algorithm itself implements a breadth-first search on a graph in which mutually visible e-spaces suited for wayfinding are connected. The results of a case study, in which the calculated paths were compared with their corresponding shortest paths, show that the proposed algorithm is a valuable alternative for Dijkstra’s shortest path algorithm. It is able to calculate a path with a minimal amount of actions that are linked to landmarks, while the path length increase is comparable to the increase observed when applying other path algorithms that adhere to natural wayfinding behaviour. However, the practicability of the proposed algorithm is highly dependent on the availability of landmarks and on the spatial configuration of the building. © 2016, © The Author(s) 2016."
"10.1177/0265813516672212","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85033464601&doi=10.1177%2f0265813516672212&partnerID=40&md5=5de5b77b1b24363aa88f6844f6d13502","Visiting urban parks regularly can provide significant physical and mental health benefits for children and teenagers, but these benefits are tempered by park quality, amenities, maintenance, and safety. Therefore, planning and public health scholars have developed instruments to measure park quality, but most of these tools require costly and time-consuming field surveys and only a handful focus specifically on youth. We rectify these issues by developing the QUality INdex of Parks for Youth (QUINPY) based on a robust literature review of studies on young people’s park visitation habits and an extensive validation process by academic and professional experts. Importantly, the QUINPY relies on publicly available geospatial data to measure park quality. We then successfully pilot test the QUINPY in Denver and New York City. We believe that park agencies, planning consultants, researchers, and nonprofits aiming to assess park quality will find this tool useful. The QUINPY is particularly promising given the increasing amount of publicly available geospatial data and other recent advancements in geospatial science. © 2016, © The Author(s) 2016."
"10.1177/0265813516675871","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021420146&doi=10.1177%2f0265813516675871&partnerID=40&md5=eb815b90da6dc4770aea9320e33f1355","This paper considers the experience of the implementation of the Directive 2007/2/EC of the European Parliament and of the Council of 14 March 2007 establishing an Infrastructure for Spatial Information in the European Community (INSPIRE) as a case study of qualitative monitoring in building information infrastructures. It considers the nature of information infrastructures and possible approaches to qualitative monitoring in situations of this kind and describes the outcomes of two rounds of qualitative country reports prepared by the European Union national Member States in 2010 and 2013. The findings of the analysis highlight the great diversity of approaches developed by the participating countries and the complexity of the tasks involved as well as pointing to a number of areas of potential research on the implementation of information infrastructures. © 2016, © The Author(s) 2016."
"10.1177/0265813516676487","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013683323&doi=10.1177%2f0265813516676487&partnerID=40&md5=563ed80cc29ae52a3d0d8715545df51c","In this article, we introduce the method of urban association rules and its uses for extracting frequently appearing combinations of stores that are visited together to characterize shoppers’ behaviors. The Apriori algorithm is used to extract the association rules (i.e. if -> result) from customer transaction datasets in a market-basket analysis. An application to our large-scale and anonymized bank card transaction dataset enables us to output linked trips for shopping all over the city: the method enables us to predict the other shops most likely to be visited by a customer given a particular shop that was already visited as an input. In addition, our methodology can consider all transaction activities conducted by customers for a whole city. This approach enables us to uncover not only simple linked trips such as transition movements between stores but also the edge weight for each linked trip in the specific district. Thus, the proposed methodology can complement conventional research methods. Enhancing understanding of people’s shopping behaviors could be useful for city authorities and urban practitioners for effective urban management. The results also help individual retailers to rearrange their services by accommodating the needs of their customers’ habits to enhance their shopping experience. © 2016, © The Author(s) 2016."
"10.5334/dsj-2018-004","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042390379&doi=10.5334%2fdsj-2018-004&partnerID=40&md5=152b4e8733bae29daeb5746154865cd2","Paper is dedicated to the new approach to distributed industrial systems (IS) sustainability/ vulnerability assessment. This approach is based on the unitary multiset grammars (UMG) as a flexible and convenient tool designed specially for large systems analysis and optimization. UMG description of IS technological base as well as multiset representation of order completed by the IS, its resource base and impact on the IS are presented. Criterion for recognition of IS sustainability to the impact is formulated. UMG extension for natural disasters impacts (NDI) representation is introduced, and criterion for recognition of IS sustainability to the NDI is also presented. The solution of the reverse problem, concerning part of the order, which may be completed by the affected IS, is described. Implementation issues are considered. © 2018 The Author(s)."
"10.5334/dsj-2018-002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041066520&doi=10.5334%2fdsj-2018-002&partnerID=40&md5=3c284cc44e6b0ece419e4b7c5c6a4d95","With the volume of Earth observation data expanding rapidly, cloud computing is quickly changing the way these data are processed, analyzed, and visualized. Collocating freely available Earth observation data on a cloud computing infrastructure may create opportunities unforeseen by the original data provider for innovation and value-added data re-use, but existing systems at data centers are not designed for supporting requests for large data transfers. A lack of common methodology necessitates that each data center handle such requests from different cloud vendors differently. Guidelines are needed to support enabling all cloud vendors to utilize a common methodology for bulk-downloading data from data centers, thus preventing the providers from building custom capabilities to meet the needs of individual vendors. This paper presents recommendations distilled from use cases provided by three cloud vendors (Amazon, Google, and Microsoft) and are based on the vendors’ interactions with data systems at different Federal agencies and organizations. These specific recommendations range from obvious steps for improving data usability (such as ensuring the use of standard data formats and commonly supported projections) to non-obvious undertakings important for enabling bulk data downloads at scale. These recommendations can be used to evaluate and improve existing data systems for high-volume data transfers, and their adoption can lead to cloud vendors utilizing a common methodology. © 2018 The Author(s)."
"10.5334/dsj-2018-001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041039196&doi=10.5334%2fdsj-2018-001&partnerID=40&md5=a7fa2bf438f66af585e26b521662b1ec","The Polar Data Centre (PDC) of the National Institute of Polar Research (NIPR) has a responsibility to manage polar science data as part of the National Antarctic Data Centre and the Science Committee on Antarctic Research. During the International Polar Year (IPY 2007–2008), a remarkable number of data/metadata involving multi-disciplinary science activities were compiled. Although the long-term stewardship of the accumulation of metadata falls to the data center of NIPR, the work has been in collaboration with the Global Change Master Directory, the Polar Information Commons, the World Data System and other data science bodies/communities under the International Council for Science. In addition, links with other data centers, such as the Data Integration and Analysis System Program of the Global Earth Observation System of Systems and the Polar Data Catalogue of Canada were initiated in 2014 using the Open Archives Initiative Protocol for Metadata Harvesting. The metadata compiled by the PDC were recently modified using an automatic attributing system and DataCite through the Japan Link Center. © 2018 The Author(s)."
"10.1080/23270012.2018.1424572","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057060820&doi=10.1080%2f23270012.2018.1424572&partnerID=40&md5=80bface5d737441fd7fbf8c9d8de326b","The importance of knowledge as a strategic asset for organizations has been recognized by both researchers and practitioners. To gain a competitive advantage, firms are required to effectively manage their knowledge resources. The most central activity in managing knowledge is to ensure its transfer within and between organizations. Knowledge transfer (KT) has thus been recognized as a key component of the knowledge management processes. The purpose of this research is to provide a holistic view of the KT barriers and enablers within an organization, from a multilevel and process-based perspectives. We first review the extant literature to identify the key enablers and barriers to KT. Second, we develop a multilevel conceptualization of enablers and barriers that can influence KT at different levels–individual, team/exchange and organization. The proposed model improves current understanding of KT by offering a holistic and integrated view of enablers and barriers. © 2018, © 2018 Antai College of Economics and Management, Shanghai Jiao Tong University."
"10.1080/23270012.2017.1373262","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057055850&doi=10.1080%2f23270012.2017.1373262&partnerID=40&md5=3c33d4a4cc0ff803ffb406f31db86c9d","Support Vector Regression (SVR) has already been proved to be one of the most referred and used machine learning technique in various fields. In this study, we have addressed a predictive-cum-prescriptive analysis for finalizing fund allocations by the Government at center to the schemes under Central Plan and to the schemes under States and Union Territories Plan, with a goal to maximize Gross Value Added (GVA) at factor cost. Here, we have proposed a hybrid machine learning model comprising of OFS (Orthogonal Forward Selection), TLBO (Teaching Learning Based Optimization) and SVR for the prediction of GVA at factor cost. In this model, referred as OFS–TLBO–SVR hybrid model, SVR is at the core of prediction mechanism, OFS is for identifying the relevant features, and TLBO is to support in optimizing the free parameters of SVR and again TLBO is used for optimizing the governable attributes of data. © 2017, © 2017 Antai College of Economics and Management, Shanghai Jiao Tong University."
"10.1080/23270012.2017.1410862","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056992494&doi=10.1080%2f23270012.2017.1410862&partnerID=40&md5=68056111437ea12fc6dcae813b575bc3","To get a clear sense of the “appropriate” individual payout, based on the literature reviews and the World Health Organization (WHO) reports, the study analyzes the data from two aspects: 191 WHO member states and 76 superior states (the health performance is better than that of China) by using the Panel data model, which was further validated by intention questionnaire. The results show that Chinese-appropriate individual payout intervals are respectively around 30% from the WHO angle and the superior state angle in 2020, which have been well proved by the survey of intention. For the realization of the feasible structure of total expenditure on health, we need not only the efforts from government, society and individuals but also the cooperation among hospitals, drug supply security system, health care systems and other health systems. © 2017, © 2017 Antai College of Economics and Management, Shanghai Jiao Tong University."
"10.5334/dsj-2018-031","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060891826&doi=10.5334%2fdsj-2018-031&partnerID=40&md5=e013c872ddf65c80aad71d4ba33e05ea","In several domains, privacy presents a significant obstacle to scientific and analytic research, and limits the economic, social, health and scholastic benefits that could be derived from such research. These concerns stem from the need for privacy about personally identifiable information (PII), commercial intellectual property, and other types of information. For example, businesses, researchers, and policymakers may benefit by analyzing aggregate information about markets, but individual companies may not be willing to reveal information about risks, strategies, and weaknesses that could be exploited by competitors. Extracting valuable utility from the new “big data” economy demands new privacy technologies to overcome barriers that impede sensitive data from being aggregated and analyzed. Secure multiparty computation (MPC) is a collection of cryptographic technologies that can be used to effectively cope with some of these obstacles, and provide a new means of allowing researchers to coordinate and analyze sensitive data collections, obviating the need for data-owners to share the underlying data sets with other researchers or with each other. This paper outlines the findings that were made during interdisciplinary workshops that examined potential applications of MPC to data in the social and health sciences. The primary goals of this work are to describe the computational needs of these disciplines and to develop a specific roadmap for selecting efficient algorithms and protocols that can be used as a starting point for interdisciplinary projects between cryptographers and data scientists. © 2018 The Author(s)."
"10.5334/dsj-2018-030","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060888521&doi=10.5334%2fdsj-2018-030&partnerID=40&md5=a71633dba9c614a914c0d45607c694a3","For the purpose of detecting communities in social networks, a triad percolation method is ­proposed, which first locates all close-triads and open-triads from a social network, then a ­specified close-triad or open-triad is selected as the seed to expand by utilizing the triad ­percolation method, such that a community is found when this expanding process meet a ­particular threshold. This approach can efficiently detect communities not only from a densely social network, but also from the sparsely one. Experimental results performing on real-world social benchmark networks and artificially simulated networks give a satisfactory ­correspondence. © 2018 The Author(s)."
"10.5334/dsj-2018-029","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060858241&doi=10.5334%2fdsj-2018-029&partnerID=40&md5=9e88ebe4ee235283b97b617916658976","The development of data and model integration platforms has furthered scientific inquiry and helped to solve pressing social and environmental problems. While several e-infrastructure platforms have been developed, the concept of data and model integration remains obscure, and these platforms have produced few firm results. This article investigates data and model integration on the Data Integration and Analysis System (DIAS) platform, using three case projects from water-related fields. We provide concrete examples of data and model integration by analyzing the data transfer and analysis process, and demonstrate what platform functions are needed to promote the advantages of data and model integration. In addition, we introduce the Digital Object Identifier (DOI), a valuable tool for promoting data and model integration and open science. Our investigation reveals that DIAS advances data and model integration in five main ways: it is a “sophisticated and robust integration platform”; has “rich APIs, including a metadata management system, for high-quality data archive and utilization”; functions as a “core hydrological model”; and promotes a “collaborative R&D community” and “open science and data repositories”. This article will appeal especially to researchers interested in new methods of analysis, and information technology experts responsible for developing e-infrastructure systems to support environmental and scientific research. © 2018 The Author(s)."
"10.5334/dsj-2018-032","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060720710&doi=10.5334%2fdsj-2018-032&partnerID=40&md5=d3d461d7232c06a1d1273c3c60822417","With the continuous development of Internet technology, the era of big data has come. The traditional library resource sharing which still at the mode of information island is unable to maximize the use of resources. In order to solve this problem, this study designed a library information resources co-construction and sharing system based on Squid reverse proxy technology. After the system was completed, the login function, user opening function and resource sharing function were tested. Functions of the system were basically normal, but some small details affected users’ experience. Therefore, it is necessary to optimize the system code before putting it into use. Moreover in the aspect of resource sharing, the system realized the login of the same user information into different library databases, and users can search same resources in different libraries. © 2018 The Author(s)."
"10.1186/s40537-018-0136-5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051103365&doi=10.1186%2fs40537-018-0136-5&partnerID=40&md5=2d4964826af0efb5c7c44c0648113487","In this paper, we present a statistical model performed on the basis of a patient dataset. This model predicts efficiently the brain disease risk. Multiple regression was used to build the statistical model. The least squares estimation problem usually used to estimate the parameters of regression model is solved via parallelized algebraic Adjoint method. As the parallelized algebraic Adjoint method is not the only Mapreduce-based method used to solve the least square problem, experimentations were carried out to classify the Adjoint method amongst the other methods. The calculated job completion time shows the competitive trait of the Mapreduce-based Adjoint method. © The Author(s) 2018."
"10.4018/IJBIR.2018010103","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060738113&doi=10.4018%2fIJBIR.2018010103&partnerID=40&md5=746686d1cc35e7972a8b77d0849dbf8f","This article reviews methods to the development of a decision support system (DSS) solution for small business owners/managers. The main objective of designing the DSS artefact is to support the strategic decision-making for achieving competitive advantages in the business-to-consumer (B2C) e-commerce environment. Many researchers in the DSS domain utilised various methods to design of information systems (IS) artefact, mostly intended for large businesses. Researchers have paid much attention to the business environment as a knowledge source for DSS design and development for the small business strategic decision support needs. User-centred design (UCD) principles were adopted for DSS development. Prior a novel DSS development, multiple case studies were carried out for understanding the user needs and system requirements. Also, knowledge was sourced from the external business environment via the analysis of small business website features against their overseas competitors. The findings suggested developing a DSS solution for small business needs. © 2018, IGI Global."
"10.4018/IJBIR.2018010102","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060726093&doi=10.4018%2fIJBIR.2018010102&partnerID=40&md5=6d15a696243b1150ce31fcc5be3ef60e","This article describes how in today's hyper-competitive environment, business leaders around the world are using analytics technologies to create business values, and to gain a better understanding of their customer's needs and wants. However, traditional business analytics are undergoing major changes. The Internet revolution, cloud computing, and the evolution to self-service analytics have all contributed to the changing dimensions of business intelligence. To compete effectively in a digitally driven world, business leaders must understand and address the critical shifts taking place in the field of analytics, and how these shifts impact their overall strategy. The key objective of this article is to propose a conceptual model for successful implementation of Embedded Analytics in organizations. This article also covers some of the potential benefits of analytics, explores the changing dimensions of analytics, and provides a guide to some of the opportunities that are available for using embedded analytics in business. Furthermore, this study reviews key attributes of a successful modern analytics platform and illustrates how to overcome some of the key challenges of incorporating embedded analytics into an analytic strategy in business. Finally, this article highlights successful implementation of analytics solutions in manufacturing and service industry. © 2018, IGI Global."
"10.4018/IJBIR.2018010101","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060718392&doi=10.4018%2fIJBIR.2018010101&partnerID=40&md5=c74fa9a8cab70f6147b755decc1fba51","Cost allocations for business intelligence (BI) costs create cost awareness, enhance cost transparency, and support the management of BI systems. Although BI cost allocation is highly relevant in practice, the field is widely uncharted in current scholarly research. In this article, the state of the art in scientific literature is analyzed. The review is comprised of three iterations. It shows that certain general approaches for information systems cost allocation are suitable candidates if being combined and tailored to BI systems. Based on synthesis, an agenda is derived for future research into cost allocation for BI systems. © 2018, IGI Global."
"10.4018/IJBIR.2018010104","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060716774&doi=10.4018%2fIJBIR.2018010104&partnerID=40&md5=467305df5ca6bbb121be320c42135876","Business intelligence system (BIS) reflect on the foundation of completive advantage approaches, this article enhances previous articles of BIS implementation by analyzing a 7 McKinsey model. Using a Delphi survey method with sixty of managers and experts in the BIS field, the results from twenty pharmaceutical manufacturing industries deploying business intelligence applications are presented. The author surveyed a qualitative methodology to hypothesize a proposed model. The findings demonstrated that 80% of participants believe that both the hard and soft 7 dimensions of a McKinsey model-strategy, style, structure, staff, systems, skills, and shared values-are antecedents for successful BIS implementation. Precisely, shared value dimensions is the highest value affected by BIS implementation and skills dimension is the lowest value. © 2018, IGI Global."
"10.4018/IJBIR.2018010105","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060698611&doi=10.4018%2fIJBIR.2018010105&partnerID=40&md5=a1f4daf4a819fdbdd37c11eae1da4183","In recent years, Operational Business Intelligence has emerged as an important trend in the Business Intelligence (BI) market. Majority of BI application architectures are bespoke in nature which have several architectural limitations like tightly coupled, static, historic, subjective, no performance measurement of business processes, limited user access, limited analytical processing, querying and reporting features. In this article, a generic functional architecture for Operational BI systems based on software architecture principles is presented. All functional modules of the system are derived from the key features of the system and by using top down approach of software design principles. The similar functional modules are grouped into sub-systems and a set of these sub-systems constitutes overall functional architecture. The proposed architecture overcomes the limitations of traditional BI architectures. © 2018, IGI Global."
"10.1504/IJBIDM.2018.094980","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054198195&doi=10.1504%2fIJBIDM.2018.094980&partnerID=40&md5=9983e792692998fd133ac51e2cd89b61","Tourism demand is the total number of persons who travel, or wish to travel, to use tourists' facilities and services at places away from their places of work or residence. Analysis of tourism demand helps companies understand tourists' needs and improves their marketing strategies. Current research for predicting tourism demand is targeted at foreign countries, and the little research targeted at predicting tourism demand in Egypt is based on macro forecasting and not on understanding the collective behaviour of tourists. In this paper, we devise different granularities from tourist data that we collect and use these different granularities to provide different levels of demand prediction. We develop a hybrid prediction framework to analyse tourists' behaviour and infer behaviour rules. These rules will act as recommendations that help to understand tourists' behaviour and their needs, and define future policies regarding tourism in Egypt. Copyright © 2018 Inderscience Enterprises Ltd."
"10.1504/IJBIDM.2018.094986","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054179898&doi=10.1504%2fIJBIDM.2018.094986&partnerID=40&md5=3f9ccc7b4bcfb9cb6c477e8cbdeacd0a","Support operations enhance machine conditions; additionally involve potential creation time, conceivably postponing the client orders. The target of this paper is to decide execution parameters in every work stations with foresee the cost, dependability and accessibility of the business. This estimate examination considers two sorts of various methodologies, for example, FLP ideal neural system model. At first utilising FLP to foresee the exhibitions parameters and expanding the exactness of examination by means of ANN with motivated enhancement procedure to upgrade the weights in structure. All the ideal results exhibit the way that the accomplished mistake values between the yield of the trial values and the anticipated qualities are firmly equivalent to zero in the planned system. From the outcomes the proposed KHO-based ideal neural system demonstrates the exactness is 98.23% it is contrasted with the Pareto improvement model. Copyright © 2018 Inderscience Enterprises Ltd."
"10.1504/IJBIDM.2018.094979","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054179093&doi=10.1504%2fIJBIDM.2018.094979&partnerID=40&md5=98d367cd2a01e7e8d96f5670fada1cac","The capacity to build up steady and extensive trench structures by means of headed for great degree thin fibres would have wide innovative ramifications. Here, we report a procedure to plan and make sandwich organised polyamide-6/polyacrylonitrile/polyamide-6 (PA-6/PAN/PA-6) composite membrane is considered. This is sensible for powerful air filtration via consecutive electro spinning by coordinating the elements of parts to foresee the distinctive mechanical properties with help of optimal weight of ANN structure. Distinctive inspired optimisation strategies are used to touch base at the optimal weight of the ANN procedure. All the ideal results exhibit the way that the accomplished error values between the yield of the exploratory qualities and the anticipated qualities are firmly equivalent to zero in the outlined network. In addition, the most intense filtration accuracy and lower pressure drops furthermore the result demonstrates the base error of 96.72% dictated by the ANN. This is accomplished by the artificial fish swam optimisation (AFSO) strategies. Copyright © 2018 Inderscience Enterprises Ltd."
"10.1504/IJBIDM.2018.094981","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054169946&doi=10.1504%2fIJBIDM.2018.094981&partnerID=40&md5=5045ff171b267c600b3d03281b4c901c","The purpose of this research is to analyse the performance of a real estate valuation model based on the multi objective decision making methods. The optimal price function is achieved with the goal programming model. The price function which is described as the sum of the individual objectives (criteria), and the goals are the prices of comparable properties. The model integrates with the inductive and deductive approach overcomes many of the assumptions of the best known statistical approaches. The evaluation of the proposed model is performed by comparing the results obtained by the application, to the same case study, of a multiple linear regression model and a nonlinear regression method based on penalised spline smoothing model. The comparison shows, first of all, the best interpretation capabilities of the proposed model. Copyright © 2018 Inderscience Enterprises Ltd."
"10.1504/IJBIDM.2018.094983","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054156265&doi=10.1504%2fIJBIDM.2018.094983&partnerID=40&md5=945086444da14ec5efb8d8d76a257b03","Privacy preserving data mining is a fast growing new era of research due to recent advancements in information, data mining, communications and security technologies. Government agencies and many other non-governmental organisations often need to publish sensitive data that contain information about individuals. The important problem is publishing data about individuals without revealing sensitive information about them. A breach in the security of a sensitive data may expose the private information of an individual, or the interception of a private communication may compromise the security of a sensitive data. The objective of the research is to publish data without revealing the sensitive information of individuals, at the same time the miner need to discover non-sensitive knowledge. To achieve the above objective, haphazard anonymisation, enhanced haphazard anonymisation and personalised anonymisation are proposed for privacy and utility preservation. The performances are evaluated based on vulnerability to attacks, efficiency and data utility. Copyright © 2018 Inderscience Enterprises Ltd."
"10.1504/IJBIDM.2017.10004682","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85038447444&doi=10.1504%2fIJBIDM.2017.10004682&partnerID=40&md5=ea0ea2ccf2f55925a3d7624b557f4814","Designing the high-quality software is a difficult one due to the high complexity and fault prone class. To reduce the complexity and predict the fault-prone class in the object orient software design proposed a new empirical approach. This proposed approach concentrates more on to increase the software quality in the object oriented programming structures. This technique will collect the dataset and metric values from CK-based metrics. And then complexity will be calculated based on the weighted approach. The fault prediction will be done, based on the low usage of the dataset and high complexity dataset. This helps to increase the software quality. In simulation section, the proposed approach has performed and analysed the parameters such as accuracy, fairness, recall, prediction rate and efficiency. The experimental results have shown that the proposed approach increases the prediction rate, accuracy and efficiency. Copyright © 2018 Inderscience Enterprises Ltd."
"10.1504/IJBIDM.2017.10004072","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85038445206&doi=10.1504%2fIJBIDM.2017.10004072&partnerID=40&md5=4cf4708dd26b3cb1077093bde2ad4a1a","Presently, sight and sound information becomes available quickly because of the propelled interactive media catching gadgets such as computerised video recorder, portable camera, etc. As ordinary inquiry by-content recovery does not fulfil clients’ needs in finding the desired recordings viably, content-based video recovery stands out as the most sensible answer to enhance the recovery quality. Moreover, video recovery utilising inquiry by picture is not effective in partnership with the recordings to the client’s advantage. In this manuscript, we intend a creative strategy to accomplish the high calibre substance-based video recovery by finding the transient examples. On premise of the found lobbyist designs, a productive indexing procedure and a powerful grouping coordinating strategy are incorporated to diminish the calculation cost and to raise the recovery precision. Trial result uncover that our methodology is extremely encouraging in upgrading content-based video recovery with regard to proficiency and adequacy in NPTEL. Copyright © 2018 Inderscience Enterprises Ltd."
"10.1504/IJBIDM.2017.10003632","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85038440623&doi=10.1504%2fIJBIDM.2017.10003632&partnerID=40&md5=375d502f9a057aeb40c3398dea8b16fc","In this paper, modified non-dominated sorting genetic algorithm-II (MNSGA-II)-based optimal damping control of unified power flow controller (UPFC) has been designed to enhance the damping of low frequency oscillations in power systems. The robust damping of UPFC controller design is formulated as a multi-objective optimisation problem, thereby minimising the integral squared error (ISE) of speed deviation and input control signal (u) under a wide range of operating conditions. The effectiveness of the proposed controller is confirmed through nonlinear time domain simulation and Eigen value analysis. The results are compared with NSGA-II and conventional method. Simulation result reveals that the obtained Pareto-front using MNSGA-II-based UPFC controllers are better and uniformly distributed due to the controlled elitism and dynamic crowding distance concepts. The proposed modulation index of shunt inverter (mE)-based damping controller is superior to the other damping controllers under different loading conditions and improves the stability of system. Copyright © 2018 Inderscience Enterprises Ltd."
"10.1504/IJBIDM.2016.10002431","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85038439416&doi=10.1504%2fIJBIDM.2016.10002431&partnerID=40&md5=3bb3ccab09d94a1ea2277139210ac705","Browsing information through web has become part and parcel of life. There is hardly any user who does not browse through web. Generally, any user when in search for content only looks at the top ten pages that get displayed in the web search. Therefore, it has been proposed that the information such as the link that is created between both visited and unvisited web pages along with the path that is chosen in the search query needs a novel technique to give the best performance. The operation feature matrix (OFM) is used as one of the novel functionalities that have been used for extracting the data from the web. The automatically identified user profile is the graph-based that is called as the modified page outlook (MPO) graph was proposed that involves a link between the visited and the unvisited web pages. Copyright © 2018 Inderscience Enterprises Ltd."
"10.1504/IJBIDM.2017.10004685","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85038437119&doi=10.1504%2fIJBIDM.2017.10004685&partnerID=40&md5=309e6f1ca8abf9fd9416ffeeb882757a","The aim of this paper is to create an efficient hardware architecture for the multi-alphabet arithmetic coding (MA-AC) in semicustom and full custom application specific integrated circuit (ASIC). Generally, hardware realisation of MA-AC involves numerical processing and entropy coding, which employ the floating point (FP) arithmetic which is replaced by integer implementation in such a way that the symbol counts are used instead of probabilities. Novel hardware architecture is designed by modifying the update equations for upper and lower limits of multi-alphabet arithmetic encoder and decoder based on the update equation of the FP implementation. The proposed hardware architectures are synthesised in Xilinx and Altera Field Programmable Gate Array (FPGA) devices to evaluate resource utilisation and speed. Also, the physical design is encountered as ASIC device using Cadence Design environment tsmc 0.18 µm technology which shows area reduction of 12.75% and 23.61% and power consumption of 29.86% and 38.89% for encoder and decoder, respectively. Copyright © 2018 Inderscience Enterprises Ltd."
"10.1504/IJBIDM.2017.10004943","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85038434929&doi=10.1504%2fIJBIDM.2017.10004943&partnerID=40&md5=472989e9ef58abea46e985a96b48215c","In real classification problems, common learning algorithms generally fail to describe instances that require complicated classification logic. Additionally, it is often difficult to ensure a satisfying amount of classified data for their training. In this work, we propose and examine a new learning algorithm that also integrates expert logic. Essentially, this algorithm takes advantage of unclassified data to produce a self-generated fuzzy inference system that is eventually used as a classifier. It also utilises a mere sample of classified data in order to compare various classifiers constructed from different algorithm options, thus finally achieving an assumingly more accurate result. As part of our study, this algorithm was compared with six well-known supervised learning algorithms such as artificial neural networks, support vector machine and random forest. We used the ten-fold cross-validation technique with Kappa statistic to assess algorithm performance. Subsequently, in order to find statistically significant dissimilarities among the algorithms, we used a two-tailed Friedman test. After the null hypothesis was rejected, we used a Nemenyi post-hoc test to prove differences between pairs of algorithms. Consequently, despite lacking in efficiency and scalability, our algorithm proved to be highly competitive and demonstrated excellent classification potential. Copyright © 2018 Inderscience Enterprises Ltd."
"10.1504/IJBIDM.2017.10003958","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85038433614&doi=10.1504%2fIJBIDM.2017.10003958&partnerID=40&md5=402c3348a0e9c3441869543bf805b8a1","In traditional routing, there are various routing protocol used for MANETs depending on the environment. The opportunistic routing protocol is used as the basic protocol. In the CORBG, location-based adaptive mechanism was created and improves the network performance. The basic function of OR is its ability to overhear the transmitted packet and to coordinate among relaying nodes. A new routing protocol named cooperative opportunistic routing based on geographic (CORBG) location has been implemented to get better network performance. In the proposed work, network optimisation might be attained by CORBG protocol with cross-layer between network and transport layer that evaluates the performance based on the network QoS parameters like throughput, delay and energy consumption. In MANETs by reason of dynamic communications and their decentralised admin makes the network becomes more risk to have more attack. To prevent such network problems, security mechanisms were introduced. Copyright © 2018 Inderscience Enterprises Ltd."
"10.1504/IJBIDM.2017.10003611","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85038432753&doi=10.1504%2fIJBIDM.2017.10003611&partnerID=40&md5=068ec286d89f7482a657638a79de7626","In Twitter and in other social media channels, detecting events is very important and has many applications. However, this task is very challenging because of the huge number of tweets that are posted every minute and the massive scale of the spamming activities. In this paper, we present an innovative approach for detecting events using data posted to Twitter. The proposed approach is based on the concept of user’s attention by quantitatively modelling the diversity of hashtags using Shannon’s index. Our method records the diversity values on an hourly basis time-series. Using statistical techniques, the method locates the intervals having diversity values that fall outside the range of forecasted ones (normal state). We also present the labelling and ranking techniques that are implemented in this research. Experimental results on a dataset consisting of 15 million Arabic tweets show that our proposed approach can effectively detect real-world events in Twitter. Copyright © 2018 Inderscience Enterprises Ltd."
"10.1504/IJBIDM.2017.10004788","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85038432731&doi=10.1504%2fIJBIDM.2017.10004788&partnerID=40&md5=7796090f53f5ee7f54087d9c12271bde","In this study, urban development process in the Walloon region (Belgium) has been analysed. Two main aspects of development are quantitatively measured: the development type and the definition of the main drivers of the urbanisation process. Unlike most existing studies that consider the urban development as a binary process, this research considers the urban development as a continuous process, characterised by different levels of urban density. Eight urban classes are defined based on the Belgian cadastral data for years 2000 and 2010. A multinomial logistic regression model is employed to examine the main driving forces of the different densities. Sixteen drivers were selected, including accessibility, geo-physical features, policies and socio-economic factors. Finally, the changes from the non-urban to one of the urban density classes are detected and classified into different development types. The results indicate that zoning status (political factor), slope, distance to roads, population densities and mean land price, respectively, have impact on the urbanisation process whatever maybe the density. The results also show that the impact of these factors highly varies from one density to another. Copyright © 2018 Inderscience Enterprises Ltd."
"10.1504/IJBIDM.2017.10004683","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85038432341&doi=10.1504%2fIJBIDM.2017.10004683&partnerID=40&md5=fd3c309887a2d6a55cca52d73d34fdb1","Hadoop Distributed File System, Talend, MapReduce (MR), YARN and Cloudera model have gotten to be prevalent advancements for expansive scale information association and investigation. In our work, we distinguish the prerequisites of the covered information association and propose an augmentation to the present programming model, called Comprehensive Hadoop Distributed File System along with MapReduce (C-HDFS-MR), to address them. The expanded interface is exhibited as application programming interface and actualised with regards to image processing application space. In our work, we show viability of C-HDFS-MR through contextual investigations of picture handling capacities along with the outcomes. Despite the fact that C-HDFS-MR has minimal overhead in information stockpiling and I/O operations, it enormously upgrades the framework execution and improves the application advancement process. Our proposed framework, C-HDFS-MR, works in the absence of progressions for the current prototypes, and is used by numerous applications to prerequisite of covered information. Copyright © 2018 Inderscience Enterprises Ltd."
"10.1504/IJBIDM.2017.10005166","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85038429441&doi=10.1504%2fIJBIDM.2017.10005166&partnerID=40&md5=a39d4851b88f55f2f5c06fbb803fd565","There has been fast development happening in the multimedia and the related technologies, particularly associated with visual tracking and search operations. Moving target detection has been comprehensively engaged in various arenas but has the disadvantage that the scheme is frequently complex and also that tracking is affected numerous external factors. In this article, multiple objects recognition and tracking is projected so as to progress the method and make it more robust and general with assistance of shape-based features and Kalman filter. Primarily, the input video is rehabilitated to frames and then manually segmented for object segmentation. Consequently, the objects are tracked with the help of Kalman filtering. The method is assessed under standard evaluation metrics of error value and the score value. The technique achieved maximum score values of 95% and minimum error value of 25%. The results validate the effectiveness of the technique. Copyright © 2018 Inderscience Enterprises Ltd."
"10.1504/IJBIDM.2017.10003508","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85038428835&doi=10.1504%2fIJBIDM.2017.10003508&partnerID=40&md5=cc0945101fcf97ce260eefe339d48eb8","Early diagnosis of cutaneous melanoma is very hard for experienced dermatologists. Even though a lot of advanced imaging techniques and clinical diagnostic algorithms such as dermoscopy and the ABCD rule of dermoscopy respectively are available. The accuracy is an issue of distress (estimated to be about 75%–85%) especially with oblique pigmented lesions. An effective diagnosis can be achieved by reducing the viewer variability’s found in dermatologists’ examinations. In order to improve some of existing methods and budding new techniques to ease accurate, fast and reliable diagnosis of cutaneous melanoma. In this paper, different types diagnostic system of melanoma namely, pre-processing feature extraction, feature selection and classification is explained. The results of feature selection were optimised from advanced classes of classification techniques; namely, two weighted k-nearest neighbour (k-NN) classifiers (k = 1, 30), a decision tree (DT), and the random forest (RF) algorithm are employed. Copyright © 2018 Inderscience Enterprises Ltd."
"10.1504/IJBIDM.2017.10004374","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85038426468&doi=10.1504%2fIJBIDM.2017.10004374&partnerID=40&md5=a8f4a0bfa320f6e2ecdd35788da2ddf2","According to World Health Organization, over 5% of the world’s population have hearing and speaking disabilities. The primary language of communication for people who are deaf and mute is the sign language. The proposed system aims to recognise the American Sign Language and converts it to text. Input given to the system is an image of the hand depicting the necessary alphabet. The histogram of the input image is then computed and checked for similarity with the histograms of pre-saved images by using the Bhattacharyya Distance Metric. Implementation of the system will be a small step in overcoming the social barrier of communication between the deaf-mute people and the people who do not understand sign language. OpenCV is used as a tool for implementing proposed system. Copyright © 2018 Inderscience Enterprises Ltd."
"10.1504/IJBIDM.2016.10002433","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85038419971&doi=10.1504%2fIJBIDM.2016.10002433&partnerID=40&md5=2fa997151e99d63c252c1289afa7f459","An ambitious task is to conduct a project related to enterprise resource planning (ERP). In any of the ERP business enterprises, the technological, psychosomatic and sociological characteristics are included. Managing such characteristics in any enterprises is high complex. The ERP as such categorised on the basis of implementation, software, supply chain management, resources, management and optimisation. These characteristics are high risk to handle. The system still seems to be a growing system that has to be moulded into many forms. The aim of this manuscript is to present a risk assessment and management (RAM) in ERP by advanced system engineering theory. Copyright © 2018 Inderscience Enterprises Ltd."
"10.1504/IJBIDM.2017.10003120","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85038419478&doi=10.1504%2fIJBIDM.2017.10003120&partnerID=40&md5=8ad64c536d1ac4b47429c3c25d5540fa","The process of service discovery is the most important task in web services. But this service discovery process may degrade network performance due to the mobile environment. To overcome these issues, a new approach called ‘semantic mobile web services (SMWS)’ is proposed. This proposed semantic mobile web services is used for a better discovery process even in the high mobile environment. By using the query request (QRY REQ), response (RSP) packets, user can identify the location of the mobile node as well as discover the web services with the minimum utilisation of bandwidth resources. During the service discovery process, a service registrar is included between service requester and service provider to reduce overhead. The process of matchmaking produces the exact match responses for the respective requestor queries. This helps to increase the quality of performance and network efficiency. Simulation has analysed the performance of the proposed semantic mobile web services. Copyright © 2018 Inderscience Enterprises Ltd."
"10.1504/IJBIDM.2017.10004803","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85038416835&doi=10.1504%2fIJBIDM.2017.10004803&partnerID=40&md5=1cd13f6cc834c73becb5c9cddbb7ecf2","Cloud computing environment provides on-demand virtualised resources for cloud application. The scheduling of tasks in cloud application is a well-known NP-hard problem. The task scheduling problem is more complicated while satisfying multiple objectives, which are conflict in nature. In this paper, Epsilon-fuzzy dominance based composite discrete artificial bee colony (EDCABC) approach is used to generate Pareto optimal solutions for multi-objective task scheduling problem in cloud. Three conflicting objectives, such as makespan, execution cost and resource utilisation, are considered for task scheduling problem. The Epsilon-fuzzy dominance sort approach is used to choose the best solutions from the Pareto optimal solution set in the multi-objective domain. EDCABC with composite mutation strategies and fast local search method are used to enrich the local searching behaviours which help to avoid the premature convergence. The performance and efficiency of the proposed algorithm is compared with NSGA-II and MOPSO algorithms. The simulation results express that proposed EDCABC algorithm substantially minimises the makespan, execution cost and ensures the proper resource utilisation when compare to specified existing algorithm. Copyright © 2018 Inderscience Enterprises Ltd."
"10.1504/IJBIDM.2017.10004686","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85038414970&doi=10.1504%2fIJBIDM.2017.10004686&partnerID=40&md5=0ee8ab71be2999643c4adbaf03adfbd1","Data mining environment gives a quick response to the user by fast and correctly pick-out the item from the large database is a very challenging task. Previously, multiple algorithms were proposed to identify the frequent item since they are scanning database at multiple times. To overcome those problems, we proposed Rehashing based Apriori Technique in which hashing technology is used to store the data in horizontal and vertical formats. Rehash Based Apriori uses hashing function to reduce the size of candidate item set and scanning of database, eliminate non-frequent items and avoid hash collision. After finding frequent item sets, perform level wise subspace clustering. We instigate generalised self organised tree based (GSTB) mechanism to adaptively selecting root to construct the path from the cluster head to neighbours when constructing the tree. Our experimental results show that our proposed mechanisms reduce the computational time of overall process. Copyright © 2018 Inderscience Enterprises Ltd."
"10.1504/IJBIDM.2017.10004684","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85038412093&doi=10.1504%2fIJBIDM.2017.10004684&partnerID=40&md5=30d35e457f39a9f606d5001fd3ba5dee","This paper concentrates on the cell formation problem for the ratio level data to the design of cellular manufacturing system. The aim of this paper is to identify the machine cells and part family and as a result to create production cells in order to reduce the cell load variation. A competent Tabu Search (TS) algorithm is proposed to investigate the search space of all possible solutions with a chain of moves. This method is an iterative process for seeking a global optimum for the discrete combinatorial optimisation problems. The ratio level data is calculated in terms of time in seconds based on the data collected from the processing time of the part, production volume of the part and availability of the machine. A modified grouping efficiency (MGE) is used to compute the cell formation problem. The results clearly indicated that this proposed TS yield good results compared to the chosen benchmark problems. Copyright © 2018 Inderscience Enterprises Ltd."
"10.1504/IJBIDM.2017.10005098","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85038411786&doi=10.1504%2fIJBIDM.2017.10005098&partnerID=40&md5=dfc9ae827f3413a749eae6a17e56b037","Constructing fast and accurate classifiers for large data sets is an important task in data mining. Associative classification can produce more efficient and accurate classifiers than traditional classification techniques. Weighted class association rule (WCAR) mining reflects significance of items by considering their weight. Moreover, real time databases are dynamic. This influences the need for incremental approach for classification. Existing incremental classification algorithms suffer from issues like longer execution time and higher memory usage. This paper proposes an algorithm which uses hash structure to store weighted frequent items and the concept of difference of object identifiers to compute the support faster. For mining incremental databases, pre-large concept is used to reduce the number of re-scans over the original database. The proposed algorithm was implemented and tested on experimental data sets taken from UCI repository. The results show that proposed algorithm for mining WCARs gives better results compared to existing algorithm. Copyright © 2018 Inderscience Enterprises Ltd."
"10.4018/IJBAN.2018010101","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046297741&doi=10.4018%2fIJBAN.2018010101&partnerID=40&md5=a967c66bd170b1c056ad26afc1417e9d","This study examines the influence of Electronic Word of Mouth (eWOM) on the box office revenue generation of movies in the U.S domestic market using the technique of Aspect-Based Sentiment Analysis (ABSA) and aspect identification. The analysis was conducted on the sentiment score and frequency of five movie aspects from the user reviews collected from high grossing 2014 movies. This study revealed a significant dependence on the aspect-based sentiment frequency of the movie’s Story aspect. Surprisingly, the data also showed a strong dependence of movie success on the negative sentiment frequency on the Casting aspect. The findings of the study suggest that the eWOM present in online movie reviews can be used to predict the performance of a movie at the box office by monitoring the aspect’s frequency of sentiment, which can be referred to as a metric of the online “buzz” of the movie. Copyright © 2018, IGI Global."
"10.4018/IJBAN.2018010103","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046289262&doi=10.4018%2fIJBAN.2018010103&partnerID=40&md5=756581c608f66bdf445fa5b56eece386","This paper studies the impact of consumers’ individual attitudes towards load shifting in electricity consumption in an electricity market that includes a single electricity provider and multiple consumers. A Stackelberg game model is formulated in which the provider uses price discounts over a finite number of periods in order to induce incentives for consumers to shift their peak period loads to off-peak periods. The equilibrium outcomes are investigated and the analytical results are derived for this type of market, where not only the response behaviors of independent consumers are diverse but also an individual consumer’s valuation of electricity consumption varies across periods. The obtained results demonstrate that consumer sensitivities to price discounts significantly impact price discounts and load-shifts, which are not necessarily monotonic. The authors also observe that a diverse market leads to lower peak-to-average values and provider payoffs compared to a homogenous market unless the latter one is composed of consumers with relatively lower inconvenience costs during the peak periods. Copyright © 2018, IGI Global."
"10.4018/IJBAN.2018010104","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046283576&doi=10.4018%2fIJBAN.2018010104&partnerID=40&md5=89eb9a68cf26c8fa083fcf2183f9ea28","This paper considers methods of estimation of choice probability using Maximum Difference (MaxDiff) technique, also known as Best-Worst Scaling (BWS). The paper shows that on the aggregate level the choice probabilities can be obtained using analytical closed-form solution and other approaches such as Thurstone scaling, Bradley-Terry maximum likelihood, and Markov modeling via Chapman-Kolmogorov equations for steady-states probabilities. On the individual level, to account for the exact combinations presented in each task, the Cox hazard model is employed, as well as new approaches of least squares objective for maximum difference, and maximum likelihood in order statistics. The results are useful in the practical MaxDiff applications for items prioritization in marketing research. Copyright © 2018, IGI Global."
"10.4018/IJBAN.2018010102","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046251985&doi=10.4018%2fIJBAN.2018010102&partnerID=40&md5=28809e7be60361ac913c985444716dae","Integrated supply chain information systems (ISCIS) face various barriers including lack of alignment between IT and business model, security and privacy concerns, behavioral and cultural issues, and heterogeneous software applications. This paper develops an architecture for ISCIS and validate it by interviewing experts. The proposed architecture is an intermediary to integrate in-house information systems as well as cloud-based systems across distributed heterogeneous supply chain networks. The developed ISCIS architecture works in three layers of data, processes, and knowledge and facilitates the alignment of information systems and decision making with business. Copyright © 2018, IGI Global."
"10.1177/0265813516668854","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040373553&doi=10.1177%2f0265813516668854&partnerID=40&md5=2c89b13d5b30d76adbfc1f99c8396a55","Urban development cannot proceed without the commitment of multiple actors, because decision-making processes have become more and more interdependent. This article supports better decision-making in brownfield redevelopment projects in cases where the project realization depends on a public–private partnership between a municipality and a developer. In a broader sense, in this research, we investigate how the negotiation process in brownfield redevelopment projects can be improved by providing an understanding of the characteristics of a brownfield area and the interaction between the parties involved. In order to improve the process, a negotiation decision model is proposed. This is a hybrid model consisting of five phases. Its novelty lies in the combination of a latent class model and a strategic choice model that can be formulated as a prescriptive-interactive approach in decision theory. The hybrid negotiation model is applied to a reconstructed case study in order to present its possibilities. Accordingly, three applications of the model are introduced, and in each application the beneficiary is a municipality. Although using decision models for brownfield redevelopment projects has already been thoroughly studied, there is little evidence concerning the prescriptive-interactive component built into existing models. © 2016, © The Author(s) 2016."
"10.1177/0265813516668855","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040357387&doi=10.1177%2f0265813516668855&partnerID=40&md5=7defce4d110b56f751fbd9d94068a989","This article highlights the importance for urban planning, of the under-researched notion of superstar destinations. Furthermore, it presents and compares destination choice models that generate a convex demand for destination quality, and thereby explain and predict the existence of so-called superstar destinations. When compared to their competition, superstar destinations are much more popular than differences in quality between the superstar and other destinations would suggest at first sight. Although convexity of demand for quality is a known precondition for the existence of superstars, it remains unclear what mechanism might cause this imperfect substitution between different quality levels. The article proposes several choice models that generate a convexity of demand for quality, thereby paving the way for (modelling) the existence of superstar destinations. These models are compared using numerical simulations, which show that each of the proposed models has the potential to generate superstar effects, although for most models the effect decreases for larger choice sets. Results suggest that including reference-dependency into choice models helps overcome this potential limitation, as it leads to superstar effects for larger choice sets typically encountered in real life destination choice situations. © 2016, © The Author(s) 2016."
"10.1177/0265813516665361","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040350276&doi=10.1177%2f0265813516665361&partnerID=40&md5=d5a6e71abc69dbeadbf43d4d06e13b3a","The Traditional Chinese Private Garden is a historically and socially significant landscape type that features multiple complex planning elements. Whereas there are many different examples of Traditional Chinese Private Gardens, the small gardens of Suzhou make up a distinct subset. This paper describes a method for mathematically capturing and then parametrically generating, new instances of what might be called the small ‘Suzhou type’, which features some of the same social and cognitive spatial properties as the historic cases. The research commences with a mathematical analysis of three historic Suzhou Traditional Chinese Private Gardens before using connectivity graphs to investigate their properties. Mathematical measurements derived from the Traditional Chinese Private Gardens are then used as rules for a parametric system to generate new instances of the Suzhou type. In the paper, three new Suzhou type connectivity and spatio-typological systems are generated and tested against the properties of the historic cases. Through this process, the paper demonstrates a method for capturing distinct social and cognitive properties in a parametric system and thereby derives possible new insights into these important heritage sites. This method may also be applied to the analysis and generation of different spatial types. © 2016, © The Author(s) 2016."
"10.1177/0265813516667300","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040341594&doi=10.1177%2f0265813516667300&partnerID=40&md5=26400be3d094d0e90eb5baef3a7076eb","A description grammar, in conjunction with a shape grammar, serves to generate verbal descriptions of designs, next to the spatial descriptions. These verbal descriptions can also assist in guiding the generative process. This paper presents a general notation for descriptions and description rules that accounts, extensively if not entirely, for many of the applications of description grammars found in literature. Specifically, a review of the notation with respect to these description schemes supports the explication of its strengths and limitations and the identification of future work. A follow-up paper revisits selected applications of description grammars and demonstrates the applicability of this general notation to these case studies. © 2016, © The Author(s) 2016."
"10.1177/0265813516667301","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040340897&doi=10.1177%2f0265813516667301&partnerID=40&md5=b2693505377407eb2f098abb62e1af90","A description grammar, in conjunction with a shape grammar, serves to generate verbal descriptions of designs, next to the spatial descriptions. These verbal descriptions can also assist in guiding the generative process. This paper revisits applications of description grammars found in literature and demonstrates how they can be recast and redeveloped to make use of a general notation and implementation for description grammars. The review of this notation was the topic of a previous paper; this paper is both meant as an illustration and as a confirmation of those review analysis results. © 2016, © The Author(s) 2016."
"10.1177/0265813516668856","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040339736&doi=10.1177%2f0265813516668856&partnerID=40&md5=dcf678348af819ebae6c670ec5bfcfad","At European level, the different methodologies used for the classification of urban areas rely on the spatial allocation of population to 1 km2 grid cells, failing therefore to identify small-sized settlements that play an important role in urban systems mostly composed by small towns, such as the Portuguese. This paper reports the development of alternative methodologies which overcome the problem stated, successfully enabling the automated recognition and delimitation of small-sized urban settlements – the prime goal of this work. Two alternative methodologies (A and B) were developed and later compared. The settlements identified by A are clusters of census tracts, previously classified using an urban–rural typology proposed by the authors. In B an adaptation of the Urban Morphological Zones methodology published by the European Environment Agency was used, whereby settlements are clusters of specific Land Use/Land Cover classes combined with the urbanised areas defined by Municipal Master Plans. © 2016, © The Author(s) 2016."
"10.1177/0265813516663932","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040338489&doi=10.1177%2f0265813516663932&partnerID=40&md5=31a9a5ab0d052534ec4733fd9ce0fe35","Many empirical studies assessing the economic benefits of urban green space have continually documented that green space tends to increase both value and sale price of nearby residential properties. Previous studies, however, have not fully captured the quality of neighborhood level landscape spatial patterns on housing prices. To fill this literature gap, this study examined the association between landscape spatial patterns of urban green spaces and single-family home sale transactions using a spatial regression model. The research was conducted through the analysis of 11,326 housing transaction records from 2010 to 2012 in Austin, TX, USA. Variables measuring the structural, locational and neighborhood characteristics of housing were coupled with Geographic Information Systems, remote sensing and FRAGSTATS to calculate several landscape indices measuring the quality of existing landscape spatial patterns. After controlling for any spatial autocorrelation effects, we found that that larger tree and urban forest areas surrounding single-family homes positively contributed to property values, while more fragmented, isolated and irregularly shaped landscape spatial patterns resulted in the inverse. The results of this research increase awareness of the role of urban green spaces while informing community design/planning practices about the linkages between landscape spatial structure and economic benefits. © 2016, © The Author(s) 2016."
"10.1177/0265813516667299","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040335839&doi=10.1177%2f0265813516667299&partnerID=40&md5=ab1a4be7abbba651a0fe9e62ba355baa","From Ebenezer Howard’s concept of garden cities to visions of the sustainable development of ecocities, people have engaged in numerous attempts to curb hazards of residential environments and excessive capitalized development resulting from mass industrialization. However, many countries have adopted widely used green building assessment tools to conduct neighborhood sustainability assessment of ecocities. An ecocommunity assessment tool of Taiwan was established according to current green building policies, the content of which focuses on technological aspects rather than on the closely correlated social and economic aspects within ecocities. To thoroughly review neighborhood sustainability assessment tools, this study conducted a qualitative and quantitative analysis of internationally renowned assessment tools and tools used in developed Asian countries. This study concluded that (a) a number of countries have proposed neighborhood sustainability assessment tools that strongly emphasize resource and energy categories but neglect economic development; (b) the neighborhood sustainability assessment tools in Asia possess special characteristics; and (c) indicators of neighborhood sustainability assessment tools must enable mutual links and public participation. © 2016, © The Author(s) 2016."
"10.1177/0265813516665618","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040330297&doi=10.1177%2f0265813516665618&partnerID=40&md5=62c2fa0ddb4f935a285bcdf76fd6dd64","This paper presents a hybrid approach that selectively merges aspects of both the theories of Shape Grammar and Space Syntax to investigate spatial design patterns. The paper describes the development of a generic Justified Plan Graph (g-JPG) grammar. This grammatically nuanced, syntactically derived approach is then demonstrated through a more specific JPG (s-JPG) grammar to identify spatial design patterns in the rural domestic architecture of Glenn Murcutt. The results are then discussed in terms of Murcutt's architecture from four perspectives: grammatical transformation of syntax, epistemological questions, similarity or disparity and finally in terms of JPG variations. The findings of this paper suggest that the combined analytic approach facilitates the exploration of both the grammatical and syntactical genotypes of sets of architectural designs. © 2016, © The Author(s) 2016."
"10.1177/0265813516660716","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040323267&doi=10.1177%2f0265813516660716&partnerID=40&md5=6c9685ed24cde2cde9ab19361a4bf096","For centuries, landscape architects, architects, and urban planners have been designing outdoor green spaces for one to contemplate. In today’s urban realm, we can understand a contemplative space more specifically as one joining esthetic and environmental values with mental health benefits for its visitors. So far, the concept of contemplativeness of a space has not been operationalized and a definitive list of design principles of such a space has not been developed. In response to this gap of knowledge, we have identified a set of features that may be used in order to design and create a space of contemplation within seven categories: Landscape Layers, Landform, Vegetation, Light and Color, Compatibility, Archetypal Elements, and a Character of Peace and Silence. The developed framework is based on development and analysis of a Contemplative Landscape Questionnaire. This instrument was developed based on literature review and Delphi expert evaluation of multiple landscapes. The statistical tests on the Contemplative Landscape Questionnaire revealed satisfactory reliability and validity measures, which provided evidence-based support for the efficacy of designed spaces. This approach could enhance the practice of landscape architects and urban designers by reinforcing intuition-based designs with scientific evidence. The developed framework can also serve to identify contemplative spaces for subsequent research purposes. © 2016, © The Author(s) 2016."
"10.1089/big.2017.0047","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042550364&doi=10.1089%2fbig.2017.0047&partnerID=40&md5=ba25d3ef2d3a9af7e46ee4308d625570","The problem of accurately predicting vote counts in elections is considered in this article. Typically, small-sample polls are used to estimate or predict election outcomes. In this study, a machine-learning hybrid approach is proposed. This approach utilizes multiple sets of static data sources, such as voter registration data, and dynamic data sources, such as polls and donor data, to develop individualized voter scores for each member of the population. These voter scores are used to estimate expected vote counts under different turnout scenarios. The proposed technique has been tested with data collected during U.S. Senate and Louisiana gubernatorial elections. The predicted results (expected vote counts, predicted several days before the actual election) were accurate within 1%. © 2017, Mary Ann Liebert, Inc."
"10.1089/big.2017.0049","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042548840&doi=10.1089%2fbig.2017.0049&partnerID=40&md5=8511355795a5de82fd25a652fbf69d6e","In this article, we present results on the identification and behavioral analysis of social bots in a sample of 542,584 Tweets, collected before and after Japan's 2014 general election. Typical forms of bot activity include massive Retweeting and repeated posting of (nearly) the same message, sometimes used in combination. We focus on the second method and present (1) a case study on several patterns of bot activity, (2) methodological considerations on the automatic identification of such patterns and the prerequisite near-duplicate detection, and (3) we give qualitative insights into the purposes behind the usage of social/political bots. We argue that it was in the latency of the semi-public sphere of social media-and not in the visible or manifest public sphere (official campaign platform, mass media)-where Shinzō Abe's hidden nationalist agenda interlocked and overlapped with the one propagated by organizations such as Nippon Kaigi and Internet right-wingers (netto uyo) during the election campaign, the latter potentially forming an enormous online support army of Abe's agenda. © 2017, Mary Ann Liebert, Inc."
"10.1089/big.2017.0038","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042548167&doi=10.1089%2fbig.2017.0038&partnerID=40&md5=2e58afb9b4a2507535e1f7ef706b552a","Automated and semiautomated Twitter accounts, bots, have recently gained significant public attention due to their potential interference in the political realm. In this study, we develop a methodology for detecting bots on Twitter using an ensemble of classifiers and apply it to study bot activity within political discussions in the Russian Twittersphere. We focus on the interval from February 2014 to December 2015, an especially consequential period in Russian politics. Among accounts actively Tweeting about Russian politics, we find that on the majority of days, the proportion of Tweets produced by bots exceeds 50%. We reveal bot characteristics that distinguish them from humans in this corpus, and find that the software platform used for Tweeting is among the best predictors of bots. Finally, we find suggestive evidence that one prominent activity that bots were involved in on Russian political Twitter is the spread of news stories and promotion of media who produce them. © 2017, Mary Ann Liebert, Inc."
"10.1089/big.2017.0055","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042543184&doi=10.1089%2fbig.2017.0055&partnerID=40&md5=9417fae2169acbff0661e26222deff1f","Peace processes are complex, protracted, and contentious involving significant bargaining and compromising among various societal and political stakeholders. In civil war terminations, it is pertinent to measure the pulse of the nation to ensure that the peace process is responsive to citizens' concerns. Social media yields tremendous power as a tool for dialogue, debate, organization, and mobilization, thereby adding more complexity to the peace process. Using Colombia's final peace agreement and national referendum as a case study, we investigate the influence of two important indicators: intergroup polarization and public sentiment toward the peace process. We present a detailed linguistic analysis to detect intergroup polarization and a predictive model that leverages Tweet structure, content, and user-based features to predict public sentiment toward the Colombian peace process. We demonstrate that had proaccord stakeholders leveraged public opinion from social media, the outcome of the Colombian referendum could have been different. © 2017, Mary Ann Liebert, Inc."
"10.1089/big.2017.0071","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042537413&doi=10.1089%2fbig.2017.0071&partnerID=40&md5=a583adbf75532e8670250340c3fb90ff","In this article, we introduce a prototype of an innovative technology for proving the origins of captured digital media. In an era of fake news, when someone shows us a video or picture of some event, how can we trust its authenticity? It seems that the public no longer believe that traditional media is a reliable reference of fact, perhaps due, in part, to the onset of many diverse sources of conflicting information, via social media. Indeed, the issue of ""fake"" reached a crescendo during the 2016 U.S. Presidential Election, when the winner, Donald Trump, claimed that The New York Times was trying to discredit him by pushing disinformation. Current research into overcoming the problem of fake news does not focus on establishing the ownership of media resources used in such stories-the blockchain-based application introduced in this article is technology that is capable of indicating the authenticity of digital media. Put simply, using the trust mechanisms of blockchain technology, the tool can show, beyond doubt, the provenance of any source of digital media, including images used out of context in attempts to mislead. Although the application is an early prototype and its capability to find fake resources is somewhat limited, we outline future improvements that would overcome such limitations. Furthermore, we believe that our application (and its use of blockchain technology and standardized metadata) introduces a novel approach to overcoming falsities in news reporting and the provenance of media resources used therein. However, while our application has the potential to be able to verify the originality of media resources, we believe that technology is only capable of providing a partial solution to fake news. That is because it is incapable of proving the authenticity of a news story as a whole. We believe that takes human skills. © 2017, Mary Ann Liebert, Inc."
"10.1089/big.2017.0044","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042528130&doi=10.1089%2fbig.2017.0044&partnerID=40&md5=1c4f09db3936cefef474ae4048089333","Social bots are currently regarded an influential but also somewhat mysterious factor in public discourse and opinion making. They are considered to be capable of massively distributing propaganda in social and online media, and their application is even suspected to be partly responsible for recent election results. Astonishingly, the term social bot is not well defined and different scientific disciplines use divergent definitions. This work starts with a balanced definition attempt, before providing an overview of how social bots actually work (taking the example of Twitter) and what their current technical limitations are. Despite recent research progress in Deep Learning and Big Data, there are many activities bots cannot handle well. We then discuss how bot capabilities can be extended and controlled by integrating humans into the process and reason that this is currently the most promising way to realize meaningful interactions with other humans. This finally leads to the conclusion that hybridization is a challenge for current detection mechanisms and has to be handled with more sophisticated approaches to identify political propaganda distributed with social bots. © 2017, Mary Ann Liebert, Inc."
"10.1016/j.bdr.2017.10.004","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85033477822&doi=10.1016%2fj.bdr.2017.10.004&partnerID=40&md5=129c7bf6c9201f497c6ca184040d521b","In today's world, large volumes of data are being continuously generated by many scientific applications, such as bioinformatics or networking. Since each monitored event is usually characterized by a variety of features, high-dimensional datasets have been continuously generated. To extract value from these complex collections of data, different exploratory data mining algorithms can be used to discover hidden and non-trivial correlations among data. Frequent closed itemset mining is an effective but computational expensive technique that is usually used to support data exploration. Thanks to the spread of distributed and parallel frameworks, the development of scalable approaches able to deal with the so called Big Data has been extended to frequent itemset mining. Unfortunately, most of the current algorithms are designed to cope with low-dimensional datasets, delivering poor performances in those use cases characterized by high-dimensional data. This work introduces PaMPa-HD, a MapReduce-based frequent closed itemset mining algorithm for high dimensional datasets. An efficient solution has been proposed to parallelize and speed up the mining process. Furthermore, different strategies have been proposed to easily configure the algorithm parameter. The experimental results, performed on real-life high-dimensional use cases, show the efficiency of the proposed approach in terms of execution time, load balancing and robustness to memory issues. © 2017 Elsevier Inc."
"10.1016/j.bdr.2017.10.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85033405976&doi=10.1016%2fj.bdr.2017.10.001&partnerID=40&md5=dc92c4467a7e9e8d6817ca639e19f43d","Visualization plays an important role in enabling understanding of big data. Graphs are crucial tools for visual analytics of big data networks such as social, biological, traffic and security networks. Graph drawing has been intensively researched to enhance aesthetic features (i.e., layouts, symmetry, cross-free edges). Early physic-inspired techniques have focused on synthetic abstract graphs whose weights/distances of the edges are often ignored or assumed equal. Although recent approaches have been extended to sophisticated realistic networks, most are not designed to address very large-scale weighted graphs, which are important for visual analyses. The difficulty lies in the fact that the drawing process, governed by these physical properties, oscillates in large graphs and conflicts with specified distances leading to poor visual results. Our research attempts to alleviate these obstacles. This paper presents a simple graph visualization technique that aims to efficiently draw aesthetically pleasing large-scale straight-line weighted edge graphs. Our approach uses relevant physic-inspired techniques to promote aesthetic graphs and proposes a weak constraint-based approach to handle large-scale computing and competing goals to satisfy both weight requirements and aesthetic properties. The paper describes the approach along with experiments on both synthetic and real large-scale weighted graphs including that of over 10,000 nodes and comparisons with state-of-the-art approaches. The results obtained show enhanced and promising outcomes toward a general-purpose graph drawing technique for both big synthetic and real network data analytics. © 2017 Elsevier Inc."
"10.1016/j.bdr.2017.10.003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85033385560&doi=10.1016%2fj.bdr.2017.10.003&partnerID=40&md5=2f95e2f3b47a3e3236ed567be0800c98","One of the most relevant and widely studied structural properties of networks is their community structure. Detecting communities is of great importance in social networks where systems are often represented as graphs. With the advent of web-based social networks like Twitter, Facebook and LinkedIn. community detection became even more difficult due to the massive network size, which can reach up to hundreds of millions of vertices and edges. This large graph structured data cannot be processed without using distributed algorithms due to memory constraints of one machine and also the need to achieve high performance. In this paper, we present a novel hybrid (shared + distributed memory) parallel algorithm to efficiently detect high quality communities in massive social networks. For our simulations, we use synthetic graphs ranging from 100K to 16M vertices to show the scalability and quality performance of our algorithm. We also use two massive real world networks: (a) section of Twitter-2010 network having ≈41M vertices and ≈1.4B edges (b) UK-2007 (.uk web domain) having ≈105M vertices and ≈3.3B edges. Simulation results on MPI setup with 8 compute nodes having 16 cores each show that, upto ≈6X speedup is achieved for synthetic graphs in detecting communities without compromising the quality of the results. © 2017 Elsevier Inc."
"10.1016/j.bdr.2017.10.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032255841&doi=10.1016%2fj.bdr.2017.10.002&partnerID=40&md5=4b8628fb6cee40ec4dd09708e47e414c","Big data has arrived. Myriad applications, systems generate data of humongous volumes, variety and velocity which traditional computing systems and databases are unable to manage. The proliferation of sensors in every possible device is also becoming one of the major generators of Big data. Of particular interest in this article is how context aware computing systems which derive context from data and act accordingly, deal with such huge amounts of data. Big industry players namely Google, Yahoo, and Amazon are already developing context aware applications using user data from emails, chat messages, browsing and shopping histories etc. For instance, Gmail reminds us of our flight schedule by understanding flight booking related content in our emails. Similarly, Amazon understands user preference and recommends items of interest to shop and so on. In this paper, we survey context aware computing systems from a Big data perspective. We first propose a taxonomy of existing work on the basis of sensing platforms and then discuss the latest developments in this field of Big data context aware systems focusing on how such systems deal with various Big data challenges. We conclude the paper with an insight on open research issues involving designing and developing context aware Big data generating systems. © 2017 Elsevier Inc."
"10.1016/j.bdr.2017.07.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028621414&doi=10.1016%2fj.bdr.2017.07.002&partnerID=40&md5=025c428e4fdd0f15e7df0a60ec425fe1","Conventional Extreme Learning Machines utilize Moore–Penrose generalized pseudo-inverse to solve hidden layer activation matrix and perform analytical determination of output weights. Scalability is the major concern to be addressed in Extreme Learning Machines while dealing with large dataset. Motivated by these scalability concerns, this paper proposes a novel tensor decomposition based Extreme Learning Machine which utilize PARAFAC and TUCKER decomposition based techniques in a SPARK platform. This proposed Extreme Learning Machine achieve reduced training time and better accuracy when compared with a conventional Extreme Learning Machine. © 2017 Elsevier Inc."
"10.1140/epjds/s13688-017-0127-3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85038209991&doi=10.1140%2fepjds%2fs13688-017-0127-3&partnerID=40&md5=1a52330c4546c91eadf777d4ce404395","Social networks are made out of strong and weak ties having very different structural and dynamical properties. But what features of human interaction build a strong tie? Here we approach this question from a practical way by finding what are the properties of social interactions that make ties more persistent and thus stronger to maintain social interactions in the future. Using a large longitudinal mobile phone database we build a predictive model of tie persistence based on intensity, intimacy, structural and temporal patterns of social interaction. While our results confirm that structural (embeddedness) and intensity (number of calls) features are correlated with tie persistence, temporal features of communication events are better and more efficient predictors for tie persistence. Specifically, although communication within ties is always bursty we find that ties that are more bursty than the average are more likely to decay, signaling that tie strength is not only reflected in the intensity or topology of the network, but also on how individuals distribute time or attention across their relationships. We also found that stable relationships have and require a constant rhythm and if communication is halted for more than 8 times the previous communication frequency, most likely the tie will decay. Our results not only are important to understand the strength of social relationships but also to unveil the entanglement between the different temporal scales in networks, from microscopic tie burstiness and rhythm to macroscopic network evolution. © 2018, The Author(s)."
"10.1140/epjds/s13688-017-0126-4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85035241183&doi=10.1140%2fepjds%2fs13688-017-0126-4&partnerID=40&md5=69d268455c2d6203ca6ceb8c6368e701","There has been growing interest in leveraging Web-based social and communication technologies for better crisis response. How might the Web platforms be used as an observatory to systematically understand the dynamics of the public’s attention during disaster events? And how could we monitor such attention in a cost-effective way? In this work, we propose an ‘attention shift network’ framework to systematically observe, measure, and analyze the dynamics of collective attention in response to real-world exogenous shocks such as disasters. Through tracing hashtags that appeared in Twitter users’ complete timeline around several violent terrorist attacks, we study the properties of network structures and reveal the temporal dynamics of the collective attention across multiple disasters. Further, to enable an efficient monitoring of the collective attention dynamics, we propose an effective stochastic sampling approach that accounts for the users’ hashtag adoption frequency, connectedness and diversity, as well as data variability. We conduct extensive experiments to show that the proposed sampling approach significantly outperforms several alternative methods in both retaining the network structures and preserving the information with a small set of sampling targets, suggesting the utility of the proposed method in various realistic settings. © 2017, The Author(s)."
"10.1140/epjds/s13688-017-0125-5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85033497874&doi=10.1140%2fepjds%2fs13688-017-0125-5&partnerID=40&md5=dc61813c7a9bb2c88f93f70860e14580","Emerging trends in the use of smartphones, online mapping applications, and social media, in addition to the geo-located data they generate, provide opportunities to trace users’ socio-economic activities in an unprecedentedly granular and direct fashion and have triggered a revolution in empirical research. These vast mobile data offer new perspectives and approaches to measure economic dynamics, and they are broadening the social science and economics fields. In this paper, we explore the potential for using mobile data to measure economic activity in China from a bottom-up view. First, we build indices for gauging employment and consumer trends based on billions of geo-positioning data. Second, we advance the estimation of offline store foot traffic via location search data derived from Baidu Maps, which is then applied to predict Apple’s revenues in China and to accurately detect box-office fraud. Third, we construct consumption indicators to track trends in various service sector industries and verify them with several existing indicators. To the best of our knowledge, this is the first study to measure the world’s second-largest economy by mining such unprecedentedly large-scale and fine-granular spatial-temporal data. In this way, our research provides new approaches and insights into measuring economic activity. © 2017, The Author(s)."
"10.1140/epjds/s13688-017-0124-6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032590846&doi=10.1140%2fepjds%2fs13688-017-0124-6&partnerID=40&md5=f8f6b218de2fd30a91f18828f86eb509","Understanding and modeling the mobility of individuals is of paramount importance for public health. In particular, mobility characterization is key to predict the spatial and temporal diffusion of human-transmitted infections. However, the mobility behavior of a person can also reveal relevant information about her/his health conditions. In this paper, we study the impact of people mobility behaviors for predicting the future presence of flu-like and cold symptoms (i.e. fever, sore throat, cough, shortness of breath, headache, muscle pain, malaise, and cold). To this end, we use the mobility traces from mobile phones and the daily self-reported flu-like and cold symptoms of 29 individuals from February 20, 2013 to March 21, 2013. First of all, we demonstrate that daily symptoms of an individual can be predicted by using his/her mobility trace characteristics (e.g. total displacement, radius of gyration, number of unique visited places, etc.). Then, we present and validate models that are able to successfully predict the future presence of symptoms by analyzing the mobility patterns of our individuals. The proposed methodology could have a societal impact opening the way to customized mobile phone applications, which may detect and suggest to the user specific actions in order to prevent disease spreading and minimize the risk of contagion. © 2017, The Author(s)."
"10.1140/epjds/s13688-017-0121-9","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032565890&doi=10.1140%2fepjds%2fs13688-017-0121-9&partnerID=40&md5=510182caca37e3a50a60afd0b81bf9b2","The emergence and global adoption of social media has rendered possible the real-time estimation of population-scale sentiment, an extraordinary capacity which has profound implications for our understanding of human behavior. Given the growing assortment of sentiment-measuring instruments, it is imperative to understand which aspects of sentiment dictionaries contribute to both their classification accuracy and their ability to provide richer understanding of texts. Here, we perform detailed, quantitative tests and qualitative assessments of 6 dictionary-based methods applied to 4 different corpora, and briefly examine a further 20 methods. We show that while inappropriate for sentences, dictionary-based methods are generally robust in their classification accuracy for longer texts. Most importantly they can aid understanding of texts with reliable and meaningful word shift graphs if (1) the dictionary covers a sufficiently large portion of a given text’s lexicon when weighted by word usage frequency; and (2) words are scored on a continuous scale. © 2017, The Author(s)."
"10.1140/epjds/s13688-017-0123-7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031496158&doi=10.1140%2fepjds%2fs13688-017-0123-7&partnerID=40&md5=d1137c322b7e5398ed7b1d6346489b8e","We propose an indicator to measure the degree to which a particular news article is novel, as well as an indicator to measure the degree to which a particular news item attracts attention from investors. The novelty measure is obtained by comparing the extent to which a particular news article is similar to earlier news articles, and an article is regarded as novel if there was no similar article before it. On the other hand, we say a news item receives a lot of attention and thus is highly topical if it is simultaneously reported by many news agencies and read by many investors who receive news from those agencies. The topicality measure for a news item is obtained by counting the number of news articles whose content is similar to an original news article but which are delivered by other news agencies. To check the performance of the indicators, we empirically examine how these indicators are correlated with intraday financial market indicators such as the number of transactions and price volatility. Specifically, we use a dataset consisting of over 90 million business news articles reported in English and a dataset consisting of minute-by-minute stock prices on the New York Stock Exchange and the NASDAQ Stock Market from 2003 to 2014, and show that price volatility, transaction volumes, and the number of transactions exhibited a significant response to a news article when it was novel and topical. © 2017, The Author(s)."
"10.1140/epjds/s13688-017-0122-8","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030848172&doi=10.1140%2fepjds%2fs13688-017-0122-8&partnerID=40&md5=1f4449add81d40c2442569a4badcac1b","The immense growth of the social Web, which has made a large amount of user data easily and publicly available, has opened a whole new spectrum for research in social behavioral sciences. However, as the volume of social media content increases at a very fast rate, it becomes extremely difficult to systematically obtain high-level information from this data. As a consequence, tasks related to the analysis of historical news events based on social media data have not been explored, which limits any type of comparative historical research, causality analysis, and discovery of knowledge from patterns extracted from aggregated social media event information. In this work, we target this issue by proposing a compact high-level representation of news events using social media information. This representation explicitly includes temporal information about the event and information about locations, in particular of geopolitical entities. We call this a spatio-temporal context-aware event representation. Our hypothesis is that by including social, temporal, and spatial information in the event representation, we are enabling the analysis of historical world news from a social and geopolitical perspective. This facilitates, new information retrieval tasks related to historical event information extraction and international relations analysis. We support our claims by presenting two applications of this idea: the first, a visual tool, named Galean, for retrieval and exploration of historical news events within their geopolitical and temporal context. The second, a quantitative analysis of a 2-year Twitter dataset of news events reported by U.S. and U.K. media, which we explore using data mining techniques on our event representations. We present two case studies of event exploration using Galean and user evaluation of this tool, as well as details of our data mining empirical results. © 2017, The Author(s)."
"10.1140/epjds/s13688-017-0120-x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030454223&doi=10.1140%2fepjds%2fs13688-017-0120-x&partnerID=40&md5=b584f2456cd77fba37c5ecf999dd33ca","The emergence of large stores of transactional data generated by increasing use of digital devices presents a huge opportunity for policymakers to improve their knowledge of the local environment and thus make more informed and better decisions. A research frontier is hence emerging which involves exploring the type of measures that can be drawn from data stores such as mobile phone logs, Internet searches and contributions to social media platforms and the extent to which these measures are accurate reflections of the wider population. This paper contributes to this research frontier, by exploring the extent to which local commuting patterns can be estimated from data drawn from Twitter. It makes three contributions in particular. First, it shows that heuristics applied to geolocated Twitter data offer a good proxy for local commuting patterns; one which outperforms the current best method for estimating these patterns (the radiation model). This finding is of particular significance because we make use of relatively coarse geolocation data (at the city level) and use simple heuristics based on frequency counts. Second, it investigates sources of error in the proxy measure, showing that the model performs better on short trips with higher volumes of commuters; it also looks at demographic biases but finds that, surprisingly, measurements are not significantly affected by the fact that the demographic makeup of Twitter users differs significantly from the population as a whole. Finally, it looks at potential ways of going beyond simple frequency heuristics by incorporating temporal information into models. © 2017, The Author(s)."
"10.1140/epjds/s13688-017-0119-3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029741812&doi=10.1140%2fepjds%2fs13688-017-0119-3&partnerID=40&md5=df166402d58ecb9861f0753b7dbdd9cc","Pokémon Go, a location-based game that uses augmented reality techniques, received unprecedented media coverage due to claims that it allowed for greater access to public spaces, increasing the number of people out on the streets, and generally improving health, social, and security indices. However, the true impact of Pokémon Go on people’s mobility patterns in a city is still largely unknown. In this paper, we perform a natural experiment using data from mobile phone networks to evaluate the effect of Pokémon Go on the pulse of a big city: Santiago, capital of Chile. We found significant effects of the game on the floating population of Santiago compared to movement prior to the game’s release in August 2016: in the following week, up to 13.8% more people spent time outside at certain times of the day, even if they do not seem to go out of their usual way. These effects were found by performing regressions using count models over the states of the cellphone network during each day under study. The models used controlled for land use, daily patterns, and points of interest in the city. Our results indicate that, on business days, there are more people on the street at commuting times, meaning that people did not change their daily routines but slightly adapted them to play the game. Conversely, on Saturday and Sunday night, people indeed went out to play, but favored places close to where they live. Even if the statistical effects of the game do not reflect the massive change in mobility behavior portrayed by the media, at least in terms of expanse, they do show how ‘the street’ may become a new place of leisure. This change should have an impact on long-term infrastructure investment by city officials, and on the drafting of public policies aimed at stimulating pedestrian traffic. © 2017, The Author(s)."
"10.1140/epjds/s13688-017-0117-5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029155002&doi=10.1140%2fepjds%2fs13688-017-0117-5&partnerID=40&md5=cf840012b018fd37f6a2f1e0c26f336b","We analyze large-scale data sets about collaborations from two different domains: economics, specifically 22,000 R&D alliances between 14,500 firms, and science, specifically 300,000 co-authorship relations between 95,000 scientists. Considering the different domains of the data sets, we address two questions: (a) to what extent do the collaboration networks reconstructed from the data share common structural features, and (b) can their structure be reproduced by the same agent-based model. In our data-driven modeling approach we use aggregated network data to calibrate the probabilities at which agents establish collaborations with either newcomers or established agents. The model is then validated by its ability to reproduce network features not used for calibration, including distributions of degrees, path lengths, local clustering coefficients and sizes of disconnected components. Emphasis is put on comparing domains, but also sub-domains (economic sectors, scientific specializations). Interpreting the link probabilities as strategies for link formation, we find that in R&D collaborations newcomers prefer links with established agents, while in co-authorship relations newcomers prefer links with other newcomers. Our results shed new light on the long-standing question about the role of endogenous and exogenous factors (i.e., different information available to the initiator of a collaboration) in network formation. © 2017, The Author(s)."
"10.1140/epjds/s13688-017-0116-6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028666683&doi=10.1140%2fepjds%2fs13688-017-0116-6&partnerID=40&md5=85cd187f697b0982dbaf149f04a4f992","Contemporary collective action, much of which involves social media and other Internet-based platforms, leaves a digital imprint which may be harvested to better understand the dynamics of mobilization. Petition signing is an example of collective action which has gained in popularity with rising use of social media and provides such data for the whole population of petition signatories for a given platform. This paper tracks the growth curves of all 20,000 petitions to the UK government petitions website (http://epetitions.direct.gov.uk) and 1,800 petitions to the US White House site (https://petitions.whitehouse.gov), analyzing the rate of growth and outreach mechanism. Previous research has suggested the importance of the first day to the ultimate success of a petition, but has not examined early growth within that day, made possible here through hourly resolution in the data. The analysis shows that the vast majority of petitions do not achieve any measure of success; over 99 percent fail to get the 10,000 signatures required for an official response and only 0.1 percent attain the 100,000 required for a parliamentary debate (0.7 percent in the US). We analyze the data through a multiplicative process model framework to explain the heterogeneous growth of signatures at the population level. We define and measure an average outreach factor for petitions and show that it decays very fast (reducing to 0.1% after 10 hours in the UK and 30 hours in the US). After a day or two, a petition’s fate is virtually set. The findings challenge conventional analyses of collective action from economics and political science, where the production function has been assumed to follow an S-shaped curve. © 2017, The Author(s)."
"10.1140/epjds/s13688-017-0115-7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028423949&doi=10.1140%2fepjds%2fs13688-017-0115-7&partnerID=40&md5=423aa66930671853257eb4901bd419e3","Existing studies have developed different indices based on various approaches including network connectivity, delay time and flow capacity, estimating the traffic reliability states from different angles. However, these indices mainly estimate traffic reliability from single view and rarely consider the combined effect of city traffic dynamics and underlying network structure. Based on percolation theory, Li et al. has developed a traffic reliability index to address this issue (Proc. Natl. Acad. Sci. USA 112(3):669-672, 2015) [1]. Here we compare this percolation-based index with one of the well-known index - congestion delay index (CDI). Using real traffic data of Beijing and Shenzhen (two large cities in China), we compare the two indices in the macroscopic trends and microscopic extreme values. The two indices are found to indicate the state of real-time traffic reliability in different consideration. Our results can be used for better evaluation of traffic system reliability and mitigation measures of traffic jams. © 2017, The Author(s)."
"10.1140/epjds/s13688-017-0114-8","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028308213&doi=10.1140%2fepjds%2fs13688-017-0114-8&partnerID=40&md5=24f80420e1a9baff64907161c7f17c63","The structure of scientific collaborations has been the object of intense study both for its importance for innovation and scientific advancement, and as a model system for social group coordination and formation thanks to the availability of authorship data. Over the last years, complex networks approach to this problem have yielded important insights and shaped our understanding of scientific communities. In this paper we propose to complement the picture provided by network tools with that coming from using simplicial descriptions of publications and the corresponding topological methods. We show that it is natural to extend the concept of triadic closure to simplicial complexes and show the presence of strong simplicial closure. Focusing on the differences between scientific fields, we find that, while categories are characterized by different collaboration size distributions, the distributions of how many collaborations to which an author is able to participate is conserved across fields pointing to underlying attentional and temporal constraints. We then show that homological cycles, that can intuitively be thought as hole in the network fabric, are an important part of the underlying community linking structure. © 2017, The Author(s)."
"10.1140/epjds/s13688-017-0109-5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027692546&doi=10.1140%2fepjds%2fs13688-017-0109-5&partnerID=40&md5=e8b442dfb0cb708833bdefdffaa10e12","Persistent homology (PH) is a method used in topological data analysis (TDA) to study qualitative features of data that persist across multiple scales. It is robust to perturbations of input data, independent of dimensions and coordinates, and provides a compact representation of the qualitative features of the input. The computation of PH is an open area with numerous important and fascinating challenges. The field of PH computation is evolving rapidly, and new algorithms and software implementations are being updated and released at a rapid pace. The purposes of our article are to (1) introduce theory and computational methods for PH to a broad range of computational scientists and (2) provide benchmarks of state-of-the-art implementations for the computation of PH. We give a friendly introduction to PH, navigate the pipeline for the computation of PH with an eye towards applications, and use a range of synthetic and real-world data sets to evaluate currently available open-source implementations for the computation of PH. Based on our benchmarking, we indicate which algorithms and implementations are best suited to different types of data sets. In an accompanying tutorial, we provide guidelines for the computation of PH. We make publicly available all scripts that we wrote for the tutorial, and we make available the processed version of the data sets used in the benchmarking. © 2017, The Author(s)."
"10.1140/epjds/s13688-017-0110-z","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027011738&doi=10.1140%2fepjds%2fs13688-017-0110-z&partnerID=40&md5=12591ab9e3de234208ab344a688f4cec","Using Instagram data from 166 individuals, we applied machine learning tools to successfully identify markers of depression. Statistical features were computationally extracted from 43,950 participant Instagram photos, using color analysis, metadata components, and algorithmic face detection. Resulting models outperformed general practitioners’ average unassisted diagnostic success rate for depression. These results held even when the analysis was restricted to posts made before depressed individuals were first diagnosed. Human ratings of photo attributes (happy, sad, etc.) were weaker predictors of depression, and were uncorrelated with computationally-generated features. These results suggest new avenues for early screening and detection of mental illness. © 2017, The Author(s)."
"10.1140/epjds/s13688-017-0113-9","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027004062&doi=10.1140%2fepjds%2fs13688-017-0113-9&partnerID=40&md5=54b22d11431b450329ade08a2e6e9220","In a representative democracy it is important that politicians have knowledge of the desires, aspirations and concerns of their constituents. Opportunities to gauge these opinions are however limited and, in the era of novel data, thoughts turn to what alternative, secondary, data sources may be available to keep politicians informed about local concerns. One such source of data are signatories to electronic petitions (e-petitions). Such e-petitions have risen greatly in popularity over the past decade and allow members of the public to initiate and sign an e-petition online, with popular e-petitions resulting in media attention, a response from the government or ultimately a debate in parliament. These data are thus novel in their availability and have not yet been widely used for research purposes. In this article we will use the e-petition data to show how semantic classes of Westminster Parliamentary constituencies, fitted as Gaussian finite mixture models via EM algorithm, can be used to typify constituencies. We identify four classes: Domestic Liberals; International Liberals; Nostalgic Brits and Rural Concerns, and illustrate how they map onto electoral results. The findings and the utility of this approach to incorporate new e-petitions and adapt to changes in electoral geography are discussed. © 2017, The Author(s)."
"10.1140/epjds/s13688-017-0112-x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85023764650&doi=10.1140%2fepjds%2fs13688-017-0112-x&partnerID=40&md5=2fce789518df1cac7a9e70d7e3219c43","By modeling macro-economical indicators using digital traces of human activities on mobile or social networks, we can provide important insights to processes previously assessed via paper-based surveys or polls only. We collected aggregated workday activity timelines of US counties from the normalized number of messages sent in each hour on the online social network Twitter. In this paper, we show how county employment and unemployment statistics are encoded in the daily rhythm of people by decomposing the activity timelines into a linear combination of two dominant patterns. The mixing ratio of these patterns defines a measure for each county, that correlates significantly with employment (0.46 ± 0.02) and unemployment rates (− 0.34 ± 0.02). Thus, the two dominant activity patterns can be linked to rhythms signaling presence or lack of regular working hours of individuals. The analysis could provide policy makers a better insight into the processes governing employment, where problems could not only be identified based on the number of officially registered unemployed, but also on the basis of the digital footprints people leave on different platforms. © 2017, The Author(s)."
"10.1140/epjds/s13688-017-0111-y","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021772851&doi=10.1140%2fepjds%2fs13688-017-0111-y&partnerID=40&md5=dff6873bb61ba8f6cbe311caddfb3088","Social media expose millions of users every day to information campaigns - some emerging organically from grassroots activity, others sustained by advertising or other coordinated efforts. These campaigns contribute to the shaping of collective opinions. While most information campaigns are benign, some may be deployed for nefarious purposes, including terrorist propaganda, political astroturf, and financial market manipulation. It is therefore important to be able to detect whether a meme is being artificially promoted at the very moment it becomes wildly popular. This problem has important social implications and poses numerous technical challenges. As a first step, here we focus on discriminating between trending memes that are either organic or promoted by means of advertisement. The classification is not trivial: ads cause bursts of attention that can be easily mistaken for those of organic trends. We designed a machine learning framework to classify memes that have been labeled as trending on Twitter. After trending, we can rely on a large volume of activity data. Early detection, occurring immediately at trending time, is a more challenging problem due to the minimal volume of activity data that is available prior to trending. Our supervised learning framework exploits hundreds of time-varying features to capture changing network and diffusion patterns, content and sentiment information, timing signals, and user meta-data. We explore different methods for encoding feature time series. Using millions of tweets containing trending hashtags, we achieve 75% AUC score for early detection, increasing to above 95% after trending. We evaluate the robustness of the algorithms by introducing random temporal shifts on the trend time series. Feature selection analysis reveals that content cues provide consistently useful signals; user features are more informative for early detection, while network and timing features are more helpful once more data is available. © 2017, The Author(s)."
"10.1140/epjds/s13688-017-0107-7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021144032&doi=10.1140%2fepjds%2fs13688-017-0107-7&partnerID=40&md5=12c9268d999b89201a60efedd7dca2ed","Next place prediction algorithms are invaluable tools, capable of increasing the efficiency of a wide variety of tasks, ranging from reducing the spreading of diseases to better resource management in areas such as urban planning. In this work we estimate upper and lower limits on the predictability of human mobility to help assess the performance of competing algorithms. We do this using GPS traces from 604 individuals participating in a multi year long experiment, The Copenhagen Networks study. Earlier works, focusing on the prediction of a participant’s whereabouts in the next time bin, have found very high upper limits (> 90 %). We show that these upper limits are highly dependent on the choice of a spatiotemporal scales and mostly reflect stationarity, i.e. the fact that people tend to not move during small changes in time. This leads us to propose an alternative approach, which aims to predict the next location, rather than the location in the next bin. Our approach is independent of the temporal scale and introduces a natural length scale. By removing the effects of stationarity we show that the predictability of the next location is significantly lower (71%) than the predictability of the location in the next bin. © 2017, The Author(s)."
"10.1140/epjds/s13688-017-0102-z","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020618360&doi=10.1140%2fepjds%2fs13688-017-0102-z&partnerID=40&md5=f4a3431628e0ef96141cae35a7f19235","Military populations present a small, unique community whose mental and physical health impacts the security of the nation. Recent literature has explored social media’s ability to enhance disease surveillance and characterize distinct communities with encouraging results. We present a novel analysis of the relationships between influenza-like illnesses (ILI) clinical data and affects (i.e., emotions and sentiments) extracted from social media around military facilities. Our analyses examine (1) differences in affects expressed by military and control populations, (2) affect changes over time by users, (3) differences in affects expressed during high and low ILI seasons, and (4) correlations and cross-correlations between ILI clinical visits and affects from an unprecedented scale - 171M geo-tagged tweets across 31 global geolocations. Key findings include: Military and control populations differ in the way they express affects in social media over space and time. Control populations express more positive and less negative sentiments and less sadness, fear, disgust, and anger emotions than military. However, affects expressed in social media by both populations within the same area correlate similarly with ILI visits to military health facilities. We have identified potential responsible cofactors leading to location variability, e.g., region or state locale, military service type and/or the ratio of military to civilian populations. For most locations, ILI proportions positively correlate with sadness and neutral sentiment, which are the affects most often expressed during high ILI season. The ILI proportions negatively correlate with fear, disgust, surprise, and positive sentiment. These results are similar to the low ILI season where anger, surprise, and positive sentiment are highest. Finally, cross-correlation analysis shows that most affects lead ILI clinical visits, i.e. are predictive of ILI data, with affect-ILI leading intervals dependent on geolocation and affect type. Overall, information gained in this study exemplifies a usage of social media data to understand the correlation between psychological behavior and health in the military population and the potential for use of social media affects for prediction of ILI cases. © 2017, The Author(s)."
"10.1140/epjds/s13688-017-0108-6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020521512&doi=10.1140%2fepjds%2fs13688-017-0108-6&partnerID=40&md5=1470699b33bc9ba0fe2975f5cd62ff4c","A longitudinal mobile phone data that include both location and communication logs is analyzed to infer social influence in terms of ego-network effect in the commute mode choice. The results show that person’s strong ties are more important to determine if driving is the person’s transport mode choice, whereas weak ties are more important to determine if public transit is the person’s choice. It is also evident from the results that social ties that are geographically closer are more influential for the commute mode choice than the ones who are farther away. For public transit, access distance is also one of the influential factors. The portion of transit users decreases as the access distance becomes larger. Moreover, social network is shown to influence the commute mode choice, as the likelihood of choosing a particular mode choice rises with the portion of social ties choosing that specific mode. © 2017, The Author(s)."
"10.1140/epjds/s13688-017-0105-9","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020503332&doi=10.1140%2fepjds%2fs13688-017-0105-9&partnerID=40&md5=a909ccaac8ac345a0db64d6075583286","The availability of big data recorded from massively multiplayer online role-playing games (MMORPGs) allows us to gain a deeper understanding of the potential connection between individuals’ network positions and their economic outputs. We use a statistical filtering method to construct dependence networks from weighted friendship networks of individuals. We investigate the 30 distinct motif positions in the 13 directed triadic motifs which represent microscopic dependences among individuals. Based on the structural similarity of motif positions, we further classify individuals into different groups. The node position diversity of individuals is found to be positively correlated with their economic outputs. We also find that the economic outputs of leaf nodes are significantly lower than that of the other nodes in the same motif. Our findings shed light on understanding the influence of network structure on economic activities and outputs in socioeconomic systems. © 2017, The Author(s)."
"10.1140/epjds/s13688-017-0103-y","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020201729&doi=10.1140%2fepjds%2fs13688-017-0103-y&partnerID=40&md5=231f6de3ae54276231b6407932a21215","Humans interact through numerous communication channels to build and maintain social connections: they meet face-to-face, make phone calls or send text messages, and interact via social media. Although it is known that the network of physical contacts, for example, is distinct from the network arising from communication events via phone calls and instant messages, the extent to which these networks differ is not clear. We show here that the network structure of these channels show large structural variations. The various channels account for diverse relationships between pairs of individuals and the corresponding interaction patterns across channels differ to an extent that social ties cannot easily be reduced to a single layer. Each network of interactions, however, contains both central and peripheral individuals: central members are characterized by higher connectivity and can reach a large fraction of the network within a low number of steps, in contrast to the nodes on the periphery. The origin and purpose of each communication network also determine the role of their respective central members: highly connected individuals in the person-to-person networks interact with their environment in a regular manner, while members central in the social communication networks display irregular behavior with respect to their physical contacts and are more active through irregular social events. Our results suggest that due to the inherently different functions of communication channels, each one favors different social behaviors and different strategies for interacting with the environment. These findings can facilitate the understanding of the varying roles and impact individuals have on the population, which can further shed light on the prediction and prevention of epidemic outbreaks, or information propagation. © 2017, The Author(s)."
"10.1140/epjds/s13688-017-0100-1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019394310&doi=10.1140%2fepjds%2fs13688-017-0100-1&partnerID=40&md5=d6c3696de094dfff8de28801899e31d4","Most individuals in social networks experience a so-called Friendship Paradox: they are less popular than their friends on average. This effect may explain recent findings that widespread social network media use leads to reduced happiness. However the relation between popularity and happiness is poorly understood. A Friendship paradox does not necessarily imply a Happiness paradox where most individuals are less happy than their friends. Here we report the first direct observation of a significant Happiness Paradox in a large-scale online social network of 39,110 Twitter users. Our results reveal that popular individuals are indeed happier and that a majority of individuals experience a significant Happiness paradox. The magnitude of the latter effect is shaped by complex interactions between individual popularity, happiness, and the fact that users strongly cluster by similar level of happiness. Our results indicate that the topology of online social networks, combined with how happiness is distributed in some populations, may be associated with significant psycho-social effects. © 2017, The Author(s)."
"10.1140/epjds/s13688-017-0101-0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019392777&doi=10.1140%2fepjds%2fs13688-017-0101-0&partnerID=40&md5=69f43bf85107954dc15ba91e44d82422","Gender differences is a phenomenon around the world actively researched by social scientists. Traditionally, the data used to support such studies is manually obtained, often through surveys with volunteers. However, due to their inherent high costs because of manual steps, such traditional methods do not quickly scale to large-size studies. We here investigate a particular aspect of gender differences: preferences for venues. To that end, we explore the use of check-in data collected from Foursquare to estimate cultural gender preferences for venues in the physical world. For that, we first demonstrate that by analyzing the check-in data in various regions of the world we can find significant differences in preferences for specific venues between gender groups. Some of these significant differences reflect well-known cultural patterns. Moreover, we also gathered evidence that our methodology offers useful information about gender preference for venues in a given region in the real world. This suggests that gender and venue preferences observed may not be independent. Our results indicate that our proposed methodology could be a promising tool to support studies on gender preferences for venues at different spatial granularities around the world, being faster and cheaper than traditional methods, besides quickly capturing changes in the real world. © 2017, The Author(s)."
"10.1140/epjds/s13688-017-0099-3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019244546&doi=10.1140%2fepjds%2fs13688-017-0099-3&partnerID=40&md5=92bfe87a64fa1df50fc511201fab3381","Mobile phones are one of the fastest growing technologies in the developing world with global penetration rates reaching 90%. Mobile phone data, also called CDR, are generated everytime phones are used and recorded by carriers at scale. CDR have generated groundbreaking insights in public health, official statistics, and logistics. However, the fact that most phones in developing countries are prepaid means that the data lacks key information about the user, including gender and other demographic variables. This precludes numerous uses of this data in social science and development economic research. It furthermore severely prevents the development of humanitarian applications such as the use of mobile phone data to target aid towards the most vulnerable groups during crisis. We developed a framework to extract more than 1400 features from standard mobile phone data and used them to predict useful individual characteristics and group estimates. We here present a systematic cross-country study of the applicability of machine learning for dataset augmentation at low cost. We validate our framework by showing how it can be used to reliably predict gender and other information for more than half a million people in two countries. We show how standard machine learning algorithms trained on only 10,000 users are sufficient to predict individual’s gender with an accuracy ranging from 74.3 to 88.4% in a developed country and from 74.5 to 79.7% in a developing country using only metadata. This is significantly higher than previous approaches and, once calibrated, gives highly accurate estimates of gender balance in groups. Performance suffers only marginally if we reduce the training size to 5,000, but significantly decreases in a smaller training set. We finally show that our indicators capture a large range of behavioral traits using factor analysis and that the framework can be used to predict other indicators of vulnerability such as age or socio-economic status. Mobile phone data has a great potential for good and our framework allows this data to be augmented with vulnerability and other information at a fraction of the cost. © 2017, The Author(s)."
"10.1140/epjds/s13688-017-0098-4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018499478&doi=10.1140%2fepjds%2fs13688-017-0098-4&partnerID=40&md5=2fed66f1387e0be8f24d3f8a2206512d","The emerging domain of data-enabled science necessitates development of algorithms and tools for knowledge discovery. Human interaction with data through well-constructed graphical representation can take special advantage of our visual ability to identify patterns. We develop a data visualization framework, called BiFold, for exploratory analysis of bipartite datasets that describe binary relationships between groups of objects. Typical data examples would include voting records, organizational memberships, and pairwise associations, or other binary datasets. BiFold provides a low dimensional embedding of data that represents similarity by visual nearness, analogous to Multidimensional Scaling (MDS). The unique and new feature of BiFold is its ability to simultaneously capture both within-group and between-group relationships among objects, enhancing knowledge discovery. We benchmark BiFold using the Southern Women Dataset, where social groups are now visually evident. We construct BiFold plots for two US voting datasets: For the presidential election outcomes since 1976, BiFold illustrates the evolving geopolitical structures that underlie these election results. For Senate congressional voting, BiFold identifies a partisan coordinate, separating senators into two parties while simultaneously visualizing a bipartisan-coalition coordinate which captures the ultimate fate of the bills (pass/fail). Finally, we consider a global cuisine dataset of the association between recipes and food ingredients. BiFold allows us to visually compare and contrast cuisines while also allowing identification of signature ingredients of individual cuisines. © 2017, The Author(s)."
"10.1140/epjds/s13688-016-0097-x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85008698574&doi=10.1140%2fepjds%2fs13688-016-0097-x&partnerID=40&md5=3711a5a83401325f73cdbebb3ca2f5b4","Social networks require active relationship maintenance if they are to be kept at a constant level of emotional closeness. For primates, including humans, failure to interact leads inexorably to a decline in relationship quality, and a consequent loss of the benefits that derive from individual relationships. As a result, many social species compensate for weakened relationships by investing more heavily in them. Here we study how humans behave in similar situations, using data from mobile call detail records from a European country. For the less frequent contacts between pairs of communicating individuals we observe a logarithmic dependence of the duration of the succeeding call on the time gap with the previous call. We find that such behaviour is likely when the individuals in these dyadic pairs have the same gender and are in the same age bracket as well as being geographically distant. Our results indicate that these pairs deliberately invest more time in communication so as to reinforce their social bonding and prevent their relationships decaying when these are threatened by lack of interaction. © 2017, The Author(s)."
"10.1186/s40537-017-0095-2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042372255&doi=10.1186%2fs40537-017-0095-2&partnerID=40&md5=3614973de63440958ac6497f8281b4a5","Classifying short texts to one category or clustering semantically related texts is challenging, and the importance of both is growing due to the rise of microblogging platforms, digital news feeds, and the like. We can accomplish this classifying and clustering with the help of a deep neural network which produces compact binary representations of a short text, and can assign the same category to texts that have similar binary representations. But problems arise when there is little contextual information on the short texts, which makes it difficult for the deep neural network to produce similar binary codes for semantically related texts. We propose to address this issue using semantic enrichment. This is accomplished by taking the nouns, and verbs used in the short texts and generating the concepts and co-occurring words with the help of those terms. The nouns are used to generate concepts within the given short text, whereas the verbs are used to prune the ambiguous context (if any) present in the text. The enriched text then goes through a deep neural network to produce a prediction label for that short text representing it’s category. © 2017, The Author(s)."
"10.1186/s40537-017-0108-1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85039920313&doi=10.1186%2fs40537-017-0108-1&partnerID=40&md5=92b6cc591e24b887bee7967be410c65e","The huge variety of NoSQL Big Data has tossed a need for new pathways to store, process and analyze it. The quantum of data created is inconceivable along with a mixed breath of unknown veracity and creative visualization. The new trials of frameworks help to find substantial unidentified values from massive data sets. They have added an exceptional dimension to the pre-processing and contextual conversion of the data sets for needful analysis. In addition, handling of ambitious imbalanced data sets has acknowledged an intimation of alarm. Traditional classifiers are unable to discourse the precise need of grouping for such data sets. Over_sampling of the minority classes help to improve the performance. Updated Class Purity Maximization Over_Sampling Technique (UCPMOT) is a rationalized technique proposed to handle imbalanced data sets using exclusive safe-level based synthetic sample creation. It addresses the multi-class problem in alignment to a newly induced method namely lowest versus highest. The projected technique experiments with several data sets from the UCI repository. The underlying bed of mapreduce environment encompasses the distributed processing approach on Apache Hadoop framework. Several classifiers help to authorize the classification results using parameters like F-measure and AUC values. The experimental conclusions quote the dominance of UCPMOT over the benchmarking techniques. © 2017, The Author(s)."
"10.1186/s40537-017-0099-y","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85039069353&doi=10.1186%2fs40537-017-0099-y&partnerID=40&md5=8e57391894d8d36c47442375cc7932f0","Socio-economic status measurement is an ongoing problem where different suggested measurements are given by researchers. This work investigates a socio-economic status measurement derived from natural correlations of variables which can better and meaningfully cluster African countries for the level of status. The researcher used 48 African countries socio-economic yearly time series data from 1993 to 2013 of IMF 2013 data set for data management (i.e, 2737 variables for 21 years), however, the analysis is reasonably done based on recent 14 years time series data. In data management, missing values are treated (imputed) by using regression estimates, Lagrange interpolation, linear interpolation and linear spline interpolation based on the appropriate method which best fits for the trend of data with minimum error at each time level. From principal component and factor analysis of average time series data, 7 principal factors contributed by 84 variables which explain 70 % of the variation in the data set are suggested as a socio-economic status measuring components and as a result the considered clustering methods (K-mean Method, Average linkage method, Ward’s method and Bootstrap Ward’s method) are agreed on six clusters of countries, those are statistically significant at 95 % , where as three countries each where suggested as outlier-countries made an individual cluster. © 2017, The Author(s)."
"10.1186/s40537-017-0109-0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85038874951&doi=10.1186%2fs40537-017-0109-0&partnerID=40&md5=361597f8c59b090f88c1e0a703298eb5","Common clustering algorithms require multiple scans of all the data to achieve convergence, and this is prohibitive when large databases, with data arriving in streams, must be processed. Some algorithms to extend the popular K-means method to the analysis of streaming data are present in literature since 1998 (Bradley et al. in Scaling clustering algorithms to large databases. In: KDD. p. 9–15, 1998; O’Callaghan et al. in Streaming-data algorithms for high-quality clustering. In: Proceedings of IEEE international conference on data engineering. p. 685, 2001), based on the memorization and recursive update of a small number of summary statistics, but they either don’t take into account the specific variability of the clusters, or assume that the random vectors which are processed and grouped have uncorrelated components. Unfortunately this is not the case in many practical situations. We here propose a new algorithm to process data streams, with data having correlated components and coming from clusters with different covariance matrices. Such covariance matrices are estimated via an optimal double shrinkage method, which provides positive definite estimates even in presence of a few data points, or of data having components with small variance. This is needed to invert the matrices and compute the Mahalanobis distances that we use for the data assignment to the clusters. We also estimate the total number of clusters from the data. © 2017, The Author(s)."
"10.1186/s40537-017-0106-3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85038125231&doi=10.1186%2fs40537-017-0106-3&partnerID=40&md5=ecf68c46453bf9a76005fb831bb17700","This paper addresses the problem of designing an efficient platform for pairs-trading implementation in real time. Capturing the stylised features of a spread process, i.e., the evolution of the differential between the returns from a pair of stocks, exhibiting a heavy-tailed mean-reverting process is also dealt with. Likewise, the optimal recovery of time-varying parameters in a return-spread model is tackled. It is important to solve such issues in an integrated manner to carry out the execution of trading strategies in a dynamic market environment. The Kalman and hidden Markov model (HMM) multi-regime dynamic filtering approaches are fused together to provide a powerful method for pairs-trading actualisation. Practitioners’ considerations are taken into account in the way the new filtering method is automated. The synthesis of the HMM’s expectation–maximisation algorithm and Kalman filtering procedure gives rise to a set of self-updating optimal parameter estimates. The method put forward in this paper is a hybridisation of signal-processing algorithms. It highlights the critical role and beneficial utility of data fusion methods. Its appropriateness and novelty support the advancements of accurate predictive analytics involving big financial data sets. The algorithm’s performance is tested on historical return spread between Coca-Cola and Pepsi Inc.’s equities. Through a back-testing trade, a hypothetical trader might earn a non-zero profit under the assumption of no transaction costs and bid-ask spreads. The method’s success is illustrated by a trading simulation. The findings from this work show that there is high potential to gain when the transaction fees are low, and an investor is able to benefit from the proposed interplay of the two filtering methods. © 2017, The Author(s)."
"10.1186/s40537-017-0104-5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85037667019&doi=10.1186%2fs40537-017-0104-5&partnerID=40&md5=220b70d81864cbaa571a00bc8ad9b97c","Big data is predominantly associated with data retrieval, storage, and analytics. Data analytics is prone to privacy violations and data disclosures, which can be partly attributed to the multi-user characteristics of big data environments. Adversaries may link data to external resources, try to access confidential data, or deduce private information from the large number of data pieces that they can obtain. Data anonymization can address some of these concerns by providing tools to mask and can help with concealing the vulnerable data. Currently available anonymization methods, however, are not capable of accommodating the big data scalability, granularity, and performance in efficient manners. In this paper, we introduce a novel framework that implements SQL-like Hadoop ecosystems, incorporating Pig Latin with the additional splitting of data. The splitting reduces data masking and increases the information gained from the anonymized data. Our solution provides a fine-grained masking and concealment, which is based on access level privileges of the user. We also introduce a simple classification technique that can accurately measure the anonymization extent in any anonymized data. The results of testing this classification technique and the proposed sensitivity-based anonymization method using different samples will also be discussed. These results show the significant benefits of the proposed approach, particularly regarding reduced information loss associated with the anonymization processes. © 2017, The Author(s)."
"10.1186/s40537-017-0103-6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85037665704&doi=10.1186%2fs40537-017-0103-6&partnerID=40&md5=49b56c7d3e68a63d921cf80c2e78cb4a","In the era of big data, researchers interested in developing statistical models are challenged with how to achieve parsimony. Usually, some sort of dimension reduction strategy is employed. Classic strategies are often in the form of traditional inference procedures, such as hypothesis testing; however, the increase in computing capabilities has led to the development of more sophisticated methods. In particular, sufficient dimension reduction has emerged as an area of broad and current interest. While these types of dimension reduction strategies have been employed for numerous data problems, they are scantly discussed in the context of analyzing survey data. This paper provides an overview of some classic and modern dimension reduction methods, followed by a discussion of how to use the transformed variables in the context of analyzing survey data. We highlight some of these methods with an analysis of health insurance coverage using the US Census Bureau’s 2015 Planning Database. © 2017, The Author(s)."
"10.1186/s40537-017-0107-2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85037613389&doi=10.1186%2fs40537-017-0107-2&partnerID=40&md5=dc97e68b5a99b1e8d53892569696ddc7","Supervised learning algorithms are nowadays successfully scaling up to datasets that are very large in volume, leveraging the potential of in-memory cluster-computing Big Data frameworks. Still, massive datasets with a number of large-domain categorical features are a difficult challenge for any classifier. Most off-the-shelf solutions cannot cope with this problem. In this work we introduce DAC, a Distributed Associative Classifier. DAC exploits ensemble learning to distribute the training of an associative classifier among parallel workers and improve the final quality of the model. Furthermore, it adopts several novel techniques to reach high scalability without sacrificing quality, among which a preventive pruning of classification rules in the extraction phase based on Gini impurity. We ran experiments on Apache Spark, on a real large-scale dataset with more than 4 billion records and 800 million distinct categories. The results showed that DAC improves on a state-of-the-art solution in both prediction quality and execution time. Since the generated model is human-readable, it can not only classify new records, but also allow understanding both the logic behind the prediction and the properties of the model, becoming a useful aid for decision makers. © 2017, The Author(s)."
"10.1186/s40537-017-0105-4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85036543658&doi=10.1186%2fs40537-017-0105-4&partnerID=40&md5=60821a7a768a70b00693deb4f02b7729","Data mining techniques and extracting patterns from large datasets play a vital role in knowledge discovery. Most of the decision makers encounter a large number of decision rules resulted from association rules mining. Moreover, the volume of datasets brings a new challenge to extract patterns such as the cost of computing and inefficiency to achieve the relevant rules. To overcome these challenges, this paper aims to build a learning model based on FP-growth and Apache Spark framework to process and to extract relevant association rules. We also integrate the multi-criteria decision analysis to prioritize the extracted rules by taking into account the decision makers subjective judgment. We believe that this approach would be a useful model to follow, particularly for decision makers who are suffering from conflicts between extracted rules, and difficulties of building only the most interesting rules. Experimental results on road accidents analysis show that the proposed approach can be efficiently achieved more association rules with a higher accuracy rate and improve the response time of the proposed algorithm. The results make clear that the proposed approach performs well and can provide useful information that could help the decision makers to improve road safety. © 2017, The Author(s)."
"10.1186/s40537-017-0102-7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85035058904&doi=10.1186%2fs40537-017-0102-7&partnerID=40&md5=cef5cd0f1cd2d8fc22f7ff2275c36670","The world of DNA sequencing has not only been a difficult field since it was first worked upon, but it is also growing at an exponential rate. The amount of data involved in DNA searching is huge, thereby normal tools or algorithms are not suitable to handle this degree of data processing. BLAST is a tool given by National Center for Biotechnology Information (NCBI) to compare nucleotide or protein sequences to sequence databases and calculate the statistical significance of matches. Many variants of BLAST such as blastn, blastp, blastx, etc. are used to search for nucleotides, proteins, nucleotides-to-proteins sequences respectively. GPU-BLAST and HBLAST have already been proposed to handle the vast amount of data involved in searching DNA sequencing and they also speedup the searching process. In this article, we propose a new model for searching DNA sequences—HCudaBLAST. It involves CUDA processing and Hadoop combined for efficient searching. The results recorded after implementing HCudaBLAST are shown. This solution combines the multi-core parallelism of GPGPUs and the scalability feature provided by the Hadoop framework. © 2017, The Author(s)."
"10.1186/s40537-017-0101-8","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85034847422&doi=10.1186%2fs40537-017-0101-8&partnerID=40&md5=f029bc0e7e8e5710397f0176f4f25591","Convolutional neural networks are widely adopted for solving problems in image classification. In this work, we aim to gain a better understanding of deep learning through exploring the miss-classified cases in facial and emotion recognitions. Particularly, we propose the backtracking algorithm in order to track down the activated pixels among the last layer of feature maps. We then are able to visualize the facial features that lead to the miss-classifications, by applying the feature tracking algorithm. A comparative analysis of the activated pixels reveals that for the facial recognition, the activations of the common pixels are decisive for the result of classification; for the emotion recognition, the activations of the unique pixels indeed determine the result of classification. © 2017, The Author(s)."
"10.1186/s40537-017-0091-6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85033720936&doi=10.1186%2fs40537-017-0091-6&partnerID=40&md5=5d3a08d1940b6e8d2cd93282ce74ab83","Data sensing, information processing, and networking technologies are being fast embedded into the very fabric of the contemporary city to enable the use of innovative solutions to overcome the challenges of sustainability and urbanization. This has been boosted by the new digital transition in ICT. Driving such transition predominantly are big data analytics and context-aware computing and their increasing amalgamation within a number of urban domains, especially as their functionality involve more or less the same core enabling technologies, namely sensing devices, cloud computing infrastructures, data processing platforms, middleware architectures, and wireless networks. Topical studies tend to only pass reference to such technologies or to largely focus on one particular technology as part of big data and context-aware ecosystems in the realm of smart cities. Moreover, empirical research on the topic, with some exceptions, is generally limited to case studies without the use of any common conceptual frameworks. In addition, relatively little attention has been given to the integration of big data analytics and context-aware computing as advanced forms of ICT in the context of smart sustainable cities. This endeavor is a first attempt to address these two major strands of ICT of the new wave of computing in relation to the informational landscape of smart sustainable cities. Therefore, the purpose of this study is to review and synthesize the relevant literature with the objective of identifying and distilling the core enabling technologies of big data analytics and context-aware computing as ecosystems in relevance to smart sustainable cities, as well as to illustrate the key computational and analytical techniques and processes associated with the functioning of such ecosystems. In doing so, we develop, elucidate, and evaluate the most relevant frameworks pertaining to big data analytics and context-aware computing in the context of smart sustainable cities, bringing together research directed at a more conceptual, analytical, and overarching level to stimulate new ways of investigating their role in advancing urban sustainability. In terms of originality, a review and synthesis of the technical literature has not been undertaken to date in the urban literature, and in doing so, we provide a basis for urban researchers to draw on a set of conceptual frameworks in future research. The proposed frameworks, which can be replicated and tested in empirical research, will add additional depth and rigor to studies in the field. In addition to reviewing the important works, we highlight important applications as well as challenges and open issues. We argue that big data analytics and context-aware computing are prerequisite technologies for the functioning of smart sustainable cities of the future, as their effects reinforce one another as to their efforts for bringing a whole new dimension to the operating and organizing processes of urban life in terms of employing a wide variety of big data and context-aware applications for advancing sustainability. © 2017, The Author(s)."
"10.1186/s40537-017-0096-1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85033480072&doi=10.1186%2fs40537-017-0096-1&partnerID=40&md5=533331addfb8575bcd0fccf28729f422","An important source of information presently is social media, which reports any major event including natural disasters. Social media also includes conversational data. As a result, the volume of data on social media has an enormous increase. During the time of natural disaster like floods, tsunami, earthquake, landslide, etc., people require information in those situations, so that relief operations like help, medical facilities can save many lives (Bifet et al. in J Mach Learn Res Proc Track 17:5–11, 2011). An attempt is made in this article on Geoparsing which will identify the places of disaster on a Map. Geoparsing is a process of converting free text description of locations into the geographical identifier in an unambiguous manner with the help of longitude and latitude. With the help of geographical coordinates, it can be mapped and entered into geographical information system. A real-time, reliable at robust twitter messages which are the source of the information can handle a large amount of data. After collecting tweets at the real time we can parse them for the disaster situation and its location. This information will help to identify the exact location of the event. For knowing information on the natural disaster, tweets are extracted from twitter to R-Studio environment. First the extracted tweets from twitter are parsed using R about “Natural Disaster”. Later we parsed the tweets and store in CSV format in R database. For all posted data tweets are calculated and stored in a file. Later visual analysis is performed for the data store using R Statistical Software. Further, it is useful to assess the severity of the natural disaster. Sentiment analysis (Rahmath in IJAIEM 3(5):1–3, 2014) of user tweets is useful for decision making (Rao et al. in Int J Comput Sci Inf Technol 6(3):2923–7, 2015). © 2017, The Author(s)."
"10.1186/s40537-017-0098-z","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032840332&doi=10.1186%2fs40537-017-0098-z&partnerID=40&md5=98be4a72d717723ecd56bc72c0f3f612","Background: Recent US legislation imposes financial penalties on hospitals with excessive patient readmissions. Predictive analytics for hospital readmissions have seen an increase in research due to the passage of this legislation. However, many current systems ignore the formulas used by the Centers for Medicare and Medicaid Services for imposing penalties. This research expands upon current methodologies and directly incorporates federal penalization formulas when selecting patients for which to dedicate resources. Methods: Hospital discharge summaries are structured using clinical natural language processing techniques. Naïve Bayes classifiers are then used to assign a probability of readmission to each patient. Hospital Readmission Reductions Program formulas and probability of readmission are applied using four readmission scenarios to estimate the cost of readmission. The highest cost patients are identified and readmission mitigation efforts are attempted. Results: The results show that the average penalty savings over currently employed binary classification to be 51.93%. Binary classification is also shown to select more patients than necessary for readmission intervention. Additionally, intervening in only high-risk patients saved an average of 90.07% compared to providing all patients with costly aftercare. Conclusion: Focusing resources toward the potentially most expensive patients offers considerably better results than unfocused efforts. Utilizing direct calculation to estimate readmission costs has shown to be a more efficient use of resources than current readmission reduction methods. © 2017, The Author(s)."
"10.1186/s40537-017-0064-9","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032722947&doi=10.1186%2fs40537-017-0064-9&partnerID=40&md5=b6e70eac7de0cb3db1341dffb02110ff","Background: Expansion of Internet and its use for on-line activities such as E-Commerce and social networking are producing large volumes of transactional data. This huge data volume resulted from these activities facilitates the analysis and understanding of global trends and interesting patterns used for several decisive purposes. Analytics involved in these processes expose sensitive information present in these datasets, which is a serious privacy threat. To overcome this challenge, few sequential heuristics have been used in past where volumes of data were comparatively accommodating to these sequential heuristics; the current situation is not that much in-line and often results in high execution time. This new challenge of scalability paves a way for experimenting with Big Data approaches (e.g., MapReduce Framework). We have agglomerated the MapReduce framework with adopted heuristics to overcome this challenge of scalability along with much-needed privacy preservation and yields efficient analytic results within bounded execution times. Methods: MapReduce is a parallel programming framework [16] which provides us the opportunity to leverage largely distributed resources to deal with the Big Data analytics. MapReduce allows the resource of a largely distributed system to be utilized in a parallel fashion. The simplicity and high fault-tolerance are the key features which make MapReduce a promising framework. Therefore, we have proposed a two-phase MapReduce version of these adopted heuristics. MapReduce framework divides the whole data into ŉ' number of data chunks D = {d 1 d ∪ 2 ∪ d 3 ….. ∪ d n } and distributes them over ŉ' computing nodes to achieve the parallelization. The first phase of MapReduce job runs on each data chunk in order to generate intermediate results, which are further sorted and merged in the second phase to generate final sanitized dataset. Results: We conducted three set of experiments, each with five different scenarios corresponding to the different cluster sizes i.e., n = 1,2,3,4,5 where ŉ' is a number of computing nodes. We compared the approaches with respect to real as well as synthetically generated large datasets. For varying data sizes and varying number of computing nodes, it has been observed that sanitization time required by the MapReduce-based algorithm for same size dataset is much less than the sequential traditional approach. Further, the scalability can be improved by using more number of computing nodes. Lastly, another set of experiments explores the change in sanitization time with varying sizes of the sensitive content present in a dataset. We evaluated the effectiveness of proposed approach in different scenarios, with varying cluster size from 1 to 5 nodes. It has been observed that still the execution time of our approach is much less than traditional schemes. Further, no hiding failure, artifactual patterns have been observed during the experiments as well as in terms of misses cost also the MapReduce version performance is same as of traditional approaches. Conclusion: Traditional approaches for data hiding primarily MaxFIA and SWA were lacking with due inability to tackle large voluminous data. To subjugate the new challenge of scalability, we have implemented these basic heuristics with Big Data approach i.e., MapReduce framework. Quantitative evaluations have shown that the fusion of MapReduce framework with these adopted heuristics fulfills its obligatory responsibility of being scalable and many-fold faster for yielding efficient analytic results. © The Author(s) 2017."
"10.1186/s40537-017-0100-9","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032667180&doi=10.1186%2fs40537-017-0100-9&partnerID=40&md5=6b7f2baa0520bd64ec31e35c676d0452","Big datasets are often stored in flat files and can contain contradictory data. Contradictory data undermines the soundness of the information from a noisy dataset. Traditional tools such as pie chart and bar chart are overwhelmed when used to visually identify contradictory data in multidimensional attribute-values of a big dataset. This work explains the importance of identifying contradictions in a noisy dataset. It also examines how contradictory data in a large and noisy dataset can be mined and visually analysed. The authors developed ‘ConTra’, an open source application which applies mutual exclusion rule in identifying contradictory data, existing in comma separated values (CSV) dataset. ConTra’s capability to enable the identification of contradictory data in different sizes of datasets is examined. The results show that ConTra can process large dataset when hosted in servers with fast processors. It is also shown in this work that ConTra is 100% accurate in identifying contradictory data of objects whose attribute values do not conform to the mutual exclusion rule of a dataset in CSV format. Different approaches through which ConTra can mine and identify contradictory data are also presented. © 2017, The Author(s)."
"10.1186/s40537-017-0097-0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032582624&doi=10.1186%2fs40537-017-0097-0&partnerID=40&md5=1375f7e8b8e798f74f4ea028e07c539d","This paper proposes a reinforcement learning based message transfer model for transferring news report messages through a selected path in a trusted provenance network with the objective of maximizing the reward values based on trust or importance based and network congestion or utility based cost measures. The reward values are calculated along a dynamically defined policy path connecting start topic or event node to a goal topic or event or issue nodes for incrementally defined time windows for a given network congestion situation. A hierarchy of agents of trusted roles is used to accomplish the sub-goals associated with sub-story or subtopic in the provenance structure where an agent role may assume the semantic role of the associated sub-topic. The twitted news story thread or plan of events is defined in this work from the starting topic or event node to the goal topic or event node for incrementally defined intervals of time. The graphs are clustered into subtopic and these sub-goals or sub topic nodes of a topic node at every level of granularity are associated with cluster of news reports which describe activities associated with sub-goal or sub-topic events. Such cluster of nodes may also represent drilled down sequence of sub-events describing a sub-topic or sub-goal node. The policy path in a topic or story graph model is defined by applying reinforcement learning principles on dynamically defined event models associated with evolution of topic definition observed from incrementally acquired samples of input training data spanning multiple time windows. We provide a methodology for unifying similar provenance graph models for adapting and averaging the policy path classifiers associated with individual models to produce a reduced set of unified models derived during training. A minimum set cover of classifiers is identified for the models and a clustering procedure of the models is suggested based on these classifiers. Other database clustering methods have also been suggested as alternatives for clustering these models. A collection of unified models are identified from the models identified within a cluster and the policy path classifiers associated with these models provide the story or topic descriptions destined to goal topic or event nodes characterizing these models within a cluster. © 2017, The Author(s)."
"10.1186/s40537-017-0094-3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031995615&doi=10.1186%2fs40537-017-0094-3&partnerID=40&md5=0872c6e976c6cb0c266897ede21938d6","The restrictions that are related to using single distribution resampling for some specific computing devices’ memory gives developers several difficulties as a result of the increased effort and time needed for the development of a particle filter. Thus, one needs a new sequential resampling algorithm that is flexible enough to allow it to be used with various computing devices. Therefore, this paper formulated a new single distribution resampling called the adaptive memory size-based single distribution resampling (AMSSDR). This resampling method integrates traditional variation resampling and traditional resampling in one architecture. The algorithm changes the resampling algorithm using the memory in a computing device. This helps the developer formulate a particle filter without over considering the computing devices’ memory utilisation during the development of different particle filters. At the start of the operational process, it uses the AMSSDR selector to choose an appropriate resampling algorithm (for example, rounding copy resampling or systematic resampling), based on the current computing devices’ physical memory. If one chooses systematic resampling, the resampling will sample every particle for every cycle. On the other hand, if it chooses the rounding copy resampling, the resampling will sample more than one of each cycle’s particle. This illustrates that the method (AMSSDR) being proposed is capable of switching resampling algorithms based on various physical memory requirements. The aim of the authors is to extend this research in the future by applying their proposed method in various emerging applications such as real-time locator systems or medical applications. © 2017, The Author(s)."
"10.1186/s40537-017-0093-4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031399940&doi=10.1186%2fs40537-017-0093-4&partnerID=40&md5=ea4d863f1c273fd73a29205929320c0b","The recent technology development in the concern of microarray experiments has provided many new potentialities in terms of simultaneous measurement. But new challenges have arisen from these massive quantities of information qualified as Big Data. The challenge consists to extract the main information containing the sense from the data. To this end researchers are using various techniques as “hierarchical clustering”, “mutual information” and “self-organizing maps” to name a few. However, the management and analysis of the millions resulting dataset haven’t yet reached a satisfactory level, and there is no clear consensus about the best method/methods revealing patterns of gene expression. Thus, many efforts are required to strengthen the methodologies for optimal analysis of Big Data. In this paper, we propose a new processing approach which is structured on feature extraction and selection. The feature extraction, is based on correlation and rank analysis and leads to a reduction of the number of variables. The feature selection, consists in eliminating redundant or irrelevant variables, using some adapted techniques of discriminant analysis. Our approach is tested on three type of cancer gene expression microarray and compared with concurrent other approaches. It performs well, in terms of prediction results, computation and processing time. © 2017, The Author(s)."
"10.1186/s40537-017-0092-5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030856570&doi=10.1186%2fs40537-017-0092-5&partnerID=40&md5=f50e3852ede484d21979407258317f7f","Predicting stock market price is considered as a challenging task of financial time series analysis, which is of great interest to stock investors, stock traders and applied researchers. Many machine learning techniques have been used in this area to predict the stock market price, including regression algorithms which can be useful tools to provide good performance of financial time series prediction. Support Vector Regression is one of the most powerful algorithms in machine learning. There have been countless successes in utilizing SVR algorithm for stock market prediction. In this paper, we propose a novel hybrid approach based on machine learning and filtering techniques. Our proposed approach combines Support Vector Regression and Hodrick–Prescott filter in order to optimize the prediction of stock price. To assess the performance of this proposed approach, we have conducted several experiments using real world datasets. The principle objective of this paper is to demonstrate the improvement in predictive performance of stock market and verify the works of our proposed model in comparison with other optimized models. The experimental results confirm that the proposed algorithm constitutes a powerful model for predicting stock market prices. © 2017, The Author(s)."
"10.1186/s40537-017-0088-1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030459021&doi=10.1186%2fs40537-017-0088-1&partnerID=40&md5=187a68516c141ef4a734481a78dd64a2","The explosive growing number of data from mobile devices, social media, Internet of Things and other applications has highlighted the emergence of big data. This paper aims to determine the worldwide research trends on the field of big data and its most relevant research areas. A bibliometric approach was performed to analyse a total of 6572 papers including 28 highly cited papers and only papers that were published in the Web of ScienceTM Core Collection database from 1980 to 19 March 2015 were selected. The results were refined by all relevant Web of Science categories to computer science, and then the bibliometric information for all the papers was obtained. Microsoft Excel version 2013 was used for analyzing the general concentration, dispersion and movement of the pool of data from the papers. The t test and ANOVA were used to prove the hypothesis statistically and characterize the relationship among the variables. A comprehensive analysis of the publication trends is provided by document type and language, year of publication, contribution of countries, analysis of journals, analysis of research areas, analysis of web of science categories, analysis of authors, analysis of author keyword and keyword plus. In addition, the novelty of this study is that it provides a formula from multi-regression analysis for citation analysis based on the number of authors, number of pages and number of references. © 2017, The Author(s)."
"10.1186/s40537-017-0089-0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029935438&doi=10.1186%2fs40537-017-0089-0&partnerID=40&md5=dc9151b0bfedb06b8a355bb928ce9cea","Transfer learning has been demonstrated to be effective for many real-world applications as it exploits knowledge present in labeled training data from a source domain to enhance a model’s performance in a target domain, which has little or no labeled target training data. Utilizing a labeled source, or auxiliary, domain for aiding a target task can greatly reduce the cost and effort of collecting sufficient training labels to create an effective model in the new target distribution. Currently, most transfer learning methods assume the source and target domains consist of the same feature spaces which greatly limits their applications. This is because it may be difficult to collect auxiliary labeled source domain data that shares the same feature space as the target domain. Recently, heterogeneous transfer learning methods have been developed to address such limitations. This, in effect, expands the application of transfer learning to many other real-world tasks such as cross-language text categorization, text-to-image classification, and many others. Heterogeneous transfer learning is characterized by the source and target domains having differing feature spaces, but may also be combined with other issues such as differing data distributions and label spaces. These can present significant challenges, as one must develop a method to bridge the feature spaces, data distributions, and other gaps which may be present in these cross-domain learning tasks. This paper contributes a comprehensive survey and analysis of current methods designed for performing heterogeneous transfer learning tasks to provide an updated, centralized outlook into current methodologies. © 2017, The Author(s)."
"10.1186/s40537-017-0090-7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029845375&doi=10.1186%2fs40537-017-0090-7&partnerID=40&md5=397430b134550b2a0403d2a71014f619","The traditional methods of clustering are unable to cope with the exploding volume of data that the world is currently facing. As a solution to this problem, the research is intensified in the direction of parallel clustering methods. Although there is a variety of parallel programming models, the MapReduce paradigm is considered as the most prominent model for problems of large scale data processing of which the clustering. This paper introduces a new parallel design of a recently appeared heuristic for hard clustering using the MapReduce programming model. In this heuristic, clustering is performed by efficiently partitioning categorical large data sets according to the relational analysis approach. The proposed design, called PMR-Transitive, is a single-scan and parameter-free heuristic which determines the number of clusters automatically. The experimental results on real-life and synthetic data sets demonstrate that PMR-Transitive produces good quality results. © 2017, The Author(s)."
"10.1186/s40537-017-0087-2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029073491&doi=10.1186%2fs40537-017-0087-2&partnerID=40&md5=5886f82afe7132772437dd96806ac658","Big data has become popular for processing, storing and managing massive volumes of data. The clustering of datasets has become a challenging issue in the field of big data analytics. The K-means algorithm is best suited for finding similarities between entities based on distance measures with small datasets. Existing clustering algorithms require scalable solutions to manage large datasets. This study presents two approaches to the clustering of large datasets using MapReduce. The first approach, K-Means Hadoop MapReduce (KM-HMR), focuses on the MapReduce implementation of standard K-means. The second approach enhances the quality of clusters to produce clusters with maximum intra-cluster and minimum inter-cluster distances for large datasets. The results of the proposed approaches show significant improvements in the efficiency of clustering in terms of execution times. Experiments conducted on standard K-means and proposed solutions show that the KM-I2C approach is both effective and efficient. © 2017, The Author(s)."
"10.1186/s40537-017-0082-7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028314925&doi=10.1186%2fs40537-017-0082-7&partnerID=40&md5=4ef312725e979355da1dc2a3137ffbb4","Predictive analytics has gained a lot of reputation in the emerging technology Big data. Predictive analytics is an advanced form of analytics. Predictive analytics goes beyond data mining. A huge amount of medical data is available today regarding the disease, their symptoms, reasons for illness, and their effects on health. But this data is not analysed properly to predict or to study a disease. The aim of this paper is to give a detailed version of predictive models from base to state-of-art, describing various types of predictive models, steps to develop a predictive model, their applications in health care in a broader way and particularly in diabetes. © 2017, The Author(s)."
"10.1186/s40537-017-0074-7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027869929&doi=10.1186%2fs40537-017-0074-7&partnerID=40&md5=c173f882f027468d5963cdaa3b37c3e0","Detecting botnets in a network is crucial because bots impact numerous areas such as cyber security, finance, health care, law enforcement, and more. Botnets are becoming more sophisticated and dangerous day-by-day, and most of the existing rule based and flow based detection methods may not be capable of detecting bot activities in an efficient and effective manner. Hence, designing a robust and fast botnet detection method is of high significance. In this study, we propose a novel botnet detection methodology based on topological features of nodes within a graph: in degree, out degree, in degree weight, out degree weight, clustering coefficient, node betweenness, and eigenvector centrality. A self-organizing map clustering method is applied to establish clusters of nodes in the network based on these features. Our method is capable of isolating bots in clusters of small sizes while containing the majority of normal nodes in the same big cluster. Thus, bots can be detected by searching a limited number of nodes. A filtering procedure is also developed to further enhance the algorithm efficiency by removing inactive nodes from consideration. The methodology is verified using the CTU-13 datasets, and benchmarked against a classification-based detection method. The results show that our proposed method can efficiently detect the bots despite their varying behaviors. © 2017, The Author(s)."
"10.1186/s40537-017-0083-6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026345553&doi=10.1186%2fs40537-017-0083-6&partnerID=40&md5=e6cb3eb71a8388c5e8684b8dc528f1ce","Text similarity measurement aims to find the commonality existing among text documents, which is fundamental to most information extraction, information retrieval, and text mining problems. Cosine similarity based on Euclidean distance is currently one of the most widely used similarity measurements. However, Euclidean distance is generally not an effective metric for dealing with probabilities, which are often used in text analytics. In this paper, we propose a new similarity measure based on sqrt-cosine similarity. We apply the proposed improved sqrt-cosine similarity to a variety of document-understanding tasks, such as text classification, clustering, and query search. Comprehensive experiments are then conducted to evaluate our new similarity measurement in comparison to existing methods. These experimental results show that our proposed method is indeed effective. © 2017, The Author(s)."
"10.1186/s40537-017-0086-3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026290164&doi=10.1186%2fs40537-017-0086-3&partnerID=40&md5=c0913282f4a930361b235e01b391470c","It is true that data is never static; it keeps growing and changing over time. New data is added and old data can either be modified or deleted. This incremental nature of data motivates the development of new systems to perform large-scale data computations incrementally. MapReduce was recently introduced to provide an efficient approach for handling large-scale data computations. Nevertheless, it turned to be inefficient in supporting the processing of small incremental data. While many previous systems have extended MapReduce to perform iterative or incremental computations, these systems are still inefficient and too expensive to perform large-scale iterative computations on changing data. In this paper, we present a new system called iiHadoop, an extension of Hadoop framework, optimized for incremental iterative computations. iiHadoop accelerates program execution by performing the incremental computations on the small fraction of data that is affected by changes rather than the whole data. In addition, iiHadoop improves the performance by executing iterations asynchronously, and employing locality-aware scheduling for the map and reduce tasks taking into account the incremental and iterative behavior. An evaluation for the proposed iiHadoop framework is presented using examples of iterative algorithms, and the results showed significant performance improvements over comparable existing frameworks. © 2017, The Author(s)."
"10.1186/s40537-017-0085-4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85024876648&doi=10.1186%2fs40537-017-0085-4&partnerID=40&md5=ec7ce42a3488372105d44b7355b4ddb8","Route prediction is an essential requirement for many intelligent transport systems (ITS) services like VANETS, traffic congestion estimation, resource prediction in grid computing etc. This work focuses on building an end-to-end horizontally scalable route prediction application based on statistical modeling of user travel data. Probabilistic suffix tree (PST) is one of widely used sequence indexing technique which serves a model for prediction. The probabilistic generalized suffix tree (PGST) is a variant of PST and is essentially a suffix tree built from a huge number of smaller sequences. We construct generalized suffix tree model from a large number of trips completed by the users. User trip raw GPS traces is mapped to the digitized road network by parallelizing map matching technique leveraging map reduce framework. PGST construction from the huge volume of data by processing sequentially is a bottleneck in the practical realization. Most of the existing works focused on time-space tradeoffs on a single machine. Proposed technique solves this problem by a two-step process which is intuitive to execute in the map-reduce framework. In the first step, computes all the suffixes along with their frequency of occurrences and in the second step, builds probabilistic generalized suffix tree. The probabilistic aspect of the tree is also taken care so that it can be used as a model for prediction application. Dataset used are road network spatial data and GPS traces of users. Experiments carried out on real datasets available in public domain. © 2017, The Author(s)."
"10.1186/s40537-017-0084-5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85024831970&doi=10.1186%2fs40537-017-0084-5&partnerID=40&md5=065bc3bf08fa1b6dc0a9e31bdcfcf2fb","With the increasing demand for examining and extracting patterns from massive amounts of data, it is critical to be able to train large models to fulfill the needs that recent advances in the machine learning area create. L-BFGS (Limited-memory Broyden Fletcher Goldfarb Shanno) is a numeric optimization method that has been effectively used for parameter estimation to train various machine learning models. As the number of parameters increase, implementing this algorithm on one single machine can be insufficient, due to the limited number of computational resources available. In this paper, we present a parallelized implementation of the L-BFGS algorithm on a distributed system which includes a cluster of commodity computing machines. We use open source HPCC Systems (High-Performance Computing Cluster) platform as the underlying distributed system to implement the L-BFGS algorithm. We initially provide an overview of the HPCC Systems framework and how it allows for the parallel and distributed computations important for Big Data analytics and, subsequently, we explain our implementation of the L-BFGS algorithm on this platform. Our experimental results show that our large-scale implementation of the L-BFGS algorithm can easily scale from training models with millions of parameters to models with billions of parameters by simply increasing the number of commodity computational nodes. © 2017, The Author(s)."
"10.1186/s40537-017-0081-8","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85023173224&doi=10.1186%2fs40537-017-0081-8&partnerID=40&md5=69c337b319ec6e4a5482679c11abc7fd","Ever since the emergence of big data concept, researchers have started applying the concept to various fields and tried to assess the level of acceptance of it with renown models like technology acceptance model (TAM) and it variations. In this regard, this paper tries to look at the factors that associated with the usage of big data analytics, by synchronizing TAM with organizational learning capabilities (OLC) framework. These models are applied on the construct, intended usage of big data and also the mediation effect of the OLC constructs is assessed. The data for the study is collected from the students pertaining to information technology disciplines at University of Liverpool, online programme. Though, invitation to participate e-mails are sent to 1035 students, only 359 members responded back with filled questionnaires. This study uses structural equation modelling and multivariate regression using ordinary least squares estimation to test the proposed hypotheses using the latest statistical software R. It is proved from the analysis that compared to other models, model 4 (which is constructed by using the constructs of OLC and TAM frameworks) is able to explain 44% variation in the usage pattern of big data. In addition to this, the mediation test performed revealed that the interaction between OLC dimensions and TAM dimensions on intended usage of big data has no mediation effect. Thus, this work provided inputs to the research community to look into the relation between the constructs of OLC framework and the selection of big data technology. © 2017, The Author(s)."
"10.1186/s40537-017-0077-4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021713833&doi=10.1186%2fs40537-017-0077-4&partnerID=40&md5=337a12718f420a4c5c4fb683f899cd62","In agriculture sector where farmers and agribusinesses have to make innumerable decisions every day and intricate complexities involves the various factors influencing them. An essential issue for agricultural planning intention is the accurate yield estimation for the numerous crops involved in the planning. Data mining techniques are necessary approach for accomplishing practical and effective solutions for this problem. Agriculture has been an obvious target for big data. Environmental conditions, variability in soil, input levels, combinations and commodity prices have made it all the more relevant for farmers to use information and get help to make critical farming decisions. This paper focuses on the analysis of the agriculture data and finding optimal parameters to maximize the crop production using data mining techniques like PAM, CLARA, DBSCAN and Multiple Linear Regression. Mining the large amount of existing crop, soil and climatic data, and analysing new, non-experimental data optimizes the production and makes agriculture more resilient to climatic change. © 2017, The Author(s)."
"10.1186/s40537-017-0079-2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021445287&doi=10.1186%2fs40537-017-0079-2&partnerID=40&md5=547eb29ecfc7c71dc6eb121e6b00e3fa","Most scientists are accustomed to make predictions based on consolidated and accepted theories pertaining to the domain of prediction. However, nowadays big data analytics (BDA) is able to deliver predictions based on executing a sequence of data processing while seemingly abstaining from being theoretically informed about the subject matter. This paper discusses how to deal with the shift from theory-driven to process-driven prediction through analyzing the BDA steps and identifying the epistemological challenges and various needs of theoretically informing BDA throughout data acquisition, preprocessing, analysis, and interpretation. We suggest a theory-driven guidance for the BDA process including acquisition, pre-processing, analytics and interpretation. That is, we propose—in association with these BDA process steps—a lightweight theory-driven approach in order to safeguard the analytics process from epistemological pitfalls. This study may serve as a guideline for researchers and practitioners to consider while conducting future big data analytics.[Figure not available: see fulltext.]. © 2017, The Author(s)."
"10.1186/s40537-017-0078-3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021138081&doi=10.1186%2fs40537-017-0078-3&partnerID=40&md5=d69cce420dd8b3bf19aea05db6ba1048","Determining user’s perception of a brand in short periods of time has become crucial for business. Distilling brand perception directly from people’s comments in social media has promise. Current techniques for determining brand perception, such as surveys of handpicked users by mail, in person, phone or online, are time consuming and increasingly inadequate. The DERIV system distills storylines from open data representing direct consumer voice into a brand perception. The framework summarizes perception of a brand in comparison to peer brands with in-memory distributed algorithms utilizing supervised machine learning techniques. Experiments performed with open data and models built with storylines of known peer brands show the technique as highly scalable and accurate in capturing brand perception from vast amounts of social data compared to sentiment analysis. © 2017, The Author(s)."
"10.1186/s40537-017-0080-9","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021067968&doi=10.1186%2fs40537-017-0080-9&partnerID=40&md5=fc9bfc9345b828d78a36a005dca5d934","The performance gap between compute and storage is fairly considerable. This results in a mismatch between the application needs from storage and what storage can deliver. The full potential of storage devices cannot be harnessed till all layers of I/O hierarchy function efficiently. Despite advanced optimizations applied across various layers along the odyssey of data access, the I/O stack still remains volatile. The problems associated due to the inefficiencies in data management get amplified in Big Data shared resource environments. The Linux OS (host) block layer is the most critical part of the I/O hierarchy, as it orchestrates the I/O requests from different applications to the underlying storage. Unfortunately, despite it’s significance, the block layer, essentially the block I/O scheduler, hasn’t evolved to meet the needs of Big Data. We have designed and developed two contention avoidance storage solutions, collectively known as “BID: Bulk I/O Dispatch” in the Linux block layer specifically to suit multi-tenant, multi-tasking shared Big Data environments. Hard disk drives (HDDs) form the backbone of data center storage. The data access time in HDDs is majorly governed by disk arm movements, which usually occurs when data is not accessed sequentially. Big Data applications exhibit evident sequentiality but due to the contentions amongst other I/O submitting applications, the I/O accesses get multiplexed which leads to higher disk arm movements. BID schemes aim to exploit the inherent I/O sequentiality of Big Data applications to improve the overall I/O completion time by reducing the avoidable disk arm movements. In the first part, we propose a dynamically adaptable block I/O scheduling scheme BID-HDD for disk based storage. BID-HDD tries to recreate the sequentiality in I/O access in order to provide performance isolation to each I/O submitting process. Through trace driven simulation based experiments with cloud emulating MapReduce benchmarks, we show the effectiveness of BID-HDD which results in 28–52% lesser time for all I/O requests than the best performing Linux disk schedulers. In the second part, we propose a hybrid scheme BID-Hybrid to exploit SCM’s (SSDs) superior random performance to further avoid contentions at disk based storage. BID-Hybrid is able to efficiently offload non-bulky interruptions from HDD request queue to SSD queue using BID-HDD for disk request processing and multi-q FIFO architecture for SSD. This results in performance gain of 6–23% for MapReduce workloads when compared to BID-HDD and 33–54% over best performing Linux scheduling scheme. BID schemes as a whole is aimed to avoid contentions for disk based storage I/Os following system constraints without compromising SLAs. © 2017, The Author(s)."
"10.1186/s40537-017-0076-5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020190103&doi=10.1186%2fs40537-017-0076-5&partnerID=40&md5=cddddb7f0afc0a4dcacdda0e5b498e2f","Network theory concepts form the core of algorithms that are designed to uncover valuable insights from various datasets. Especially, network centrality measures such as Eigenvector centrality, Katz centrality, PageRank centrality etc., are used in retrieving top-K viral information propagators in social networks,while web page ranking in efficient information retrieval, etc. In this paper, we propose a novel method for identifying top-K viral information propagators from a reduced search space. Our algorithm computes the Katz centrality and Local average centrality values of each node and tests the values against two threshold (constraints) values. Only those nodes, which satisfy these constraints, form the search space for top-K propagators. Our proposed algorithm is tested against four datasets and the results show that the proposed algorithm is capable of reducing the number of nodes in search space at least by 70%. We also considered the parameter (α and β) dependency of Katz centrality values in our experiments and established a relationship between the α values, number of nodes in search space and network characteristics. Later, we compare the top-K results of our approach against the top-K results of degree centrality. © 2017, The Author(s)."
"10.1186/s40537-017-0075-6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019205986&doi=10.1186%2fs40537-017-0075-6&partnerID=40&md5=787c055935d2890761a5b5aa5a0d0b00","Background: Online consumer reviews have become a baseline for new consumers to try out a business or a new product. The reviews provide a quick look into the application and experience of the business/product and market it to new customers. However, some businesses or reviewers use these reviews to spread fake information about the business/product. The fake information can be used to promote a relatively average product/business or can be used to malign their competition. This activity is known as reviewer fraud or opinion spam. The paper proposes a feature set, capturing the user social interaction behavior to identify fraud. The problem being solved is one of the characteristics that lead to fraud rather than detecting fraud. Methods: Neural network algorithm is used to evaluate the proposed feature set and compare it against the state-of-the-art feature sets in detecting fraud. The feature set considers the user’s social interaction on the Yelp platform to determine if the user is committing fraud. The neural network algorithm helps in comparing the feature set with other feature sets used to detect fraud. Any attempt to find the characteristics that lead to fraud has a prerequisite to be good enough to detect fraud as well. Results: The F1 score obtained using neural networks is on par with all the well-known methods for detecting fraud, a value of 0.95. The effectiveness of the feature set is in rivaling the other approaches to fraud detection. Conclusions: A user’s social interaction on a digital platform such as Yelp is equally important in evaluating the user as social interaction is in real life. The characteristics that lead to fraud can be intuitively captured. The characteristics such as number of friends, number of followers and the number of times the user has provided a review which was helpful to multiple people provide the neural network with a base to form a relationship between opinion fraud and social interaction characteristics. © 2017, The Author(s)."
"10.1186/s40537-017-0072-9","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018868379&doi=10.1186%2fs40537-017-0072-9&partnerID=40&md5=e29bd0c830c40e01c9d462356f513b27","The ability to process large volumes of data on the fly, as soon as they become available, is a fundamental requirement in today’s information systems. Modern distributed stream processing engines (SPEs) address this requirement and provide low-latency and high-throughput data stream processing in cluster platforms, offering high-level programming interfaces that abstract from low-level details such as data distribution and hardware failures. The last decade saw a rapid increase in the number of available SPEs. However, each SPE defines its own processing model and standardized execution semantics have not emerged yet. This paper tackles this problem and analyzes the execution semantics of some widely adopted modern SPEs, namely Flink, Storm, Spark Streaming, Google Dataflow, and Azure Stream Analytics. We specifically target the notions of windowing and time, traditionally considered the key distinguishing factors that characterize the behavior of SPEs. We rely on the SECRET model, introduced in 2010 to analyze the windowing semantics for the SPEs available at that time. We show that SECRET models well some aspects of the behavior of modern SPEs, and we shed light on the evolution of SPEs after the introduction of SECRET by analyzing the elements that SECRET cannot fully capture. In this way, the paper contributes to the research in the area of stream processing by: (1) contrasting and comparing some widely used modern SPEs based on a formal model of their execution semantics; (2) discussing the evolution of SPEs since the introduction of the SECRET model; (3) suggesting promising research directions to direct further modeling efforts. © 2017, The Author(s)."
"10.1186/s40537-017-0073-8","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018865430&doi=10.1186%2fs40537-017-0073-8&partnerID=40&md5=a95f183285b9d1d0e26003f9ce3c5f36","Outliers are samples that are generated by different mechanisms from other normal data samples. Graphs, in particular social network graphs, may contain nodes and edges that are made by scammers, malicious programs or mistakenly by normal users. Detecting outlier nodes and edges is important for data mining and graph analytics. However, previous research in the field has merely focused on detecting outlier nodes. In this article, we study the properties of edges and propose effective outlier edge detection algorithm. The proposed algorithms are inspired by community structures that are very common in social networks. We found that the graph structure around an edge holds critical information for determining the authenticity of the edge. We evaluated the proposed algorithms by injecting outlier edges into some real-world graph data. Experiment results show that the proposed algorithms can effectively detect outlier edges. In particular, the algorithm based on the Preferential Attachment Random Graph Generation model consistently gives good performance regardless of the test graph data. More important, by analyzing the authenticity of the edges in a graph, we are able to reveal underlying structure and properties of a graph. Thus, the proposed algorithms are not limited in the area of outlier edge detection. We demonstrate three different applications that benefit from the proposed algorithms: (1) a preprocessing tool that improves the performance of graph clustering algorithms; (2) an outlier node detection algorithm; and (3) a novel noisy data clustering algorithm. These applications show the great potential of the proposed outlier edge detection techniques. They also address the importance of analyzing the edges in graph mining—a topic that has been mostly neglected by researchers. © 2017, The Author(s)."
"10.1186/s40537-017-0070-y","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018457429&doi=10.1186%2fs40537-017-0070-y&partnerID=40&md5=93c1d781f84daf23431971bcf4d4bdc5","A smart grid is an intelligent electricity grid that optimizes the generation, distribution and consumption of electricity through the introduction of Information and Communication Technologies on the electricity grid. In essence, smart grids bring profound changes in the information systems that drive them: new information flows coming from the electricity grid, new players such as decentralized producers of renewable energies, new uses such as electric vehicles and connected houses and new communicating equipments such as smart meters, sensors and remote control points. All this will cause a deluge of data that the energy companies will have to face. Big Data technologies offers suitable solutions for utilities, but the decision about which Big Data technology to use is critical. In this paper, we provide an overview of data management for smart grids, summarise the added value of Big Data technologies for this kind of data, and discuss the technical requirements, the tools and the main steps to implement Big Data solutions in the smart grid context. © 2017, The Author(s)."
"10.1186/s40537-017-0071-x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017015939&doi=10.1186%2fs40537-017-0071-x&partnerID=40&md5=ffc4bea79a32827d4464c5645feeb3bd","Cloud data stores that can handle very large amounts of data, such as Apache HBase, have accelerated the use of non-relational databases (coined as NoSQL databases) as a way of addressing RDB database limitations with regards to scalability and performance of existing systems. Converting existing systems and their databases to this technology is not trivial, given that RDB practitioners tend to use the relational model design mindset when converting existing databases. This can result in inefficient NoSQL design, leading to suboptimal query speed or inefficient database schema. This paper reports on a two-phase experiment: (1) a conversion from RDB to HBase database, a NoSQL type of database, without the use of conversion rules and based on a heuristic approach and (2) an experiment with different schema designs to uncover and extract conversion rules that could be useful conversion guidelines for the industry. © 2017, The Author(s)."
"10.1186/s40537-017-0065-8","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017014012&doi=10.1186%2fs40537-017-0065-8&partnerID=40&md5=fbe68c7ea07ed92aeb88d8923d636933","Using traditional machine learning approaches, there is no single feature engineering solution for all text mining and learning tasks. Thus, researchers must determine and implement the best feature engineering approach for each text classification task; however, deep learning allows us to skip this step by extracting and learning high-level features automatically from low-level text representations. Convolutional neural networks, a popular type of neural network for deep learning, have been shown to be effective at performing feature extraction and classification for many domains including text. Recently, it was demonstrated that convolutional neural networks can be used to train classifiers from character-level representations of text. This approach achieved superior performance compared to classifiers trained on word-level text representations, likely due to the use of character-level representations preserving more information from the data. Training neural networks from character level data requires a large volume of instances; however, the large volume of training data and model complexity makes training these networks a slow and computationally expensive task. In this paper, we propose a new method of creating character-level representations of text to reduce the computational costs associated with training a deep convolutional neural network. We demonstrate that our method of character embedding greatly reduces training time and memory use, while significantly improving classification performance. Additionally, we show that our proposed embedding can be used with padded convolutional layers to enable the use of current convolutional network architectures, while still facilitating faster training and higher performance than the previous approach for learning from character-level text. © 2017, The Author(s)."
"10.1186/s40537-017-0067-6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016926720&doi=10.1186%2fs40537-017-0067-6&partnerID=40&md5=fb2389decadaf4c15b41c02004e3bc99","Background: Chronic Obstructive Pulmonary Disease (COPD) is a chronic lung disease that affects airflow to the lungs. Discovering the co-occurrence of COPD with other diseases, symptoms, and medications is invaluable to medical staff. Building co-occurrence indexes and finding causal relationships with COPD can be difficult because often times disease prevalence within a population influences results. A method which can better separate occurrence within COPD patients from population prevalence would be desirable. Large hospital systems may potentially have tens of millions of patient records spanning decades of collection and a big data approach that is scalable is desirable. The presented method, Co-Occurring Evidence Discovery (COED), presents a methodology and framework to address these issues. Methods: Natural Language Processing methods are used to examine 64,371 deidentified clinical notes and discover associations between COPD and medical terms. Apache cTAKES is leveraged to annotate and structure clinical notes. Several extensions to cTAKES have been written to parallelize the annotation of large sets of clinical notes. A co-occurrence score is presented which can penalize scores based on term prevalence, as well as a baseline method traditionally used for finding co-occurrence. These scoring systems are implemented using Apache Spark. Dictionaries of ground truth terms for diseases, medications, and symptoms have been created using clinical domain knowledge. COED and baseline methods are compared using precision, recall, and F1 score. Results: The highest scoring diseases using COED are lung and respiratory diseases. In contrast, baseline methods for co-occurrence rank diseases with high population prevalence highest. Medications and symptoms evaluated with COED share similar results. When evaluated against ground truth dictionaries, the maximum improvements in recall for symptoms, diseases, and medications were 0.212, 0.130, and 0.174. The maximum improvements in precision for symptoms, diseases, and medications were 0.303, 0.333, and 0.180. Median increase in F1 score for symptoms, diseases, and medications were 38.1%, 23.0%, and 17.1%. A paired t-test was performed and F1 score increases were found to be statistically significant, where p < 0.01. Conclusion: Penalizing terms which are highly frequent in the corpus results in better precision and recall performance. Penalizing frequently occurring terms gives a better picture of the diseases, symptoms, and medications co-occurring with COPD. Using a mathematical and computational approach rather than purely expert driven approach, large dictionaries of COPD related terms can be assembled in a short amount of time. © 2017, The Author(s)."
"10.1186/s40537-017-0068-5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016764906&doi=10.1186%2fs40537-017-0068-5&partnerID=40&md5=78c55576beb5ff2b9fc52b63c767205c","Proliferation of structural, semi-structural and no-structural data, has challenged the scalability, flexibility and processability of the traditional relational database management systems (RDBMS). The next generation systems demand horizontal scaling by distributing data over autonomously addable nodes to a running system. For schema flexibility, they also want to process and store different data formats along the sequence factor in the data. NoSQL approaches are solutions to these, hence big data solutions are vital nowadays. But in monitoring scenarios sensors transmit the data continuously over certain intervals of time and temporal factor is the main property of the data. Therefore the key research aspect is to investigate schema flexibility and temporal data integration aspects together. We need to know that: what data modelling should we adopt for a data driven real-time scenario; that we could store the data effectively and evolve the schema accordingly during data integration in NoSQL environments without losing big data advantages. In this paper we explain a middleware based schema model to support the temporal oriented storage of real-time data of ANT+ sensors as hierarchical documents. We explain how to adopt a schema for the data integration by using an algorithm based approach for flexible evolution of the model for a document oriented database, i.e, MongoDB. The proposed model is logical, compact for storage and evolves seamlessly upon new data integration. © 2017, The Author(s)."
"10.1186/s40537-017-0066-7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016150777&doi=10.1186%2fs40537-017-0066-7&partnerID=40&md5=5226b6df4d96248153e20fcda13c4cf4","Social media data has provided various insights into the behaviour of consumers and businesses. However, extracted data may be erroneous, or could have originated from a malicious source. Thus, quality of social media should be managed. Also, it should be understood how data quality can be managed across a big data pipeline, which may consist of several processing and analysis phases. The contribution of this paper is evaluation of data quality management architecture for social media data. The theoretical concepts based on previous work have been implemented for data quality evaluation of Twitter-based data sets. Particularly, reference architecture for quality management in social media data has been extended and evaluated based on the implementation architecture. Experiments indicate that 150–800 tweets/s can be evaluated with two cloud nodes depending on the configuration. © 2017, The Author(s)."
"10.1186/s40537-017-0069-4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016137091&doi=10.1186%2fs40537-017-0069-4&partnerID=40&md5=5fe5998135440bdbb9b9583a1388f372","A fully self-managed DBMS which does not require administrator intervention is the ultimate goal of database developers. This system should automate deploying, configuration, administration, monitoring, and tuning tasks. Although there are some advances in this field, self-managed technology is largely not ready for industrial use and remains an active area of research. One of the most crucial tasks for such a system is automated physical design tuning. A self-managed approach for this task implies that the physical design of a database should be automatically adapted to changing workloads. The problems of materialized view and index selection, data allocation, horizontal and vertical partitioning were studied for a long time, and hundreds of approaches were developed. However, most of these approaches were static, thus, unsuitable for self-managed systems. In this paper we discuss the prospects of an adaptive distributed relational column-store. We show that the column-store approach holds a great promise for construction of an efficient self-managed database. At first, we present a short survey of existing physical design studies and provide a classification of approaches. In the survey, we highlight the self-managed aspects. Then, we provide some views on the organization of a self-managed distributed column-store system. We discuss its three core components: an alerter, a reorganization controller and a set of physical design options (actions) available to such a system. We present possible approaches for each of these components and evaluate them. Several physical design problems are formulated and discussed. This study is the first step towards a creation of an adaptive distributed column-store system. © 2017, The Author(s)."
"10.1186/s40537-017-0063-x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85011004513&doi=10.1186%2fs40537-017-0063-x&partnerID=40&md5=983d653070eb7027db13196daed8a058","The popularity of social media and computer-mediated communication has resulted in high-volume and highly semantic data about digital social interactions. This constantly accumulating data has been termed as Big Social Data or Social Big Data, and various visions about how to utilize that have been presented. However, as relatively new concepts, there are no solid and commonly agreed definitions of them. We argue that the emerging research field around these concepts would benefit from understanding about the very substance of the concept and the different viewpoints to it. With our review of earlier research, we highlight various perspectives to this multi-disciplinary field and point out conceptual gaps, the diversity of perspectives and lack of consensus in what Big Social Data means. Based on detailed analysis of related work and earlier conceptualizations, we propose a synthesized definition of the term, as well as outline the types of data that Big Social Data covers. With this, we aim to foster future research activities around this intriguing, yet untapped type of Big Data. © 2017, The Author(s)."
"10.1186/s40537-016-0061-4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85009485696&doi=10.1186%2fs40537-016-0061-4&partnerID=40&md5=fc8bed24aaa907ceebfe5b03d73a453e","Network infections that are already in progress cause challenges to those officers trying to preserve those nodes not yet infected. Static solutions can take advantage of global knowledge of the network to produce quick and approximate answers for those members who should be vaccinated. In dynamic situations however, small changes can severely alter those static solutions making them irrelevant. Yet in dynamic situations it can not be known with certainty which small changes will affect the solution and those that will not. Computational resources are wasted recalculating a global solution for the entire network, when a local recalculation may be enough. This paper presents a dynamic node vaccination solution that seeks to take advantage of these local recalculations. © 2017, The Author(s)."
"10.1186/s40537-016-0062-3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85009223452&doi=10.1186%2fs40537-016-0062-3&partnerID=40&md5=4abed6b0628e027ac1f21ff8367e66ef","In order to forecast prices of arbitrary agricultural commodity in different wholesale markets in one city, this paper proposes a mixed model, which combines ARIMA model and PLS regression method based on time and space factors. This mixed model is able to obtain the forecasting results of weekly prices of agricultural commodities in different markets. Meanwhile, this paper sets up variables to measure the price changing trend based on the change of exogenous variables and prices, thus achieves the warning of daily price changes using neural networks. The model is tested with the data of several types of agricultural commodities and error analysis is made. The result shows that the mixed model is more accurate in forecasting agricultural commodity prices than each single model does, and has better accuracy in warning values. The mixed model, to some extent, forecasts the daily price changes of agricultural commodities. © 2017, The Author(s)."
"10.1007/s41019-017-0052-2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065046497&doi=10.1007%2fs41019-017-0052-2&partnerID=40&md5=1b5c71e9c36b47eebbc9bc1130973f09","Clinical Decision Support (CDS) is widely seen as an information retrieval (IR) application in the medical domain. The goal of CDS is to help physicians find useful information from a collection of medical articles with respect to the given patient records, in order to take the best care of their patients. Most of the existing CDS methods do not sufficiently consider the semantic relation between texts, hence the potential in improving the performance in biomedical articles retrieval. This paper proposes a novel feedback-based approach which considers the semantic association between a retrieved biomedical article and a pseudo feedback set. Evaluation results show that our method outperforms the strong baselines and is able to improve over the best runs in the TREC CDS tasks. © 2017, The Author(s)."
"10.1007/s41019-017-0054-0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065017179&doi=10.1007%2fs41019-017-0054-0&partnerID=40&md5=ba8582e9bc7bcb7e2f8d380c79f648e5","High-frequency trading (HFT) has always been welcomed because it benefits not only personal benefits but also the whole social welfare. While the recent advance of portfolio selection in HFT market enables to bring about more profit, it yields much contended OLTP workloads. Featuring exploiting the abundant parallelism, transaction pipeline, the state-of-the-art concurrency control (CC) mechanism, however, suffers from limited concurrency confronted with HFT workloads. Its variants that enable more parallel execution by leveraging fine-grained contention information also take little effect. To solve this problem, we for the first time observe and formulate the source of restricted concurrency as harmful ordering of transaction statements. To resolve harmful ordering, we propose PARE, a pipeline-aware reordered execution, to improve application performance by rearranging statements in order of their degrees of contention. In concrete, two mechanisms are devised to ensure the correctness of statement rearrangement and identify the degrees of contention of statements, respectively. We also study the off-line reordering problem. We prove that this problem is NP-hard and present an off-line reordering approach to approximate the optimal reordering strategy. Experiment results show that PARE can improve transaction throughput and reduce transaction latency on HFT applications by up to an order of magnitude than the state-of-the-art CC mechanism. © 2017, The Author(s)."
"10.1007/s41019-017-0043-3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063879657&doi=10.1007%2fs41019-017-0043-3&partnerID=40&md5=0aaadb6ca6304d6fe86d1c037c60ac26","The emergence of new hardware architectures, and the continuous production of data open new challenges for data management. It is no longer pertinent to reason with respect to a predefined set of resources (i.e., computing, storage and main memory). Instead, it is necessary to design data processing algorithms and processes considering unlimited resources via the “pay-as-you-go” model. According to this model, resources provision must consider the economic cost of the processes versus the use and parallel exploitation of available computing resources. In consequence, new methodologies, algorithms and tools for querying, deploying and programming data management functions have to be provided in scalable and elastic architectures that can cope with the characteristics of Big Data aware systems (intelligent systems, decision making, virtual environments, smart cities, drug personalization). These functions, must respect QoS properties (e.g., security, reliability, fault tolerance, dynamic evolution and adaptability) and behavior properties (e.g., transactional execution) according to application requirements. Mature and novel system architectures propose models and mechanisms for adding these properties to new efficient data management and processing functions delivered as services. This paper gives an overview of the different architectures in which efficient data management functions can be delivered for addressing Big Data processing challenges. © 2017, The Author(s)."
"10.1007/s41019-017-0051-3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053978489&doi=10.1007%2fs41019-017-0051-3&partnerID=40&md5=f44b47f9942148bd99e965fe88e949fb","Community search problem, which is to find good communities given a set of query nodes in a graph, has attracted increasing research interest recently. Though various measurement models have been proposed to define and solve community search problem. Few of them could define a community concisely and have good quality of query results. They either involve additional constraints for modeling communities, such as size and diameter, or suffer from the free rider effect, i.e., include irrelevant subgraphs. In this paper, we propose a new k-plex based community model for community search. We show that our model not only is simple and clear, but also meets with basic requirements of defining a community search problem. We formulate the maximum k-plex community query (MCKPQ) problem, that is, given a set of query nodes Q, searching for optimal k-plex containing Q. We prove that MCKPQ is NP-hard, and it is hard to approximate in any constant factor. We first give exact solutions. Then, we propose an efficient branch-and-bound (B&B) method and design an effective upper bound function and a pruning strategy. Furthermore, we optimize the basic B&B by fast candidate generation. We also give a fast heuristic solution, which produces high-quality results in practice. The effectiveness of our model of community and the efficiency of our methods are verified by elaborate experiments. © 2017, The Author(s)."
"10.1007/s41019-017-0053-1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051537634&doi=10.1007%2fs41019-017-0053-1&partnerID=40&md5=d929b409a1fc1b76f0e8a14ddfb0aa1f","Most of the traditional top-k algorithms are based on a single-server setting. They may be highly inefficient and/or cause huge communication overhead when applied to a distributed system environment. Therefore, the problem of top-k monitoring in distributed environments has been intensively investigated recently. This paper studies how to monitor the top-k data objects with the largest aggregate numeric values from distributed data streams within a fixed-size monitoring window W, while minimizing communication cost across the network. We propose a novel algorithm, which adaptively reallocates numeric values of data objects among distributed nodes by assigning revision factors when local constraints are violated and keeps the local top-k result at distributed nodes in line with the global top-k result. We also develop a framework that combines a distributed data stream monitoring architecture with a sliding window model. Based on this framework, extensive experiments are conducted on top of Apache Storm to verify the efficiency and scalability of the proposed algorithm. © 2017, The Author(s)."
"10.1007/s41019-017-0055-z","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048041003&doi=10.1007%2fs41019-017-0055-z&partnerID=40&md5=e63087158eb2eb64af85380419a3cbab","Extracting keyphrases from documents automatically is an important and interesting task since keyphrases provide a quick summarization for documents. Although lots of efforts have been made on keyphrase extraction, most of the existing methods (the co-occurrence-based methods and the statistic-based methods) do not take semantics into full consideration. The co-occurrence-based methods heavily depend on the co-occurrence relations between two words in the input document, which may ignore many semantic relations. The statistic-based methods exploit the external text corpus to enrich the document, which introduce more unrelated relations inevitably. In this paper, we propose a novel approach to extract keyphrases using knowledge graphs, based on which we could detect the latent relations of two keyterms (i.e., noun words and named entities) without introducing many noises. Extensive experiments over real data show that our method outperforms the state-of-the-art methods including the graph-based co-occurrence methods and statistic-based clustering methods. © 2017, The Author(s)."
"10.5334/dsj-2017-051","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040253354&doi=10.5334%2fdsj-2017-051&partnerID=40&md5=ce87f495e510e5d13f06ecf7be511874","NASA has been collecting Earth observation data from spaceborne instruments since 1960. Today, there are tens of satellites orbiting the Earth and collecting frequent global observations for the benefit of mankind. Collaboration between NASA and organizations in the US and other countries has been extremely important in maintaining the Earth observation capabilities as well as collecting, organizing and managing the data. These collaborations have occurred in the form of: 1. NASA’s developing and launching spacecraft and instruments for operation by other agencies; 2. Instruments from collaborating organizations being flown on NASA satellites; and 3. Instruments from NASA being flown on satellites from collaborating organizations. In addition, there are collaborations such as joint science teams, data exchanges, and participation in international organizations to promote interoperability of various data systems. The purpose of this paper is to describe some of the Earth science data-related collaborative efforts in which NASA participates, and highlight a few results relevant to Earth system science research obtained through such collaborations. © 2017 The Author(s)."
"10.5334/dsj-2017-050","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040223610&doi=10.5334%2fdsj-2017-050&partnerID=40&md5=d15068b711c2b747e6a33ed2c1477bc0","Efforts to support the building of resilient pastoralism have been stepped up in Uganda through a number of activities. One of the activity is the provision of seasonal and medium-range climate forecasts to enable decisions concerning livestock herding. Seasonal weather forecasts are critical but there are challenges of timeliness and usability of the forecasts. The challenges are associated with the multiplicity of information sources, methods for data integration and dissemination channels. Institutions including public and Civil Society Organizations usually invest in collecting weather and other data which should be accessible. Often times this data remains hoarded necessitating other organizations to collect similar data. The inter-institutional relations notwithstanding, the lack of data sharing leads to minimal data available for open access. This paper illustrates that this challenge can be addressed by using combined multiple methods to elicit data on weather and other biophysical conditions for pastoralism in Karamoja. In this paper we additionally analyse the opportunities and challenges of using multiple sources of pastoral-relevant data to couple with weather information in support of herding decisions. Building resilient pastoralism that utilizes pasture and water availability will have to utilize available data. It is evident that more robust approaches for data sharing at global, regional and local levels are needed to understand how pastoralists can respond to climate shocks and changes. The paper illustrates the use of a multifaceted-methods approach including open data to develop climate forecast information for risk-reduction oriented information for decision-making. Integration of this data provides insights on how pastoralists have long adapted to a variable and changing climate, the methods and processes of adaptation to losses and damages from the climate shocks. © 2017 The Author(s)."
"10.5334/dsj-2017-049","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040244247&doi=10.5334%2fdsj-2017-049&partnerID=40&md5=452e9016a78b79389bfaa3986c7e2b81","Genomics is the study of the genetic material that constitutes the genomes of organisms. This genetic material can be sequenced and it provides a powerful tool for the study of human, plant and animal evolutionary history and diseases. Genomics research is becoming increasingly commonplace due to significant advances in and reducing costs of technologies such as sequencing. This has led to new challenges including increasing cost and complexity of data. There is, therefore, an increasing need for computing infrastructure and skills to manage, store, analyze and interpret the data. In addition, there is a significant cost associated with recruitment of participants and collection and processing of biological samples, particularly for large human genetics studies on specific diseases. As a result, researchers are often reluctant to share the data due to the effort and associated cost. In Africa, where researchers are most commonly at the study recruitment, determination of phenotypes and collection of biological samples end of the genomic research spectrum, rather than the generation of genomic data, data sharing without adequate safeguards for the interests of the primary data generators is a concern. There are substantial ethical considerations in the sharing of human genomics data. The broad consent for data sharing preferred by genomics researchers and funders does not necessarily align with the expectations of researchers, research participants, legal authorities and bioethicists. In Africa, this is complicated by concerns about comprehension of genomics research studies, quality of research ethics reviews and understanding of the implications of broad consent, secondary analyses of shared data, return of results and incidental findings. Additional challenges with genomics research in Africa include the inability to transfer, store, process and analyze large-scale genomics data on the continent, because this requires highly specialized skills and expensive computing infrastructure which are often unavailable. Recently initiatives such as H3Africa and H3ABioNet which aim to build capacity for large-scale genomics projects in Africa have emerged. Here we describe such initiatives, including the challenges faced in the generation, analysis and sharing of genomic data and how these challenges are being overcome. © 2017 The Author(s)."
"10.1177/0265813516657341","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85033481728&doi=10.1177%2f0265813516657341&partnerID=40&md5=eb8b07173b62d2dd57065cf40644913a","Today, urban planning processes involve many stakeholders and efficient dialogue tools are needed to support communication in transdisciplinary environments. The aim of our study is to identify visualization challenges in urban planning. Based on a state of the art study and a thematic analysis of 114 articles, published in 2004–2014 and found through snowball sampling, the development and implementation of digital visualization tools for dialogue are discussed. A wide range of examples of visualization tools for dialogue has been found; either based on 2D maps, 3D environments or gaming. The initiators of the development originate from different disciplines, such as geographic information (GI) science, computer graphics, 3D modelling, Virtual Reality, interaction design and urban planning. There has been an increasing amount of usability studies during recent years. There is a tendency for the usability studies to have gone from experimental and prototype studies to more and more concern real planning processes and implementation. Studies of implemented tools in real planning processes are, however, still rare. Gaming appears more and more frequently. Challenges are related to integration of qualitative and quantitative data, representation of data as regard appropriate levels of realism and detailing, as well as the user’s experience and the appearance of the digital models. There is a need to consider how we can achieve the full potential of visualization tools, including optimal effectiveness of visualization tools and processes for dialogue as well as how they can be implemented. Organizational preparedness is necessary, including clear ownership, allocation of resources for maintenance, competence and access to tools and technology. © 2016, © The Author(s) 2016."
"10.1177/0265813516659286","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85033480568&doi=10.1177%2f0265813516659286&partnerID=40&md5=396e0acdf4eb923647752b06f70d3e13","Walking is a form of active transportation with numerous benefits, including better health outcomes, lower environmental impacts and stronger communities. Understanding built environmental associations with walking behavior is a key step towards identifying design features that support walking. Human mobility data available through GPS receivers and cell phones, combined with high resolution walkability data, provide a rich source of georeferenced data for analyzing environmental associations with walking behavior. However, traditional techniques such as route choice models have difficulty with highly dimensioned data. This paper develops a novel combination of a data-driven technique with route choice modeling for leveraging walkability audits. Using data from a study in Salt Lake City, UT, USA, we apply the data-driven technique of random forests to select variables for use in walking route choice models. We estimate data-driven route choice models and theory-driven models based on predefined walkability dimensions. Results indicate that the random forest technique selects variables that dramatically improve goodness of fit of walking route choice models relative to models based on predefined walkability dimensions. We compare the theory-driven and data-driven walking route choice models based on interpretability and policy relevance. © 2016, © The Author(s) 2016."
"10.1177/0265813516659071","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85033475313&doi=10.1177%2f0265813516659071&partnerID=40&md5=9135f82c629d8f7294c27593c90bd5c7","All over Europe, it is a known fact that cities are shrinking. One of the main causes is population decline, but the consequent reduction of urban area is neither immediate nor easy to foresee spatially. Questions arise such as where do cities start to ‘shrink’ first? What are the most fragile areas that face the risk of becoming derelict? What are the most vulnerable social groups? And how does this affect real estate values across the city? Existing models for projecting the effects of shrinkage have been criticized for lacking spatial-explicitness, being excessively data-dependent, and failing to incorporate various socio-economic, urban and environmental aspects in the assessment of attractiveness of urban areas and of decisions by households. In this article, we attempt to overcome this criticism by applying the spatially-explicit Sustainable Urbanizing Landscape Development decision support tool (SULD), based on hedonic pricing theory, in two cities in southern Europe (Aveiro, Portugal and Imperia, Italy). SULD is used to project, assess and compare changes in land-use, household type distribution, real estate values and household densities, in three different scenarios of population decline (−5%, −10% and −15%). Results quantify the amount of contraction of urban area, housing quantity and living space; highlight the most problematic areas; and uncover low income households as the least affected, whereas the relocation of high income households may cause gentrification of medium income households in some areas of the historical city centre. © 2016, © The Author(s) 2016."
"10.1177/0265813516659070","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85033471916&doi=10.1177%2f0265813516659070&partnerID=40&md5=5722e0293bbad07415eba31edfafef49","To preserve the urban fabric or characteristics in specific quarters, there is often a need to either strengthen or lessen the homogeneity of the urban fabric when inserting new buildings. Evaluating the form of urban fabric is fundamentally important for urban design practice and relevant policy making. However, the quantitative methods and attempts are rare due to the lack of available methods. To address this deficiency, this article presents a GIS-based method to measure the homogeneity of urban fabric by extracting attributes directly from the geometry of 2D building footprints, including the angles between buildings, areas of building footprints, and distances between buildings. These attributes are calculated for separate overlaid grids in the open space between buildings, where each grid holds the measured values for one attribute. We test the method on a prototype, which we applied on four real sites using OpenStreetMap data. The results show how to categorize different kinds of urban fabric based on the new measure of homogeneity. The method can be used to interactively inform urban planners how new design proposals would affect the homogeneity of a neighborhood. Furthermore, the measure can be used to synthesize new design variants with a defined homogeneity. © 2016, © The Author(s) 2016."
"10.1177/0265813516658477","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85033465384&doi=10.1177%2f0265813516658477&partnerID=40&md5=6cf0bd4fc53f5f11778787b8603a0f82","Urban areas contribute to about 75% of global fossil fuel CO2 emissions and are a primary driver of climate change. If greenhouse gas emissions for the top 20 emitting urban areas were aggregated into a one country, it would rank third behind China and the US. With urban areas forecasted to triple between 2010 and 2030 and urban population expected to increase by more than 2.5 billion, sustainable development will require a better understanding of how different types of urbanization affect energy use. However, there is a scarcity of data on energy use at the urban level that are available globally. Nighttime light satellite data have been shown to be related to energy use, but to date there has not been a systematic comparison of how well different sources of nighttime light data and their derived products can proxy electricity use. This paper fills this gap. First, we perform a comparative analysis of different types of nighttime light satellite data to proxy for electricity use for US cities. Second, we examine how the different types of nighttime light satellite data scale with the size of urban settlements and connect these findings to recent theoretical advances in scaling. We find that (1) all measures of nighttime light and urban electricity use in the US are strongly correlated and (2) different nighttime light-derived data can measure distinct urban energy characteristics such as energy infrastructure volume versus energy use. Our results do not show a clear best nighttime light proxy for total electricity consumption, despite of the use of higher spatial and temporal resolution data. © 2016, © The Author(s) 2016."
"10.1177/0265813516657083","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85033458422&doi=10.1177%2f0265813516657083&partnerID=40&md5=fbee774178d9075ff3e13f52fd8eac25","A recent advancement in location-allocation modeling proposes a new objective of minimizing inequality of accessibility. Existing work considers the planning problem as either selecting new sites or adjusting facility capacities, separately. This paper develops a two-step hybrid approach to the problem by optimizing both locations and capacities of facilities towards equal accessibility. A genetic algorithm is first employed to find the best locations to site new facilities, and then a quadratic programming method is used to determine the best capacity of each facility within a pre-defined range. The sequence is consistent with many decision-making practices. Results of a series of experiments demonstrate that location optimization reduces inequality in accessibility more significantly than capacity optimization. The two-step optimization method can be applied for sequential allocation decision-makings towards maximum equal accessibility. © 2016, © The Author(s) 2016."
"10.1177/0265813516657342","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85033439386&doi=10.1177%2f0265813516657342&partnerID=40&md5=16d8fd000b0803c2c8434c5476e5d888","Access to air conditioned space is critical for protecting urban populations from the adverse effects of heat exposure. Yet there remains fairly limited knowledge of the penetration of private (home air conditioning) and distribution of public (cooling centers and commercial space) cooled space across cities. Furthermore, the deployment of government-sponsored cooling centers is likely to be inadequately informed with respect to the location of existing cooling resources (residential air conditioning and air conditioned public space), raising questions of the equitability of access to heat refuges. We explore the distribution of private and public cooling resources and access inequities at the household level in two major US urban areas: Los Angeles County, California and Maricopa County, Arizona (whose county seat is Phoenix). We evaluate the presence of in-home air conditioning and develop a walking-based accessibility measure to air conditioned public space using a combined cumulative opportunities-gravity approach. We find significant variations in the distribution of residential air conditioning across both regions which are largely attributable to building age and inter/intra-regional climate differences. There are also regional disparities in walkable access to public cooled space. At average walking speeds, we find that official cooling centers are only accessible to a small fraction of households (3% in Los Angeles, 2% in Maricopa) while a significantly higher number of households (80% in Los Angeles, 39% in Maricopa) have access to at least one other type of public cooling resource such as a library or commercial establishment. Aggregated to a neighborhood level, we find that there are areas within each region where access to cooled space (either public or private) is limited which may increase heat-related health risks. © 2016, © The Author(s) 2016."
"10.1177/0265813516658031","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85033433765&doi=10.1177%2f0265813516658031&partnerID=40&md5=8c2d722c6d1017de57b4870de72fb2b3","Research in Urban Morphology has long been exploring the form of cities and their changes over time, especially by establishing links with the parallel dynamics of these cities’ social, economic and political environments. The capacity of an adaptable and resilient urban form to provide a fertile environment for economic prosperity and social cohesion is at the forefront of discussion. Gentrification has emerged in the past few decades as an important topic of research in urban sociology, geography and economy, addressing the social impact of some forms of urban evolution. To some extent, these studies emphasize the form of the environment in which gentrification takes place. However, a systematic and quantitative method for a detailed characterization of this type of urban form is still far from being achieved. With this article, we make a first step towards the establishment of an approach based on ‘urban morphometrics’. To this end, we measure and compare key morphological features of five London neighbourhoods that have undergone a process of piecemeal gentrification. Findings suggest that these five case studies display similar and recognizable morphological patterns in terms of their built form, geographical location of main and local roads and physical relationships between street fronts and street types. These initial results, while not implying any causal or universal relationship between morphological and social dynamics, nevertheless contribute to (a) highlight the benefits of a rigorous quantitative approach towards interpreting urban form beyond the disciplinary boundaries of Urban Morphology and (b) define the statistical recurrence of a few, specific morphological features amongst the five cases of gentrified areas in London. © The Author(s) 2016"
"10.5334/dsj-2017-048","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040235308&doi=10.5334%2fdsj-2017-048&partnerID=40&md5=83621beb1e6f13bec413afacad266365","The Canadian Cryospheric Information Network and Polar Data Catalogue (CCIN/PDC) provide: (1) a trusted archive to store data from Canadian cryospheric research and (2) a public access portal to this information. The CCIN/PDC has since expanded its collection to include data from health, ecological, social, and other sciences. Since its inception, CCIN/PDC has engaged Indigenous and northern Canadians to understand and meet their information needs. This paper describes three instances of such engagement and next steps for enhanced interaction and support. First, feedback from northern and Indigenous communities led to the development of PDC Lite. Compared to the full-featured online PDC Search application, PDC Lite accommodates slower Internet speeds and allows one to search by particular northern communities. PDC Lite continues to be improved by input from the people that it serves. Next, to facilitate discussion and strengthen collaborative relationships within the polar data community, CCIN/PDC co-hosted two major meetings in 2015. Emerging from both these events was a need to prioritize what has been termed human interoperability and the need to have Indigenous and northern community involvement at all levels of data management. Future plans for CCIN/PDC include more effective partnerships in which we work with and listen to northern and Indigenous Canadians to better understand their requirements for data management services and expertise. Our ultimate goal is to provide, through collaboration with partners, data, information, and expertise that facilitate northern and Indigenous Canadians’ access to publicly-archived data and enable and support management of their own data and resources. © 2017 The Author(s)."
"10.5334/dsj-2017-047","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030845479&doi=10.5334%2fdsj-2017-047&partnerID=40&md5=c6d392d00a508afc3308a103196dd647","This pilot study of 110 scientific papers utilizing environmental sensor data from the Oklahoma Mesonet during its first two decades of operations demonstrates the diversity of potential purposes in scientific research for a robust, rigorously maintained, accessible source of environmental sensor data, as well as the challenges involved in identifying uses of that data within scientific papers. The study authors selected three publication years (1995, 2005, 2015) from an extensive corpus of peer-reviewed journal publications, identified each paper’s specific citation of and uses of the Mesonet’s environmental sensor data, and derived a typology of those usages (assimilation, experimentation, observation, simulation, utilization, validation) found to be most common. The rapid increase in data assimilation research projects today is discussed in terms of the difficulty and importance of correct attribution to individual data sources in these complex research projects. The study examines the possible role played by highly-cited papers that describe the quality assurance procedures in sensor data sources, which may serve as surrogates to signal the quality of the data provided by such sources, and which may also provide a useful contribution towards understanding data citation as a special form of scholarly citation. © 2017 The Author(s)."
"10.1080/23270012.2017.1373260","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057029965&doi=10.1080%2f23270012.2017.1373260&partnerID=40&md5=7caf6bac4f706faf3a9f57bda28d59cb","News and Media websites have evolved over time, with increasing complexity in their design, content, and monetization strategy. In this study we examined and reported the trends of rich media (images and video), social sharing (newer and older social media), and ad placements (display ads and native ads) for eight high-ranked news and media websites in four categories: online television news (CNN, FoxNews), online newspapers (LA Times, NY Times), online magazines (Wired, Forbes), and technology blogs (TechCrunch, TheNextWeb) over the course of 10 years. © 2017, © 2017 Antai College of Economics and Management, Shanghai Jiao Tong University."
"10.4018/IJBAN.2017100103","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046296393&doi=10.4018%2fIJBAN.2017100103&partnerID=40&md5=cfc63a2852ee8eb18405716fe70a01bf","Today, long-term relationship plays a vital role in supplier selection for supply chain management. The main reason is that long-term relationships can act as a mechanism for shifting the chains strategic focus from price to value and priorities long-term benefit over short-term gains. Since, in this paper we tried to address a method for optimal long-term alternative prediction and selection, focusing on purchase volume factor. For this, Markov chain model had been used and the final result showed improved effectiveness. Copyright © 2017, IGI Global."
"10.4018/IJBAN.2017100102","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046273297&doi=10.4018%2fIJBAN.2017100102&partnerID=40&md5=e8048bde2e723e4173236deefbb25f76","The insights that firms gain from big data analytics (BDA) in real time is used to direct, automate and optimize the decision making to successfully achieve their organizational goals. Data management (DM) and advance analytics (AA) tools and techniques are some of the key contributors to making BDA possible. This paper aims to investigate the characteristics of BD, processes of data management, AA techniques, applications across sectors and issues that are related to their effective implementation and management within broader context of BDA. A range of recently published literature on the characteristics of BD, DM processes, AA techniques are reviewed to explore their current state, applications, issues and challenges learned from their practice. The finding discusses different characteristics of BD, a framework for BDA using data management processes and AA techniques. It also discusses the opportunities/applications and challenges managers dealing with these technologies face for gaining competitive advantages in businesses. The study findings are intended to assist academicians and managers in effectively quantifying the data available in an organization into BD by understanding its properties, understanding the emerging technologies, applications and issues behind BDA implementation. Copyright © 2017, IGI Global."
"10.4018/IJBAN.2017100104","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046266104&doi=10.4018%2fIJBAN.2017100104&partnerID=40&md5=b58365da64a01c8655623aacc059dfa3","Revenue management is the art and science of making the right product or service available to the right customer at the right time through the right channel at right price. Dynamic pricing plays a crucial role in the implementation of revenue management in passenger airline reservation system. The liberalization of domestic aviation sector in countries such as India has seen many new market entrants resulting in higher competition while setting the flight fares. The variation in flight fares of Delhi – Mumbai passenger airline sector is studied for a departure date based on the number of days in advance the booking is made. Descriptive and inferential statistical analyses of the fares reveal the impact of airlines, booking channels and departure time windows on the pricing decisions of flight fares. The analysis framework of this study could be used as a basis for a continuous tracking study of flight fares by airline revenue managers to help them arrive at the right fare for each fare class of a flight. Copyright © 2017, IGI Global."
"10.4018/IJBAN.2017100101","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046261165&doi=10.4018%2fIJBAN.2017100101&partnerID=40&md5=acb22032a34d92842fef889fd730b046","In marketing one of the most common important tasks is to assign campaigns to sets of customers. These sets of customers, the target groups, consist of persons with similar properties, for example a high buying affinity for a certain product. Database marketers would not only assign a campaign by general economic or promotional consideration, but they take into account learning from databases by algorithms. The basic assumptions are already determined clusters to which campaigns, representing the products, should be assigned. The assignment can be done in the most optimal way by formal optimization, which is usually stochastic due to unknown campaign success in the future. The authors model the financial risk of the campaign success for enterprise practice. Their proposal is to use triangular distributions, known from financial and supply chain management applications. In an example, they demonstrate the benefits of the proposed procedure for the marketing task. © 2017, IGI Global."
"10.5334/dsj-2017-046","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030839525&doi=10.5334%2fdsj-2017-046&partnerID=40&md5=0beee9675ef66f27ae92406f5633f374","Big Data has great potential to be applied to research in the field of geosciences. Motivated by the opportunity provided by the Data Integration and Analysis System (DIAS) of Japan, we organized an intensive two-week course that aims to educate participants on Big Data and its exploitation to solve water management problems. When developing and implementing the Program, we identified two main challenges: (1) assuring that the training has a lasting effect and (2) developing an interdisciplinary curriculum suitable for participants of diverse professional backgrounds. To address these challenges, we introduced several distinctive features. The Program was based on experiential learning – the participants were required to solve real problems and worked in international and multidisciplinary teams. The lectures were strictly relevant to the case-study problems. Significant time was devoted to hands-on exercises, and participants received immediate feedback on individual assignments to ensure skills development. Our evaluation of the two occasions of the Program in 2015 and 2016 indicates significant positive outcomes. The successful completion of the individual assignments confirmed that the participants gained key skills related to the usage of DIAS and other tools. The final solutions to the case-study problems showed that the participants were able to integrate and apply the obtained knowledge, indicating that the Program’s format and curriculum were effective. We found that participants used DIAS in subsequent studies and work, thus suggesting that the Program had long-lasting effects. Our experience indicates that despite time constraints, short courses can effectively encourage researchers and practitioners to explore opportunities provided by Big Data. © 2017 The Author(s)."
"10.5334/dsj-2017-045","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030868184&doi=10.5334%2fdsj-2017-045&partnerID=40&md5=5d89bc039f82f9200978666b4e724635","Managing dynamic information in large multi-site, multi-species, and multi-discipline consortia is a challenging task for data management applications. Often in academic research studies the goals for informatics teams are to build applications that provide extract-transform-load (ETL) functionality to archive and catalog source data that has been collected by the research teams. In consortia that cross species and methodological or scientific domains, building interfaces which supply data in a usable fashion and make intuitive sense to scientists from dramatically different backgrounds increases the complexity for developers. Further, reusing source data from outside one’s scientific domain is fraught with ambiguities in understanding the data types, analysis methodologies, and how to combine the data with those from other research teams. We report on the design, implementation, and performance of a semantic data management application to support the NIMH funded Conte Center at the University of California, Irvine. The Center is testing a theory of the consequences of “fragmented” (unpredictable, high entropy) early-life experiences on adolescent cognitive and emotional outcomes in both humans and rodents. It employs cross-species neuroimaging, epigenomic, molecular, and neuroanatomical approaches in humans and rodents to assess the potential consequences of fragmented unpredictable experience on brain structure and circuitry. To address this multi-technology, multi-species approach, the system uses semantic web techniques based on the Neuroimaging Data Model (NIDM) to facilitate data ETL functionality. We find this approach enables a low-cost, easy to maintain, and semantically meaningful information management system, enabling the diverse research teams to access and use the data. © 2017 The Author(s)."
"10.5334/dsj-2017-044","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030832715&doi=10.5334%2fdsj-2017-044&partnerID=40&md5=63d4d1f9d64c5160993cbb9a93e01a24","In recent years, the promotion of data sharing has come with the recognition that not all scientists around the world are equally placed to partake in such activities. Notably, those within developing countries are sometimes regarded as experiencing hardware infrastructure challenges and data management skill shortages. Proposed remedies often focus on the provision of information and communication technology as well as enhanced data management training. Building on prior empirical social research undertaken in sub-Sahara Africa, this article provides a complementary but alternative proposal; namely, fostering data openness by enabling research. Towards this end, the underlying rationale is outlined for a ‘bottom-up’ system of research support that addresses the day-to-day demands in low-resourced environments. This approach draws on lessons from development financial assistance programs in recent decades. In doing so, this article provides an initial framework for science funding that call for holding together concerns for ensuring research can be undertaken in low-resourced laboratory environments with concerns about the data generated in such settings can be shared. © 2017 The Author(s)."
"10.5334/dsj-2017-043","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030868226&doi=10.5334%2fdsj-2017-043&partnerID=40&md5=9413ea07ecd7df6d61896364e2cdc7c9","While data sharing is becoming increasingly common in quantitative social inquiry, qualitative data are rarely shared. One factor inhibiting data sharing is a concern about human participant protections and privacy. Protecting the confidentiality and safety of research participants is a concern for both quantitative and qualitative researchers, but it raises specific concerns within the epistemic context of qualitative research. Thus, the applicability of emerging protection models from the quantitative realm must be carefully evaluated for application to the qualitative realm. At the same time, qualitative scholars already employ a variety of strategies for human-participant protection implicitly or informally during the research process. In this practice paper, we assess available strategies for protecting human participants and how they can be deployed. We describe a spectrum of possible data management options, such as de-identification and applying access controls, including some already employed by the Qualitative Data Repository (QDR) in tandem with its pilot depositors. Throughout the discussion, we consider the tension between modifying data or restricting access to them, and retaining their analytic value. We argue that developing explicit guidelines for sharing qualitative data generated through interaction with humans will allow scholars to address privacy concerns and increase the secondary use of their data. © 2017 The Author(s)."
"10.5334/dsj-2017-042","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030871788&doi=10.5334%2fdsj-2017-042&partnerID=40&md5=0a5a26933f470ad2d4e3ee3f3b6b2104","Researchers are increasingly required to make research data publicly available in data repositories. Although several organisations propose criteria to recommend and evaluate the quality of data repositories, there is no consensus of what constitutes a good data repository. In this paper, we investigate, first, which data repositories are recommended by various stakeholders (publishers, funders, and community organizations) and second, which repositories are certified by a number of organisations. We then compare these two lists of repositories, and the criteria for recommendation and certification. We find that criteria used by organisations recommending and certifying repositories are similar, although the certification criteria are generally more detailed. We distil the lists of criteria into seven main categories: “Mission”, “Community/Recognition”, “Legal and Contractual Compliance”, “Access/Accessibility”, “Technical Structure/Interface”, “Retrievability” and “Preservation”. Although the criteria are similar, the lists of repositories that are recommended by the various agencies are very different. Out of all of the recommended repositories, less than 6% obtained certification. As certification is becoming more important, steps should be taken to decrease this gap between recommended and certified repositories, and ensure that certification standards become applicable, and applied, to the repositories which researchers are currently using. © 2017 The Author(s)."
"10.5334/dsj-2017-041","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020513039&doi=10.5334%2fdsj-2017-041&partnerID=40&md5=77d9bd85e2c9c6159c8c3590a0e54157","In 2015, global attempts were made to reconcile the relationship between development and environmental issues. This led to the adoption of key agreements such as the Sustainable Development Goals. In this regard, it is important to identify and evaluate under-recognized disaster risks that hinder sustainable development: measures to mitigate climate change are the same as those that build resilience against climate-related disasters. To do this we need to advance scientific and technical knowledge, build data infrastructure that allows us to predict events with greater accuracy, and develop data archives. For this reason we have developed the Data Integration and Analysis System (DIAS). DIAS incorporates analysis, data and models from many fields and disciplines. It collects and stores data from satellites, ground observation stations and numerical weather prediction models; integrates this data with geographical and socio-economic information; then generates results for crisis management of global environmental issues. This article gives an overview of DIAS and summarizes its application to climate change analysis and disaster risk reduction. As the article shows, DIAS aims to initiate cooperation between different stakeholders, and contribute to the creation of scientific knowledge. DIAS provides a model for sharing transdisciplinary research data that is essential for achieving the goal of sustainable development. © 2017 The Author(s)."
"10.1089/big.2017.0025","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049047209&doi=10.1089%2fbig.2017.0025&partnerID=40&md5=9feedd97020821f98545d9b91419b940","Storytelling connects entities (people, organizations) using their observed relationships to establish meaningful storylines. This can be extended to spatiotemporal storytelling that incorporates locations, time, and graph computations to enhance coherence and meaning. But when performed sequentially these computations become a bottleneck because the massive number of entities make space and time complexity untenable. This article presents DISCRN, or distributed spatiotemporal ConceptSearch-based storytelling, a distributed framework for performing spatiotemporal storytelling. The framework extracts entities from microblogs and event data, and links these entities using a novel ConceptSearch to derive storylines in a distributed fashion utilizing key-value pair paradigm. Performing these operations at scale allows deeper and broader analysis of storylines. The novel parallelization techniques speed up the generation and filtering of storylines on massive datasets. Experiments with microblog posts such as Twitter data and Global Database of Events, Language, and Tone events show the efficiency of the techniques in DISCRN. © 2017, Mary Ann Liebert, Inc. 2017."
"10.1089/big.2017.0028","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049019764&doi=10.1089%2fbig.2017.0028&partnerID=40&md5=fb15534f254fde1ab08320d20001b184","We introduce a method called neighbor-based bootstrapping (NB2) that can be used to quantify the geospatial variation of a variable. We applied this method to an analysis of the incidence rates of disease from electronic medical record data (International Classification of Diseases, Ninth Revision codes) for ∼100 million individuals in the United States over a period of 8 years. We considered the incidence rate of disease in each county and its geospatially contiguous neighbors and rank ordered diseases in terms of their degree of geospatial variation as quantified by the NB2 method. We show that this method yields results in good agreement with established methods for detecting spatial autocorrelation (Moran's I method and kriging). Moreover, the NB2 method can be tuned to identify both large area and small area geospatial variations. This method also applies more generally in any parameter space that can be partitioned to consist of regions and their neighbors. © Copyright 2017, Mary Ann Liebert, Inc. 2017."
"10.1089/big.2017.0074","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049007878&doi=10.1089%2fbig.2017.0074&partnerID=40&md5=72f333b2681d122d653d60383f831438","Recent studies show the remarkable power of fine-grained information disclosed by users on social network sites to infer users' personal characteristics via predictive modeling. Similar fine-grained data are being used successfully in other commercial applications. In response, attention is turning increasingly to the transparency that organizations provide to users as to what inferences are drawn and why, as well as to what sort of control users can be given over inferences that are drawn about them. In this article, we focus on inferences about personal characteristics based on information disclosed by users' online actions. As a use case, we explore personal inferences that are made possible from ""Likes"" on Facebook. We first present a means for providing transparency into the information responsible for inferences drawn by data-driven models. We then introduce the ""cloaking device"" - a mechanism for users to inhibit the use of particular pieces of information in inference. Using these analytical tools we ask two main questions: (1) How much information must users cloak to significantly affect inferences about their personal traits? We find that usually users must cloak only a small portion of their actions to inhibit inference. We also find that, encouragingly, false-positive inferences are significantly easier to cloak than true-positive inferences. (2) Can firms change their modeling behavior to make cloaking more difficult? The answer is a definitive yes. We demonstrate a simple modeling change that requires users to cloak substantially more information to affect the inferences drawn. The upshot is that organizations can provide transparency and control even into complicated, predictive model-driven inferences, but they also can make control easier or harder for their users. © 2017, Mary Ann Liebert, Inc. 2017."
"10.1089/big.2016.0074","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049007652&doi=10.1089%2fbig.2016.0074&partnerID=40&md5=01e648b8d7496c3664bb249f5200aeb8","Significant research challenges must be addressed in the cleaning, transformation, integration, modeling, and analytics of Big Data sources for finance. This article surveys the progress made so far in this direction and obstacles yet to be overcome. These are issues that are of interest to data-driven financial institutions in both corporate finance and consumer finance. These challenges are also of interest to the legal profession as well as to regulators. The discussion is relevant to technology firms that support the growing field of FinTech. © Copyright 2017, Mary Ann Liebert, Inc. 2017."
"10.1089/big.2016.0051","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049006775&doi=10.1089%2fbig.2016.0051&partnerID=40&md5=f6b38104c6455d55b1ac032066343b46","Machine learning algorithms increasingly influence our decisions and interact with us in all parts of our daily lives. Therefore, just as we consider the safety of power plants, highways, and a variety of other engineered socio-technical systems, we must also take into account the safety of systems involving machine learning. Heretofore, the definition of safety has not been formalized in a machine learning context. In this article, we do so by defining machine learning safety in terms of risk, epistemic uncertainty, and the harm incurred by unwanted outcomes. We then use this definition to examine safety in all sorts of applications in cyber-physical systems, decision sciences, and data products. We find that the foundational principle of modern statistical machine learning, empirical risk minimization, is not always a sufficient objective. We discuss how four different categories of strategies for achieving safety in engineering, including inherently safe design, safety reserves, safe fail, and procedural safeguards can be mapped to a machine learning context. We then discuss example techniques that can be adopted in each category, such as considering interpretability and causality of predictive models, objective functions beyond expected prediction accuracy, human involvement for labeling difficult or rare examples, and user experience design of software and open data. © Copyright 2017, Mary Ann Liebert, Inc. 2017."
"10.1089/big.2017.0020","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048999266&doi=10.1089%2fbig.2017.0020&partnerID=40&md5=f725911459d6d86c8cea468ca5f4e9a6","Sentiment classification, the task of assigning a positive or negative label to a text segment, is a key component of mainstream applications such as reputation monitoring, sentiment summarization, and item recommendation. Even though the performance of sentiment classification methods has steadily improved over time, their ever-increasing complexity renders them comprehensible by only a shrinking minority of expert practitioners. For all others, such highly complex methods are black-box predictors that are hard to tune and even harder to justify to decision makers. Motivated by these shortcomings, we introduce BigCounter: a new algorithm for sentiment classification that substitutes algorithmic complexity with Big Data. Our algorithm combines standard data structures with statistical testing to deliver accurate and interpretable predictions. It is also parameter free and suitable for use virtually ""out of the box,"" which makes it appealing for organizations wanting to leverage their troves of unstructured data without incurring the significant expense of creating in-house teams of data scientists. Finally, BigCounter's efficient and parallelizable design makes it applicable to very large data sets. We apply our method on such data sets toward a study on the limits of Big Data for sentiment classification. Our study finds that, after a certain point, predictive performance tends to converge and additional data have little benefit. Our algorithmic design and findings provide the foundations for future research on the data-over-computation paradigm for classification problems. © 2017, Mary Ann Liebert, Inc. 2017."
"10.1016/j.bdr.2017.07.003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028618822&doi=10.1016%2fj.bdr.2017.07.003&partnerID=40&md5=75e587f034474748aa4fa4d477d9709a","Big Data is one of the major challenges of statistical science and has numerous consequences from algorithmic and theoretical viewpoints. Big Data always involve massive data but they also often include online data and data heterogeneity. Recently some statistical methods have been adapted to process Big Data, like linear regression models, clustering methods and bootstrapping schemes. Based on decision trees combined with aggregation and bootstrap ideas, random forests were introduced by Breiman in 2001. They are a powerful nonparametric statistical method allowing to consider in a single and versatile framework regression problems, as well as two-class and multi-class classification problems. Focusing on classification problems, this paper proposes a selective review of available proposals that deal with scaling random forests to Big Data problems. These proposals rely on parallel environments or on online adaptations of random forests. We also describe how out-of-bag error is addressed in these methods. Then, we formulate various remarks for random forests in the Big Data context. Finally, we experiment five variants on two massive datasets (15 and 120 millions of observations), a simulated one as well as real world data. One variant relies on subsampling while three others are related to parallel implementations of random forests and involve either various adaptations of bootstrap to Big Data or “divide-and-conquer” approaches. The fifth variant is related to online learning of random forests. These numerical experiments lead to highlight the relative performance of the different variants, as well as some of their limitations. © 2017 Elsevier Inc."
"10.1016/j.bdr.2017.06.005","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028469993&doi=10.1016%2fj.bdr.2017.06.005&partnerID=40&md5=afcc12e339579d3d05849d00052879f3","Many scientific applications consist of heavy computational and analysis workload on data, and often require producing intermediate data for ongoing calculations. For instance, chemical dynamics simulations are known as heavy workload applications in terms of calculation in many aspects. There is a strong desire of seeking a solution to minimize expensive calculations by replacing them with light-weight ones. VENUS is one of these chemical dynamic simulation software packages known as classical chemical dynamics simulation, with scalar executing code and heavy calculation process. In this research, we introduce an innovative approximation method by storing, managing, and leveraging intermediate data (results) in order to speed up expensive calculations. The key idea is a newly introduced data interpolation method that leverages data points from previous calculations. The newly proposed method is a general approach that can be applied to a variety of scientific applications and disciplines. In this research, we focus on chemical dynamics simulations and the VENUS code and have developed a prototype of the data interpolation method for reduced computations. The proposed computation reduction method through increased data re-use can increase the efficiency and productivity of scientific simulations, thus can have an impact on scientific discovery powered by high performance computing simulations. © 2017 Elsevier Inc."
"10.1016/j.bdr.2017.05.003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028459044&doi=10.1016%2fj.bdr.2017.05.003&partnerID=40&md5=966b5250b68244dcff11cb83e3facc89","Recently, distributed processing of large dynamic graphs has become very popular, especially in certain domains such as social network analysis, Web graph analysis and spatial network analysis. In this context, many distributed/parallel graph processing systems have been proposed, such as Pregel, PowerGraph, GraphLab, and Trinity. However, these systems deal only with static graphs and do not consider the issue of processing evolving and dynamic graphs. In this paper, we are considering the issues of scale and dynamism in the case of graph processing systems. We present BLADYG, a graph processing framework that addresses the issue of dynamism in large-scale graphs. We present an implementation of BLADYG on top of AKKA framework. We experimentally evaluate the performance of the proposed framework by applying it to problems such as distributed k-core decomposition and partitioning of large dynamic graphs. The experimental results show that the performance and scalability of BLADYG are satisfying for large-scale dynamic graphs. © 2017 Elsevier Inc."
"10.1016/j.bdr.2017.07.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028454921&doi=10.1016%2fj.bdr.2017.07.001&partnerID=40&md5=4bdea1be5dd3702010c8b5764c312b8f","Multilevel association rule mining in distributed environment plays an important role in big data analysis for making marketing strategy. Multilevel association rule provides more significant information than single level rule, and also discovers the conceptual hierarchy of knowledge from the hierarchical dataset. In this era of internet, various online marketing sites and social networking sites are generating enormous amount of data so that it becomes very difficult to process and analyze it using conventional approaches as it consumes more time. This paper overcomes the computing limitation of single node by distributing the task on multi-node cluster. The proposed method initially extracts multilevel association rules including level-crossing for each zone using distributed multilevel frequent pattern mining algorithm (DMFPM). These generated multilevel association rules are so large that it becomes complex to analyze it. Thus, MapReduce based multilevel consistent and inconsistent rule detection (MR-MCIRD) algorithm is proposed to detect the consistent and inconsistent multilevel rules from big hierarchical data which provide useful and actionable knowledge to the domain experts. These pruned interesting rules also give useful knowledge for better marketing strategy. The extracted multilevel consistent and inconsistent rules are evaluated and compared based on different interestingness measures presented together with experimental results that lead to the final conclusions. © 2017 Elsevier Inc."
"10.1016/j.bdr.2017.06.004","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028450899&doi=10.1016%2fj.bdr.2017.06.004&partnerID=40&md5=ae4cac4a29a7cfc28194d6b183d03a17","In-memory transactional data girds, often referred to as NoSQL data grids demand high concurrency for scalability and high performance in data-intensive applications. As an alternative concurrency control model, distributed transactional memory (DTM) promises to alleviate the difficulties of lock-based distributed synchronization. However, if a transaction aborts, DTM suffers from additional communication delays to remotely request and retrieve all its objects again, resulting in degraded performance. To avoid unnecessary aborts, the multi-versioning (MV) model of using multiple object versions in DTM can be considered. MV transactional memory inherently guarantees commits of read-only transactions, but limits concurrency of write transactions. We present a new transactional scheduler, called partial rollback-based transactional scheduler (or PTS), for a multi-versioned DTM model. The model supports multiple object versions to exploit concurrency of read-only transactions, and detects conflicts of write transactions at an object level. Instead of aborting a transaction, PTS assigns backoff times for conflicting transactions, and the transaction is rolled-back partially. We implemented PTS on Infinispan, and conducted comprehensive experimental studies on no and partial replication models. Our implementation reveals that PTS improves transactional throughput over MV-Transactional Forwarding Algorithm without PTS and a scalable one-copy serializable partial replication protocol (SCORe) by as much as 2.4× and 1.3×, respectively. © 2017"
"10.1016/j.bdr.2017.04.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020635899&doi=10.1016%2fj.bdr.2017.04.002&partnerID=40&md5=7339b3c53e7ff73168860bbf6137d2c9","Cardinality estimation algorithms receive a stream of elements whose order might be arbitrary, with possible repetitions, and return the number of distinct elements. Such algorithms usually seek to minimize the required storage and processing at the price of inaccuracy in their output. Real-world applications of these algorithms are required to process large volumes of monitored data, making it impractical to collect and analyze the entire input stream. In such cases, it is common practice to sample and process only a small part of the stream elements. This paper presents and analyzes a generic algorithm for combining every cardinality estimation algorithm with a sampling process. We show that the proposed sampling algorithm does not affect the estimator's asymptotic unbiasedness, and we analyze the sampling effect on the estimator's variance. © 2017 Elsevier Inc."
"10.1016/j.bdr.2017.02.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019476481&doi=10.1016%2fj.bdr.2017.02.001&partnerID=40&md5=cc22dced25f2ba26e0a416941c117df3","To ease the proliferation of big data, it frequently is transformed, be it by compression, be it by anonymization. Such transformations however modify characteristics of the data. In the case of time series, important characteristics are the occurrence of certain changes or patterns in the data, also referred to as events. Clearly, the less transformations modify events, the better for subsequent analyses. More specifically, the severity of those modifications depends on the application scenario, and quantifying it is far from trivial. In this paper, we propose MILTON, a flexible and robust Measure for quantifying the Impact of Lossy Transformations on subsequent event detectiON. MILTON is applicable to any lossy transformation technique on time-series data and to any general-purpose event-detection approach. We have evaluated it with several real-world use cases. Our evaluation shows that MILTON allows to quantify the impact of lossy transformations and to choose the best one from a class of transformation techniques for a given application scenario. © 2017 Elsevier Inc."
"10.1016/j.bdr.2017.02.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018658777&doi=10.1016%2fj.bdr.2017.02.002&partnerID=40&md5=54bb6adc8b11985f216e0d94c5f57865","Big data streams are generated continuously at unprecedented speed by thousands of data sources. The analysis of such streams need cloud resources. Due to growth of big data over cloud, allocating appropriate cloud resources has emerged as a major research problem. The current methodologies allocate cloud resources based upon data characteristics. But due to random nature of data generation, the characteristics of data in big data streams are unknown. This poses difficulty in selecting and allocating appropriate resources to big data stream. Solving this problem, an efficient resource management system is proposed in this paper. The proposed system initially estimates the data characteristics of big data stream in terms of volume, velocity, variety and variability. The estimated values are expressed in terms of a vector called Characteristics of Data (CoD). On the other hand, clusters of cloud resources are created dynamically with the help of Self-Organizing Maps (SOM). SOM uses CoD to create and allocate cluster to big data stream. Moreover, the topological ordering of clusters formed by SOM is used to reduce waiting time. The proposed system is tested experimentally. The experimental results show that the proposed system not only efficiently predicts data characteristics but also effectively enhanced the performance of cloud resources. © 2017 Elsevier Inc."
"10.1007/s41019-017-0050-4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061050778&doi=10.1007%2fs41019-017-0050-4&partnerID=40&md5=92048eefbee0eebe2dc8e32360dc8799","Information integration and workflow technologies for data analysis have always been major fields of investigation in bioinformatics. A range of popular workflow suites are available to support analyses in computational biology. Commercial providers tend to offer prepared applications remote to their clients. However, for most academic environments with local expertise, novel data collection techniques or novel data analysis, it is essential to have all the flexibility of open-source tools and open-source workflow descriptions. Workflows in data-driven science such as computational biology have considerably gained in complexity. New tools or new releases with additional features arrive at an enormous pace, and new reference data or concepts for quality control are emerging. A well-abstracted workflow and the exchange of the same across work groups have an enormous impact on the efficiency of research and the further development of the field. High-throughput sequencing adds to the avalanche of data available in the field; efficient computation and, in particular, parallel execution motivate the transition from traditional scripts and Makefiles to workflows. We here review the extant software development and distribution model with a focus on the role of integration testing and discuss the effect of common workflow language on distributions of open-source scientific software to swiftly and reliably provide the tools demanded for the execution of such formally described workflows. It is contended that, alleviated from technical differences for the execution on local machines, clusters or the cloud, communities also gain the technical means to test workflow-driven interaction across several software packages. © 2017, The Author(s)."
"10.1007/s41019-017-0047-z","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050827846&doi=10.1007%2fs41019-017-0047-z&partnerID=40&md5=6862c94b4c00ef47727e404ed5eb2148","Scalability is increasingly important for bioinformatics analysis services, since these must handle larger datasets, more jobs, and more users. The pipelines used to implement analyses must therefore scale with respect to the resources on a single compute node, the number of nodes on a cluster, and also to cost-performance. Here, we survey several scalable bioinformatics pipelines and compare their design and their use of underlying frameworks and infrastructures. We also discuss current trends for bioinformatics pipeline development. © 2017, The Author(s)."
"10.1007/s41019-017-0048-y","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048714553&doi=10.1007%2fs41019-017-0048-y&partnerID=40&md5=b05d291e58c25f158c5ef627ea016c58","Data streams have arisen as a relevant topic during the last few years as an efficient method for extracting knowledge from big data. In the robust layered ensemble model (RLEM) proposed in this paper for short-term traffic flow forecasting, incoming traffic flow data of all connected road links are organized in chunks corresponding to an optimal time lag. The RLEM model is composed of two layers. In the first layer, we cluster the chunks by using the Graded Possibilistic c-Means method. The second layer is made up by an ensemble of forecasters, each of them trained for short-term traffic flow forecasting on the chunks belonging to a specific cluster. In the operational phase, as a new chunk of traffic flow data presented as input to the RLEM, its memberships to all clusters are evaluated, and if it is not recognized as an outlier, the outputs of all forecasters are combined in an ensemble, obtaining in this a way a forecasting of traffic flow for a short-term time horizon. The proposed RLEM model is evaluated on a synthetic data set, on a traffic flow data simulator and on two real-world traffic flow data sets. The model gives an accurate forecasting of the traffic flow rates with outlier detection and shows a good adaptation to non-stationary traffic regimes. Given its characteristics of outlier detection, accuracy, and robustness, RLEM can be fruitfully integrated in traffic flow management systems. © 2017, The Author(s)."
"10.1007/s41019-017-0044-2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046547925&doi=10.1007%2fs41019-017-0044-2&partnerID=40&md5=5c438e100cee074419a522657e67ef91","The suitability of the GUHA data mining method in analyzing a big data matrix is studied in this report in general, and, in particular, a data matrix containing more than 80,000 road traffic accidents occurred in Finland in 2004–2008 is examined by LISp-Miner, a software implementation of GUHA. The general principles of GUHA are first outlined, and then, the road accident data is analyzed. As a result, more than 10,000 associations and dependencies, called hypothesis in the GUHA language, were found; some easily understandable of them are presented here. Our conclusion is that the GUHA method is useful, in particular when one wants to explore relatively small size, but still significant dependencies in a given large data matrix. © 2017, The Author(s)."
"10.1007/s41019-017-0045-1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045145313&doi=10.1007%2fs41019-017-0045-1&partnerID=40&md5=c726d20be2087147d52d24fdc752e1d8","As a recommendation technique based on historical user information, collaborative filtering typically predicts the classification of items using a single criterion for a given user. However, many application domains can benefit from the analysis of multiple criteria, e.g. tourists usually rate attractions (hotels, attractions, restaurants, etc.) using multiple criteria. In this paper, we argue that the personalised combination of multi-criteria data together with the creation and application of trust models should not only refine the tourist profile, but also improve the quality of the collaborative recommendations. The main contributions of this work are: (1) a novel profiling approach which takes advantage of the multi-criteria crowdsourced data and builds pairwise trust models and (2) the k-NN prediction of user ratings using trust-based neighbour selection. Significant experimental work has been performed using crowdsourced datasets from the Expedia and TripAdvisor platforms. © 2017, The Author(s)."
"10.1177/0265813516652115","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029440960&doi=10.1177%2f0265813516652115&partnerID=40&md5=dc49fd5ee07b0ddfd29906b12ba1e18d","Land consolidation, which aims to promote sustainable development of rural areas, involves the reorganization of space through land reallocation, both in terms of ownership and land parcel boundaries. Land reallocation, which is the core part of such schemes, is based on land values because each landowner is entitled to receive a property with approximately the same land value after land consolidation. Therefore, land value, which in the case of Cyprus is the market value, is a critical parameter, and hence it should be reliable, accurate, and fairly valued. However, the conventional land valuation process has some weaknesses. It is carried out manually and empirically by a five-member Land Valuation Committee, which visits every unique parcel in the consolidated area to assign a market value. As a result, it is time consuming and hence costly. Moreover, the outcomes can be inconsistent across valuators for whom, in the case of such a mass appraisal procedure, it is hard to analytically calculate the scores for a series of land valuation factors and compare all of these for hundreds of land parcels using a manual process. A solution to these shortcomings is the use of automated valuation models. In this context, this paper presents the development, implementation, and evaluation of an artificial neural network automated valuation model combined with a geographical information system applied in a land consolidation case study area in Cyprus. The model has been tested for quality assurance based on international standards. The evaluation showed that a sample of 15% of the selected land parcel values provided by the Land Valuation Committee is adequate for appraising the land values of all parcels in the land consolidation area with a high or acceptable accuracy, reliability, and consistency. Consequently, the automated valuation model is highly efficient compared to the conventional land valuation method since it may reduce time and resources used by up to 80%. Although the new process is based partly on the Land Valuation Committee sample, which inherently carries inconsistencies, it is systematic, analytical, and standardized, hence enhancing transparency. The comparison of artificial neural networks with similar linear and nonlinear models applied to the same case study area showed that it is capable of producing better results than the former and similar outcomes to the latter. © 2016, © The Author(s) 2016."
"10.1177/0265813516651085","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029438875&doi=10.1177%2f0265813516651085&partnerID=40&md5=3f36375d5cb42580c92ce0354a4e6058","Cellular Automata (CA) models are powerful simulation tools to study complex urban systems. Although there has been considerable amount of CA-based modeling work reported in the literature, few have been used as actual planning or policy making tools. The goal of this research aims to address the problem of “unapplicable CA models” by combining CA models with the framework of geodesign. Inspired by Batty's new science of cities, we put forward a geodesign-based framework for CA modeling to integrate the positive and normative dimension of the new science of cities as envisioned by Batty. Instead of merely applying CA models to positive urban studies, which typically requires more accurate predictions, we advocate the integration of positive (i.e. urban simulation) and normative (i.e. design) city science via the application of CA modeling to geodesign. By linking CA simulation with the geodesign framework, a geodesign-based CA model is developed and six urban design scenarios are integrated into the model, to test the urban growth impacts of these scenarios and generate a new design scenario accordingly. Using a case study of urban growth in Changping District, Beijing, we demonstrate that the normative and the positive dimension of Batty's new science for cities can be integrated. Geodesign, as a new conceptual framework, is helpful for streamlining the CA modelling process to evaluate different design scenarios and design new scenarios. In return, CA models could also contribute to the geodesign process by offering a tool for simulating and evaluating impacts of different design scenarios on urban growth. © 2016, © The Author(s) 2016."
"10.1177/0265813516655547","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029434595&doi=10.1177%2f0265813516655547&partnerID=40&md5=17c0f82b5bd9af415d39569a3dd5b5a1","Urban renewal provides valuable opportunities for sustainable development. Sustainability assessment is considered a useful tool in ensuring sustainable development in practice. Although a number of studies have been conducted to evaluate the potential of urban renewal, studies on sustainability assessment in urban renewal at a neighborhood scale are often ignored. However, urban renewal is normally accompanied by many social, economic, and environmental conflicts among various stakeholders. The present paper proposes a framework for assessing neighborhood sustainability to support urban renewal decision making in high-density cities such as Hong Kong. This framework includes two components: (1) sustainability and building condition and (2) a decision-making matrix for urban renewal strategies. A case study was conducted to illustrate how this framework can be applied in the decision-making process of urban renewal projects. The results are expected to provide references for urban renewal decision making in high-density cities. © 2016, © The Author(s) 2016."
"10.1177/0265813516654473","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029434508&doi=10.1177%2f0265813516654473&partnerID=40&md5=e5a4c1eeb5917215e3363ce2df828ed0","Ephemeral drainage patterns in the prairie pothole region of southern Alberta are not well understood at the landscape level. Municipal land use planning generally places very few constraints on development, which can leave the existing landscape topography and drainage patterns highly modified and engineered. Few if any features that exist within the pre-development landscape remain post-development. Part of the residential or industrial land development process is the creation of master drainage plans which focus on collecting and moving precipitation or snow melt away from roads and buildings through drainage ponds and piping systems. However, in prairie pothole landscapes, there is a landscape hydrology system that connects wetlands and sub-surface soil moisture flows and involves significant ephemeral components. These existing landscape flow systems provide ecosystem services in both flood and drought conditions. However, conventional land conversion processes do not generally recognize existing landscape processes like hydraulic connectivity in the development process. This creates a gap between the standard engineering approach and landscape structure and function which puts landscape processes and services at risk of being lost over time. The method demonstrated in this paper has been designed to bridge pre-development and post-development conditions for hydrologic flow systems. This method can be used as an additional cross-scalar information “layer” for use in the planning process to identify how utilities, roads and building sites can be spatially organized to complement rather than conflict with existing landscape flow systems in areas with minimal topographic relief and specifically in Prairie Pothole Region landscapes. This relatively simple technique can help reduce infrastructure costs and enables development to maintain natural flow systems and cross-scalar hydraulic connectivity. © 2016, © The Author(s) 2016."
"10.1177/0265813516655799","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029433084&doi=10.1177%2f0265813516655799&partnerID=40&md5=f55f3879b3e4c22ca72e79a6e7ba5858","Urban floods are becoming a great concern of growing cities. Urban growth pressed by poverty and social drivers, together with possible climate change, may pose difficult challenges and increasing risk to safety and urban livability. In the face of this growing risk, urban drainage management is being pressed to move towards a flood risk management approach and that builds city resilience, or the capacity to continue functioning even in future hazardous conditions. In this context, this study proposes the development of an integrated Flood Resilience Index, departing from mathematical modelling tools and flood risk concepts. The Flood Resilience Index was built to support decision-making process in choosing design alternatives that improve flood control responses in future scenarios that surpass design standards. This way, flood control design decisions would be made under a quantitative assessment of the performance of a design alternative on potential flooding events in the long term. Flood Resilience Index was successfully tested in a watershed in the metropolitan region of Rio de Janeiro/Brazil where there is uncontrolled urban growth. It identified the best alternative to be a combined approach including sustainable urban drainage measures with river restoration techniques. When looking to the city centre area, this alternative scored a Flood Resilience Index of 47 over 100 against a conservative alternative of a dam, which only scored 20. © 2016, © The Author(s) 2016."
"10.1177/0265813516649596","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029428625&doi=10.1177%2f0265813516649596&partnerID=40&md5=9af73b55ed5e60f25a28295c9f4575f2","Population density is heterogeneous, and using large spatial areas as a basis for estimates from highly urbanised areas leads to unrepresentative values. This work shows that population density estimated at the census district level (average 225 dwellings) in Canberra, Australia, poorly reflects dwelling types. Data at the individual block level (net or gross block area) greatly improve the estimates. Eight typical dwelling types in Canberra are used to show that there is a relationship between building form and estimated population density only when population density is calculated using the ‘net block’ area. To estimate population density at a finer scale than census district, the number of occupants in individual dwellings must be estimated. Assuming a city-wide constant occupancy rate in all dwelling types results in a twofold overestimation of population density in high-density dwellings. Fitting a polynomial function to the occupancy-rate and block-area data for different dwelling types of the city also provides a closer estimate than a categorical (step-wise) estimate; the occupancy rate estimate is then easily calculated from a single variable, the mean gross block size in the census district where the dwelling is located. In high-density dwellings in Canberra (e.g. more than 10 storeys), the occupancy rate was approximately 1.3 people per dwelling and in low-density dwellings (e.g. &gt; 1000 m 2 per dwelling) the occupancy rate approached 2.8 people. This work is of value to researchers and planners who use measures of population density for assessing, for example, the per capita resource sustainability of different buildings. © 2016, © The Author(s) 2016."
"10.1177/0265813516656375","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029425914&doi=10.1177%2f0265813516656375&partnerID=40&md5=34c1749dca68c85f136d06711119b58c","As use of mobile technology grows rapidly, planning agencies experiment and adopt new policies to accommodate this increase. While a significant body of literature has extolled the growth and opportunity this technological change presents, little is looking about its use by local government and urban planning organizations. This research investigates whether planning departments are capable of responding to increased mobile users given their current web and information technology tools, applying a replicable method for benchmarking website capacity. Based on a 2014 survey of planning department websites (N = 523), this study evaluates whether or not web technology, in planning departments, is tracking growth in mobile technology finding that while 99% of planning agencies have a web presence, very few planning websites have mobile responsive frameworks. Additionally, few websites have online GIS or e-permitting capability. Of those with such capacity, many use a common content management system for data management. These findings offer key design lessons for cities that want to pursue responsive websites that better match the user profile of the trend toward accessing websites from smartphones and tablets—providing useful and timely information to citizens, and respond to changing user needs. © 2016, © The Author(s) 2016."
"10.1177/0265813516650678","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029413522&doi=10.1177%2f0265813516650678&partnerID=40&md5=a7c760506d9214cd8a821a36c5c3fdaa","With increased interest in the use of network analysis to study the urban and regional environment, it is important to understand the sensitivity of centrality analysis results to the so-called “edge effect”. Most street network models have artificial boundaries, and there are principles that can be applied to minimise or eliminate the effect of the boundary condition. However, the extent of this impact has not been systematically studied and remains little understood. In this article we present an empirical study on the impact of different network model boundaries on the results of closeness and betweenness centrality analysis of street networks. The results demonstrate that the centrality measures are affected differently by the edge effect, and that the same centrality measure is affected differently depending on the type of network distance used. These results highlight the importance, in any study of street networks, of defining the network's boundary in a way that is relevant to the research question, and of selecting appropriate analysis parameters and statistics. © 2016, © The Author(s) 2016."
"10.1177/0265813516656862","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029408748&doi=10.1177%2f0265813516656862&partnerID=40&md5=6e4c6f608da8296318253bb2a88a87a8","The main purpose of this research is to provide new insights for reducing greenhouse gas (GHG) emissions linked to transportation, furthering our knowledge on linkages between urban form and economic constraints, travel behaviour, and ability-to-pay of households based on residential choices and property ownership statuses. With Quebec City (Canada) as a case study, it combines an origin-destination (OD) survey, population census data and land use records for 2006 and rests on a series of structural equations models developed at the grid cell level (3,892 cells), which allows for testing for both direct and indirect effects of urban form, accessibility and socio-economic attributes on GHG emissions, households’ transportation and housing financial burdens and motorization rate. As expected, findings suggest that GHG emissions increase with higher incomes (and education), but mainly for homeowners. Tenants displaying a high expenditure-to-income ratio for housing tend to stay close to the city centre (and jobs), thereby minimizing their overall expenditures for transportation while lowering GHG emissions. Potential accessibility by car promotes urban sprawl, thereby contributing to increased GHG emissions. In contrast, increasing residential density and land use mix while providing a better walking access to jobs and local shops tends to favour active transportation, leading to a significant reduction in GHG emissions. © 2016, © The Author(s) 2016."
"10.5334/dsj-2017-039","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030859282&doi=10.5334%2fdsj-2017-039&partnerID=40&md5=f58de2dd4a0b9cdab234f2da7aa980f3","In this paper we present a draft vocabulary for making “persistence statements.” These are simple tools for pragmatically addressing the concern that anyone feels upon experiencing a broken web link. Scholars increasingly use scientific and cultural assets in digital form, but choosing which among many objects to cite for the long term can be difficult. There are few well-defined terms to describe the various kinds and qualities of persistence that object repositories and identifier resolvers do or don’t provide. Given an object’s identifier, one should be able to query a provider to retrieve human- and machine-readable information to help judge the level of service to expect and help gauge whether the identifier is durable enough, as a sort of long-term bet, to include in a citation. The vocabulary should enable providers to articulate persistence policies and set user expectations. © 2017 The Author(s)."
"10.5334/dsj-2017-037","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030854398&doi=10.5334%2fdsj-2017-037&partnerID=40&md5=713288ebb43ffa0df8621fdfc5fd9063","Incomplete data are ubiquitous in social sciences; as a consequence, available data are inefficient (ineffective) and often biased. In the literature, multiple imputation is known to be the standard method to handle missing data. While the theory of multiple imputation has been known for decades, the implementation is difficult due to the complicated nature of random draws from the posterior distribution. Thus, there are several computational algorithms in software: Data Augmentation (DA), Fully Conditional Specification (FCS), and Expectation-Maximization with Bootstrapping (EMB). Although the literature is full of comparisons between joint modeling (DA, EMB) and conditional modeling (FCS), little is known about the relative superiority between the MCMC algorithms (DA, FCS) and the non-MCMC algorithm (EMB), where MCMC stands for Markov chain Monte Carlo. Based on simulation experiments, the current study contends that EMB is a confidence proper (confidence-supporting) multiple imputation algorithm without between-imputation iterations; thus, EMB is more user-friendly than DA and FCS. © 2017 The Author(s)."
"10.1080/23270012.2017.1344938","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054361188&doi=10.1080%2f23270012.2017.1344938&partnerID=40&md5=3e5bbb7ff96892163f12d72213bbb24b","This paper studies the effects of integrating four inventory classification methods into constant work-in-process (CONWIP) systems in a multistage batch production. The inventory classification methods are multiple closed-loop CONWIP (MCC), parallel CONWIP (PC), analytical hierarchy procedure, and weighted linear optimization (WLO). Discrete-event simulation models were built. A full-factorial experimental design was employed. Response surface methodology generated suitable regression models for performance comparison of different ABC analysis methods. In the second stage of experiment, the ABC analysis methods from the first stage were fine-tuned and compared. PC and WLO are the most preferred methods because of higher total outputs, lower work-in-process levels, and shorter flow times compared with the other two methods. However, the revenue accrued through WLO is higher than that accrued via PC. This study complements existing literature on price setting in manufacturing organizations by delineating classification of finished goods based on both exogenous and production factors. © 2017, © 2017 Antai College of Economics and Management, Shanghai Jiao Tong University."
"10.1080/23270012.2017.1344939","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053535552&doi=10.1080%2f23270012.2017.1344939&partnerID=40&md5=6c475e5f91fa32f55d21c1f565528869","This paper considers a model that deals with an imperfect production process where both perfect and imperfect quality items are produced. Here, demand depends on selling price and reliability of the product. Each manufacturing company expects to produce perfect quality items. But due to the long-run process, several kinds of problem such as labor, machinery, and technology arise. As a result, the manufacturing system becomes out-of-control state and consequently produces both perfect and imperfect quality items. Perfect items are ready to sell but imperfect items are reworked at a cost to become perfect. Reworking cost, reliability of the product and reliability parameter of the manufacturing system can be improved by introducing the development cost and also by improving the quality of the raw material of the production system. Under such circumstances, a profit function has been developed to find the optimum values of reliability parameter of the manufacturing system, reliability of the product and duration of production such that a manufacturer gets a maximum profit. Finally, the model has been illustrated with some numerical examples exploring the sensitivity analysis with respect to some parameters. © 2017, © 2017 Antai College of Economics and Management, Shanghai Jiao Tong University."
"10.1080/23270012.2017.1304291","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052839032&doi=10.1080%2f23270012.2017.1304291&partnerID=40&md5=ecc297e74754a974234699f2e2fa03c7","The project success is critical to the business performance in the era of fierce competition and globalization. The basis for project success lies in the capabilities of managing risks effectively. Innovation has always been considerably risky; however, managing Research and Development (R&D) project risks has become even more important given today’s tight schedules and limited resources. Risk management has to be an integral part of the development process. The purpose of this research is to develop a model to assess and estimate the risk exposure of an R&D project. A risk quantification model based on the Bayesian belief network is proposed, which is effective in capturing the interaction between various risk factors. The aim of this model is to empower the project managers to predict the failure risk probability of R&D projects. © 2017, © 2017 Antai College of Economics and Management, Shanghai Jiao Tong University."
"10.1080/23270012.2017.1357508","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052520403&doi=10.1080%2f23270012.2017.1357508&partnerID=40&md5=50303b519d028d219ac9765123c2f88d","In this paper, we consider a supply chain with one supplier and one retailer who jointly make their efforts to improve quality along supply chain. Investment decisions of the retailer and the supplier as well as order quantity of the retailer are made consequently in order to maximize each party’s own profit. From analytical results, we find that the retailer only places the order if the joint investment exceeds certain threshold. The region in which the supplier and retailer make joint efforts in the investment is provided when the demand is exponentially distributed. Numerical studies are given to reveal the effect of the parameters. © 2017, © 2017 Antai College of Economics and Management, Shanghai Jiao Tong University."
"10.1080/23270012.2017.1310000","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044944615&doi=10.1080%2f23270012.2017.1310000&partnerID=40&md5=7a79d83b6e8af877df9258be0e1da9e3","With the rapid rising of distributed appliance, smart home appliances and mobile Internet, smart home manufacturers in Europe and America have all developed energy management solutions for individual usage, while domestic manufacturers have also launched similar products. This technique helps people to manage their energy consumption more efficiently. This article focuses on the factors that affect energy management behavior (EMB) at the individual level. By reviewing academic literature, conducting surveys in Beijing, Shanghai and Guangzhou, the author built an integrated model of the energy management of Chinese people. This article is the first to separate EMB into two types of behaviors: the more innovative energy management intention (EMI) and the more traditional energy saving intention (ESI). The author conducts statistical analysis on these two behavioral concepts. An individual’s EMB is mainly driven by EMI. EMI is affected by behavioral attitudes, subjective norm and perceived behavioral control (PBC). Among these three key factors, PBC has the strongest influence. This implies that the promotion of the energy management concept is mainly driven by good user experience, which is based on products’ features. The traditional ESI also demonstrates a positive influence on EMB, but its impact is weaker than those three factors of EMI. In other words, the government and manufacturers will not be able to promote individual EMB by only selling the concept of traditional energy saving. At the same time, the study finds that the government may achieve better advertising results by developing subsidy policy. © 2017, © 2017 Antai College of Economics and Management, Shanghai Jiao Tong University."
"10.1080/23270012.2017.1284622","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020935137&doi=10.1080%2f23270012.2017.1284622&partnerID=40&md5=ef47aec2a7bf867d246cb8c1508eb129","The aim of the paper is to benchmark the performance of the Indian fertilizer-manufacturing organizations based on the ranking of efficiencies using a fuzzy data envelopment analysis (FDEA). FDEA has been used to find the relative efficiency and ranking of the fertilizer-manufacturing organizations. The last few years’ data have been converted into the fuzzy inputs and outputs as minimum, mean, and maximum values, respectively. The performance of the fertilizer manufacturing organizations is based on the output maximization model of DEA. The frontier organizations set the benchmark for the lagging organizations for further improvement in the performance. This method can also be used to incorporate the data of the several years for multiple inputs and outputs instead of consideration of data of only one year. The proposed approach in this study may help organizations to improve its efficiency to fulfill its goal. © 2017, © 2017 Antai College of Economics and Management, Shanghai Jiao Tong University."
"10.1016/j.bdr.2017.04.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017418943&doi=10.1016%2fj.bdr.2017.04.001&partnerID=40&md5=b48dd766f621646362ff6d012758d8e8","For many scientific disciplines, the transition to using advanced cyberinfrastructure comes not out of a desire to use the most advanced or most powerful resources available, but because their current operational model is no longer sufficient to meet their computational needs. Many researchers begin their computations on their desktop or local workstation, only to discover that the time required to simulate their problem, analyze their instrument data, or score the multitude of entities that they want to would require far more time than they have available. Launcher is a simple utility which enables the execution of high throughput computing workloads on managed HPC systems quickly and with as little effort as possible on the part of the user. Basic usage of the Launcher is straightforward, but Launcher provides several more advanced capabilities including use of Intel® Xeon Phi™ coprocessor cards and task binding support for multi-/many-core architectures. We step through the processes of setting up a basic Launcher job, including creating a job file, setting appropriate environment variables, and using scheduler integration. We also describe how to enable use of the Intel® Xeon Phi™ coprocessor cards, take advantage of Launcher's task binding system, and execute many parallel (OpenMP/MPI) applications at once. © 2017 Elsevier Inc."
"10.1016/j.bdr.2017.01.004","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013491920&doi=10.1016%2fj.bdr.2017.01.004&partnerID=40&md5=ba949461766b1e26ef2be6df82fa580e","The enabling of scientific experiments increasingly includes data, software, computational and simulation elements, often embarrassingly parallel, long running and data-intensive. Frequently, such experiments are run in a cloud environment or on high-end clusters and supercomputers. Many disciplines in sciences and engineering (and outside computer science) find the requisite computational skills attractive on the one hand but distracting from their science domain on the other. We developed Chiminey under directions by quantum physicists and molecular biologists, to ease the steep learning curve in data management and software platforms, required for the complex computational target systems. Chiminey is a smart connector mediating running specialist algorithms developed for workstations with moderately large data set and relatively small computational grunt. This connector allows the domain scientists to choose the target platform and then manages it automatically; it accepts all the necessary parameters to run many instances of their program regardless of whether this runs on a peak supercomputer, a commercial cloud like Amazon EC2 or (in Australia) the national federated university cloud system NeCTAR. Chiminey negotiates with target system schedulers, dashboards and data bases and provides an easy-to-use dashboard interface to the running jobs, regardless of the specific target platform. The smart connector encapsulates and virtualises a number of further aspects that the domain scientists directing our effort found necessary or desirable. In this article we present Chiminey and guide the reader through a hands-on tutorial of this open-source platform. The only requirement is that the reader has access to one of the supported clouds or cluster platforms – and very likely there is a matching one. The tutorial stages range in difficulty from requiring no to little technical background through to advanced sections, such as programming your own domain-specific extension on top of Chiminey application programmer interfaces. The different exercises we demonstrate include: installing the Docker deployment environment and Chiminey system; registering resources for file stores, Hadoop MapReduce and cloud virtual machines; activating hrmclite and wordcount smart connectors – two demonstrators; running a smart connector and investigating the resulting output files; and building a new smart connector. We also discuss briefly where to find more detailed information on, and what is involved in, contributing to the Chiminey open source code base. © 2017"
"10.1016/j.bdr.2017.01.005","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013391731&doi=10.1016%2fj.bdr.2017.01.005&partnerID=40&md5=44591940187020b98d9c96f39bdb139a","Big Data concerns with large-volume complex growing data. Given the fast development of data storage and network, organizations are collecting large ever-growing datasets that can have useful information. In order to extract information from these datasets within useful time, it is important to use distributed and parallel algorithms. One common usage of big data is machine learning, in which collected data is used to predict future behavior. Deep-Learning using Artificial Neural Networks is one of the popular methods for extracting information from complex datasets. Deep-learning is capable of more creating complex models than traditional probabilistic machine learning techniques. This work presents a step-by-step guide on how to prototype a Deep-Learning application that executes both on GPU and CPU clusters. Python and Redis are the core supporting tools of this guide. This tutorial will allow the reader to understand the basics of building a distributed high performance GPU application in a few hours. Since we do not depend on any deep-learning application or framework—we use low-level building blocks—this tutorial can be adjusted for any other parallel algorithm the reader might want to prototype on Big Data. Finally, we will discuss how to move from a prototype to a fully blown production application. © 2017 Elsevier Inc."
"10.1016/j.bdr.2017.01.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85010972097&doi=10.1016%2fj.bdr.2017.01.002&partnerID=40&md5=0c3155f40d8dff5d0dc7f3047d001973","Many scientific investigations require data-intensive research where big data are collected and analyzed. To get big insights from big data, we need to first develop our initial hypotheses from the data and then test and validate our hypotheses about the data. Visualization is often considered a good means to suggest hypotheses from a given dataset. Computational algorithms, coupled with scalable computing, can perform hypothesis testing with big data. Furthermore, interactive visual interfaces can allow domain experts to directly interact with data and participate in the loop to refine their research questions and redirect their research directions. In this paper we discuss a framework that integrates information visualization, scalable computing, and user interfaces to explore large-scale multi-modal data streams. Discovering new knowledge from the data requires the means to exploratively analyze datasets of this scale—allowing us to freely “wander” around the data, and make discoveries by combining bottom-up pattern discovery and top-down human knowledge to leverage the power of the human perceptual system. We start with a novel interactive temporal data mining method that allows us to discover reliable sequential patterns and precise timing information of multivariate time series. We then proceed to a parallelized solution that can fulfill the task of extracting reliable patterns from large-scale time series using iterative MapReduce tasks. Our work exploits visual-based information technologies to allow scientists to interactively explore, visualize and make sense of their data. For example, the parallel mining algorithm running on HPC is accessible to users through asynchronous web service. In this way, scientists can compare the intermediate data to extract and propose new rounds of analysis for more scientifically meaningful and statistically reliable patterns, and therefore statistical computing and visualization can bootstrap each another. Furthermore, visual interfaces in the framework allows scientists to directly participate in the loop and can redirect the analysis direction. All these combine to reveal an effective and efficient way to perform closed-loop big data analysis with visualization and scalable computing. © 2017"
"10.1016/j.bdr.2017.01.003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85009802470&doi=10.1016%2fj.bdr.2017.01.003&partnerID=40&md5=0fda1fbce3d6fda8b4e26518a1a10550","Excessive energy consumption is a major constraint in designing and deploying the next generation of supercomputers. Minimizing energy consumption of high performance computing and big data applications requires novel energy-conscious technologies (both hardware and software) at multiple layers from architecture, system support, and applications. In the past decade, we have witnessed the significant progress toward developing more energy-efficient hardware and facility infrastructure. However, the energy efficiency of software has not been improved much. One obstacle that hinders the exploration of green software technologies is the lack of tools and systems that can provide accurate, fine-grained, and real-time power and energy measurement for technology evaluation and verification. Marcher, a heterogeneous high performance computing infrastructure, is built to fill the gap by providing support to research in energy-aware high performance computing and big data analytics. The Marcher system is equipped with Intel Xeon CPUs, Intel Many Integrated Cores (Xeon Phi), Nvidia GPUs, power-aware memory systems and hybrid storage with Hard Disk Drives (HDDs) and Solid State Disks (SSDs). It provides easy-to-use tools and interfaces for researchers to obtain decomposed and fine-grained power consumption data of these primary computing components. This paper presents the design of the Marcher system and demonstrates the usage of Marcher power measurement tools to obtain detailed power consumption data in various research projects. © 2017 Elsevier Inc."
"10.4018/IJBAN.2017070105","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046286810&doi=10.4018%2fIJBAN.2017070105&partnerID=40&md5=eedf631123430d519eae784e732b9976","In the current literature, there are several studies, which the supplier selection is typically a Multi Criteria Group Decision Making problem. Several solutions for the above problem are proposed (from simple approaches; like, Borda, Condorcet, etc., to complex ones; like, Multiple Criteria Decision Making model combined with intuitionistic fuzzy set, etc.). To solve this problem, different method (particularly, extended TOPSIS method) are proposed in this paper. Firstly, we have used TOPSIS to find the individual preference ordering, then, we have used the extended version of this method to find the collective preference orderings. In addition, this model is capable of considering the expert weights. Finally, the proposed approach is compared with an existed approach (i.e., TOPSIS and Borda’s function). Compared results show the advantage of our extended model over previous one. Copyright © 2017, IGI Global."
"10.1177/0265813516657828","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85022346689&doi=10.1177%2f0265813516657828&partnerID=40&md5=c6ae1f9e57cdae318170136911a52a2b","Small apartments exist in dense cities worldwide. Developing adequate quality small apartments in city centres would strongly attract current demand. Analysis, predictive tools and design concepts regulated for better interior design and lower perceived density would provide residential environments with happier tenants. This research evaluates perceived density and visual privacy in alternative minimum apartments based on the spatial openness index three-dimensional visibility analysis, i.e. their accumulated three-dimensional visibility calculations. The compatibility of the spatial openness index concept as predicting perceived density in minimum apartments was examined and assessed. An empirical study was conducted using a virtual reality experiment in a controlled environment – An immersive three-dimensional visualisation laboratory 1 , with more than 100 subjects participated in the experiment. Significant relations were found between perceived density and visual privacy for all apartments (single and double story). The measured volume of visible space from the door viewpoint (the entrance to minimum apartments) and the perceived density evaluated there by participants, and a general evaluation for the apartment are strongly related for both (single and double story groups). Strong correlation was found between visibility measurements from the door and the ‘visual privacy’ at the sofa/bed. According to assessment results the spatial openness index three-dimensional visibility analysis can predict the perceived density of minimum apartments. © 2016, © The Author(s) 2016."
"10.1177/0265813515624688","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85022337701&doi=10.1177%2f0265813515624688&partnerID=40&md5=3cef12ef3a118f5a5f32ad9e263f7dfa","The scale and proportions of “streetscape skeletons,” the three-dimensional spaces of streets defined by the massing and arrangement of surrounding buildings, are theoretically relevant to the way human users perceive and behave. Nonetheless, the dominant ways of measuring and identifying streets emphasize vehicular service and functionality. Moreover, existing built environment-based classifications have focused on recommended forms rather than characterizing the full range of existing conditions that must be accounted for in policy and understanding of human–environment interactions. To work toward a better streetscape measurement and classification scheme, this study investigated how large numbers of streetscapes could be efficiently measured to evaluate design patterns across and between multiple cities. Using a novel GIS-based method, 12 streetscape skeleton variables were measured on more than 120,000 block-length streetscapes in three northeastern U.S. cities: Boston, MA, New York, NY, and Baltimore, MD. Logistic regression models based on these variables were unsuccessful at distinguishing between cities, confirming that the variables were similarly applicable to each city and that the cities had comparable streetscape skeleton identities. Cluster analyses were then used to identify four streetscape skeleton classes that were also consistent between cities: upright, compact, porous, and open. These classes were distinct from the widely used highway functional class system, reinforcing the distinction between streetscape design and roadway functionality and thus the importance of accounting for them separately. The streetscape skeleton classes provide a digestible yet objective system for identifying prevalent streetscape designs that are appropriate for urban policy design, advocacy, and urban systems research. © 2016, © The Author(s) 2016."
"10.1177/0265813516641685","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85022336094&doi=10.1177%2f0265813516641685&partnerID=40&md5=39e4cd0f4e199370c5d5c2bf2c63b2a1","Pedestrian accessibility has been growing in importance as an urban planning objective. Assessing it with gravity-based or potential accessibility measures requires the selection of an impedance function in order to reflect the friction of distance. The choice of impedance function is crucial to pedestrian accessibility assessment due to the level of spatial data detail required and also because perceived distances differ from physical distances. Here, we measure and compare 20 gravity-based measures, varying the impedance function and associated parameters. Correlation analysis revealed a significant and strong correlation between the measures. Factor analysis extracted two groups of measures, differing mainly in their maximum cutoff travel distance, i.e. the distance at which the impedance function reaches zero. Spatial analysis revealed that all measures produce similar spatial results in terms of identifying high and low accessibility locations but different values for medium accessibility locations. Places located at between 200 and 400 m from an opportunity are especially sensitive to the impedance function used. We promote a cumulative–Gaussian approach to measure pedestrian accessibility, as it explicitly includes the travel tolerance concept and we found it to be the most robust measure in terms of data variability. © 2016, © The Author(s) 2016."
"10.1177/0265813516637607","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85022327486&doi=10.1177%2f0265813516637607&partnerID=40&md5=268e634d1ef5b0d665214e2ba35837d8","Information about the land use of built-up areas is required for the comprehensive planning and management of cities. However, due to the high cost of the land use surveys, land use data is out-dated or not available for many cities. Therefore, we propose the reuse of up-to-date and low-cost place data from social media applications for land use mapping purposes. As main case study, we used Foursquare place data for estimating nonresidential building block use in the city of Amsterdam. Based on the Foursquare place categories, we estimated the use of 9827 building blocks, and we compared the classification results with a reference building block use dataset. Our evaluation metric is the kappa coefficient, which determines if the classification results are significantly better than a random guess result. Using the optimal set of parameter values, we achieved the highest kappa coefficient values for the land use categories “hotels, restaurants and cafes” (0.76) and “retail” (0.65). The lowest kappa coefficients were found for the land use categories “industries” and “storage and unclear”. We have also applied the methodology in another case study area, the city of Varese in Italy, where we had similar accuracy results. We therefore conclude that Foursquare place data can be trusted only for the estimation of particular land use categories. © 2016, © The Author(s) 2016."
"10.1177/0265813515605097","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85022326655&doi=10.1177%2f0265813515605097&partnerID=40&md5=ac0efa77dccf69639270971a04faf4f1","In this article, the authors has explored the feasibility of three-dimensional visibility analysis and visual quality computation via the aid of Google SketchUp and WebGIS, which may be beneficial for measurable evaluation of the environment in urban open spaces, and referential for urban planning and design to build an environment with better visual comfort. Considered for long-term significance, three-dimensional visibility analysis and visual quality computation are potential in enabling the quantitative analysis between urban open spaces and visual perception, providing appropriate standard for the evaluation of environment, and making the future planning and design rational and reasonable. Due to the easy access for entry-level programmer and the public, Google SketchUp and WebGIS were adopted as proper and efficient tools for three-dimensional visibility analysis in this research, which considerably decreased the programming development difficulty. Both SketchUp and WebGIS are thought to be well accepted by the public, as SketchUp has been popularized in three-dimensional modelling and WebGIS has been familiar as the form of websites for a long period, which may enable the dissemination of visibility analysis to the public. From a pilot study of progresses based on the past scholars’ researches, the authors developed an improved method for three-dimensional visibility analysis, by mathematically deriving the visual factors from the spatial relationship analysis of buildings, terrain and other geographical features. A few quantitative factors such as the distance, solid angle and visual field (a distribution of occupied solid angle in all directions) valued in spherical coordinate system were adopted as the basic units for visibility levels. Starting from the space prototype, the research has also focused on several aspects possibly associated with the visual effects in open spaces, including the openness, enclosure and ground coverage for edge; the distribution and dispersion for skyline and the visibility of individual building for landmark. For further comparison, the variances of those figures were also noticed during space scales changing for the prototype, in order to find possible connection or changing trend before and after. Moreover, experiments of three-dimensional visibility analysis have been designed and taken for real scenes to discover the similarities or differences between prototype and reality, and Piazza del Campo (Siena, Italy), Piazza San Marco (Venice, Italy) and Olomouc centre area (Olomouc, Czech) have been selected as the first group of candidates. Those would be the possible references for making quantitative assessments in real scenes via collection of visual factors, enabling the comparison of similarities and differences among various urban open spaces. © 2015, © The Author(s) 2015."
"10.1177/0265813515624686","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85022326031&doi=10.1177%2f0265813515624686&partnerID=40&md5=7fe434a018c3770c76c812dcca824c1c","The aim of this paper is to discuss the representation of space in statistical models of urban crime. We argue that some important information represented by the properties of space is either lost or hardly interpretable if those properties are not explicitly introduced in the model as regressors. We illustrate the issue commenting on the shortcomings of the two standard approaches to modeling the dispersion of crime in a city: using local attributes of places as regressors, and defining a catch-all spatial component to neutralize the effect of latent spatial factors from the model. As an alternative to the current methods, the metrics of spatial configuration, including those devised by the technique called Space Syntax Analysis, provide useful variables that can be introduced as regressors. Such regressors offer interpretable information on space, behavior, and their interactions, that would otherwise be lost. We therefore consider a set of three configurational variables that represent different forms of centrality and that are thought to have influence on a wide range of human activities. We propose an innovative procedure to adapt these variables to most urban graphs and then, using data from a large area in the city of Genoa (Italy), we show that the three variables are well defined, consistent, noncollinear indicators, with evident spatial meanings. Then we build two sets of Hierarchical Bayesian count models of different urban crime types (“property crime” and “arson and criminal damage”) around some known covariates of crime and we show that the overall quality of the models is improved (with the size of improvement depending on the type of crime) when the three configurational variables are included. Furthermore, we show that what the three variables explain of the overall variability of crime is a sizeable part of what would be the spatial error term of a traditional spatial model of urban crime. While the configurational variables alone cannot provide a goodness of fit as high as the one obtained with a generic spatial term, they have a relevant role for the interpretation of the results, which is ultimately the objective of urban crime modeling. © 2015, © The Author(s) 2015."
"10.1177/0265813516637608","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85022322743&doi=10.1177%2f0265813516637608&partnerID=40&md5=59b391787f9c24daf0c58f52258252dc","Hydrological infrastructure components such as pumps, floodgates, and flood gauges are invaluable assets for mitigating flooding, which threatens millions of lives and damages property worth billions of dollars in coastal mega-cities around the world. By improving the understanding of how these hydrological infrastructure components are both spatially and topologically connected through waterways (rivers, canals, streams, etc.) within coastal mega-cities, more precise decisions can be made regarding the most appropriate hydrological infrastructure components required to mitigate flooding during emergency conditions. This paper explores the use of graph theory to create a spatio-topological model of a real world hydrological infrastructure network for one of the most representative coastal mega-cities—Jakarta, Indonesia. The network is modeled as a directed multigraph, with hydrological infrastructure represented as network nodes and waterways as edges. The article demonstrates how the network model can be used as a real-time decision support tool for responding to flooding events by alerting decision makers to the occurrence of rising water levels in any given area and, suggesting the most appropriate infrastructure components to engage in order to prevent a given area from flooding. © 2016, © The Author(s) 2016."
"10.5334/dsj-2017-035","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85024382786&doi=10.5334%2fdsj-2017-035&partnerID=40&md5=a7d96aab201618a0a5d672fca9bdced2","Even though the importance of sharing data is frequently discussed, data sharing appears to be limited to a few fields, and practices within those fields are not well understood. This study examines perspectives on sharing neutron data collected at Oak Ridge National Laboratory’s neutron sources. Operation at user facilities has traditionally focused on making data accessible to those who create them. The recent emphasis on open data is shifting the focus to ensure that the data produced are reusable by others. This mixed methods research study included a series of surveys and focus group interviews in which 13 data consumers, data managers, and data producers answered questions about their perspectives on sharing neutron data. Data consumers reported interest in reusing neutron data for comparison/verification of results against their own measurements and testing new theories using existing data. They also stressed the importance of establishing context for data, including how data are produced, how samples are prepared, units of measurement, and how temperatures are determined. Data managers expressed reservations about reusing others’ data because they were not always sure if they could trust whether the people responsible for interpreting data did so correctly. Data producers described concerns about their data being misused, competing with other users, and over-reliance on data producers to understand data. We present the Consumers Managers Producers (CMP) Model for understanding the interplay of each group regarding data sharing. We conclude with policy and system recommendations and discuss directions for future research. © 2017 The Author(s)."
"10.5334/dsj-2017-034","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017314341&doi=10.5334%2fdsj-2017-034&partnerID=40&md5=2dbd3e0150f5b5138af4c01ba10f2821","The need to identify both digital and physical objects is ubiquitous in our society. Past and present persistent identifier (PID) systems, of which there is a great variety in terms of technical and social implementation, have evolved with the advent of the Internet, which has allowed for globally unique and globally resolvable identifiers. PID systems have, by in large, catered for identifier uniqueness, integrity, and persistence, regardless of the identifier’s application domain. Trustworthiness of these systems has been measured by the criteria first defined by Bütikofer (2009) and further elaborated by Golodoniuc et al. (2016) and Car et al. (2017). Since many PID systems have been largely conceived and developed by a single organisation they faced challenges for widespread adoption and, most importantly, the ability to survive change of technology. We believe that a cause of PID systems that were once successful fading away is the centralisation of support infrastructure – both organisational and computing and data storage systems. In this paper, we propose a PID system design that implements the pillars of a trustworthy system – ensuring identifiers’ independence of any particular technology or organisation, implementation of core PID system functions, separation from data delivery, and enabling the system to adapt for future change. We propose decentralisation at all levels — persistent identifiers and information objects registration, resolution, and data delivery — using Distributed Hash Tables and traditional peer-to-peer networks with information replication and caching mechanisms, thus eliminating the need for a central PID data store. This will increase overall system fault tolerance thus ensuring its trustworthiness. We also discuss important aspects of the distributed system’s governance, such as the notion of the authoritative source and data integrity. © 2017 The Author(s)."
"10.5334/dsj-2017-033","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85024364698&doi=10.5334%2fdsj-2017-033&partnerID=40&md5=446aba70dd85e77bec2d706bdcac0e43","Rothschild and Stiglitz (1976) argued that people signal their risk profile through their insurance demand, i.e. individuals with a high risk profile would buy insurance as much as they can, while people who are not going to buy any insurance are the ones with a lower risk profile. This issue is commonly known as adverse selection. Even if their prediction seems to work quite well in a lot of different markets, Cutler et al. (2008) proved that there exist some insurance markets in United States in which the expected result is completely different. In the wake of this study, we provide empirical evidences that there are some European insurance markets in which the low risk profile agents are the ones who buy more insurance. © 2017 The Author(s)."
"10.5334/dsj-2017-032","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85024406058&doi=10.5334%2fdsj-2017-032&partnerID=40&md5=f2803cbda8649785a419eef5ba71ac60","This paper reflects on the relation between international debates around data quality assessment and the diversity characterising research practices, goals and environments within the life sciences. Since the emergence of molecular approaches, many biologists have focused their research, and related methods and instruments for data production, on the study of genes and genomes. While this trend is now shifting, prominent institutions and companies with stakes in molecular biology continue to set standards for what counts as ‘good science’ worldwide, resulting in the use of specific data production technologies as proxy for assessing data quality. This is problematic considering (1) the variability in research cultures, goals and the very characteristics of biological systems, which can give rise to countless different approaches to knowledge production; and (2) the existence of research environments that produce high-quality, significant datasets despite not availing themselves of the latest technologies. Ethnographic research carried out in such environments evidences a widespread fear among researchers that providing extensive information about their experimental set-up will affect the perceived quality of their data, making their findings vulnerable to criticisms by better-resourced peers. These fears can make scientists resistant to sharing data or describing their provenance. To counter this, debates around Open Data need to include critical reflection on how data quality is evaluated, and the extent to which that evaluation requires a localised assessment of the needs, means and goals of each research environment. © 2017 The Author(s)."
"10.5334/dsj-2017-031","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85024376421&doi=10.5334%2fdsj-2017-031&partnerID=40&md5=4dc46bc8747d473a2416cd4c98ae502b","The research was aimed at evaluating how research data are being managed in research institutions in Zimbabwe. The study also sought to assess the challenges that are faced in research data management by research institutions in Zimbabwe. Twenty five institutions of higher learning and other organisations that deal with research were selected using purposive sampling to participate in the study. An online questionnaire on SurveyMonkey was sent to the selected participants and telephone interviews were done to follow up on participants who failed to respond on time. Data that were collected using interviews were entered manually into SurveyMonkey for easy analysis. It was found out that proper research data management is not being done. Researchers were managing their own research data. Most of the research data were in textual and spreadsheet format. Graphical, audio, video, database, structured text formats and software applications research data were also available. Lack of guidelines on good practice, inadequate human resources, technological obsolescence, insecure infrastructure, use of different vocabulary between librarians and researchers, inadequate financial resources, absence of research data management policies and lack of support by institutional authorities and researchers negatively impacted on research data management. Authors recommend the establishment of research data repositories and use of existing research data repositories that are registered with the Registry of Research Data Repositories to ensure that research data standards are adhered to when doing research. © 2017 The Author(s)."
"10.5334/dsj-2017-030","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85024408274&doi=10.5334%2fdsj-2017-030&partnerID=40&md5=3a4775abc5511308ce5f5850c05d2982","Data citations have become widely accepted. Technical infrastructures as well as principles and recommendations for data citation are in place but best practices or guidelines for their implementation are not yet available. On the other hand, the scientific climate community requests early citations on evolving data for credit, e.g. for CMIP6 (Coupled Model Intercomparison Project Phase 6). The data citation concept for CMIP6 is presented. The main challenges lie in limited resources, a strict project timeline and the dependency on changes of the data dissemination infrastructure ESGF (Earth System Grid Federation) to meet the data citation requirements. Therefore a pragmatic, flexible and extendible approach for the CMIP6 data citation service was developed, consisting of a citation for the full evolving data superset and a data cart approach for citing the concrete used data subset. This two citation approach can be implemented according to the RDA recommendations for evolving data. Because of resource constraints and missing project policies, the implementation of the second part of the citation concept is postponed to CMIP7. © 2017 The Author(s)."
"10.5334/dsj-2017-028","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85024388675&doi=10.5334%2fdsj-2017-028&partnerID=40&md5=afc08f75ea3379b02bd9de6c2b0150fc","The persistent identifier (PID) landscape extends to cover objects, individuals and organisations engaged in the process of research. Established services such as DataCite, Crossref, ORCID and ISNI are providing a foundation for a trusted ecosystem and a new generation of services. Scalable identifier systems will support researchers and capture research activity in a holistic way, across the entire lifecycle. Challenges remain – siloed services are not interoperable; important types of objects are not adequately covered, many processes remain manual, and adoption, while strong, is not consistent across disciplines. This article draws on the work of the EU-funded THOR project to take stock of the current state of interoperability across the PID landscape and to discuss the next steps towards an integrated research record. Examples illustrate how this interconnectivity is facilitated technically, as well as social and human challenges in fostering adoption. User stories highlight how this network of persistent identifier services is facilitating good practice in open research and where its limitations lie. © 2017 The Author(s)."
"10.5334/dsj-2017-029","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85024363694&doi=10.5334%2fdsj-2017-029&partnerID=40&md5=38e74de13147913cd2f75d1f0be52633","The risk of scooping is often used as a counter argument for open science, especially open data. In this case study I have examined openness strategies, practices and attitudes in two open collaboration research projects created by Finnish researchers, in order to understand what made them resistant to the fear of scooping. The radically open approach of the projects includes open by default funding proposals, co-authorship and community membership. Primary sources used are interviews of the projects’ founding members. The analysis indicates that openness requires trust in close peers, but not necessarily in research community or society at large. Based on the case study evidence, focusing on intrinsic goals, like new knowledge and bringing about ethical reform, instead of external goals such as publications, supports openness. Understanding fundaments of science, philosophy of science and research ethics, can also have a beneficial effect on willingness to share. Whether there are aspects in open sharing that makes it seem riskier from the point of view of certain demographical groups within research community, such as women, could be worth closer inspection. © 2017 The Author(s)."
"10.5334/dsj-2017-027","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020537776&doi=10.5334%2fdsj-2017-027&partnerID=40&md5=f526ad16154f928077c29f8a39d2fbf9","The 2013 Office of Science and Technology Policy (OSTP) Memo on federally-funded research directed agencies with research and development budgets above $100 million to develop and release plans to increase and broaden access to research results, both published literature and data. The agency responses have generated discussion and interest but are yet to be analyzed and compared. In this paper, we examine how 19 federal agencies responded to the memo, written by John Holdren, on issues of scientific data and the extent of their compliance to the directives outlined in the memo. We present a varied picture of the readiness of federal science agencies to comply with the memo through a comparative analysis and close reading of the contents of these responses. While some agencies, particularly those with a long history of supporting and conducting science, scored well, other responses indicate that some agencies have only taken a few steps towards implementing policies that comply with the memo. These results are of interest to the data curation community as they reveal how different agencies across the federal government approach their responsibilities for research data management, and how new policies and requirements might continue to affect scientists and research communities. © 2017 The Author(s)."
"10.1089/big.2017.29019.cfp","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055077248&doi=10.1089%2fbig.2017.29019.cfp&partnerID=40&md5=964ba74a092034570ccb69f7c05c053c",[No abstract available]
"10.1089/big.2017.29018.cfp","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055071122&doi=10.1089%2fbig.2017.29018.cfp&partnerID=40&md5=9d5f04a50c2d9a56c6992a86866d393d",[No abstract available]
"10.1089/big.2016.0048","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021165717&doi=10.1089%2fbig.2016.0048&partnerID=40&md5=e894b1de5b90458c9fcd8e0313ec44e6","Recent research has helped to cultivate growing awareness that machine-learning systems fueled by big data can create or exacerbate troubling disparities in society. Much of this research comes from outside of the practicing data science community, leaving its members with little concrete guidance to proactively address these concerns. This article introduces issues of discrimination to the data science community on its own terms. In it, we tour the familiar data-mining process while providing a taxonomy of common practices that have the potential to produce unintended discrimination. We also survey how discrimination is commonly measured, and suggest how familiar development processes can be augmented to mitigate systems' discriminatory potential. We advocate that data scientists should be intentional about modeling and reducing discriminatory outcomes. Without doing so, their efforts will result in perpetuating any systemic discrimination that may exist, but under a misleading veil of data-driven objectivity. © Copyright 2017, Mary Ann Liebert, Inc. 2017."
"10.1089/big.2016.0061","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021153252&doi=10.1089%2fbig.2016.0061&partnerID=40&md5=277bc3bd6b00a9d2cd38554669c666c8","Educators and commenters who evaluate big data-driven learning environments focus on specific questions: whether automated education platforms improve learning outcomes, invade student privacy, and promote equality. This article puts aside separate unresolved - and perhaps unresolvable - issues regarding the concrete effects of specific technologies. It instead examines how big data-driven tools alter the structure of schools' pedagogical decision-making, and, in doing so, change fundamental aspects of America's education enterprise. Technological mediation and data-driven decision-making have a particularly significant impact in learning environments because the education process primarily consists of dynamic information exchange. In this overview, I highlight three significant structural shifts that accompany school reliance on data-driven instructional platforms that perform core school functions: teaching, assessment, and credentialing. First, virtual learning environments create information technology infrastructures featuring constant data collection, continuous algorithmic assessment, and possibly infinite record retention. This undermines the traditional intellectual privacy and safety of classrooms. Second, these systems displace pedagogical decision-making from educators serving public interests to private, often for-profit, technology providers. They constrain teachers' academic autonomy, obscure student evaluation, and reduce parents' and students' ability to participate or challenge education decision-making. Third, big data-driven tools define what ""counts"" as education by mapping the concepts, creating the content, determining the metrics, and setting desired learning outcomes of instruction. These shifts cede important decision-making to private entities without public scrutiny or pedagogical examination. In contrast to the public and heated debates that accompany textbook choices, schools often adopt education technologies ad hoc. Given education's crucial impact on individual and collective success, educators and policymakers must consider the implications of data-driven education proactively and explicitly. © Copyright 2017, Mary Ann Liebert, Inc. 2017."
"10.1089/big.2016.0055","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021114038&doi=10.1089%2fbig.2016.0055&partnerID=40&md5=df0305d1d93964325881d53be682779f","""Big Data"" and data-mined inferences are affecting more and more of our lives, and concerns about their possible discriminatory effects are growing. Methods for discrimination-aware data mining and fairness-aware data mining aim at keeping decision processes supported by information technology free from unjust grounds. However, these formal approaches alone are not sufficient to solve the problem. In the present article, we describe reasons why discrimination with data can and typically does arise through the combined effects of human and machine-based reasoning, and argue that this requires a deeper understanding of the human side of decision-making with data mining. We describe results from a large-scale human-subjects experiment that investigated such decision-making, analyzing the reasoning that participants reported during their task to assess whether a loan request should or would be granted. We derive data protection by design strategies for making decision-making discrimination-aware in an accountable way, grounding these requirements in the accountability principle of the European Union General Data Protection Regulation, and outline how their implementations can integrate algorithmic, behavioral, and user interface factors. © Copyright 2017, Mary Ann Liebert, Inc. 2017."
"10.1089/big.2016.0047","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021102916&doi=10.1089%2fbig.2016.0047&partnerID=40&md5=1b979db8d74eaced7620bbdf004370dd","Recidivism prediction instruments (RPIs) provide decision-makers with an assessment of the likelihood that a criminal defendant will reoffend at a future point in time. Although such instruments are gaining increasing popularity across the country, their use is attracting tremendous controversy. Much of the controversy concerns potential discriminatory bias in the risk assessments that are produced. This article discusses several fairness criteria that have recently been applied to assess the fairness of RPIs. We demonstrate that the criteria cannot all be simultaneously satisfied when recidivism prevalence differs across groups. We then show how disparate impact can arise when an RPI fails to satisfy the criterion of error rate balance. © Copyright 2017, Mary Ann Liebert, Inc. 2017."
"10.1089/big.2016.0050","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021097126&doi=10.1089%2fbig.2016.0050&partnerID=40&md5=0a64cf9c8271467772b2ee20acf16df2","What would data science look like if its key critics were engaged to help improve it, and how might critiques of data science improve with an approach that considers the day-to-day practices of data science? This article argues for scholars to bridge the conversations that seek to critique data science and those that seek to advance data science practice to identify and create the social and organizational arrangements necessary for a more ethical data science. We summarize four critiques that are commonly made in critical data studies: data are inherently interpretive, data are inextricable from context, data are mediated through the sociomaterial arrangements that produce them, and data serve as a medium for the negotiation and communication of values. We present qualitative research with academic data scientists, ""data for good"" projects, and specialized cross-disciplinary engineering teams to show evidence of these critiques in the day-to-day experience of data scientists as they acknowledge and grapple with the complexities of their work. Using ethnographic vignettes from two large multiresearcher field sites, we develop a set of concepts for analyzing and advancing the practice of data science and improving critical data studies, including (1) communication is central to the data science endeavor; (2) making sense of data is a collective process; (3) data are starting, not end points, and (4) data are sets of stories. We conclude with two calls to action for researchers and practitioners in data science and critical data studies alike. First, creating opportunities for bringing social scientific and humanistic expertise into data science practice simultaneously will advance both data science and critical data studies. Second, practitioners should leverage the insights from critical data studies to build new kinds of organizational arrangements, which we argue will help advance a more ethical data science. Engaging the insights of critical data studies will improve data science. Careful attention to the practices of data science will improve scholarly critiques. Genuine collaborative conversations between these different communities will help push for more ethical, and better, ways of knowing in increasingly datum-saturated societies. © Copyright 2017, Mary Ann Liebert, Inc. 2017."
"10.1089/big.2016.0043","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021078697&doi=10.1089%2fbig.2016.0043&partnerID=40&md5=4a3d8e0dac1145fa36562e76cb4149b7","Behavioral big data (BBD) refers to very large and rich multidimensional data sets on human and social behaviors, actions, and interactions, which have become available to companies, governments, and researchers. A growing number of researchers in social science and management fields acquire and analyze BBD for the purpose of extracting knowledge and scientific discoveries. However, the relationships between the researcher, data, subjects, and research questions differ in the BBD context compared to traditional behavioral data. Behavioral researchers using BBD face not only methodological and technical challenges but also ethical and moral dilemmas. In this article, we discuss several dilemmas, challenges, and trade-offs related to acquiring and analyzing BBD for causal behavioral research. © Copyright 2017, Mary Ann Liebert, Inc. 2017."
"10.1007/s41019-017-0041-5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065039646&doi=10.1007%2fs41019-017-0041-5&partnerID=40&md5=9ea921f48f493663b5e8257247fbb0bd","The process of matching and integrating records that relate to the same entity from one or more datasets is known as record linkage, and it has become an increasingly important subject in many application areas, including business, government and health system. The data from these areas often contain sensitive information. To prevent privacy breaches, ideally records should be linked in a private way such that no information other than the matching result is leaked in the process, and this technique is called privacy-preserving record linkage (PPRL). With the increasing data, scalability becomes the main challenge of PPRL, and many private blocking techniques have been developed for PPRL. They are aimed at reducing the number of record pairs to be compared in the matching process by removing obvious non-matching pairs without compromising privacy. However, most of them are designed for two databases and they vary widely in their ability to balance competing goals of accuracy, efficiency and security. In this paper, we propose a novel private blocking approach for PPRL based on dynamic k-anonymous blocking and Paillier cryptosystem which can be applied on two or multiple databases. In dynamic k-anonymous blocking, our approach dynamically generates blocks satisfying k-anonymity and more accurate values to represent the blocks with varying k. We also propose a novel similarity measure method which performs on the numerical attributes and combines with Paillier cryptosystem to measure the similarity of two or more blocks in security, which provides strong privacy guarantees that none information reveals even collusion. Experiments conducted on a public dataset of voter registration records validate that our approach is scalable to large databases and keeps a high quality of blocking. We compare our method with other techniques and demonstrate the increases in security and accuracy. © 2017, The Author(s)."
"10.1007/s41019-017-0040-6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062596024&doi=10.1007%2fs41019-017-0040-6&partnerID=40&md5=f83d0994a9e7ce17e7c2388a938dcdba","With the rapid growth of social media, sentiment analysis has received growing attention from both academic and industrial fields. One line of researches for sentiment analysis is to feed bag-of-words (BOW) text representation into classifiers. Usually, raw BOW requires weighting schemes to obtain better performance, where important words are given more weights while unimportant ones are given less weights. Another line of researches focuses on neural models, where distributed text representations are learned from raw texts automatically. In this paper, we take advantages of techniques in both lines of researches. We use words’ weights to guide neural models to focus on important words. Various supervised weighting schemes are explored in this work. We discover that better text features are learned for sentiment analysis when suitable weighting schemes are applied upon neural models. © 2017, The Author(s)."
"10.1007/s41019-017-0038-0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061219703&doi=10.1007%2fs41019-017-0038-0&partnerID=40&md5=1eac66bc396635488b61e83d252311b5","Today, data exploration platforms are widely used to assist users in locating interesting objects within large volumes of scientific and business data. In those platforms, users try to make sense of the underlying data space by iteratively posing numerous queries over large databases. While diversification of query results, like other data summarization techniques, provides users with quick insights into the huge query answer space, it adds additional complexity to an already computationally expensive data exploration task. To address this challenge, in this paper we propose a diversification scheme that targets the problem of efficiently diversifying the results of multiple queries within and across different data exploratory sessions. Our proposed scheme relies on a model-based diversification method and an ordered cache. In particular, we employ an adaptive regression model to estimate the diversity of a diverse subset. Such estimation of diversity value allows us to select diverse results without scanning all the query results. In order to further expedite the diversification process, we propose an order-based caching scheme to leverage the overlap between sequence of data exploration queries. Our extensive experimental evaluation on both synthetic and real data sets shows the significant benefits provided by our scheme as compared to the existing methods. © 2017, The Author(s)."
"10.1007/s41019-016-0019-8","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052717508&doi=10.1007%2fs41019-016-0019-8&partnerID=40&md5=2ce528894be816b0090ce02f69b9e19b","Finding and fixing software vulnerabilities have become a major struggle for most software development companies. While generally without alternative, such fixing efforts are a major cost factor, which is why companies have a vital interest in focusing their secure software development activities such that they obtain an optimal return on this investment. We investigate, in this paper, quantitatively the major factors that impact the time it takes to fix a given security issue based on data collected automatically within SAP’s secure development process, and we show how the issue fix time could be used to monitor the fixing process. We use three machine learning methods and evaluate their predictive power in predicting the time to fix issues. Interestingly, the models indicate that vulnerability type has less dominant impact on issue fix time than previously believed. The time it takes to fix an issue instead seems much more related to the component in which the potential vulnerability resides, the project related to the issue, the development groups that address the issue, and the closeness of the software release date. This indicates that the software structure, the fixing processes, and the development groups are the dominant factors that impact the time spent to address security issues. SAP can use the models to implement a continuous improvement of its secure software development process and to measure the impact of individual improvements. The development teams at SAP develop different types of software, adopt different internal development processes, use different programming languages and platforms, and are located in different cities and countries. Other organizations, may use the results—with precaution—and be learning organizations. © 2016, The Author(s)."
"10.1007/s41019-017-0039-z","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049455781&doi=10.1007%2fs41019-017-0039-z&partnerID=40&md5=4eaeffcb46185b880b254bb10bfe0783","Log replication is a key component in highly available database systems. In order to guarantee data consistency and reliability, it is common for modern database systems to utilize Paxos protocol, which is responsible for replicating transactional logs from one primary node to multiple backups. However, the Paxos replication needs to store and synchronize some additional metadata, such as committed log sequence number (commit point), to guarantee the consistency of the database. This increases the overhead of storage and network, which would have a negative impact on the throughput in the update intensive work load. In this paper, we present an implementation of log replication and database recovery methods, which adopts the idea of piggybacking, i.e., commit point can be embedded in the commit logs. This practice not only retains virtues of Paxos replication, but also reduces disk and network IO effectively. We implemented and evaluated our approach in a main memory database system. Our experiments show that the piggybacking method can offer 1.3× higher throughput than typical log replication with synchronization mechanism. © 2017, The Author(s)."
"10.1007/s41019-017-0035-3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85037072421&doi=10.1007%2fs41019-017-0035-3&partnerID=40&md5=91460de9a98650a318f5d78f0a2633ad","Context plays an important role in helping users to make decisions. There are hierarchical structure between contexts and aggregation characteristics within the context in real scenarios. Exist works mainly focus on exploring the explicit hierarchy between contexts, while ignoring the aggregation characteristics within the context. In this work, we explore both of them so as to improve accuracy of prediction in recommender systems. We propose a Random Partition Factorization Machines (RPFM) by adopting random decision trees to split the contexts hierarchically to better capture the local complex interplay. The intuition here is that local homogeneous contexts tend to generate similar ratings. During prediction, our method goes through from the root to the leaves and borrows from predictions at higher level when there is sparseness at lower level. Other than estimation accuracy of ratings, RPFM also reduces the over-fitting by building an ensemble model on multiple decision trees. We test RPFM over three different benchmark contextual datasets. Experimental results demonstrate that RPFM outperforms state-of-the-art context-aware recommendation methods. © 2017, The Author(s)."
"10.1007/s41019-017-0037-1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029812794&doi=10.1007%2fs41019-017-0037-1&partnerID=40&md5=8a88c5e13c13cb66f6f98bdeaa6bb8c0","With the rapid development of mobile internet and online to offline marketing model, various spatial crowdsourcing platforms, such as Gigwalk and Gmission, are getting popular. Most existing studies assume that spatial crowdsourced tasks are simple and trivial. However, many real crowdsourced tasks are complex and need to be collaboratively finished by a team of crowd workers with different skills. Therefore, an important issue of spatial crowdsourcing platforms is to recommend some suitable teams of crowd workers to satisfy the requirements of skills in a task. In this paper, to address the issue, we first propose a more practical problem, called Top-k team recommendation in spatial crowdsourcing (TopkTR) problem. We prove that the TopkTR problem is NP-hard and designs a two-level-based framework, which includes an approximation algorithm with provable approximation ratio and an exact algorithm with pruning techniques to address it. In addition, we study a variant of the TopkTR problem, called TopkTRL, where a team leader is appointed among each recommended team of crowd workers in order to coordinate different crowd workers conveniently, and the aforementioned framework can be extended to address this variant. Finally, we verify the effectiveness and efficiency of the proposed methods through extensive experiments on real and synthetic datasets. © 2017, The Author(s)."
"10.1057/s41270-017-0018-2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031313645&doi=10.1057%2fs41270-017-0018-2&partnerID=40&md5=e6a0955418c32e3b822c0f09aefe37de","The purpose of the present study is to examine the impact of consumers’ personality traits such as value consciousness and coupon proneness on attitude towards online shopping. A self-administered questionnaire was used to collect data from 300 online shoppers. Structural equation modeling was used to analyze the data. χ2difference test was used to test the moderating effects. The results indicate that perceived usefulness and perceived risk influenced attitude towards online shopping. Value consciousness and coupon proneness significantly moderate the impact of perceived usefulness and perceived risk on attitude towards online shopping. Consumers having high value consciousness and coupon proneness are likely to be influenced by perceived usefulness and risk towards online shopping. As these consumers are experienced in redeeming coupons and rebates, ease of use does not influence their attitude towards online shopping. The study makes a significant contribution to the literature of online retailing by identifying the role of personality traits on attitude towards online shopping. The present research is one of the few to consider both value consciousness and coupon proneness. © Macmillan Publishers Ltd 2017."
"10.1057/s41270-017-0015-5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031285224&doi=10.1057%2fs41270-017-0015-5&partnerID=40&md5=015dab22039ea1e503df1ffa9902aefd","Researchers have found evidence that helpful product reviews written by other consumers have the potential to alter consumers’ purchase decision and influence overall sales. In the quest to find what makes a review helpful, prior studies have documented volume, valence, argument quality, and source certainty as determinants of helpful reviews. However, these studies used regression analysis and found significant effects of each of the determinants regardless of other variables. Taking a different perspective, the present study uncovers the “causal recipe” (combination of antecedent conditions) of helpfulness review by applying a fuzzy-set qualitative comparative analysis. Congruent with elaboration likelihood model, this study finds that high argument quality and high source certainty, together in a review, do not make a review helpful, and consumers use heuristics (peripheral cues) when reading a long review. Negative as well as long reviews are found to be helpful. Real consumer reviews collected from Amazon.com are used for this study. The study contributes to the literature by uncovering different paths (path signifies configuration of variables) that lead to helpful reviews by using a fs-QCA technique on real Amazon.com reviews and attends to the call of using sophisticated techniques in exploring new online data. © Macmillan Publishers Ltd 2017."
"10.5334/dsj-2017-026","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020525947&doi=10.5334%2fdsj-2017-026&partnerID=40&md5=355f328b49b146ffdd1a3d914f50c528","When considering the “openness” of data it is unsurprising that most conversations focus on the online environment - how data is collated, moved and recombined for multiple purposes. Nonetheless, it is important to recognize that the movements online are only part of the data lifecycle. Indeed, considering where and how data are created - namely, the research setting - are of key importance to Open Data initiatives. In particular, such insights offer key understandings of how and why scientists engage with in practices of openness, and how data transitions from personal control to public ownership. This paper examines research settings in low/middle-income countries (LMIC) to better understand how resource limitations influence Open Data buy-in. Using empirical fieldwork in Kenyan and South African laboratories it draws attention to some key issues currently overlooked in Open Data discussions. First, that many of the hesitations raised by the scientists about sharing data were as much tied to the speed of their research as to any other factor. Thus, it would seem that the longer it takes for individual scientists to create data, the more hesitant they are about sharing it. Second, that the pace of research is a multifaceted bind involving many different challenges relating to laboratory equipment and infrastructure. Indeed, it is unlikely that one single solution (such as equipment donation) will ameliorate these “binds of pace”. Third, that these “binds of pace” were used by the scientists to construct “narratives of exclusion” through which they remove themselves from responsibility for data sharing. Using an adapted model of technology first proposed by Elihu Gerson, the paper then offers key ways in which these critical “binds of pace” can be addressed in Open Data discourse. In particular, it calls for an expanded understanding of laboratory equipment and research speed to include all aspects of the research environment. It also advocates for better engagement with LMIC scientists regarding these challenges and the adoption of frugal/responsible design principles in future Open Data initiatives. © 2017 The Author(s)."
"10.5334/dsj-2017-025","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020549115&doi=10.5334%2fdsj-2017-025&partnerID=40&md5=947ebe3ad82d5ec0813ad06274b46f80","The discovery of frequent itemsets is one of the very important topics in data mining. Frequent itemset discovery techniques help in generating qualitative knowledge which gives business insight and helps the decision makers. In the Big Data era the need for a customizable algorithm to work with big data sets in a reasonable time becomes a necessity. In this paper we propose a new algorithm for frequent itemset discovery that could work in distributed manner with big datasets. Our approach is based on the original Buddy Prima algorithm and the Greatest Common Divisor (GCD) calculation between itemsets which exist in the transaction database. The proposed algorithm introduces a new method to parallelize the frequent itemset mining without the need to generate candidate itemsets and also it avoids any communication over-head between the participated nodes. It explores the parallelism abilities in the hardware in case of single node operation. The proposed approach could be implemented using map-reduce technique or Spark. It was successfully applied on different size transactions DBs and compared with two well-known algorithms: FP-Growth and Parallel Apriori with different support levels. The experiments showed that the proposed algorithm achieves major time improvement over both algorithms especially with datasets having huge number of items. © 2017 The Author(s)."
"10.5334/dsj-2017-024","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020499703&doi=10.5334%2fdsj-2017-024&partnerID=40&md5=cb220fde1114b54217a78ff412f96ec6","In early 2016, the International Committee of Medical Journal Editors (ICMJE) proposed that responsible sharing of de-identified individual-level data be required for clinical trials published in their affiliated journals. There would be a delay in implementing this policy to allow for the necessary informed consents to work their way through ethical review. Meanwhile, some researchers and policy makers have conflated the notions of de-identification and anonymity. The former is a process that seeks to mitigate disclosure risk though careful application of rules and statistical analysis, while the latter is an absolute state. The consequence of confusing the process and the state is profound. Extensions to the ICMJE proposal based on the presumed anonymity of data include: sharing unconsented data; sharing data without managing access, as Open Data; and proposals to sell data. This essay aims to show that anonymity (the state) cannot be guaranteed by de-identification (the process), and so these extensions to the ICMJE proposal should be rejected on governance grounds, if no other. This is not as negative a po-i tion as it might seem, as other disciplines have been aware of these limitations and concomitant responsibilities for many years. The essay concludes with an example from social science of managed access strategies that could be adopted by the medical field. © 2017 The Author(s)."
"10.5334/dsj-2017-023","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020553773&doi=10.5334%2fdsj-2017-023&partnerID=40&md5=d82353c08ac7e4e38a510c3caee5cd33","Considerable attention has been devoted to the use of persistent identifiers for assets of interest to scientific and other communities alike over the last two decades. Among persistent identifiers, Digital Object Identifiers (DOIs) stand out quite prominently, with approximately 133 million DOIs assigned to various objects as of February 2017. While the assignment of DOIs to objects such as scientific publications has been in place for many years, their assignment to Earth science data sets is more recent. Applying persistent identifiers to data setsenables improved tracking of their use and reuse, facilitates the crediting of data producers, and aids reproducibility through associating research with the exact data set(s) used. Maintaining provenance - i.e., tracing back lineage of significant scientific conclusions to the entities (data sets, algorithms, instruments, satellites, etc.) that lead to the conclusions, would be prohibitive without persistent identifiers. This paper provides a brief background on the use of persistent identifiers in general within the US, and DOIs more specifically. We examine their recent use for Earth science data sets, and outline successes and some remaining challenges. Among the challenges, for example, is the ability to conveniently and consistently obtain data citation statistics using the DOIs assigned by organizations that manage data sets. © 2017 The Author(s)."
"10.5334/dsj-2017-022","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020546053&doi=10.5334%2fdsj-2017-022&partnerID=40&md5=dee6bfb59731693a32a6fdf9c96a2a2a","This paper details the drivers, methods, and outcomes of the U.S. Geological Survey’s quest to establish criteria by which to judge its own digital preservation resources as Trusted Ðigital Repositories. Drivers included recent U.S. legislation focused on data and asset management conducted by federal agencies spending $100M USD or more annually on research activities. The methods entailed seeking existing evaluation criteria from national and international organizations such as International Standards Organization (ISO), U.S. Library of Congress, and Data Seal of Approval upon which to model USGS repository evaluations. Certification, complexity, cost, and usability of existing evaluation models were key considerations. The selected evaluation method was derived to allow the repository evaluation process to be transparent, understandable, and defensible; factors that are critical for judging competing, internal units. Implementing the chosen evaluation criteria involved establishing a cross-agency, multi-disciplinary team that interfaced across the organization. © 2017 The Author(s)."
"10.5334/dsj-2017-021","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020536872&doi=10.5334%2fdsj-2017-021&partnerID=40&md5=85346c992bd2f72e9f5a3d8e758d1526","In disciplines such as biomedicine and social sciences, sharing and combining sensitive individual-level data is often prohibited by ethical-legal or governance constraints and other barriers such as the control of intellectual property or the huge sample sizes. DataSHIELD (Data Aggregation Through Anonymous Summary-statistics from Harmonised Individual-levEL Databases) is a distributed approach that allows the analysis of sensitive individual-level data from one study, and the co-analysis of such data from several studies simultaneously without physically pooling them or disclosing any data. Following initial proof of principle, a stable DataSHIELD platform has now been implemented in a number of epidemiological consortia. This paper reports three new applications of ÐataSHIELD including application to post-publication sensitive data analysis, text data analysis and privacy protected data visualisation. Expansion of DataSHIELD analytic functionality and application to additional data types demonstrate the broad applications of the software beyond biomedical sciences. © 2017 The Author(s)."
"10.5334/dsj-2017-020","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020524077&doi=10.5334%2fdsj-2017-020&partnerID=40&md5=0ce62f2baf63e1dd9d31e863ac76e8ea","The National Computational Infrastructure (NCI) manages over 10 PB research data, which is co-located with the high performance computer (Raijin) and an HPC class 3000 core OpenStack cloud system (Tenjin). In support of this integrated High Performance Computing/High Performance Data (HPC/HPD) infrastructure, NCI’s data management practices includes building catalogues, DOI minting, data curation, data publishing, and data delivery through a variety of data services. The metadata catalogues, DOIs, THREDDS, and Vocabularies, all use different Uniform Resource Locator (URL) styles. A Persistent IDentifier (PID) service provides an important utility to manage URLs in a consistent, controlled and monitored manner to support the robustness of our national ‘Big Data’ infrastructure. In this paper we demonstrate NCI’s approach of utilising the NCI’s PID Service to consistently manage its persistent identifiers with various applications. © 2017 The Author(s)."
"10.5334/dsj-2017-019","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020499208&doi=10.5334%2fdsj-2017-019&partnerID=40&md5=592d7471f16792e272c03d6dab2393e1","The Perseids project provides a platform for creating, publishing, and sharing research data, in the form of textual transcriptions, annotations and analyses. An offshoot and collaborator of the Perseus Digital Library (PDL), Perseids is also an experiment in reusing and extending existing infrastructure, tools, and services. This paper discusses infrastructure in the domain of digital humanities (DH). It outlines some general approaches to facilitating data sharing in this domain, and the specific choices we made in developing Perseids to serve that goal. It concludes by identifying lessons we have learned about sustainability in the process of building Perseids, noting some critical gaps in infrastructure for the digital humanities, and suggesting some implications for the wider community. © 2017 The Author(s)."
"10.5334/dsj-2017-014","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85024364121&doi=10.5334%2fdsj-2017-014&partnerID=40&md5=f884a064d5a9fd8db97ef2c9bbf48513","The value of making research data available is broadly accepted. Policies concerning the open access to research data try to implement new norms calling for researchers to make their data more openly available. These policies either appeal to the common good or focus on publication and citation as an incentive to bring about a cultural change in how researchers share their data with their peers. But when we compare the total number of publications in the fields of science, technology and medicine with the number data publications from the same time period, the number of openly available datasets is rather small. This indicates that current policies on data sharing are not effective in changing behaviours and bringing about the wanted cultural change. By looking at research communities that are more open to data sharing we can study the social patterns that influence data sharing and point us to possible points for intervention and change. © 2017 The Author(s)."
"10.1080/23270012.2016.1265906","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057006104&doi=10.1080%2f23270012.2016.1265906&partnerID=40&md5=862eddcf05294d2e64b8ffec9aea9353","Nowadays, it is common to think about big data and its consequences, especially in information systems (IS) room. This study investigated the effects of big data on the design and development of IS, and 167 articles were examined using meta-synthesis and the articles were selected for data extraction. Critical Appraisal Skills Program (CASP) was used to evaluate the quailty of the selected articles. The findings indicate that volume affects the layer of software engineering tools, but variety and velocity affect the layer of software engineering methods. Therefore, for the development of IS, it is essential to develop systems with specific capabilities. © 2017, © 2017 Antai College of Economics and Management, Shanghai Jiao Tong University."
"10.1080/23270012.2017.1280423","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056044903&doi=10.1080%2f23270012.2017.1280423&partnerID=40&md5=8e2b6e19608feb41764087dc1d7438e9","In this paper, economic order quantity (EOQ) inventory model is considered subject to promotional efforts. We adopt a demand function which is dependent on sales teams' initiatives in which shortages are allowed which are completely backlogged under the condition of permissible delay in payments with time-dependent holding cost. The main objective of this paper is to determine the optimal replenishment schedule and optimal order quantity to maximize the total profit. Expressions for various optimal indices are provided. First, we prove that a unique optimal replenishment schedule exists. Second, we present an effective iterative algorithm to obtain the optimal solution. Furthermore, we establish some useful theorems to characterize the optimal solution to determine the values of replenishment schedule and optimal order quantity. Third, we prove that the total profit is a concave function via differential calculus and present numerical examples using SCILAB 5.5.0 to illustrate the model. Finally, we extend the numerical example by performing a sensitivity analysis of the model parameters and discuss managerial insights. This study suggests to the management of firms to determine the optimal order quantity, optimal inventory cycle length and sales teams' initiatives/promotional effort in order to achieve their maximum profits. © 2017, © 2017 Antai College of Economics and Management, Shanghai Jiao Tong University."
"10.1080/23270012.2017.1299047","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053297253&doi=10.1080%2f23270012.2017.1299047&partnerID=40&md5=59578e8dbc313955b449366abbcf7c9f","This paper examines whether and how the adoption of eXtensible Business Reporting Language (XBRL) reduces information asymmetry in loan contracting. We hypothesize that the adoption of XBRL can enhance information dissemination and mitigate the information asymmetry problem between borrowers and lenders. Using a sample of 554 US bank loan contracts, we find that borrowers who adopt XBRL enjoy more favourable price and non-price terms of bank loan contracts. Additional analyses indicate that the relations among XBRL adoption and bank loan price vary with loan structure. Overall, this research provides evidence that technology advancements such as XBRL reduce cost of bank loans by decreasing information asymmetry between borrowers and lenders. © 2017, © 2017 Antai College of Economics and Management, Shanghai Jiao Tong University."
"10.1080/23270012.2017.1299048","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85035141499&doi=10.1080%2f23270012.2017.1299048&partnerID=40&md5=d4cfe07f574ea7ea917f9e40b08bf444","Evaluating the success of changes to an existing Business Intelligence (BI) environment means that there is a need to compare the level of user satisfaction with the original and amended versions of the application. The focus of this paper is on producing an evaluation tool, which can be used to measure the success of changes to existing BI solutions to support improved BI reporting. The paper identifies the users involved in the BI process and investigates what is meant by satisfaction in this context from both a user and a technical perspective. The factors to be used to measure satisfaction and appropriate clusters of measurements are identified and an evaluation tool to be used by relevant stakeholders to measure success is developed. The approach used to validate the evaluation tool is discussed and the conclusion gives suggestions for further development and extension of the tool. © 2017, © 2017 Antai College of Economics and Management, Shanghai Jiao Tong University."
"10.1080/23270012.2017.1304292","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028663742&doi=10.1080%2f23270012.2017.1304292&partnerID=40&md5=dccf7d41d74a508fad566522d43c37a8","In the era of Big Data, many NoSQL databases emerged for the storage and later processing of vast volumes of data, using data structures that can follow columnar, key-value, document or graph formats. For analytical contexts, requiring a Big Data Warehouse, Hive is used as the driving force, allowing the analysis of vast amounts of data. Data models in Hive are usually defined taking into consideration the queries that need to be answered. In this work, a set of rules is presented for the transformation of multidimensional data models into Hive tables, making available data at different levels of detail. These several levels are suited for answering different queries, depending on the analytical needs. After the identification of the Hive tables, this paper summarizes a demonstration case in which the implementation of a specific Big Data architecture shows how the evolution from a traditional Data Warehouse to a Big Data Warehouse is possible. © 2017, © 2017 Antai College of Economics and Management, Shanghai Jiao Tong University."
"10.4018/IJBAN.2017040104","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046268735&doi=10.4018%2fIJBAN.2017040104&partnerID=40&md5=dbf2f485500fd52055dad01fad081548","Search engines acts as an intermediate between the user and web. It takes the user query as input and retrieves the pages based on query terms from its database, which is in advance populated from World Wide Web. It then applies some ranking algorithm to sort the retrieved pages and presents the results back to the user in the form of millions of web pages. But most of pages in the result are not useful to the user. This problem arises because the search engine retrieves the results based on query keywords only and no attention is paid in incorporating the user interest during the ranking process. Due to the lack of automatic mechanism for tracking user browsing patterns, user seldom gets the relevant results in the top ten links. So, in order to cater the need of individual user, an automatic user interest mining technique for retrieving quality data is being proposed here. The mechanism provides the satisfactory results to the user as each user interest is maintained separately without any hassle at the user end. Copyright © 2017, IGI Global."
"10.4018/IJBAN.2017040101","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032854607&doi=10.4018%2fIJBAN.2017040101&partnerID=40&md5=0b66b1cdc7f832de618470418059ea70","Energy saving and emission reduction are increasingly important. This paper studies a two-stage network based on the DEA-SBM model to study the two aspects. This study evaluates their efficiency, including benchmarks for improvement, of 27 regions in China from 2009 to 2013. First, the efficiency of each DMU in each stage can be obtained by the proposed model. Second, by combining the two stages, the integrated efficiency of each region can be calculated. The regions are also grouped geographically. The empirical results show that: (i)Beijing and Tianjin are the best in terms of energy system, while Gansu and Ningxia perform best in terms of environment system. (ii)From the perspective of geographical area in China, the eastern area are the best for stage one, while the western area is the best for stage two. (iii)In terms of integrated efficiency, the western area performs worst. Copyright © 2017, IGI Global."
"10.1089/big.2017.29014.cfp","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055080569&doi=10.1089%2fbig.2017.29014.cfp&partnerID=40&md5=28c4ccba095cfa4627310e0d30b0639e",[No abstract available]
"10.1089/big.2017.29015.cfp","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055074235&doi=10.1089%2fbig.2017.29015.cfp&partnerID=40&md5=7443c8b884212b08e8810a1c2aa92f15",[No abstract available]
"10.1089/big.2016.0073","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016442556&doi=10.1089%2fbig.2016.0073&partnerID=40&md5=4217d806f9ff6a5b0f1f5f319b4f8fca","This article demonstrates how time-dependent, interacting, and repeating risk factors can be used to create more accurate predictive medicine. In particular, we show how emergence of anemia can be predicted from medical history within electronic health records. We used the Veterans Affairs Informatics and Computing Infrastructure database to examine a retrospective cohort of 9,738,838 veterans over an 11-year period. Using International Clinical Diagnoses Version 9 codes organized into 25 major diagnostic categories, we measured progression of disease by examining changes in risk over time, interactions in risk of combination of diseases, and elevated risk associated with repeated hospitalization for the same diagnostic category. The maximum risk associated with each diagnostic category was used to predict anemia. The accuracy of the model was assessed using a validation cohort. Age and several diagnostic categories significantly contributed to the prediction of anemia. The largest contributors were health status (β= -1075, t = -92, p < 0.000), diseases of the endocrine (β= -1046, t = -87, p < 0.000), hepatobiliary (β= -1043, t = -72, p < 0.000), kidney (β= -1125, t = -111, p < 0.000), and respiratory systems (β= -1151, t = -89, p < 0.000). The AUC for the additive model was 0.751 (confidence interval 74.95%-75.26%). The magnitude of AUC suggests that the model may assist clinicians in determining which patients are likely to develop anemia. The procedures used for examining changes in risk factors over time may also be helpful in other predictive medicine projects. © 2017, Mary Ann Liebert, Inc."
"10.1089/big.2016.0044","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016430412&doi=10.1089%2fbig.2016.0044&partnerID=40&md5=6dd3a40913925efdb3b2b396d62c665a","Historically, domains such as business intelligence would require a single analyst to engage with data, develop a model, answer operational questions, and predict future behaviors. However, as the problems and domains become more complex, organizations are employing teams of analysts to explore and model data to generate knowledge. Furthermore, given the rapid increase in data collection, organizations are struggling to develop practices for intelligence analysis in the era of big data. Currently, a variety of machine learning and data mining techniques are available to model data and to generate insights and predictions, and developments in the field of visual analytics have focused on how to effectively link data mining algorithms with interactive visuals to enable analysts to explore, understand, and interact with data and data models. Although studies have explored the role of single analysts in the visual analytics pipeline, little work has explored the role of teamwork and visual analytics in the analysis of big data. In this article, we present an experiment integrating statistical models, visual analytics techniques, and user experiments to study the role of teamwork in predictive analytics. We frame our experiment around the analysis of social media data for box office prediction problems and compare the prediction performance of teams, groups, and individuals. Our results indicate that a team's performance is mediated by the team's characteristics such as openness of individual members to others' positions and the type of planning that goes into the team's analysis. These findings have important implications for how organizations should create teams in order to make effective use of information from their analytic models. © 2017, Mary Ann Liebert, Inc."
"10.1089/big.2017.0013","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016414897&doi=10.1089%2fbig.2017.0013&partnerID=40&md5=a02cfee021d452d1c86f6c9b4daecdfa","In a recent article by Barfar and Padmanabhan (2015), we demonstrated how television viewership data could predict presidential election outcomes in the United States. In this article, we examine predictive models using a snapshot of Nielsen's national data on television viewership. The study is conducted with high-dimensional low sample size (HDLSS) data, whereby we conduct a comparative analysis with and without feature reduction on the data from the 2012 elections. We find that simple ""single-show models"" often provided more insights and predictive accuracies than models from feature reduction. Second, beyond the state and county levels of analysis, we show that the results continue to hold at the designated market area (DMA) level, crucial for television broadcasting because programs are often targeted at the DMA level. Finally, we examine the performance of the single-show models in the 2016 election season by applying them to the viewership information during the U.S. presidential primaries. We discuss implications of our findings for research and practice. © 2017, Mary Ann Liebert, Inc."
"10.1089/big.2017.0012","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016399563&doi=10.1089%2fbig.2017.0012&partnerID=40&md5=0651d310f737a8b6cfdabb39069ac029","The ability of automatically recognizing and typing entities in natural language without prior knowledge (e.g., predefined entity types) is a major challenge in processing such data. Most existing entity typing systems are limited to certain domains, genres, and languages. In this article, we propose a novel unsupervised entity-typing framework by combining symbolic and distributional semantics. We start from learning three types of representations for each entity mention: general semantic representation, specific context representation, and knowledge representation based on knowledge bases. Then we develop a novel joint hierarchical clustering and linking algorithm to type all mentions using these representations. This framework does not rely on any annotated data, predefined typing schema, or handcrafted features; therefore, it can be quickly adapted to a new domain, genre, and/or language. Experiments on genres (news and discussion forum) show comparable performance with state-of-the-art supervised typing systems trained from a large amount of labeled data. Results on various languages (English, Chinese, Japanese, Hausa, and Yoruba) and domains (general and biomedical) demonstrate the portability of our framework. © 2017, Mary Ann Liebert, Inc."
"10.1016/j.bdr.2017.01.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85009415089&doi=10.1016%2fj.bdr.2017.01.001&partnerID=40&md5=ebbc6d80b122d3b9afe6fe2908c51373","In the atmospheric sciences, the size of simulation output continues to grow as computational resources able to handle simulations with fine-scale spatial and temporal resolutions become more accessible. As output size increases, serial data analysis methods become overwhelmed, resulting in either long delays during processing or total failures due to memory constraints. Parallel data analysis methods can alleviate these issues, however atmospheric scientists are often unfamiliar with how to achieve this. Therefore, example methods are needed to help guide the use of parallel processing in the analysis of Big Data from atmospheric simulations. In this work, practical methods are presented by which an analysis may be executed in parallel using the Message Passing Interface (MPI) and Python. These methods first consider the inherent spatial dependencies of a particular data analysis process. By identifying these dependencies, horizontal or vertical distribution of the dataset across processes can be carried out with minimal process intercommunication. In addition, an analysis method is classified as either data-transfer-limited or computationally-limited. In data-transfer-limited problems, data transfer time outweighs processing time. In computationally-limited problems, processing time outweighs data transfer time. The results show that by increasing processor count, the execution time of computationally-limited problems shows improvement. For data-transfer-limited problems, increasing node count offers the greatest improvement. To further improve the performance of computationally-limited problems, a Graphics Processing Unit (GPU) and the Compute Unified Device Architecture (CUDA) framework are used. It is shown that this GPU implementation offers further improvement over the MPI version of the analysis methods tested. © 2017 Elsevier Inc."
"10.1016/j.bdr.2016.11.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85008950153&doi=10.1016%2fj.bdr.2016.11.001&partnerID=40&md5=302738d8f0638615f6d8053dec7f1dd1","Recently, dedicated web portals and social networks for the automatization of the recruitment processes, have emerged with the expansion of the Internet, leading to a wide use of optimized algorithms. To that aim, a lot of job boards websites have been created, for disseminating and sharing at the best the job offers. Choosing the relevant job board for the broadcasting of a given job offer can be sometimes difficult for the recruiters, since they always seek to attract the best candidates, in a short period of time. Moreover, some job boards have different business categories, which can make the selection very difficult. To deal with these problems, we propose in this paper, Smart4Job a new job boards recommendation system, which proposes the adequate job boards for the dissemination of a new job offer. Our system is based on a hybrid representation on a big data platform, which includes both a domain knowledge analysis and a temporal prediction model. The semantic classification of job boards requires a textual analysis using a controlled vocabulary. The time series analysis module is used to predict the best job board for a given offer, using the history of the clicks. The answers of these modules are combined during the decision making process. The proposed system has been evaluated on real data, and preliminary results seem very promising. © 2016 Elsevier Inc."
"10.1016/j.bdr.2016.10.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85006894333&doi=10.1016%2fj.bdr.2016.10.001&partnerID=40&md5=d4acdbf766271978702a069b55c404f7","Semantic Web technologies and in particular Linked Open Data provide a means for sharing knowledge about cities as physical, social, and technical systems, so enabling the development of smart city applications. This paper presents a prototype based on the case of Catania with the aim of sharing the lessons learnt, which can be reused as reference practices in other cases with similar requirements. The importance of achieving syntactic as well as semantic interoperability – as a result of transforming heterogeneous sources into Linked Data – is discussed: semantic interoperability is solved at data level in order to ease further development on top. We present a comprehensive data model for smart cities that integrates several data sources, including, geo-referenced data, public transportation, urban fault reporting, road maintenance and municipal waste collection. We show some novel ontology design patterns for modeling public transportation, urban fault reporting and road maintenance. Domain practitioners and general members of the public have been asked to play with the prototype, and fill out a survey with questions and feedbacks. A computational experiment has been also conducted to evaluate the performance of our data model in terms of practical scalability over increasing data and efficiency under complex queries. All produced data, models, prototype and questionnaire results are publicly accessible online. © 2016 Elsevier Inc."
"10.1007/s41019-016-0025-x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050626732&doi=10.1007%2fs41019-016-0025-x&partnerID=40&md5=fc699095482515d80359ccc08f34e234","This position paper provides an overview of our recent advances in the study of big graphs, from theory to systems to applications. We introduce a theory of bounded evaluability, to query big graphs by accessing a bounded amount of the data. Based on this, we propose a framework to query big graphs with constrained resources. Beyond queries, we propose functional dependencies for graphs, to detect inconsistencies in knowledge bases and catch spams in social networks. As an example application of big graph analyses, we extend association rules from itemsets to graphs for social media marketing. We also identify open problems in connection with querying, cleaning and mining big graphs. © 2017, The Author(s)."
"10.1007/s41019-016-0024-y","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047816698&doi=10.1007%2fs41019-016-0024-y&partnerID=40&md5=aa297347d69401119be58e92d22b43f8","There are many large-scale graphs in real world such as Web graphs and social graphs. The interest in large-scale graph analysis is growing in recent years. Breadth-First Search (BFS) is one of the most fundamental graph algorithms used as a component of many graph algorithms. Our new method for distributed parallel BFS can compute BFS for one trillion vertices graph within half a second, using large supercomputers such as the K-Computer. By the use of our proposed algorithm, the K-Computer was ranked 1st in Graph500 using all the 82,944 nodes available on June and November 2015 and June 2016 38,621.4 GTEPS. Based on the hybrid BFS algorithm by Beamer (Proceedings of the 2013 IEEE 27th International Symposium on Parallel and Distributed Processing Workshops and PhD Forum, IPDPSW ’13, IEEE Computer Society, Washington, 2013), we devise sets of optimizations for scaling to extreme number of nodes, including a new efficient graph data structure and several optimization techniques such as vertex reordering and load balancing. Our performance evaluation on K-Computer shows that our new BFS is 3.19 times faster on 30,720 nodes than the base version using the previously known best techniques. © 2017, The Author(s)."
"10.1007/s41019-016-0029-6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044860545&doi=10.1007%2fs41019-016-0029-6&partnerID=40&md5=ac13666328daa2a42c314e840a97f333","The increasing size of RDF data requires efficient systems to store and query them. There have been efforts to map RDF data to a relational representation, and a number of systems exist that follow this approach. We have been investigating an alternative approach of maintaining the native graph model to represent RDF data, and utilizing graph database techniques (such as a structure-aware index and a graph matching algorithm) to address RDF data management. Since 2009, we have been developing a set of graph-based RDF data management systems that follow this approach: gStore, gStore-D and gAnswer. The first two are designed to support efficient SPARQL query evaluation in a centralized and distributed/parallel environments, respectively, while the last one aims to provide an easy-to-use interface (natural language question/answering) for users to access a RDF repository. In this paper, we give an overview of these systems and also discuss our design philosophy. © 2017, The Author(s)."
"10.1007/s41019-016-0030-0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85035150833&doi=10.1007%2fs41019-016-0030-0&partnerID=40&md5=fea5e108a60d45fe528a03767310884a","Travel planning is one of the important issues in the location-based services (LBS). Traveling salesman problem (TSP) is to find the optimal tour that traverses points exactly once in the minimum total distance. Given the hardness of TSP (NP-hard), TSP query for a given set of points, Q, is not widely studied for online LBS, and the nearest-neighbor heuristic is the only heuristic adapted to find TSP-like tours with additional constraints for LBS. The questions to ask are: Is the nearest-neighbor the best in terms of accuracy? Which heuristics among many should we use to process TSP queries online for LBS? In the literature, TSPLIB benchmarks are designed for special cases where the number of points used is large, and the existing synthetic datasets are based on uniform/normal distributions. Both do not reflect the real datasets used in real applications. Therefore, the best heuristics suggested by the TSPLIB and the existing benchmarks need to be reconsidered for LBS setting. In this work, we investigate 22 heuristics and show that the best heuristics in terms of accuracy for LBS are not the ones suggested by the existing work, and identify several heuristics by extensive performance studies over real datasets, TSPLIB benchmarks, the existing synthetic datasets and our new synthetic datasets. Among many issues, we also show that it is possible to get high-quality TSP by precomputing/indexing, even though it is hard to prove by theorem. © 2017, The Author(s)."
"10.1007/s41019-016-0023-z","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031908257&doi=10.1007%2fs41019-016-0023-z&partnerID=40&md5=ae5d86969ae3b6f9e6d924b10eeb99fd","Performing online selective queries against graphs is a challenging problem due to the unbounded nature of graph queries which leads to poor computation locality. It becomes even difficult when a graph is too large to be fit in the memory. Although there have been emerging efforts on managing large graphs in a distributed and parallel setting, e.g., Pregel, HaLoop and etc, these computing frameworks are designed from the perspective of scalability instead of the query efficiency. In this work, we present our solution methodology for online selective graph queries based on the shortest path distance semantic, which finds various applications in practice. The essential intuition is to build a distance-aware index for online distance-based query processing and to eliminate redundant graph traversal as much as possible. We discuss how the solution can be applied to two types of research problems, distance join and vertex set bonding, which are distance-based graph pattern discovery and finding the structure-wise bonding of vertices, respectively. © 2017, The Author(s)."
"10.1007/s41019-017-0034-4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030103025&doi=10.1007%2fs41019-017-0034-4&partnerID=40&md5=453acca8121f74e84c9cafe5d3fed440","There is a large demand for distributed engines that efficiently process large-scale graph data, such as social graph and web graph. The distributed graph engines execute analysis process after partitioning input graph data and assign them to distributed computers, so the quality of graph partitioning largely affects the communication cost and load balance among computers during the analysis process. We propose an effective graph partitioning technique that achieves low communication cost and good load balance among computers at the same time. We first generate more clusters than the number of computers by extending the modularity-based clustering, and then merge those clusters into balanced-size clusters until the number of clusters becomes the number of computers by using techniques designed for graph packing problem. We implemented our technique on top of distributed graph engine, PowerGraph, and made intensive experiments. The results show that our partitioning technique reduces the communication cost so it improves the response time of graph analysis patterns. In particular, PageRank computation is 3.2 times faster at most than HDRF, the state-of-the art of streaming-based partitioning approach. © 2017, The Author(s)."
"10.1057/s41270-017-0013-7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031314713&doi=10.1057%2fs41270-017-0013-7&partnerID=40&md5=cc9eb0c6878b1c8755cdf251d71837ef","This paper aims to contribute insights on data analytics methodologies when applied to direct marketing. From a business perspective, the objective is to unveil those banking customers who are most likely to respond positively to a term deposit marketing campaign. Mathematically, this is a typical classification problem; however, in our case, the class of interest is relatively rare and the dataset imbalanced. The paper offers a comparison of performance between statistical, distance-based, induction and Machine Learning classification algorithms on predicting potential depositors, when trained with imbalanced datasets. The main effort focuses on rebalancing effectively the datasets during training so as to reverse the negative effect of imbalance and to increase the correct classifications for the under-represented class. Distance-based and cluster-based resampling techniques are applied in comparison and in combination in order to understand how customer targeting could become more effective for practitioners. Using a publicly available dataset for direct marketing of bank products, we study the influence of resampling techniques on the different algorithms and conclude that our proposed cluster-based technique is overall the most effective in relation to other well-established techniques. © Macmillan Publishers Ltd 2017."
"10.1057/s41270-017-0012-8","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031304913&doi=10.1057%2fs41270-017-0012-8&partnerID=40&md5=e99310fafd35f5265bb9fcae463e7031","Since consumers’ post-purchase behaviors directly indicate the extent of customer satisfaction (CS), these behaviors are relatively objective and can signal consumers’ attitudes toward the purchased product or service. Based on the basic expectation–disconfirmation paradigm, this article proposes a generalized model in evaluating CS by integrating equity, regret, and disappointment. Within this conceptual framework, CS can be assessed in a global manner so that different marketing mixes and strategies may take to meet consumers’ expectations effectively. In addition, by adopting weight and cluster to classify behaviors signaling-defined post-purchase emotional responses, this disappointment–regret– inequity disconfirmation framework and the post-purchase signal model is expected to compromise variables of chosen and forgone options with social exchange comparisons. One major benefit of this proposed post-purchase signal model is that it may partly overcome the shortcomings and reliability issues of self-report method with the measurement of CS in decomposed dimensions based on the occurred purchasing outcomes. © Macmillan Publishers Ltd 2017."
"10.1057/s41270-017-0011-9","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031301336&doi=10.1057%2fs41270-017-0011-9&partnerID=40&md5=df2960c1768c13bad82e743bbe378908","Although aging is a multidimensional process, studies of older consumer behavior traditionally have used age as a surrogate measure of this process. As a result, the emerged relationships between age and consumer behaviors are not well understood. This paper proposes a theoretically based alternative measure of age that derives from multiple dimensions of the aging process. It uses a large sample of older Malays to test and compare the power of this measure in predicting select consumer behaviors and compares the results to those of age and cognitive age that are commonly used to explain consumer behavior in later life. The results of the empirical study suggest that the new measure of age and aging may be a better predictor of certain types of consumer behavior of the elderly and possibly could supplement or even replace the existing measures such as cognitive age. Directions for further research are suggested with regard to improving and validating this measure. © Macmillan Publishers Ltd 2017."
"10.1080/23270012.2016.1239227","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056860482&doi=10.1080%2f23270012.2016.1239227&partnerID=40&md5=0919338cf35a10fed7e96d2e7bf8cd1e","This paper investigates the ordering policies of two competitive retailers, and the coordination status of a two-echelon supply chain by considering the fairness concerns of channel members. We consider that two retailers compete with each other over price, where overstock and shortage are allowed. We assume that the demand is stochastic and considered with additive form. First, based on the Nash bargaining fairness reference point, we obtain the optimal decisions of the fairness-concerned channel members in both the centralized and the decentralized cases using a two-stage game theory. Secondly, we analyze the coordination status of the supply chain with Nash bargaining fairness concerns using ideas of optimization. Finally, numerical experiments are used to illustrate the influence of some parameters, the fairness-concerned behavioral preference of the channel members on the optimal decisions and the coordination status of supply chain. Some managerial insights are obtained. © 2016, © 2016 Antai College of Economics and Management, Shanghai Jiao Tong University."
"10.1080/23270012.2016.1259967","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049115020&doi=10.1080%2f23270012.2016.1259967&partnerID=40&md5=042486e3aa941a625f2855cd988dd6c3","The purpose of this paper is to investigate how latest technology Internet-of-Things (IoT) can enable/facilitate traditional manufacturing firms shifting to more service-centred business perspective. Service-dominant (S-D) logic has emerged to provide the right perspective, vocabulary and assumptions on which to build a service-centred alternative to the traditional goods-dominant (G-D) paradigm for understanding economic exchange and value creation and has been identified as an appropriate philosophical foundation for the development of service science ([Maglio, P. P., & Spohrer, J. (2008). Fundamentals of service science. Journal of the Academy of Marketing Science, 36, 18–20; Maglio, P. P., Vargo, S. L., Caswell, N., & Spohrer, J. (2009). The service system is the basic abstraction of service science. Information Systems and e-business Management, 7, 395–406]). S-D logic is in its current state of development is conceptual and few empirical studies exist to test such a logic realized in real business applications. On the other hand, IoT is a novel paradigm recently envisioned as a global network of machines and devices capable of interacting with each other to reach desired business goals in the real world. However, there is considerable research on IoT technical specifications but less elaboration on real business applications. This paper aims to describe IoT as a critical vehicle when manufacturing firms desire to transit to a more S-D and value co-creation business model, with an in-depth real business case study in the elevators industry. This paper aims: (1) to explain the distinction between G-D logic and S-D logic and its implication for manufacturing firms, (2) to examine how IoT can facilitate the transition from key S-D logic managerial implication perspective and (3) to conduct a case study in a new service offering of an elevator service business when transition to more S-D mindset by adoption of IoT, the purpose aims to examine whether the underlying technology can bring different ways of thinking when deploying a new industrial service offering. © 2017, © 2017 Antai College of Economics and Management, Shanghai Jiao Tong University."
"10.1080/23270012.2016.1217755","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042099155&doi=10.1080%2f23270012.2016.1217755&partnerID=40&md5=2ebc9ed30d87975aa17d5b7ed927c93b","Uncertainty is certain in the world of uncertainty. This study revisits an economic production quantity (EPQ) model with shortages for stock-dependent demand of the items with reworking and disposing of the imperfect ones over a random planning horizon under the joint effect of inflation and time value of money, where the expected time length is imprecise in nature. Transmission of learning effect has been incorporated to reduce the defective production. The total expected profit over the random planning horizon is maximized subject to the imprecise space constraint. The possibility, necessity and credibility measures have been introduced to defuzzify the model. The simulation-based genetic algorithm is used to make decision for the above EPQ model in different measures of uncertainty. The model is illustrated through an example. Sensitivity analysis shows the impacts of different parameters on the objective function in the model. © 2016, © 2016 Antai College of Economics and Management, Shanghai Jiao Tong University."
"10.5334/dsj-2017-006","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017400417&doi=10.5334%2fdsj-2017-006&partnerID=40&md5=8aed5f66e038b637de5b4149f2377ca4","The continuous evolution of data management systems affords great opportunities for the enhancement of knowledge and advancement of science research. To capitalize on these opportunities, it is essential to understand and develop methods that enable data relationships to be examined and information to be manipulated. Earth Science Data Analytics (ESDA) comprises the techniques and skills needed to holistically extract information and knowledge from all sources of available, often heterogeneous, data sets. This paper reports on the ground breaking efforts of the Earth Science Information Partners (ESIP) ESDA Cluster in defining ESDA and identifying ESDA methodologies. As a result of the void of Earth science data analytics in the literature the ESIP ESDA definition and goals serve as an initial framework for a common understanding of techniques and skills that are available, as well as those still needed to support ESDA. Through the acquisition of Earth science research use cases and categorization of ESDA result oriented research goals, ESDA techniques/skills have been assembled. The resulting ESDA techniques/skills provide the community with a definition for ESDA that is useful in articulating data management and research needs, as well as a working list of techniques and skills relevant to the different types of ESDA. © 2017 The Author(s)."
"10.5334/dsj-2017-005","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017342980&doi=10.5334%2fdsj-2017-005&partnerID=40&md5=f343a2653350dc8009f456ac8256f630","Information about the research process is gaining importance for research documentation and evaluation. With the increased usage of such research information, the requirements for data quality and interpretation consistency are increasing. An agreed understanding of the concepts of research information is therefore crucial for fair science evaluation and science policy. Initiatives like euroCRIS and CASRAI address this by standardising research information definitions. In this paper, we present an approach to systematically develop and document not only definitions of research information, but also discussed alternatives and related arguments. With that we aim to support existing RI standardisation initatives with a flexible and scalable way of documenting and communicating the standardisation process in order to increase acceptance for the resulting definitions. Our contribution is threefold: Based on the widely used IBIS notation for argumentation modelling, we first introduce semantic rules for defining research information. Secondly, a transformation algorithm is provided to reduce the complexity of those argumentations - without the loss of information -And in turn improve readability of the diagrams. Thirdly, the semantic rules of the resulting less complex RIDAL notation are provided. The presented modelling notations are evaluated in the case setting of the standardisation project for research information of the German science system ""Core Research Dataset"". © 2017 The Author(s)."
"10.5334/dsj-2017-007","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017316676&doi=10.5334%2fdsj-2017-007&partnerID=40&md5=795ec2a54988b6ce8d9a242b15f71ad7","In an effort to lead our community in following modern data citation practices by formally citing data used in published research and implementing standards to facilitate reproducible research results and data, while also producing meaningful metrics that help assess the impact of our services, the National Center for Atmospheric Research (NCAR) Earth Observing Laboratory (EOL) has implemented the use of Digital Object Identifiers (DOIs) (DataCite 2017) for both physical objects (e.g., research platforms and instruments) and datasets. We discuss why this work is important and timely, and review the development of guidelines for the use of DOIs at EOL by focusing on how decisions were made. We discuss progress in assigning DOIs to physical objects and datasets, summarize plans to cite software, describe a current collaboration to develop community tools to display citations on websites, and touch on future plans to cite workflows that document dataset processing and quality control. Finally, we will review the status of efforts to engage our scientific community in the process of using DOIs in their research publications. © 2017 The Author(s)."
"10.5334/dsj-2017-016","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017315957&doi=10.5334%2fdsj-2017-016&partnerID=40&md5=d27256f501dcf1b1b197dfa5e5e9b97b","Sharing scientific data with the objective of making it discoverable, accessible, reusable, and interoperable requires work and presents challenges being faced at the disciplinary level to define in particular how the data should be formatted and described. This paper represents the Proceedings of a session held at SciDataCon 2016 (Denver, 12-13 September 2016). It explores the way a range of disciplines, namely materials science, crystallography, astronomy, earth sciences, humanities and linguistics, get organized at the international level to address those challenges. The disciplinary culture with respect to data sharing, science drivers, organization, lessons learnt and the elements of the data infrastructure which are or could be shared with others are briefly described. Commonalities and differences are assessed. Common key elements for success are identified: data sharing should be science driven; defining the disciplinary part of the interdisciplinary standards is mandatory but challenging; sharing of applications should accompany data sharing. Incentives such as journal and funding agency requirements are also similar. For all, social aspects are more challenging than technological ones. Governance is more diverse, often specific to the discipline organization. Being problem-driven is also a key factor of success for building bridges to enable interdisciplinary research. Several international data organizations such as CODATA, RDA and WDS can facilitate the establishment of disciplinary interoperability frameworks. As a spin-off of the session, a RDA Disciplinary Interoperability Interest Group is proposed to bring together representatives across disciplines to better organize and drive the discussion for prioritizing, harmonizing and efficiently articulating disciplinary needs. © 2017 The Author(s)."
"10.5334/dsj-2017-017","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017310855&doi=10.5334%2fdsj-2017-017&partnerID=40&md5=6d37156df2c43d8a99fe5bca1e515df7","Scientific research is producing ever-increasing amounts of data. Organizing and reflecting relationships across data collections, datasets, publications, and other research objects are essential functionalities of the modern science environment, yet challenging to implement. Landing pages are often used for providing 'big picture' contextual frameworks for datasets and data collections, and many large-volume data holders are utilizing them in thoughtful, creative ways. The benefits of their organizational efforts, however, are not realized unless the user eventually sees the landing page at the end point of their search. What if that organization and 'big picture' context could benefit the user at the beginning of the search? That is a challenging approach, but The Department of Energy's (DOE) Office of Scientific and Technical Information (OSTI) is redesigning the database functionality of the DOE Data Explorer (DDE) with that goal in mind. Phase I is focused on redesigning the DDE database to leverage relationships between two existing distinct populations in DDE, data Projects and individual Datasets, and then adding a third intermediate population, data Collections. Mapped, structured linkages, designed to show user relationships, will allow users to make informed search choices. These linkages will be sustainable and scalable, created automatically with the use of new metadata fields and existing authorities. Phase II will study selected DOE Data ID Service clients, analyzing how their landing pages are organized, and how that organization might be used to improve DDE search capabilities. At the heart of both phases is the realization that adding more metadata information for cross-referencing may require additional effort for data scientists. OSTI's approach seeks to leverage existing metadata and landing page intelligence without imposing an additional burden on the data creators. © 2017 The Author(s)."
"10.5334/dsj-2017-013","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017310513&doi=10.5334%2fdsj-2017-013&partnerID=40&md5=27506d91c71f99b26026b8a5959b50e7","The identification of information objects has always been important with library collections with indexes having been created in the most ancient times. Since the digital age, many specialised and generic persistent identifier (PID) systems have been used to identify digital objects. Just as many ancient indexes have died over time, so too PID systems have had a lifecycle from inception to active phase to paralysis, and eventually a fall into oblivion. Where the indexes within the Great Library at Alexandria finally succumbed to fire, technology change has been the destroyer of more recent digital indexes. We distil four PID system design principles from observations over the years that we think should be implemented by PID system architects to ensure that their systems survive change. The principles: describe how to ensure identifiers' system and organisation independence; codify the delivery of essential PID system functions; mandate a separation of PID functions from data delivery mechanisms; and require generation of policies detailing how change is handled. In addition to suggesting specific items for each principle, we propose that a platform-independent model (PIM) be established for persistent identifiers - of any sort and with any resolver technology - in order to enable transition between present and future systems and the preservation of the identifiers' functioning. We detail our PID system-The PID Service-That implements the proposed principles and a data model to some extent and we describe an implementation case study of an organisation's implementation of PID systems that implement the Pillars further but still not completely. Penultimately, we describe in a Future Work section, an opportunity for the use of both the Pillars and the PIM; that of the World Wide Web Consortium's Permanent Identifier Community Group who is seeking to ""set up and maintain a secure permanent, URL re-direction service for the web"". © 2017 The Author(s)."
"10.5334/dsj-2017-010","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017299021&doi=10.5334%2fdsj-2017-010&partnerID=40&md5=041c180734bc37888dc22afa0c60d407","Repository managers can never be one hundred percent sure of the security of hosted research data. Even assuming that human errors and technical faults will never happen, repositories can be subject to hacking attacks. Therefore, repositories accepting personal/sensitive data (or other forms of restricted data) should have workflows in place with defined procedures to be followed should things go wrong and restricted data is inappropriately released. In this paper we will report on our considerations and procedures when restricted data from our institution was inappropriately released. © 2017 The Author(s)."
"10.5334/dsj-2017-012","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017297131&doi=10.5334%2fdsj-2017-012&partnerID=40&md5=f7e5dda13720f235f0bf273edc1e8cce","As public investment in archiving research data grows, there has been increasing attention to the longevity or sustainability of the data repositories that curate such data. While there have been many conceptual frameworks developed and case reports of individual archives and digital repositories, there have been few empirical studies of how such archives persist over time. In this paper, we draw upon organizational studies theories to approach the issue of sustainability from an organizational perspective, focusing specifically on the organizational histories of three social science data archives (SSDA): ICPSR, UKDA, and LIS. Using a framework of organizational resilience to understand how archives perceive crisis, respond to it, and learn from experience, this article reports on an empirical study of sustainability in these long-lived SSDAs. The study draws from archival documents and interviews to examine how sustainability can and should be conceptualized as on-going processes over time and not as a quality at a single moment. Implications for research and practice in data archive sustainability are discussed."
"10.5334/dsj-2017-011","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017294799&doi=10.5334%2fdsj-2017-011&partnerID=40&md5=bfd477931611083b2e8917689426c7f0","Public transcriptomic assets in the nuclear receptor (NR) signaling field hold considerable collective potential for exposing underappreciated aspects of NR regulation of gene expression. This potential is undermined however by a series of enduring informatic pain points that retard the routine re-use of these datasets. Here we describe a coordinated biocuration and web develop-ment approach to redress this situation that is closely aligned with ideals articulated in the FAIR (findable, accessible, interoperable, re-usable) principles on data stewardship. To improve findability, biocurators engage authors of studies in collaborating journals to secure datasets for deposition in public archives. Annotated derivatives of the archived datasets are assigned digital object identifiers and regulatory molecule identifiers that support persistent linkages between datasets and their associated research articles, integration in relevant records in gene and small molecule knowledgebases, and indexing by dataset search engines. To enhance their accessibility and interoperability, datasets are visualizable in responsively designed web pages, retrievable in machine-readable spreadsheets, or through an application programming interface. Re-use of the datasets is supported by their interrogation as a universe of data points through the Transcriptomine search engine, highlighting transcriptional intersections between NR signaling pathways, physiological processes and disease states. We illustrate the value of our approach in connecting disparate research communities using a use case of persistent interoperability between the Nuclear Receptor Signaling Atlas and the Pharmacogenomics Knowledgebase. Our FAIR-Aligned model demonstrates the enduring value of discovery-scale datasets that accrues from their systematic compilation, biocuration and distribution across the digital biomedical research enterprise."
"10.5334/dsj-2017-018","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017291784&doi=10.5334%2fdsj-2017-018&partnerID=40&md5=964c6d3d05350c854b00e94bc5d3ec2f","Open data is considered the new oil. As oil can be used to produce fertilisers, pesticides, lubricants, plastics and many other derivatives, so data is considered the commodity to use and re-use to create value. The number of initiatives supporting free access to data has increased in the last years and open data is becoming the norm in the public sector; the approach empowers stakeholders and nurtures the economy. Even if at early stage, private companies also are adapting to the open data market. A survey was conducted to which thirteen companies of different size (from micro enterprises to world-leading pharmas) in the pharmaceutical and biotech sector and representing four business models archetypes of companies exploiting open data (aggregators, developers, enrichers and enablers) participated. The information collected provides a snapshot of the use of open data by the pharmaceutical and biotech industry in 2015-2016. The companies interviewed use open data to complement proprietary data for research purposes, to implement licensing-in/licensing-out strategies, to map partnerships and connections among players or to identify key expertise and hire staff. Pharmaceutical and biotech companies have made of the protection of knowledge a dogma at the foundation of their business models, but using and contributing to the open data movement may change their approach to intellectual property and innovation. © 2017 The Author(s)."
"10.5334/dsj-2017-015","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017289022&doi=10.5334%2fdsj-2017-015&partnerID=40&md5=8f4ae7977f8e84b180d5d49ae65d7e4c","NASA's Earth Science Data and Information System (ESDIS) Project began investigating the use of Digital Object Identifiers (DOIs) in 2010 with the goal of assigning DOIs to various data products. These Earth science research data products produced using Earth observations and models are archived and distributed by twelve Distributed Active Archive Centers (DAACs) located across the United States. Each data center serves a different Earth science discipline user community and, accordingly, has a unique approach and process for generating and archiving a variety of data products. These varied approaches present a challenge for developing a DOI solution. To address this challenge, the ESDIS Project has developed processes, guidelines, and several models for creating and assigning DOIs. Initially the DOI assignment and registration process was started as a prototype but now it is fully operational. In February 2012, the ESDIS Project started using the California Digital Library (CDL) EZID for registering DOIs. The DOI assignments were initially labor-intensive. The system is now automated, and the assignments are progressing rapidly. As of February 28, 2017, over 50% of the data products at the DAACs had been assigned DOIs. Citations using the DOIs increased from about 100 to over 370 between 2015 and 2016. © 2017 The Author(s)."
"10.5334/dsj-2017-002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85011914765&doi=10.5334%2fdsj-2017-002&partnerID=40&md5=cfca9156dd84a19446da6cf92d5aa894","The International Geo Sample Number (IGSN) is a globally unique persistent identifier (PID) for physical samples that provides discovery functionality of digital sample descriptions via the internet. In this article we describe the implementation of a registration service for IGSNs of the Helmholtz Centre Potsdam - GFZ German Research Centre for Geosciences. This includes the adaption of the metadata schema developed within the context of the System for Earth Sample Registration (SESAR1) to better describe the complex sample hierarchy of drilling cores, core sections and samples of scientific drilling projects. Our case study is the COSC-1 expedition2 (Collisional Orogeny in the Scandinavian Caledonides) supported by the International Continental Scientific Drilling Program3 (ICDP). COSC-1 prompted for the first time in ICDP's history to assign and register IGSNs during an on-going drilling campaign preserving the original parent-child relationship of the sample objects. IGSN-associated data and metadata are distributed and shared with the world wide community through novel web portals, one of which is currently evolving as part of ICDP's collaborative efforts within the GFZ Potsdam and researchers from ICDP's COSC clientele. Thus, COSC-1 can be considered as a 'Prime-Example' for ICDP projects to further improve the quality of scientific research output through a transparent process of producing and managing large quantities of data as they are normally acquired during a typical scientific drilling operation. The IGSN is an important new player in the general publication landscape that can be cited in scholarly literature and also cross-referenced in DOI-bearing scholarly and data publications. © 2017 The Author(s)."
"10.5334/dsj-2017-003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85011898344&doi=10.5334%2fdsj-2017-003&partnerID=40&md5=fa62e3710582fbad1fadad1110d437e5","Human knowledge of the polar region is a unique blend of Western scientific knowledge and local and indigenous knowledge. It is increasingly recognized that to exclude Traditional Knowledge from repositories of polar data would both limit the value of such repositories and perpetuate colonial legacies of exclusion and exploitation. However, the inclusion of Traditional Knowledge within repositories that are conceived and designed for Western scientific knowledge raises its own unique challenges. There is increasing acceptance of the need to make these two knowledge systems interoperable but in addition to the technical challenge there are legal and ethical issues involved. These relate to 'ownership' or custodianship of the knowledge; obtaining appropriate consent to gather, use and incorporate this knowledge; being sensitive to potentially different norms regarding access to and sharing of some types of knowledge; and appropriate acknowledgement for data contributors. In some cases, respectful incorporation of Traditional Knowledge may challenge standard conceptions regarding the sharing of data, including through open data licensing. These issues have not been fully addressed in the existing literature on legal interoperability which does not adequately deal with Traditional Knowledge. In this paper we identify legal and ethical norms regarding the use of Traditional Knowledge and explore their application in the particular context of polar data. Drawing upon our earlier work on cybercartography and Traditional Knowledge we identify the elements required in the development of a framework for the inclusion of Traditional Knowledge within data infrastructures. © 2017 The Author(s)."
"10.5334/dsj-2017-004","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85011878119&doi=10.5334%2fdsj-2017-004&partnerID=40&md5=17f2a4d43ac2784656b9cefe5e2b07ce","Earth Science researchers require access to integrated, cross-disciplinary data in order to answer critical research questions. Partially due to these science drivers, it is common for disciplinary data systems to expand from their original scope in order to accommodate collaborative research. The result is multiple disparate databases with overlapping but incompatible data. In order to enable more complete data integration and analysis, the Observations Data Model Version 2 (ODM2) was developed to be a general information model, with one of its major goals to integrate data collected by in situ sensors with those by ex-situ analyses of field specimens. Four use cases with different science drivers and disciplines have adopted ODM2 because of benefits to their users. The disciplines behind the four cases are diverse - hydrology, rock geochemistry, soil geochemistry, and biogeochemistry. For each case, we outline the benefits, challenges, and rationale for adopting ODM2. In each case, the decision to implement ODM2 was made to increase interoperability and expand data and metadata capabilities. One of the common benefits was the ability to use the flexible handling and comprehensive description of specimens and data collection sites in ODM2's sampling feature concept. We also summarize best practices for implementing ODM2 based on the experience of these initial adopters. The descriptions here should help other potential adopters of ODM2 implement their own instances or to modify ODM2 to suit their needs. © 2017 The Author(s)."
"10.5334/dsj-2017-001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85011798213&doi=10.5334%2fdsj-2017-001&partnerID=40&md5=24c9426556684acd71d76bb8af0c5fbd","EarthCube is a U.S. National Science Foundation initiative that aims to create a cyberinfrastructure (CI) for all the geosciences. An initial set of ""building blocks"" was funded to develop potential components of that CI. The Brokering Building Block (BCube) created a brokering framework to demonstrate cross-disciplinary data access based on a set of use cases developed by scientists from the domains of hydrology, oceanography, polar science and climate/weather. While some successes were achieved, considerable challenges were encountered. We present a synopsis of the processes and outcomes of the BCube experiment. © 2017 The Author(s)."
"10.1504/IJBIDM.2017.10003121","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042638435&doi=10.1504%2fIJBIDM.2017.10003121&partnerID=40&md5=7c1e20d2abf1c4ff0ed98d2715fe6451","One of the most popular clustering algorithms is K-means cluster due to its simplicity and efficiency. Although clustering using K-means algorithm is fast and produces good results, it still has a number of limitations including initial centroid selection and local optima. The purpose of this research is to develop a hybrid algorithm that address k-means clustering limitations and improve its performance by finding optimal cluster centre. In this paper, Lévy-flights or Lévy motion is one of non-Gaussian random processes used to solve the initial centroid problem. Bees algorithm is a population-based algorithm which has been proposed to overcome the local optima problem, used along with its local memory to enhance the efficiency of K-means. The proposed algorithm applied to different datasets and compared with K-means and basic Bess algorithm. The results show that the proposed algorithm gives better performance and avoid local optima problem. Copyright © 2017 Inderscience Enterprises Ltd."
"10.1504/IJBIDM.2017.086986","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030791139&doi=10.1504%2fIJBIDM.2017.086986&partnerID=40&md5=26fa0cd39139c254a6d57995d0e8f739","Business process management and business intelligence initiatives are commonly seen as separated organisational projects, suffering from lack of coordination, leading to a poor alignment between strategic management and operational business processes execution. Information systems researchers andprofessionals have recognised that business processes are the key for identifying the user needs for developing the software that supports those requirements. This paper presents a process based approach for identifying an analytical data model using as input a set of interrelated business processes, modelled with business process model and notation (BPMN), and the corresponding persistent operational data model. This process-based approach extends the BPMN language allowing the integration of behavioural aspects and processes performance measures in the persistent operational data model. The proposed approach ensures the identification of an analytical data model for a data warehouse, integrating dimensions, facts, relationships and measures, providing useful data analytics perspectives of the data under analysis. Copyright © 2017 Inderscience Enterprises Ltd."
"10.1504/IJBIDM.2017.086983","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030787486&doi=10.1504%2fIJBIDM.2017.086983&partnerID=40&md5=f4610196be2d888de76dcef45268f32b","Regression testing is one among the most serious activities of software development and conservation. The main influence of our study is regression test case generation, factors documentation, clustering for test case prioritisation and optimisation of ordered test case. In this investigation, the K-means clustering algorithm will be utilised to discrete the pertinent test cases from immaterial test cases. Pertinent test cases signify the prioritised test cases. We will reflect only these pertinent test cases subsequent from the clustering algorithm to optimise it along with hybrid fire fly algorithm (HFFA). The hybridisation of artificial bee colony (ABC) algorithm and also the firefly (FF) algorithm are utilised for the function of HFFA. The FF will be administered within the scout bee constituent of ABC that leads to fast conjunction and restricted search space controlled depended on optimisation of locations in ourHFFA optimisation algorithm. Therefore we will acquire effective prioritized test cases. Copyright © 2017 Inderscience Enterprises."
"10.1504/IJBIDM.2017.086984","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030787146&doi=10.1504%2fIJBIDM.2017.086984&partnerID=40&md5=76823ca827bb8fa134c7a84dc7bb26df","Vision enhancement in night surveillance robot (FLC-VENSR) can be obtained by implementing an area efficient reconfigurable architecture of fuzzy logic comparator. The comparator contains a fuzzy logic algorithm for comparator design. The reconfigurable computing method will work very efficiently for images and videos captured under any sort of environment. The speed and area constraints can be met by quantifying the lessening inprocessing speed over and above FPGA resources that can be accomplished if acomponent of the image/video processing system is embedded onto a hardware depended platform like an FPGA. The proposed processor is executed and synthesised using Xilinx integrated software environment (ISE) and Spartan-3E XC3S5000E family. It is observed that the used area by the proposed processor is less compared to the available processor AFLC-PIDAM. Copyright © 2017 Inderscience Enterprises Ltd."
"10.1504/IJBIDM.2017.086985","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030759055&doi=10.1504%2fIJBIDM.2017.086985&partnerID=40&md5=83c2bff67af1d056b59239bbc4994962","Friction-stir-welding (FSW) is a firm stipulation dual procedure and its possessions are reliant on the welding procedure confines. Investigational results are executed by an advance speed revolving apparatus negotiating AA 6063 aluminium alloy 4 mm width laminate substance by FSW apparatus which is formed by HSS M2 substance. The DWT is utilised to decay the vibration signal in different welded dual. Following the signal decay procedure, the signals are indicated to categorisation procedure. Arithmetical and chronological limits of decay vibration signals by wavelet transform have been employed as the input of the FFBN. For improving the classification performance optimises the network structure hidden the layer and hidden neuron optimisation techniques are used. The optimal hidden layer and neuron attained in OGA technique to organise the vibration signals. This experimental and simulation analysis proves that vibration signal analysis method could be used to concern in process condition monitoring in friction stir welding process Copyright © 2017 Inderscience Enterprises Ltd."
"10.1504/IJBIDM.2017.086987","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030754445&doi=10.1504%2fIJBIDM.2017.086987&partnerID=40&md5=f39d7119df3aa3007e55b3a28cce538d","Freshwater is considered one of the most important renewable natural resources of the planet. In this sense, it is vital to study and evaluate the water quality in rivers and basins. The USA and especially the border states like California face the same water problems as its southern neighbours, such as the deterioration of public drinking water systems and the continued appearance of pollutants that threaten domestic water sources. This implies the need to monitor and analyse the water supplies in each region. Several researches have been conducted to develop water quality detection systems through supervised learning algorithms. However, these research approaches set aside the data processing to improve the performance of supervised learning algorithms. This paper presents an improvement of data processing techniques for a water quality detection system based on supervised learning and data quality techniques for the California estuary. Copyright © 2017 Inderscience Enterprises Ltd."
"10.1504/IJBIDM.2017.085090","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85024123267&doi=10.1504%2fIJBIDM.2017.085090&partnerID=40&md5=6e0b8a649a7c3cd2677526a06c88f753","This research aims to provide a robust evidence base contributing to improving the quality of policy formation from local to national level in Ireland. The distributions of businesses within key economic sectors in Ireland are explored aiming to find clustering effects occurring across the country. The density mapping and hot spot analysis approaches were applied to find statistically significant clusters of companies for specific business sectors. The research was implemented in collaboration with Dublin Regional Authority and Dublin City Council to inform key policy makers in Ireland. It assists in the assessments of the nationwide spatial distribution of economic activities adding to the overall body of evidence on business intensive regions. The results show the continued statistically proven significance of the main urban growth centres or gateways as key centres for the main business sectors in Ireland, while public policy has prioritised rebalancing economic development to other regions. Copyright © 2017 Inderscience Enterprises Ltd."
"10.1504/IJBIDM.2017.085092","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85024122382&doi=10.1504%2fIJBIDM.2017.085092&partnerID=40&md5=00103aa4261b6cda48eda8c385e959c0","In this paper, an efficient spam classification technique is proposed using weighted-based multiple classifier and F-GSO algorithm. At first, input data is given to the feature selection to select the suitable feature for spam classification. Here, firefly and GSO algorithm is effectively hybridised to select the suitable features. Once the best feature is identified through hybrid algorithm, the spam classification is done using the weighted-based multiple classifiers. Here, three categories of classifiers like, rule classifier, lazy classifier and learning classifiers is combined using weight rule. These three classifiers have their own advantages and disadvantages so the hybridisation of classifiers leads to provide overall improvements by rectifying their disadvantages by other algorithms and retaining their advantages. Accordingly, decision tree (rule), lazy classifier (naïve Bayes) and neural network classifier (learning) are combined using voting-based weighted rule. Our experiment result shows the proposed systems have outperformed by having better accuracy value of 98.83%. Copyright © 2017 Inderscience Enterprises Ltd."
"10.1504/IJBIDM.2017.085088","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85024118844&doi=10.1504%2fIJBIDM.2017.085088&partnerID=40&md5=8b57baa0454c3ddb4b8934e80534de6a","Cardiovascular disease remains the biggest cause of deaths worldwide and the heart disease prediction at the early stage is very important. The classification problem of assigning several observations into different disjoint groups plays an important role in business decision making and many other areas. A heart disease dataset is analysed using neural network approach. The main aim of data mining is to find relationships in data and to predict outcomes. Classification is one of the important data mining techniques for classifying given set of input data. Many real world problems in various fields such as business, science, industry and medicine, can be solved by using classification approach. For the better classification and improve the accuracy, optimisation technique is used. To optimise the weight of the ANN structure, GSO technique is used. From the classification results process, the maximum accuracy is 82.23% in heart diseases database classification process. Copyright © 2017 Inderscience Enterprises Ltd."
"10.1504/IJBIDM.2017.085087","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85024112342&doi=10.1504%2fIJBIDM.2017.085087&partnerID=40&md5=91a7d04c9596fa0455c63a9797a3c0a8","Time series forecasting based on pattern matching has received a lot of interest in recent years due to its simplicity and the ability to predict complex nonlinear behaviours. The choice of the metric to measure the similarity between two time series depends mainly on the specific features of the considered data and it can influence on forecasting results. In this paper, unlike the conventional method, we propose an improved pattern matching-based prediction method using a linear combination of two measures, Euclidean distance and dynamic time warping, in order to achieve a better forecasting result. These two distance measures are chosen because they are the two most commonly used metrics for pattern matching in time series. The experimental results showed that our approach can produce better results on time series forecasting work in comparison to the pattern matching-based method under Euclidean distance or dynamic time warping in terms of prediction accuracy. Copyright © 2017 Inderscience Enterprises Ltd."
"10.1504/IJBIDM.2017.085089","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85024110268&doi=10.1504%2fIJBIDM.2017.085089&partnerID=40&md5=f2516798fafd3863740c06a47dcf6d5b","Clustering-based document retrieval system offers to find similar documents for a given user's query. This study explores the scope of kernel fuzzy c-means (KFCM) with the genetic algorithm on document retrieval issue. Initially, genetic algorithm-based kernel fuzzy c-means algorithm (GKFCM) is proposed to make the clustering of documents in the library. For each cluster, an index is created, which contains a common significant keywords of the documents for that cluster. Once the user enters the keyword as the input to the system, it will process the keywords with the WORDNET ontology to achieve the neighbourhood keywords and related synset keywords. Lastly, the documents inside the cluster are released at first as the resultant-related documents for the query keyword, which clusters have the maximum matching score values. Experiments results prove that GKFCM-based proposed system outperforms better performance than existing methods. Copyright © 2017 Inderscience Enterprises Ltd."
"10.1504/IJBIDM.2017.084283","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020054563&doi=10.1504%2fIJBIDM.2017.084283&partnerID=40&md5=3e8b577eba2c950f72406f94c566a1db","We proposed a symmetrical nonlinear regression model to fit interval-valued data. An important feature of this new model is that the estimate and prediction are less sensitive in the presence of outliers than a nonlinear model proposed in the literature. Monte Carlo simulation studies have been developed to investigate the performance of the model on different scenarios in precense of the some percentage of outliers. The results based on the mean magnitude of the relative errors are presented and discussed. The model was fitted to one real symbolic dataset with noticeable interval outliers, and the forecast accuracy has been considered. © Copyright 2017 Inderscience Enterprises Ltd."
"10.1504/IJBIDM.2017.084278","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020036830&doi=10.1504%2fIJBIDM.2017.084278&partnerID=40&md5=01b4abb71f18615b40c1a06c89056e14","The multidimensional nature of socio-economic hardship requires a multidimensional research approaches, oriented toward advanced solutions, able to capture the changing dimensions of the problem at hand. One of such approaches consists in abandoning traditional dichotomous logic in favour of a semantically richer fuzzy classification, in which each unit belongs and, at the same time, does not belong to a given category. Cluster analysis allows to identify the profiles families who meet certain descriptive characteristics not defined a priori. The approach used in this work to synthesise and measure hardship conditions is based on a clustering procedure known as fuzzy clustering by local approximation of membership (FLAME), and based on defining the neighbourhood of each object and identifying cluster-supporting objects. This clustering method not only allows for each instance of a dataset to belong to a unique main cluster, but also that each instance can be shared by two or more clusters on the ground of suitably defined 'fuzzy profiles'. © Copyright 2017 Inderscience Enterprises Ltd."
"10.1504/IJBIDM.2017.084284","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020023283&doi=10.1504%2fIJBIDM.2017.084284&partnerID=40&md5=e70016025f1526d2eefec935a03a48a3","The main objective of this paper is to design intrusion detection from suspicious URLs using optimal fuzzy logic system. Basically, the system consists of three modules such as: 1) feature extraction; 2) feature selection; 3) classification. At first, we extract the four kinds of feature from the dataset which have a total of 30 features. Among that, we select the important features using hybridisation of firefly and cuckoo search algorithm (HFFCS). Then, we train the selected features using fuzzy logic classifier and then we calculate the fuzzy logic score. Finally in testing, the fuzzy logic classifier detected the malicious URL based on the fuzzy score. In this work, we use two types of database such as URL reputation dataset and phishing websites dataset. The experimental results demonstrate that the proposed malicious URL detection method outperforms other existing methods. © Copyright 2017 Inderscience Enterprises Ltd."
"10.1504/IJBIDM.2017.084280","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019985863&doi=10.1504%2fIJBIDM.2017.084280&partnerID=40&md5=8bb772d7637ef2298cdcd5ccff933567","In this paper, the proposed adaptive technique is used for improving the performance of the concentrating solar power (CSP) system. Initially, the neural network (NN) generated the dataset based on the reference signal and the normal signal. Then the cuckoo search (CS) algorithm improved the effectiveness of the proposed algorithm and the weight and the biases can optimised. The novelty of this paper is to evaluate the performance of the CSP system and improve the effectiveness also of gathered maximum energy. Thereafter, to support the accuracy and efficiency of the system, parameters such as regression coefficient, root mean square error (RMSE) value and error variation are estimated. The proposed optimisation process is implemented in MATLAB/Simulink platform and also the result is estimated and analysed based on the error parameters of the CSP system. Based on these, the proposed optimisation process is evaluated and compared with other traditional methods. © Copyright 2017 Inderscience Enterprises Ltd."
"10.1504/IJBIDM.2017.082704","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85015344764&doi=10.1504%2fIJBIDM.2017.082704&partnerID=40&md5=28de42e9fd608ef3a78871259a353277","The ever-zooming issue of personalisation in the domain of the e-learning or the adaptive e-learning has emerged as a hot subject of intriguing debate among the inquisitive investigators in the last few years. In the learning contents module, all the critical learning materials in the shape of text, video or audio are collected and saved by means of the data management approach so as to effectively orchestrate the entire learning material. In the profile ontology module, the learner profile is saved as the ontology with a clear-cut framework and data. The Markov model is also utilised in this regard. In the instructor module, the online tutoring for each and every learner in accordance with the protocol devised by means of the captioned techniques is carried out. The execution is performed through the Java programming and protégé device and the performance is subjected to assessment. © 2017 Inderscience Enterprises Ltd."
"10.1504/IJBIDM.2017.082706","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85015341543&doi=10.1504%2fIJBIDM.2017.082706&partnerID=40&md5=d178ef63d1e0f8e727b337934b55ed0e","One of the most popular clustering algorithms is K-means cluster due to its simplicity and efficiency. Although clustering using K-means algorithm is fast and produces good results, it still has a number of limitations including initial centroid selection and local optima. The purpose of this research is to develop a hybrid algorithm that address k-means clustering limitations and improve its performance by finding optimal cluster centre. In this paper, Lévy-flights or Lévy motion is one of non-Gaussian random processes used to solve the initial centroid problem. Bees algorithm is a population-based algorithm which has been proposed to overcome the local optima problem, used along with its local memory to enhance the efficiency of K-means. The proposed algorithm applied to different datasets and compared with K-means and basic Bess algorithm. The results show that the proposed algorithm gives better performance and avoid local optima problem. © 2017 Inderscience Enterprises Ltd."
"10.1504/IJBIDM.2017.082703","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85015306401&doi=10.1504%2fIJBIDM.2017.082703&partnerID=40&md5=5d9cb46e1eca80fdb269571dbc7bf3e0","Co-location pattern discovery is intended towards the processing data with spatial contexts to discover classes of spatial objects that are frequently located together. The existing moving vehicle location prediction technique not analyses the moving vehicles co-location instance. So, we improve the previous technique process by mining spatially co-located moving objects using spatial data mining techniques. Initially, the neighbour relationship is computed by the prim's algorithm. After that, the candidate co-locations are pruned according to the presence of candidate co-location in the input data and the final stage of co-location instances selection is performed by compute neighbourhood and node membership functions. The values obtained using neighbourhood membership function is compared with the dynamic threshold values. The co-location instances are selected which satisfy the dynamic threshold value. Moreover, the proposed co-location pattern mining with dynamic thresholding technique is compared with the existing co-location pattern mining technique. © 2017 Inderscience Enterprises Ltd."
"10.1504/IJBIDM.2017.082701","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85015294815&doi=10.1504%2fIJBIDM.2017.082701&partnerID=40&md5=dc4fe3df3f40edd928b8291e0c095d36","Recently, the speed of change related with enterprise management is getting faster than ever owing to the competition among companies, technique diffusion, shortening of product lifecycle, excessive supply of market The compliance condition from the ship owner is getting complicated and the needs for the new product such as FPSO, FSRU This paradigm shift emphasise the flexibility, agility and resilience rather than the competitive price or a optimal perspective for a survival in shipbuilding industry In this paper, I am going to define a standard shipbuilding production management system, and to propose a unified ship production management system through detail analysis of the activities and the data flow of ship production management systems. And, the system functions for the strategic approach are investigated through the business administration tools -performance pyramid, VDT and BSC. Lastly, the research of applying strategic KPI to the digital shipyard as virtual execution platform is conducted. © 2017 Inderscience Enterprises Ltd."
"10.1504/IJBIDM.2017.082702","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85015290958&doi=10.1504%2fIJBIDM.2017.082702&partnerID=40&md5=0e8a55c32b3701b11f1c68f85e30d84a","Cloud computing is an budding information technology of data storage, of local networks as well as software and has its advantages of scalability, reliability, high performance and comparatively low cost feasible solution contrast to committed infrastructures. In this paper, an effective encryption algorithm is proposed with the employment of advanced encryption standard (AES) encryption. Here, we modify the shift operation which highly eliminates data privacy leakage. Here, initially the data owner encrypts the searchable index with enhanced AES encryption. The storage would in the form of vector space. Subsequently, the data user will decrypt the scores and select out the top-K highest scoring files identifiers to request to the cloud server. The technique is implemented using Java and evaluated using standard metrics. © 2017 Inderscience Enterprises Ltd."
"10.1177/0265813516641684","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028624072&doi=10.1177%2f0265813516641684&partnerID=40&md5=7fb599a4152c39f6800424fd9b7fbf37","A framework was developed for the construction of an objectives hierarchy for multicriteria decisions in land use planning. The process began through identification of fundamental objectives; these were iteratively decomposed into a hierarchy of subobjectives until a level was reached at which subobjectives had measurable attributes. Values were derived for attributes through a variety of methods and weights assigned to objectives through preference elicitation. The framework assumed that the objectives could be incorporated into a linear value function; this required attributes to satisfy preference and difference independence conditions. Strategies were developed to address typical features that distinguish land use decisions from many other multicriteria decisions. The methodology was illustrated with a case study of land use planning in a forestry concession in the Merauke region of Papua Province, Indonesia. The problem involved severe hard constraints; the analysis showed how these can be accommodated within the framework. Results integrated interests and preferences of a diverse set of stakeholders (resident peoples, developers, and conservation professionals) and were intended for implementation. This methodology is extendible to other land use problems. © The Author(s) 2016."
"10.1177/0265813516642226","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028623811&doi=10.1177%2f0265813516642226&partnerID=40&md5=9ebc6144528cad63db783f66cc364eff","The spatial economy can be visualised as a web of overlapping markets or functional areas linked to different activities. Unfortunately, very often administrative regions and local authority areas are used as proxies for functional areas, but they have serious shortcomings for spatial economic analysis and the implementation of local policies. The spatial economy is viewed here as a complex network of economic flows within a hierarchical urban system. This paper constructs a comprehensive geography of functional economic areas by first using consistent criteria based on commuting and migration flows for England. The analysis next superimposes a central place hierarchy derived from retail sales and office centre rents. It then classifies these functional economic areas offering insights into the spatial economy of England. The geography queries the relevance of core/surrounding models of urban form as large local labour market areas comprise in some cases more than 20 housing market areas. While the research uses England as a case study the analysis is of generic significance to the spatial economies of developed countries. The paper also raises a number of methodological and data issues for further development. © The Author(s) 2016."
"10.1177/0265813515626924","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028622668&doi=10.1177%2f0265813515626924&partnerID=40&md5=0c59a22133dffc7b4781ed2d2c0f36e6","France has developed a high quality motorway system that has been rapidly rationalised and matured in the late 20th century yet has been founded on ancient, Roman infrastructures. The development of the motorway system is thus an iterative method associated with hierarchical ‘top-down’ processes taking into consideration factors such as population density, network demand, location of natural resources, civil engineering challenges and population growth. At the opposite extreme to this approach is the development of transport networks within simple biological systems which are typically decentralised, dynamic and emerge from simple, local and ‘bottom-up’ interactions. We examine the notion, and to what extent, that the structure of a complex motorway network could be predicted by the transport network of the single-celled slime mould Physarum polycephalum. This comparison is explored through its ability to ‘deduce’ the French motorway network in a series of analogue and digital experiments. We compare Physarum network and motorway network topology in relation to proximity graphs and assess the trade-off between connectivity and minimal network length with a bottom-up model of a virtual plasmodium. We demonstrate that despite the apparent complexity of the challenge Physarum can successfully apply its embodied intelligence to rationalise the motorway topology. We also demonstrate that such calculations prove challenging in the face of significant obstacles such as, mountainous terrain and may account for the missing route between Nice, Grenoble Avignon and Lyon. Finally, we discuss the topological findings with respect to circle and spoke city planning infrastructures and certain species of web-building spiders. © The Author(s) 2016."
"10.1177/0265813515608641","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028621262&doi=10.1177%2f0265813515608641&partnerID=40&md5=df1dec3506585f0034b3b21b4a45682e","Various uncertainties exist in most urban land-use allocation systems; however, they have not been considered in most traditional urban land-use allocation methods. In this study, an interval-probabilistic urban land-use allocation model is developed based on a hybrid intervalprobabilistic programming method. The developed interval-probabilistic urban land-use allocation model can deal with uncertainties expressed as intervals and probability distributions; moreover, it can also help examine the reliability of satisfying (or risk of violating) system constraints under uncertainty; the interval-probabilistic urban land-use allocation model not only considers economic factors, but also involves environmental and ecological constraints, which can effectively reflect various interrelations among different aspects in the urban land-use system. The developed model is applied to a case of long-term land-use allocation planning in the city of Wuhan, China. Interval solutions associated with different risk levels of constraint violation are obtained. The desired system benefit from the land-use system will be between $ [1781.921, 2290.970]×109 under the minimum violating probabilities, and in this condition, the optimized areas of industrial land, commercial land and landfill will be [35,739, 42,402] ha, [58,572, 62,450] ha, and [903, 1087] ha. Results provide the decision makers of Wuhan with desired land-use allocation patterns and environmental policies, which are related to a variety of trade-offs between system benefit and constraint-violation risk. Willingness to accept low benefit from land-use system will guarantee meeting the environmental protection objective. A strong desire to acquire high system benefit will run into the risk of violating environmental constraint. © The Author(s) 2015."
"10.1177/0265813515624685","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028619116&doi=10.1177%2f0265813515624685&partnerID=40&md5=c6841ec4e5d4a53f9c59a47dde9447f7","In this study, we empirically model the interactions between 2D and 3D geospatial information and both daytime and nighttime urban heat islands, and estimate the relative importance of various urban heat islands drivers. While previous studies have explored the relationship between the urban heat islands and 2D urban features, the interactions with 3D urban features and neighboring surface characteristics have not been adequately explored. This paper specifies the impacts of these urban features on the urban heat islands intensity during daytime and nighttime, which tend to be quite different. The empirical evidence from this study suggests that while vegetation is the dominant factor for urban heat islands intensity during daytime, the urban canyon has stronger impacts on the urban heat islands than vegetation at night. In addition adjacent surfaces are more likely to influence nighttime surface temperatures. These results could be used to develop urban design solutions for mitigating the urban heat islands. © The Author(s) 2015."
"10.1177/0265813516647061","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028617756&doi=10.1177%2f0265813516647061&partnerID=40&md5=b21cac265271bab62b53e94a65314149","In this study, we present the potential of the conjunction between space syntax and Q-analysis methodologies for investigating patterns of movement flows in buildings. This potential is illustrated by an implementation of the two methodologies by comparing between two shopping malls in Israel, which are characterized by distinct spatial configurations. Q-analysis was used to identify the spread of individuals’ movement paths and their conjunction into movement areas, while space syntax enabled to examine how these aspects of movement flow are affected by the malls’ spatial configuration. The results indicate that high spatial integration and intelligibility levels not only promote intensive movement in accessible and central areas (as found in previous studies) but also the spread of movement paths and their formation to integrated circulation systems. The combined implementation of space syntax and Q-analysis improves our understanding about the role of building’ spatial configuration in shaping movement flows. We discuss the potential of this implementation for monitoring and improving movement flow and usability of spaces in buildings. © The Author(s) 2016."
"10.1177/0265813515624687","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028617671&doi=10.1177%2f0265813515624687&partnerID=40&md5=abbd2238930e81790abe2020c388e8bf","The present study aims at exploring whether aspects of urban form (compactness ratio and elongation ratio) are associated with urban smog (particulate matter) in China. Quantitative indicators relating to urban form and urban smog were selected and quantified for 30 Chinese cities, for the reference years 2000, 2007, and 2010, by using a combination of compiled statistical data, remote sensing, and geographical information system data. Panel data analysis was used to evaluate the degree of association between measures of urban form and urban smog, while controlling for urban population, built-up area green coverage rate, power consumption, SO2 emissions, gross value of industrial output, gross industrial output, and buses per capita. The results indicate that urban compactness and urban elongation were positively correlated to urban particulate matter. It is therefore recommended to consider the implication of urban form on smog as part of urban planning and as part of ongoing strategies to mitigate the deleterious consequences of air pollution. © The Author(s) 2016."
"10.1177/0265813515610337","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028617111&doi=10.1177%2f0265813515610337&partnerID=40&md5=baf37514570ee0b95a36e66ac2e39c9f","The emergence of web technology creates tremendous opportunity to advance good government, through increased information, interaction with the public, and cost-effective, efficient means of conducting public transactions. Some have proposed that e-government tools have two major roles: (1) receptive and (2) interactive. We argue that there is a third role for planning and e-government technology - the transactive. To evaluate this, we survey public sector planning officials on their professional use of mobile technology. Results confirm the recent trends in increased smartphone use, but indicated that only a limited amount of this increased use is for work purposes. We find that there are still planners who not only do not rely on web technologies at all, within even desktop computer access. Furthermore, our results suggest that the current use of mobile technology appears to be less transactive than the literature suggests. This finding provides room for growth in these transactive and more dynamic exchanges, especially with the increasing prevalence of mobile devices. To assist in this, we provide a taxonomy to help define how mobile technology can change planning and local governance. Such a tool can provide a roadmap for increasing transactive exchanges between local government and citizens in the future. © The Author(s) 2015."
"10.1177/0265813515611417","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028615600&doi=10.1177%2f0265813515611417&partnerID=40&md5=6a21cb2ab4c97e3d10e7e728d76e3b5c","This paper discusses the significance of biomimicry as a design methodology within the context of urban infrastructure planning and design. The application of biomimicry principles to urban infrastructure problems is examined by analysing case studies that used biomimicry inspired designs rather than ‘mainstream’ infrastructure approaches. Biomimicry is presented as an ontology of the city that fosters innovative and collaborative urban infrastructure design and management, supplements dominant future city paradigms like the ‘smart’ city and is worthy of further, detailed study. © The Author(s) 2015."
"10.1177/0265813515608640","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028614002&doi=10.1177%2f0265813515608640&partnerID=40&md5=676afca936c291d106906680718fa229","Urban policy has increasingly emphasized the compact city and higher density urban forms in reaching sustainability goals.Although environmental and economic advantages of densification have been empirically supported, the relation between higher density environments and social sustainability has been more contentious. Concerns have been raised regarding the social outcomes of high-density urban contexts; however, these connections have neither beenwell explored nor understood. Using the city ofAmsterdam, considered a case of high-quality compact city form, our study looks at how specific neighbourhood built form relates to key measures of sustainability of community. Despite previous concerns regarding the effects of density, the study reveals that higher densities have no significant impact on local social capital, sense of community or resident satisfaction. Rather, other built-form measures such as scale, existence of local stores, degree of automobile dominance and construction period were of greater importance. The study of high-quality urban environments in Amsterdam challenges notions that higher densities are detrimental to social and community experience and proposes that the specific urban form of higher density neighbourhoods is of greater importance than absolute density. © The Author(s) 2015."
"10.1177/0265813516638340","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028610710&doi=10.1177%2f0265813516638340&partnerID=40&md5=bca235e47207c8cd65b35d70e81c50f2","We have assembled CO2 emission figures from collections of urban GHG emission estimates published in peer-reviewed journals or reports from research institutes and non-governmental organizations. Analyzing the scaling with population size, we find that the exponent is development dependent with a transition from super- to sub-linear scaling. From the climate change mitigation point of view, the results suggest that urbanization is desirable in developed countries. Further, we compare this analysis with a second scaling relation, namely the fundamental allometry between city population and area, and propose that density might be a decisive quantity too. Last, we derive the theoretical country-wide urban emissions by integration and obtain a dependence on the size of the largest city. © The Author(s) 2016."
"10.1177/0265813516684136","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028610689&doi=10.1177%2f0265813516684136&partnerID=40&md5=d4dc8edbf00ecd913c837c977cf41048","We propose a GIS-based method to enable the understanding of how global street-network properties emerge from the temporal accumulation of individual street-network increments. The method entails the adoption of quantitative descriptions of individual street-patterns and of classification algorithms, in order to obtain numerically defined typomorphologies, which may then be statistically associated with the numerical outputs of street-network analysis. We apply the method to the case of Oporto Metropolitan Area, whose development we observed over 60 years. We isolate each increment of development entailing the creation of new streets (4208 objects), we quantify the morphology of their street-layouts, and we classify them into typomorphologies with clustering techniques. Through the investigation of the temporal and spatial frequencies of those typomorphologies, we assess their impacts on the street-networks of a set of selected civil-parishes of the metropolitan region, demonstrating that different typomorphological frequencies result in also different global street-network properties. We conclude by summarising the advantages of the method to generic urban morphological research and by suggesting that it may also contribute to inform bottom-up metropolitan spatial planning. © The Author(s) 2016."
"10.1177/0265813516637606","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028608229&doi=10.1177%2f0265813516637606&partnerID=40&md5=38aabc63e74cbcf6f1c1ddf7548fc958","China’s economic reforms of 1978, which led to the country’s transition from a centrally planned to a market-oriented economy, ushered in a phase of accelerated urbanization. Influenced by the economic transition and taking advantage of its privileged geographic and historic position, Tianjin has seen dramatic changes in its social landscape during the last three decades. Given this context, this study aims at understanding the different urban socio-spatial patterns of Tianjin and their mechanisms in three distinctive economic contexts by adapting both statistical and spatial approaches. Due to increasing population mobility caused by the economic reforms, the urban social landscape of Tianjin has become increasingly multifaceted, characterized by a '‘one axis, two nuclei’' urban morphology. The rise of the Binhai New Area (TBNA) in the southeast is creating a dual-core urban social structure in Tianjin, with its traditional Urban Core located in the center of the city. In terms of the Urban Core’s expansion and population movements southeast toward the TBNA, an asymmetric suburbanization process is evident in Tianjin. Meanwhile, an additional population shift toward Beijing in the northwest is significant during 2000-2010, illustrating the changing relationship between these two neighboring municipalities. By integrating itself with Beijing, Tianjin has not only recovered from under Beijing’s shadow during the centrally planned economy period, but is also benefitting from Beijing in order to flourish. © The Author(s) 2016."
"10.1177/0265813516685565","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028606941&doi=10.1177%2f0265813516685565&partnerID=40&md5=201e26b114bb7df7151f24c6d026714d","In light of the urgent threats presented by climate change and rapid urbanisation, interest in ‘smart city systems’ is mounting. In contrast to scholarship that poses ‘smartness’ as something that needs to be added to cities, recent developments in spatial morphology research pursue a view of the built fabric of cities as an extension of the cognitive human apparatus, as well as a material formulation of social, cultural and economic relations and processes. The built fabric of cities needs to be understood as a highly intelligent artefact in itself, rather than simple, dead matter. The current focus on high-tech systems risks concealing the fact that the machine is already there. In contrast to the technological ‘implements’ of smart city systems, this article looks at cities as ‘facilities’ - that is, as technologies that slow down, store and maintain energy as a resource for a variety of purposes. The article builds on space syntax research in order to give precision to the understanding of the affordances the cities offer their various processes and the ways in which cities operate as information storage and retrieval devices for individuals and for society. The city must be considered, we argue, in terms of a range of tangled, interdependent systems, reaching from individual buildings to the whole city, an understanding anchored in notions of ‘diversity’ and ‘density’ (recently gathered under the concept of ‘spatial capital’) and in research addressing how the distribution of space and artefacts serve as means of knowledge communication (specifically in complex buildings such as libraries and department stores). In conclusion, we argue that existing discussions on ‘smart city systems’ would benefit acknowledgement of the role of cities as facilities. © The Author(s) 2016."
"10.1177/0265813516652898","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028605123&doi=10.1177%2f0265813516652898&partnerID=40&md5=0c4bb5257cf6f7ad3c75dbe2f58cbcf1","This article describes the development of a new three-dimensional model of the British building stock, called '3DStock’. The model differs from other 3D urban and stock models, in that it represents explicitly and in detail the spatial relationships between ‘premises’ and ‘buildings’. It also represents the pattern of activities on different floors within buildings. The geometrical/ geographical structure of the model is assembled automatically from two existing national data sets. Additional data from other sources including figures for electricity and gas consumption are then attached. Some sample results are given for energy use intensities. The first purpose of the model is in the analysis of energy use in the building stock. With actual energy data for very large numbers of premises, it is possible to take a completely new type of statistical approach, in which consumption can be related to a range of characteristics including activity, built form, construction and materials. Models have been built to date of the London Borough of Camden and the cities of Leicester, Tamworth and Swindon. Work is in progress to extend the modelling to other parts of Britain. Because of the coverage of the data, this will be limited however to England and Wales. © The Author(s) 2016."
"10.1177/0265813515608849","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028604711&doi=10.1177%2f0265813515608849&partnerID=40&md5=0423915dd49e9e880bad83fbd982f4f7","Research on the fringe-belt concept has grown significantly in the past decade. This is particularly evident in parts of the world in which interest in urban morphology has been slight until recently. The main emphasis continues to be the light that this concept can shed on the historicogeographical grain of urban areas. This paper reports a morphogenetic investigation into fringe belts that gives particular attention to the fixation lines associated with Chinese city walls. Discussion is concentrated on a fringe belt related to one of the world’s longest and most massive city walls, that of Nanjing. The formation, consolidation and, in places, alienation of the Ming fringe belt of Nanjing has been influenced by natural and artificial fixation lines and the political economy of an authoritarian society. Understanding the changing spatial structure of fringe-belt landscapes has implications for the management of urban form in ways sensitive to its historico-geographical development. © The Author(s) 2015."
"10.1177/0265813515611420","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028600843&doi=10.1177%2f0265813515611420&partnerID=40&md5=729c135483a86311c7a8cd82e782bae7","One of the most common relativization techniques in life sciences quantifies body condition based on residuals from the linear regression in the log-log plot of body mass against a linear measure of size. Given the network-based analogy between organisms and cities, the method is applied to comparative urban studies to formulate relativized allometric measures based on the allometry to size of metric and topological measures of street networks. The analysis of a sample of 70 cities from a confined region considered in three historical stages demonstrates that the more allometric scaling of a measure to size diverges from the linear, the more allometric measures show discrepancy to the existing relativization methods that are based on mean measures and ratios between measures. Allometric measures reflect the dynamics of specific regional samples of cities and therefore also differ from relativization methods that relate measures that grow exponentially with size against static theoretical yardsticks. The comparison involving two additional samples of cities from other regions suggests that the proposed allometric measures can be used to approximate size-invariant measures for cities with unknown allometry more reliably than existing relativization measures. The method can be applied to formulate relativized indices for any measure that displays allometry to size in various scales of the built environment. © The Author(s) 2015."
"10.1177/0265813516647062","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028599137&doi=10.1177%2f0265813516647062&partnerID=40&md5=59ed2cc36a5e1ec9ef94b0a7600d1319","It is an expensive and time-consuming task to develop a new model. Furthermore, a single model often cannot provide answers required for complex decision making based on multiple criteria. Coupling models are often applied to make use of existing models and analyze complex policy questions. This paper provides an overview of possible model integration approaches, briefly explains the modules that were integrated in a particular application, and focuses on the integration methods applied in this research. While the initial attempt was to integrate all models as tightly as possible, the authors developed a much more agile integration approach that allows adding and replacing individual modules easily. Python wrappers were developed to loosely couple land use, land cover, transportation, and emission models developed in different environments. ArcGIS Model Builder was used to provide a graphical user interface and to present the models’ workflow. The suggested approach is especially efficient when the models are developed in different programming languages, their source codes are not available, or the licensing restrictions make other coupling approaches impractical. © The Author(s) 2016."
"10.1177/0265813516638185","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028593746&doi=10.1177%2f0265813516638185&partnerID=40&md5=059546f5112fa777aff710825ef06b1a","The diurnal movements of pedestrians in the built environment are sometimes typified as a ‘street ballet’, where each actor or dancer has their own set role within a larger complex. Every individual in the ballet may have many influences on their behaviour including the physical layout of the environment, cognitive strategies to navigate it, experiential or affective preferences as well as social, economic and political factors, but ultimately each one seems to obey apparently choreographed actions. The aim of this article is to understand whether or not there is in fact an underlying choreography to the ballet, in that certain steps or moves are more likely than others such that a ‘dance’ through daily life is constructed. To do so, simple automata that use active perception to inhabit the world are evolved against different tasks within the environment representing different sets of moves that may be taken. It is shown that any evolved automaton appears to embody a mathematical person-space relationship that joins visual affordance with motor action: the convergence of a simple Markov model of visual movement. From the Markov model, a general model of embodied action in the environment is proposed, whereby memory of the dance is ingrained over evolutionary history, such that it forms building blocks for non-discursive action within the built environment and comprises a possible common phenomenological framework. © The Author(s) 2016."
"10.1177/0265813515607474","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028591450&doi=10.1177%2f0265813515607474&partnerID=40&md5=93c477ec21b9e10f3a3d95ebf78a30e7","In 1985, San Francisco adopted a downtown plan on ground-level wind currents intended to mitigate the negative effects of wind on pedestrians’ perceived comfort in public open spaces. The plan mandates that new buildings in designated parts of the city associated with high density or development potential be designed or adopt measures to not cause wind in excess of accepted comfort levels. This study examines whether and to what degree the plan has successfully shaped an urban formthatmitigates wind by comparing the ground-level wind environment in 1985 and 2013. A series of wind tunnel tests found that during San Francisco’s windiest season when the westerly winds are prevalent, the overall mean wind speed ratio measured at 318 locations in four areas of the city dropped by 22%. However, there still exist many excessively windy places that are associated with specific urban form conditions, including streets oriented to have direct exposure to westerly winds, flat façades on high-rise buildings, and horizontal street walls where building façades align. Recommendations based on the findings include incorporating more tangible guidance on the built form conditions, expanding the plan’s reach to cover more parts of the city, and learning from strategies used elsewhere. By evaluating the urban form impacts of a wind mitigation policy that has been in place for 30 years, the research offers insights for other cities that have implemented or plan to adopt similar approach and sheds light on issues related to wind comfort in high-density urban areas. © The Author(s) 2015."
"10.1177/0265813516647733","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028586040&doi=10.1177%2f0265813516647733&partnerID=40&md5=1afe87cf25fc64b651ecd3d192097c3f","To improve the acoustic environment of residential blocks, noise mapping is employed in this study to analyze traffic noise distribution and the influence factors of four types of residential blocks in China. The study shows that high-rise small blocks have the highest average noise level (Lavg) for ground and building facades, followed by small low-rise blocks while modern residential blocks yield the lowest value. An analysis of the standard deviation (STD) of spatial statistical noise level (Ln) shows that the STD of the ground and building façade of two types of small blocks is higher than that of other blocks. The analysis of influence factors indicates that the lot area of residential block has significant negative correlation with ground and building facade average noise level (Lavg), and street coverage ratio (SCR) has significant positive correlation with ground and building facade average noise level (Lavg). In low-rise and high-rise small blocks, ground space index (GSI) has significant negative correlation with ground and building facade average noise level (Lavg); street interface density (SID) has significant positive correlation with the STDs of ground and building facade noise. Floor space index (FSI) shows significant positive correlation with the STDs of ground and building facade noise in low-rise small blocks. © The Author(s) 2016."
"10.1177/0265813516638849","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028583707&doi=10.1177%2f0265813516638849&partnerID=40&md5=07bbd20a615b81fa3ff45e0c55a14e63","By studying the mathematical properties of metrics, we identify three fundamental characteristics of distance, which are optimality, detour and break. In this paper, we explore the implications of these properties for transport planning, urbanism and spatial planning. We state that distances contain the idea of optimum and that any distance is associated to a search for optimisation. Pedestrian movements obey this principle and sometimes depart from designed routes. Local suboptimality conveyed by public transport maps has to be corrected by interventions on public space to relieve the load on central parts of networks. The second principle we state is that detour in distances is most often a means to optimise movement. Fast transport systems generate most of the detour observed in geographical spaces at regional scale. This is why detour has to be taken into account in regional transport policies. The third statement is that breaks in movement contribute to optimising distances. Benches, cafés, pieces of art, railway stations are examples of the urban break. These facilities of break represent an urban paradox: they organise the possibility of a break, of a waste of time in a trip, and they also contribute to optimising distances in a wider network. In that sense, break should be considered as a relevant principle for the design of urban space in order to support a pedestrian-oriented urban form. © The Author(s) 2016."
"10.1177/0265813515599982","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028583513&doi=10.1177%2f0265813515599982&partnerID=40&md5=2cd109452bb1cd3ce2d03de66b99789b","The street network is an important aspect of cities and contains crucial information about their organization and evolution. Characterizing and comparing various street networks could then be helpful for a better understanding of the mechanisms governing the formation and evolution of these systems. Their characterization is however not easy: there are no simple tools to classify planar networks and most of the measures developed for complex networks are not useful when space is relevant. Here, we describe recent efforts in this direction and new methods adapted to spatial networks. We will first discuss measures based on the structure of shortest paths, among which the betweenness centrality. In particular for time-evolving road networks, we will show that the spatial distribution of the betweenness centrality is able to reveal the impact of important structural transformations. Shortest paths are however not the only relevant ones. In particular they can be very different from those with the smallest number of turns-the simplest paths. The statistical comparison of the lengths of the shortest and simplest paths provides a nontrivial and nonlocal information about the spatial organization of planar graphs. We define the simplicity index as the average ratio of these lengths and the simplicity profile characterizes the simplicity at different scales. Measuring these quantities on artificial (roads, highways, railways) and natural networks (leaves, insect wings) show that there are fundamental differences-probably related to their different function-in the organization of urban and biological systems: there is a clear hierarchy of the lengths of straight lines in biological cases, but they are randomly distributed in urban systems. The paths are however not enough to fully characterize the spatial pattern of planar networks such as streets and roads. Another promising direction is to analyze the statistics of blocks of the planar network. More precisely, we can use the conditional probability distribution of the shape factor of blocks with a given area, and define what could constitute the fingerprint of a city. These fingerprints can then serve as a basis for a classification of cities based on their street patterns. This method applied on more than 130 cities in the world leads to four broad families of cities characterized by different abundances of blocks of a certain area and shape. This classification will be helpful for identifying dominant mechanisms governing the formation and evolution of street patterns. © The Author(s) 2015."
"10.1177/0265813515624684","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025615770&doi=10.1177%2f0265813515624684&partnerID=40&md5=3a1c924c2f00f9c6710cd173c77bc9c1","High levels of out-of-centre foodstore developments in the 1980s and early 1990s significantly altered the commercial landscape of the UK, and were widely seen as threatening the vitality and viability of small and medium-sized centres. The progressive tightening of retail planning regulation in the decade that followed, and retailer adaptation to that tightening, resulted in the development of more flexible foodstore formats suited to in-centre or edge-of-centre sites, which worked ‘with the grain’ of the ‘town centre first’ approach to retail planning policy. Since then academic research has started to suggest a more positive role for such developments than hitherto, and to indicate that they can play an important role in anchoring small centres. The key mechanism underlining this potential positive role is that of linked trips, whereby the spatial externality generated by a foodstore development is transmitted to the existing retail structure of the centre in which development has occurred. Even though UK planning policy has consistently viewed the role of linked shopping trips as critical to town centre vitality, available evidence on this key issue remains remarkably scarce and dated in terms of the planning regulation context from which it was generated. This paper aims to fill that gap. We make use of a large and unique database on consumer shopping behaviour collected over the period August 2007-November 2009 in selected UK centres, and employ the difference-in-differences method to obtain insight into the hypothesised uplift in linked trip propensity which can be attributed to a foodstore development. Our results indicate that the development of new-generation foodstores in in-centre and edge-of-centre locations does indeed increase the propensity of shoppers to link their trips between foodstores and town centre shops/services. Controlling for shoppers’ individual characteristics, that increase is shown to be over seven percentage points. The exact numerical value is likely to be sample specific, and its typical range will only be established by replication. However, the importance of the finding is that using sophisticated but appropriate statistical methodology and a large sample of data from a transparently designed and rigorously conducted study, the development of ŉew-generation’ town-centre first foodstores is clearly associated with increased linked trip propensities. To our knowledge, this is the first time unambiguous evidence of the existence of this hypothesised ‘town centre first era’ linked-trip effect has been demonstrated. © The Author(s) 2016."
"10.1177/0265813515625641","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85023599948&doi=10.1177%2f0265813515625641&partnerID=40&md5=10ae9213ff3b52df886b118784bc25a5","This article tests the extent to which a measure of walkable access is a good proxy for the quality of the walking environment. Based on existing findings on inequalities of walkability, we ask whether this relation varies between neighborhoods with low and high incomes. Walk Score is used to measure walkable access while the State of Place Index is applied to synthesize the qualitative urban form dimensions collected as part of the Irvine Minnesota Inventory. Simple bivariate correlations and difference-in-means tests assess the relationship and difference in average scores between the two. We draw on an existing sample of 115 walkable neighborhoods in the Washington, DC metro area that Mariela Alfonzo and colleagues had collected for previous research and that we augmented to include additional low-income neighborhoods. Our results reveal a strong and positive overall association between walkable access (Walk Score) and walkability (State of Place). However, this association masks problems with the quality of the walking environment that are significantly larger in low-income neighborhoods (even those with very good walkable access), especially regarding connectivity personal safety, and the presence of litter and graffiti. As a proxy for walkability, Walk Score’s walkable access measure is, therefore, not equally strong across all neighborhoods but declines with income. In this sense, Walker’s Paradise is more walkable in higher than low-income neighborhoods. © The Author(s) 2016."
"10.1177/0265813516647063","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021694581&doi=10.1177%2f0265813516647063&partnerID=40&md5=211ffd1acd071d98de3bc2f0d303f4a3","In his original 1965 article, Christopher Alexander argued that master planned cities ultimately failed because the designs elaborated followed a tree structure as opposed to a more desirable semilattice structure present in organic cities. In this article, I argue that a similar claim can be made with urban infrastructure systems planning. As cities expanded and became increasingly complex in the 20th century, the responsibility to plan and design urban infrastructure was distributed to separate agencies that seldom communicate and coordinate with one another. In the global context to make cities more sustainable and resilient, a better integration of infrastructure systems may hold much potential. After recalling Alexander’s main concepts I examine how current infrastructure systems are naturally interdependent. I then discuss the role of integration, by notably proposing an integration-decentralization matrix, with four quadrants, illustrated by using practical examples. The quadrants are current paradigm, siloed distribution, localized integration, and integrated decentralization. Overall, a better integration of urban infrastructure can oer significant benefits to a city, and it may be time to seriously revisit our current urban infrastructure systems planning practice. © The Author(s) 2016."
"10.1089/big.2016.0062","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85007275343&doi=10.1089%2fbig.2016.0062&partnerID=40&md5=bde3326fece2a5103678bff00e18b32f","Autonomous robots often rely on models of their sensing and actions for intelligent decision making. However, when operating in unconstrained environments, the complexity of the world makes it infeasible to create models that are accurate in every situation. This article addresses the problem of using potentially large and high-dimensional sets of robot execution data to detect situations in which a robot model is inaccurate - that is, detecting context-dependent model inaccuracies in a high-dimensional context space. To find inaccuracies tractably, the robot conducts an informed search through low-dimensional projections of execution data to find parametric Regions of Inaccurate Modeling (RIMs). Empirical evidence from two robot domains shows that this approach significantly enhances the detection power of existing RIM-detection algorithms in high-dimensional spaces. © 2016, Mary Ann Liebert, Inc."
"10.1089/big.2016.0028","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85007273826&doi=10.1089%2fbig.2016.0028&partnerID=40&md5=ab979a8e5091112e9907b35db70ceb9f","Linking human motion and natural language is of great interest for the generation of semantic representations of human activities as well as for the generation of robot activities based on natural language input. However, although there have been years of research in this area, no standardized and openly available data set exists to support the development and evaluation of such systems. We, therefore, propose the Karlsruhe Institute of Technology (KIT) Motion-Language Dataset, which is large, open, and extensible. We aggregate data from multiple motion capture databases and include them in our data set using a unified representation that is independent of the capture system or marker set, making it easy to work with the data regardless of its origin. To obtain motion annotations in natural language, we apply a crowd-sourcing approach and a web-based tool that was specifically build for this purpose, the Motion Annotation Tool. We thoroughly document the annotation process itself and discuss gamification methods that we used to keep annotators motivated. We further propose a novel method, perplexity-based selection, which systematically selects motions for further annotation that are either under-represented in our data set or that have erroneous annotations. We show that our method mitigates the two aforementioned problems and ensures a systematic annotation process. We provide an in-depth analysis of the structure and contents of our resulting data set, which, as of October 10, 2016, contains 3911 motions with a total duration of 11.23 hours and 6278 annotations in natural language that contain 52,903 words. We believe this makes our data set an excellent choice that enables more transparent and comparable research in this important area. © 2016, Mary Ann Liebert, Inc."
"10.1089/big.2016.0038","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85007256312&doi=10.1089%2fbig.2016.0038&partnerID=40&md5=b00c9fbca0b7400b80478a254aabc233","This work seeks to leverage semantic networks containing millions of entries encoding assertions of commonsense knowledge to enable improvements in robot task execution and learning. The specific application we explore in this project is object substitution in the context of task adaptation. Humans easily adapt their plans to compensate for missing items in day-to-day tasks, substituting a wrap for bread when making a sandwich, or stirring pasta with a fork when out of spoons. Robot plan execution, however, is far less robust, with missing objects typically leading to failure if the robot is not aware of alternatives. In this article, we contribute a context-aware algorithm that leverages the linguistic information embedded in the task description to identify candidate substitution objects without reliance on explicit object affordance information. Specifically, we show that the task context provided by the task labels within the action structure of a task plan can be leveraged to disambiguate information within a noisy large-scale semantic network containing hundreds of potential object candidates to identify successful object substitutions with high accuracy. We present two extensive evaluations of our work on both abstract and real-world robot tasks, showing that the substitutions made by our system are valid, accepted by users, and lead to a statistically significant reduction in robot learning time. In addition, we report the outcomes of testing our approach with a large number of crowd workers interacting with a robot in real time. © 2016, Mary Ann Liebert, Inc."
"10.1089/big.2016.0041","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85007174620&doi=10.1089%2fbig.2016.0041&partnerID=40&md5=896661a7084d146bf092722b2b596add","It has long been hoped that model-based control will improve tracking performance while maintaining or increasing compliance. This hope hinges on having or being able to estimate an accurate inverse dynamics model. As a result, substantial effort has gone into modeling and estimating dynamics (error) models. Most recent research has focused on learning the true inverse dynamics using data points mapping observed accelerations to the torques used to generate them. Unfortunately, if the initial tracking error is bad, such learning processes may train substantially off-distribution to predict well on actual observed acceleration rather than the desired accelerations. This work takes a different approach. We define a class of gradient-based online learning algorithms we term Direct Online Optimization of Modeling Errors in Dynamics (DOOMED) that directly minimize an objective measuring the divergence between actual and desired accelerations. Our objective is defined in terms of the true system's unknown dynamics and is therefore impossible to evaluate. However, we show that its gradient is observable online from system data. We develop a novel adaptive control approach based on running online learning to directly correct (inverse) dynamics errors in real time using the data stream from the robot to accurately achieve desired accelerations during execution. © 2016, Mary Ann Liebert, Inc."
"10.1016/j.bdr.2016.08.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84999029523&doi=10.1016%2fj.bdr.2016.08.002&partnerID=40&md5=18a1a5ddc2c06fb6338c591cd84bf1b5","Entity Resolution constitutes a quadratic task that typically scales to large entity collections through blocking. The resulting blocks can be restructured by Meta-blocking to raise precision at a limited cost in recall. At the core of this procedure lies the blocking graph, where the nodes correspond to entities and the edges connect the comparable pairs. There are several configurations for Meta-blocking, but no hints on best practices. In general, the node-centric approaches are more robust and suitable for a series of applications, but suffer from low precision, due to the large number of unnecessary comparisons they retain. In this work, we present three novel methods for node-centric Meta-blocking that significantly improve precision. We also introduce a pre-processing method that restricts the size of the blocking graph by removing a large number of noisy edges. As a result, it reduces the overhead time of Meta-blocking by 2 to 5 times, while increasing precision by up to an order of magnitude for a minor cost in recall. The same technique can be applied as graph-free Meta-blocking, enabling for the first time Entity Resolution over very large datasets even on commodity hardware. We evaluate our approaches through an extensive experimental study over 19 voluminous, established datasets. The outcomes indicate best practices for the configuration of Meta-blocking and verify that our techniques reduce the resolution time of state-of-the-art methods by up to an order of magnitude. © 2016 Elsevier Inc."
"10.1016/j.bdr.2016.07.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994129965&doi=10.1016%2fj.bdr.2016.07.001&partnerID=40&md5=ea42fa7ef10c8c6e5439510aa779f384","High utility itemset mining discovers itemsets whose utility is above a given threshold, where the utility measures the importance of an itemset. It overcomes the limitation of frequent pattern mining, which uses frequency as its quality measure. To speed up the performance for mining high utility itemsets, many algorithms have been proposed which usually focus on optimizing the candidate generation process. However, memory and time performance limitations still cause scalability issues, especially when the dataset is very large. In this paper, the problem is addressed by proposing a distributed parallel algorithm, PHUI-Miner, and a sampling strategy, which can be used either separately or simultaneously. PHUI-Miner parallelizes the state-of-the-art high utility itemset mining algorithm HUI-Miner. In PHUI-Miner, the search space of the high utility itemset mining problem is divided and assigned to nodes in a cluster, which splits the workload. The sampling strategy investigates the required sample size of a dataset, in order to achieve a given accuracy. The sample size is selected based on a new theorem, which provides a theoretical guarantee on the accuracy of results. We also propose an approach combining sampling with PHUI-Miner, which mines an approximate set of results, but could provide better time performance. In our experiments, we show that PHUI-Miner has high performance on different datasets and outperforms the state-of-the-art non-parallel algorithm HUI-Miner. The sampling strategy achieves accuracies much higher than the guarantee provided by the theorems in practice. Extensive experiments are also conducted to compare the time performance of PHUI-Miner with and without sampling. © 2016 Elsevier Inc."
"10.1016/j.bdr.2016.04.003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84979690318&doi=10.1016%2fj.bdr.2016.04.003&partnerID=40&md5=0829dd1ef5cab87408bbc860890e26b4","Big data is among the most promising research trends of the decade, drawing attention from every segment of the market and society. This paper provides the scientific community with a comprehensive overview of the applications of a data processing platform designed to harness the potential of big data in the field of road transport policies in Europe. This platform relies on datasets of driving and mobility patterns collected by means of navigation systems. Two datasets from conventional fuel vehicles collected with on-board GPS systems have been used to perform an initial pilot study and develop its core algorithms. They consist of 4.5 million trips and parking events recorded by monitoring 28,000 vehicles over one month. The presented analyses address: (1) large-scale mobility statistics, (2) potential of electric vehicles in replacing conventional fuel vehicles and related modal shift, (3) energy demand coming from electric vehicles, (4) smart design of the recharge infrastructure and Vehicle-to-Grid, and (5) real-world driving and evaporative emissions assessment and mapping. The developed methodology and the presented outcomes demonstrate the potential of big data for policy assessment and better governance, focusing on the challenges and on the huge opportunities offered for future developments. This paper ultimately aims to show how big data can inspire smart policies together with public and private investments to enable the large scale deployment of the next generation of green vehicles, offering an unprecedented opportunity to shape policies for future mobility and smart cities. © 2016 The Authors"
"10.1140/epjds/s13688-016-0093-1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013885215&doi=10.1140%2fepjds%2fs13688-016-0093-1&partnerID=40&md5=a5594e11c8765cbc3caf352b39600e52","Advances in computing power, natural language processing, and digitization of text now make it possible to study a culture’s evolution through its texts using a ‘big data’ lens. Our ability to communicate relies in part upon a shared emotional experience, with stories often following distinct emotional trajectories and forming patterns that are meaningful to us. Here, by classifying the emotional arcs for a filtered subset of 1,327 stories from Project Gutenberg’s fiction collection, we find a set of six core emotional arcs which form the essential building blocks of complex emotional trajectories. We strengthen our findings by separately applying matrix decomposition, supervised learning, and unsupervised learning. For each of these six core emotional arcs, we examine the closest characteristic stories in publication today and find that particular emotional arcs enjoy greater success, as measured by downloads. © 2016, Reagan et al."
"10.1140/epjds/s13688-016-0095-z","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85004010210&doi=10.1140%2fepjds%2fs13688-016-0095-z&partnerID=40&md5=3e2e1b646d34df8c13096996189c0f14","In recent times, a phenomenon that threatens the representative democracy of many developed countries is the low voter turnout. Voting Advice Applications (VAAs) are used to inform citizens about the political stances of the parties that involved in the upcoming elections, in an effort to facilitate their decision making process and increase their participation in this democratic process. VAA is a Web application that calls the users and parties to state their position in a set of policy statements, which are based on the current affairs of their country and then it recommends to each user the party that better fits their political views. SVAAs, a social recommendation approach of VAAs, on the other hand, base their recommendation on the VAA community of users in a collaborative filtering manner. In this paper we resort to Hidden Markov Models (HMMs) in an attempt to improve the effectiveness of SVAAs. In particular, we try to model party-supporters using HMMs and then use these models to recommend each VAA user the party whose model best fits his/her answer sequence of the VAA policy statements. HMMs proved to be effective machine learning tools for sequential and correlated data and this is the main rationale behind this study. VAA policy statements are usually correlated and grouped into categories such as external policy, economy, society, etc. As a result, opting from the various answer choices in each policy statement might be related with selections in previous and subsequent policy statements. Given that the order of policy statements is kept fixed within each VAA one can assume that (a) answer patterns (sequences of choices for all policy statements included in the VAA) can be found that characterise ‘typical’ voters of particular parties, and (b) the answer choice in each policy statement can be ‘predicted’ from previous answer choices. For our experiments we use three datasets based on the 2014 elections to the European Parliament (http://www.euvox2014.eu/), which are publicly available through the Preference Matcher website (http://www.preferencematcher.org/?page_id=18). © 2016, The Author(s)."
"10.1140/epjds/s13688-016-0096-y","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84997079172&doi=10.1140%2fepjds%2fs13688-016-0096-y&partnerID=40&md5=d65bcc553e2310564a93f7d8201fdbad","Many complex phenomena, from trait selection in biological systems to hierarchy formation in social and economic entities, show signs of competition and heterogeneous performance in the temporal evolution of their components, which may eventually lead to stratified structures such as the worldwide wealth distribution. However, it is still unclear whether the road to hierarchical complexity is determined by the particularities of each phenomena, or if there are generic mechanisms of stratification common to many systems. Human sports and games, with their (varied but simple) rules of competition and measures of performance, serve as an ideal test-bed to look for universal features of hierarchy formation. With this goal in mind, we analyse here the behaviour of performance rankings over time of players and teams for several sports and games, and find statistical regularities in the dynamics of ranks. Specifically the rank diversity, a measure of the number of elements occupying a given rank over a length of time, has the same functional form in sports and games as in languages, another system where competition is determined by the use or disuse of grammatical structures. We use a Gaussian random walk model to reproduce the rank diversity of the studied sports and games. We also discuss the relation between rank diversity and the cumulative rank distribution. Our results support the notion that hierarchical phenomena may be driven by the same underlying mechanisms of rank formation, regardless of the nature of their components. Moreover, such regularities can in principle be used to predict lifetimes of rank occupancy, thus increasing our ability to forecast stratification in the presence of competition. © 2016, Morales et al."
"10.1140/epjds/s13688-016-0094-0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994708648&doi=10.1140%2fepjds%2fs13688-016-0094-0&partnerID=40&md5=14436a662ca196f0326c16c6e9214658","Data on the number of people who have committed suicide tends to be reported with a substantial time lag of around two years. We examine whether online activity measured by Google searches can help us improve estimates of the number of suicide occurrences in England before official figures are released. Specifically, we analyse how data on the number of Google searches for the terms ‘depression’ and ‘suicide’ relate to the number of suicides between 2004 and 2013. We find that estimates drawing on Google data are significantly better than estimates using previous suicide data alone. We show that a greater number of searches for the term ‘depression’ is related to fewer suicides, whereas a greater number of searches for the term ‘suicide’ is related to more suicides. Data on suicide related search behaviour can be used to improve current estimates of the number of suicide occurrences. © 2016, Kristoufek et al."
"10.1140/epjds/s13688-016-0092-2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84992147641&doi=10.1140%2fepjds%2fs13688-016-0092-2&partnerID=40&md5=dfa122c1983854948200bdeea8fff828","Predicting human mobility flows at different spatial scales is challenged by the heterogeneity of individual trajectories and the multi-scale nature of transportation networks. As vast amounts of digital traces of human behaviour become available, an opportunity arises to improve mobility models by integrating into them proxy data on mobility collected by a variety of digital platforms and location-aware services. Here we propose a hybrid model of human mobility that integrates a large-scale publicly available dataset from a popular photo-sharing system with the classical gravity model, under a stacked regression procedure. We validate the performance and generalizability of our approach using two ground-truth datasets on air travel and daily commuting in the United States: using two different cross-validation schemes we show that the hybrid model affords enhanced mobility prediction at both spatial scales. © 2016, Beiró et al."
"10.1140/epjds/s13688-016-0091-3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84986305903&doi=10.1140%2fepjds%2fs13688-016-0091-3&partnerID=40&md5=1b2253f7ff2339572cf06470b94291b7","Macroeconomic theories of growth and wealth distribution have an outsized influence on national and international social and economic policies. Yet, due to a relative lack of reliable, system wide data, many such theories remain, at best, unvalidated and, at worst, misleading. In this paper, we introduce a novel economic observatory and framework enabling high resolution comparisons and assessments of the distributional impact of economic development through the remote sensing of planet Earth’s surface. Striking visual and empirical validation is observed for a broad, global macroeconomic σ-convergence in the period immediately following the end of the Cold War. What is more, we observe strong empirical evidence that the mechanisms driving σ-convergence failed immediately after the financial crisis and the start of the Great Recession. Nevertheless, analysis of both cross-country and cross-state samples indicates that, globally, disproportionately high growth levels and excessively high decay levels have become rarer over time. We also see that urban areas, especially concentrated within short distances of major capital cities were more likely than rural or suburban areas to see relatively high growth in the aftermath of the financial crisis. Observed changes in growth polarity can be attributed plausibly to post-crisis government intervention and subsidy policies introduced around the world. Overall, the data and techniques we present here make economic evidence for the rise of China, the decline of US manufacturing, the euro crisis, the Arab Spring, and various, recent, Middle East conflicts visually evident for the first time. © 2016, Duede and Zhorin."
"10.1140/epjds/s13688-016-0089-x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84983616251&doi=10.1140%2fepjds%2fs13688-016-0089-x&partnerID=40&md5=5c4290fb0d819f160fb0e3823178853c","Organizations, irrespective of their size and type, are increasingly becoming data-driven or aspire to become data-driven. There is a rush to quantify value of their own internal data or the value of integrating their internal data with external data, and performing modeling on such data. A question that analytics teams often grapple with is whether to acquire more data or expend additional effort on more complex modeling, or both. If these decisions can be quantified a priori, it can be used to guide budget and investment decisions. To that end, we quantify the Net Present Value (NPV) of the tasks of additional data acquisition or more complex modeling, which are critical to the data science process. We develop a framework, NPVModel, for a comparative analysis of various external data acquisition and in-house model development scenarios using NPVs of costs and returns as a measure of feasibility. We then demonstrate the effectiveness of NPVModel in prescribing strategies for various scenarios. Our framework not only acts as a suggestion engine, but it also provides valuable insights into budgeting and roadmap planning for Big Data ventures. © 2016, Nagrecha and Chawla."
"10.1140/epjds/s13688-016-0088-y","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84983526975&doi=10.1140%2fepjds%2fs13688-016-0088-y&partnerID=40&md5=db8477885562ad387841a9e3d91682fe","This paper introduces a data-driven methodology to study the historical evolution of mathematical thinking and its spatial spreading. To do so, we have collected and integrated data from different online academic datasets. In its final form, the database includes a large number (N∼ 200 K) of advisor-student relationships, with affiliations and keywords on their research topic, over several centuries, from the 14th century until today. We focus on two different issues, the evolving importance of countries and of the research disciplines over time. Moreover we study the database at three levels, its global statistics, the mesoscale networks connecting countries and disciplines, and the genealogical level. © 2016, Gargiulo et al."
"10.1140/epjds/s13688-016-0090-4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84983511112&doi=10.1140%2fepjds%2fs13688-016-0090-4&partnerID=40&md5=5d8a7b0a558ac354b3902e9041a831ae","Community detection techniques are widely used to infer hidden structures within interconnected systems. Despite demonstrating high accuracy on benchmarks, they reproduce the external classification for many real-world systems with a significant level of discrepancy. A widely accepted reason behind such outcome is the unavoidable loss of non-topological information (such as node attributes) encountered when the original complex system is converted to a network. In this article we systematically show that the observed discrepancies may also be caused by a different reason: the external classification itself. For this end we use scientific publication data which (i) exhibit a well defined modular structure and (ii) hold an expert-made classification of research articles. Having represented the articles and the extracted scientific concepts both as a bipartite network and as its unipartite projection, we applied modularity optimization to uncover the inner thematic structure. The resulting clusters are shown to partly reflect the author-made classification, although some significant discrepancies are observed. A detailed analysis of these discrepancies shows that they may carry essential information about the system, mainly related to the use of similar techniques and methods across different (sub)disciplines, that is otherwise omitted when only the external classification is considered. © 2016, Palchykov et al."
"10.1140/epjds/s13688-016-0086-0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84979753535&doi=10.1140%2fepjds%2fs13688-016-0086-0&partnerID=40&md5=18f3a42eb0f6f2e5471cb283828bbe6a","Large scale social events that involve violence may have dramatic political, economic and social consequences. These events may result in higher crime rates, spreading of infectious diseases, economic crises, and even in migration phenomena (e.g., refugees across borders or internally displaced people). Hence, researchers have started using mobile phone data for developing tools to identify such emergency events in real time. In our paper, we apply a stochastic model, namely a Markov modulated Poisson process, for spatio-temporal detection of hourly and daily behavioral anomalies. We use the call volumes collected from an entire geographic region. Our work is based on the assumption that people tend to make calls when extraordinary events take place. We validate our methodology using a dataset of mobile phone records and events (emergency and non-emergency) from the Republic of Côte d’Ivoire. Our results show that we can successfully capture anomalous calling patterns associated with violent events, riots, as well as social non-emergency events such as holidays, sports events. Moreover, call volume changes also show significant temporal and spatial differences depending on the type of an event. Our results provide insights for the long-term goal of developing a real-time event detection system based on mobile phone data. © 2016, Gundogdu et al."
"10.1140/epjds/s13688-016-0087-z","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84979738912&doi=10.1140%2fepjds%2fs13688-016-0087-z&partnerID=40&md5=0d5a9b00d5c4baea1f7b9c6d132e9278","Online social systems are multiplex in nature as multiple links may exist between the same two users across different social media. In this work, we study the geo-social properties of multiplex links, spanning more than one social network and apply their structural and interaction features to the problem of link prediction across social networking services. Exploring the intersection of two popular online platforms - Twitter and location-based social network Foursquare - we represent the two together as a composite multilayer online social network, where each platform represents a layer in the network. We find that pairs of users connected on both services, have greater neighbourhood similarity and are more similar in terms of their social and spatial properties on both platforms in comparison with pairs who are connected on just one of the social networks. Our evaluation, which aims to shed light on the implications of multiplexity for the link generation process, shows that we can successfully predict links across social networking services. In addition, we also show how combining information from multiple heterogeneous networks in a multilayer configuration can provide new insights into user interactions on online social networks, and can significantly improve link prediction systems with valuable applications to social bootstrapping and friend recommendations. © 2016, Hristova et al."
"10.1140/epjds/s13688-016-0085-1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978152242&doi=10.1140%2fepjds%2fs13688-016-0085-1&partnerID=40&md5=4ce6cad03e541cdea243cd97db8eebd8","In the last few years thousands of scientific papers have investigated sentiment analysis, several startups that measure opinions on real data have emerged and a number of innovative products related to this theme have been developed. There are multiple methods for measuring sentiments, including lexical-based and supervised machine learning methods. Despite the vast interest on the theme and wide popularity of some methods, it is unclear which one is better for identifying the polarity (i.e., positive or negative) of a message. Accordingly, there is a strong need to conduct a thorough apple-to-apple comparison of sentiment analysis methods, as they are used in practice, across multiple datasets originated from different data sources. Such a comparison is key for understanding the potential limitations, advantages, and disadvantages of popular methods. This article aims at filling this gap by presenting a benchmark comparison of twenty-four popular sentiment analysis methods (which we call the state-of-the-practice methods). Our evaluation is based on a benchmark of eighteen labeled datasets, covering messages posted on social networks, movie and product reviews, as well as opinions and comments in news articles. Our results highlight the extent to which the prediction performance of these methods varies considerably across datasets. Aiming at boosting the development of this research area, we open the methods’ codes and datasets used in this article, deploying them in a benchmark system, which provides an open API for accessing and comparing sentence-level sentiment analysis methods. © 2016, Ribeiro et al."
"10.1140/epjds/s13688-016-0083-3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84976411520&doi=10.1140%2fepjds%2fs13688-016-0083-3&partnerID=40&md5=3657e84b00f58a281aaa267b475d3e2f","This aim of this article is to explore the potential use of Wikipedia page view data for predicting electoral results. Responding to previous critiques of work using socially generated data to predict elections, which have argued that these predictions take place without any understanding of the mechanism which enables them, we first develop a theoretical model which highlights why people might seek information online at election time, and how this activity might relate to overall electoral outcomes, focussing especially on information seeking incentives related to swing voters and new parties. We test this model on a novel dataset drawn from a variety of countries in the 2009 and 2014 European Parliament elections. We show that while Wikipedia offers little insight into absolute vote outcomes, it does offer good information about changes in overall turnout at elections and about changes in vote share for particular parties. These results are used to enhance existing theories about the drivers of aggregate patterns in online information seeking, by suggesting that voters are cognitive misers who seek information only when considering changing their vote. © 2016, Yasseri and Bright."
"10.1140/epjds/s13688-016-0084-2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84976407088&doi=10.1140%2fepjds%2fs13688-016-0084-2&partnerID=40&md5=af730374fe0162995550a1260024d2b5","Close proximity interactions between individuals influence how infections spread. Quantifying close contacts in developing world settings, where such data is sparse yet disease burden is high, can provide insights into the design of intervention strategies such as vaccination. Recent technological advances have enabled collection of time-resolved face-to-face human contact data using radio frequency proximity sensors. The acceptability and practicalities of using proximity devices within the developing country setting have not been investigated. We present and analyse data arising from a prospective study of 5 households in rural Kenya, followed through 3 consecutive days. Pre-study focus group discussions with key community groups were held. All residents of selected households carried wearable proximity sensors to collect data on their close (<1.5 metres) interactions. Data collection for residents of three of the 5 households was contemporaneous. Contact matrices and temporal networks for 75 individuals are defined and mixing patterns by age and time of day in household contacts determined. Our study demonstrates the stability of numbers and durations of contacts across days. The contact durations followed a broad distribution consistent with data from other settings. Contacts within households occur mainly among children and between children and adults, and are characterised by daily regular peaks in the morning, midday and evening. Inter-household contacts are between adults and more sporadic when measured over several days. Community feedback indicated privacy as a major concern especially regarding perceptions of non-participants, and that community acceptability required thorough explanation of study tools and procedures. Our results show for a low resource setting how wearable proximity sensors can be used to objectively collect high-resolution temporal data without direct supervision. The methodology appears acceptable in this population following adequate community engagement on study procedures. A target for future investigation is to determine the difference in contact networks within versus between households. We suggest that the results from this study may be used in the design of future studies using similar electronic devices targeting communities, including households and schools, in the developing world context. © 2016, Kiti et al."
"10.1140/epjds/s13688-016-0082-4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84976353292&doi=10.1140%2fepjds%2fs13688-016-0082-4&partnerID=40&md5=f3eef910dc27d8b42a5b6e4dff60e7d3","Collective search for people and information has tremendously benefited from emerging communication technologies that leverage the wisdom of the crowds, and has been increasingly influential in solving time-critical tasks such as the DARPA Network Challenge (DNC, also known as the Red Balloon Challenge). However, while collective search often invests significant resources in encouraging the crowd to contribute new information, the effort invested in verifying this information is comparable, yet often neglected in crowdsourcing models. This paper studies how the exploration-verification trade-off displayed by the teams modulated their success in the DNC, as teams had limited human resources that they had to divide between recruitment (exploration) and verification (exploitation). Our analysis suggests that team performance in the DNC can be modelled as a modified multi-armed bandit (MAB) problem, where information arrives to the team originating from sources of different levels of veracity that need to be assessed in real time. We use these insights to build a data-driven agent-based model, based on the DNC’s data, to simulate team performance. The simulation results match the observed teams’ behavior and demonstrate how to achieve the best balance between exploration and exploitation for general time-critical collective search tasks. © 2016, Chen et al."
"10.1140/epjds/s13688-016-0081-5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84966393425&doi=10.1140%2fepjds%2fs13688-016-0081-5&partnerID=40&md5=387339d322f43cc018e0bd1fbb191fb4","There is enormous interest in inferring features of human behavior in the real world from potential digital footprints created online - particularly at the collective level, where the sheer volume of online activity may indicate some changing mood within the population regarding a particular topic. Civil unrest is a prime example, involving the spontaneous appearance of large crowds of otherwise unrelated people on the street on a certain day. While indicators of brewing protests might be gleaned from individual online communications or account content (e.g. Twitter, Facebook) societal concerns regarding privacy can make such probing a politically delicate issue. Here we show that instead, a simple low-level indicator of civil unrest can be obtained from online data at the aggregate level through Google Trends or similar tools. Our study covers countries across Latin America during 2011-2014 in which diverse civil unrest events took place. In each case, we find that the combination of the volume and momentum of searches from Google Trends surrounding pairs of simple keywords, tailored for the specific cultural setting, provide good indicators of periods of civil unrest. This proof-of-concept study motivates the search for more geographically specific indicators based on geo-located searches at the urban level. © 2016, Qi et al."
"10.1140/epjds/s13688-016-0079-z","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964757904&doi=10.1140%2fepjds%2fs13688-016-0079-z&partnerID=40&md5=54194bec7c6eed60d1b16a4f64b3571a","Fame, popularity and celebrity status, frequently used tokens of success, are often loosely related to, or even divorced from professional performance. This dichotomy is partly rooted in the difficulty to distinguish performance, an individual measure that captures the actions of a performer, from success, a collective measure that captures a community’s reactions to these actions. Yet, finding the relationship between the two measures is essential for all areas that aim to objectively reward excellence, from science to business. Here we quantify the relationship between performance and success by focusing on tennis, an individual sport where the two quantities can be independently measured. We show that a predictive model, relying only on a tennis player’s performance in tournaments, can accurately predict an athlete’s popularity, both during a player’s active years and after retirement. Hence the model establishes a direct link between performance and momentary popularity. The agreement between the performance-driven and observed popularity suggests that in most areas of human achievement exceptional visibility may be rooted in detectable performance measures. © 2016, Yucesoy and Barabási."
"10.1140/epjds/s13688-016-0068-2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84963525404&doi=10.1140%2fepjds%2fs13688-016-0068-2&partnerID=40&md5=cbfe132ce4470f1711527c3064512a43","Citation networks have been widely used to study the evolution of science through the lenses of the underlying patterns of knowledge flows among academic papers, authors, research sub-fields, and scientific journals. Here we focus on citation networks to cast light on the salience of homophily, namely the principle that similarity breeds connection, for knowledge transfer between papers. To this end, we assess the degree to which citations tend to occur between papers that are concerned with seemingly related topics or research problems. Drawing on a large data set of articles published in the journals of the American Physical Society between 1893 and 2009, we propose a novel method for measuring the similarity between articles through the statistical validation of the overlap between their bibliographies. Results suggest that the probability of a citation made by one article to another is indeed an increasing function of the similarity between the two articles. Our study also enables us to uncover missing citations between pairs of highly related articles, and may thus help identify barriers to effective knowledge flows. By quantifying the proportion of missing citations, we conduct a comparative assessment of distinct journals and research sub-fields in terms of their ability to facilitate or impede the dissemination of knowledge. Findings indicate that Electromagnetism and Interdisciplinary Physics are the two sub-fields in physics with the smallest percentage of missing citations. Moreover, knowledge transfer seems to be more effectively facilitated by journals of wide visibility, such as Physical Review Letters, than by lower-impact ones. Our study has important implications for authors, editors and reviewers of scientific journals, as well as public preprint repositories, as it provides a procedure for recommending relevant yet missing references and properly integrating bibliographies of papers. © 2016, Ciotti et al."
"10.1140/epjds/s13688-016-0074-4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962657945&doi=10.1140%2fepjds%2fs13688-016-0074-4&partnerID=40&md5=87c17985e1214f5d7a0d5694bf9635eb","Participation in social sensing applications is challenged by privacy threats. Large-scale access to citizens’ data allow surveillance and discriminatory actions that may result in segregation phenomena in society. On the contrary are the benefits of accurate computing analytics required for more informed decision-making, more effective policies and regulation of techno-socio-economic systems supported by ‘Internet-of Things’ technologies. In contrast to earlier work that either focuses on privacy protection or Big Data analytics, this paper proposes a self-regulatory information sharing system that bridges this gap. This is achieved by modeling information sharing as a supply-demand system run by computational markets. On the supply side lie the citizens that make incentivized but self-determined decisions about the level of information they share. On the demand side stand data aggregators that provide rewards to citizens to receive the required data for accurate analytics. The system is empirically evaluated with two real-world datasets from two application domains: (i) Smart Grids and (ii) mobile phone sensing. Experimental results quantify trade-offs between privacy-preservation, accuracy of analytics and costs from the provided rewards under different experimental settings. Findings show a higher privacy-preservation that depends on the number of participating citizens and the type of data summarized. Moreover, analytics with summarization data tolerate high local errors without a significant influence on the global accuracy. In other words, local errors cancel out. Rewards can be optimized to be fair so that citizens with more significant sharing of information receive higher rewards. All these findings motivate a new paradigm of truly decentralized and ethical data analytics. © 2016, Pournaras et al."
"10.1140/epjds/s13688-016-0075-3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962178056&doi=10.1140%2fepjds%2fs13688-016-0075-3&partnerID=40&md5=211e798bde6a41b3fd1e1670ed91f88d","Energy efficiency is a key challenge for building sustainable societies. Due to growing populations, increasing incomes and the industrialization of developing countries, the world primary energy consumption is expected to increase annually by 1.6%. This scenario raises issues related to the increasing scarcity of natural resources, the accelerating pollution of the environment, and the looming threat of global climate change. In this paper we introduce a new and original approach to predict next week energy consumption based on human dynamics analysis derived out of the anonymized and aggregated telecom data, which is processed from GSM network call data records (CDRs). We introduce an original problem statement, analyze regularities of the source data, provide insight on the original feature extraction method and discuss peculiarities of the regression models applicable for this big data problem. The proposed solution could act on energy producers/distributors as an essential aid to smart meters data for making better decisions in reducing total primary energy consumption by limiting energy production when the demand is not predicted, reducing energy distribution costs by efficient buy-side planning in time and providing insights for peak load planning in geographic space. © 2016, Bogomolov et al."
"10.1140/epjds/s13688-016-0073-5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84961726896&doi=10.1140%2fepjds%2fs13688-016-0073-5&partnerID=40&md5=522548067ba1599828a955288504a3ac","Tourism is becoming a significant contributor to medium and long range travels in an increasingly globalized world. Leisure traveling has an important impact on the local and global economy as well as on the environment. The study of touristic trips is thus raising a considerable interest. In this work, we apply a method to assess the attractiveness of 20 of the most popular touristic sites worldwide using geolocated tweets as a proxy for human mobility. We first rank the touristic sites based on the spatial distribution of the visitors’ place of residence. The Taj Mahal, the Pisa Tower and the Eiffel Tower appear consistently in the top 5 in these rankings. We then pass to a coarser scale and classify the travelers by country of residence. Touristic site’s visiting figures are then studied by country of residence showing that the Eiffel Tower, Times Square and the London Tower welcome the majority of the visitors of each country. Finally, we build a network linking sites whenever a user has been detected in more than one site. This allow us to unveil relations between touristic sites and find which ones are more tightly interconnected. © 2016, Bassolas et al."
"10.1140/epjds/s13688-016-0072-6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84961615137&doi=10.1140%2fepjds%2fs13688-016-0072-6&partnerID=40&md5=a5f60f31331ba80ca029413d4085f739","Hateful and antagonistic content published and propagated via the World Wide Web has the potential to cause harm and suffering on an individual basis, and lead to social tension and disorder beyond cyber space. Despite new legislation aimed at prosecuting those who misuse new forms of communication to post threatening, harassing, or grossly offensive language - or cyber hate - and the fact large social media companies have committed to protecting their users from harm, it goes largely unpunished due to difficulties in policing online public spaces. To support the automatic detection of cyber hate online, specifically on Twitter, we build multiple individual models to classify cyber hate for a range of protected characteristics including race, disability and sexual orientation. We use text parsing to extract typed dependencies, which represent syntactic and grammatical relationships between words, and are shown to capture ‘othering’ language - consistently improving machine classification for different types of cyber hate beyond the use of a Bag of Words and known hateful terms. Furthermore, we build a data-driven blended model of cyber hate to improve classification where more than one protected characteristic may be attacked (e.g. race and sexual orientation), contributing to the nascent study of intersectionality in hate crime. © 2016, Burnap and Williams."
"10.1140/epjds/s13688-016-0071-7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84961589080&doi=10.1140%2fepjds%2fs13688-016-0071-7&partnerID=40&md5=1d5118774f52c7bbbdb729682dd3d1d6","We detect the backbone of the weighted bipartite network of the Japanese credit market relationships. The backbone is detected by adapting a general method used in the investigation of weighted networks. With this approach we detect a backbone that is statistically validated against a null hypothesis of uniform diversification of loans for banks and firms. Our investigation is done year by year and it covers more than thirty years during the period from 1980 to 2011. We relate some of our findings with economic events that have characterized the Japanese credit market during the last years. The study of the time evolution of the backbone allows us to detect changes occurred in network size, fraction of credit explained, and attributes characterizing the banks and the firms present in the backbone. © 2016, Marotta et al."
"10.1140/epjds/s13688-016-0070-8","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84961250907&doi=10.1140%2fepjds%2fs13688-016-0070-8&partnerID=40&md5=4972d0ae5f0469fceb9e972d89984a64","In this paper, we study the network of global interconnections between language communities, based on shared co-editing interests of Wikipedia editors, and show that although English is discussed as a potential lingua franca of the digital space, its domination disappears in the network of co-editing similarities, and instead local connections come to the forefront. Out of the hypotheses we explored, bilingualism, linguistic similarity of languages, and shared religion provide the best explanations for the similarity of interests between cultural communities. Population attraction and geographical proximity are also significant, but much weaker factors bringing communities together. In addition, we present an approach that allows for extracting significant cultural borders from editing activity of Wikipedia users, and comparing a set of hypotheses about the social mechanisms generating these borders. Our study sheds light on how culture is reflected in the collective process of archiving knowledge on Wikipedia, and demonstrates that cross-lingual interconnections on Wikipedia are not dominated by one powerful language. Our findings also raise some important policy questions for the Wikimedia Foundation. © 2016, Samoilenko et al."
"10.1140/epjds/s13688-016-0069-1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84961238481&doi=10.1140%2fepjds%2fs13688-016-0069-1&partnerID=40&md5=e6c79aa49063421a1dd48a7599295f65","We consider inventions as novel combinations of existing technological capabilities. Patent data allow us to explicitly identify such combinatorial processes in invention activities (Youn et al. in J R Soc Interface 12:20150272, 2015). Unconsidered in the previous research, not every new combination is novel to the same extent. Some combinations are naturally anticipated based on patent activities in the past or mere random choices, and some appear to deviate exceptionally from existing invention pathways. We calculate a relative likelihood that each pair of classification codes is put together at random, and a deviation from the empirical observation so as to assess the overall novelty (or conventionality) that the patent brings forth at each year. An invention is considered as unconventional if a pair of codes therein is unlikely to be used together given the statistics in the past. Temporal evolution of the distribution indicates that the patenting activities become more conventional with occasional cross-over combinations. Our analyses show that patents introducing novelty on top of the conventional units would receive higher citations, and hence have higher impact. © 2016, Kim et al."
"10.1140/epjds/s13688-016-0067-3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84959569429&doi=10.1140%2fepjds%2fs13688-016-0067-3&partnerID=40&md5=d9faf3ade1f19ec838c69a8604687238","Whenever someone makes or receives a call on a mobile telephone, a Call Detail Record (CDR) is automatically generated by the operator for billing purposes. CDRs have a wide range of applications beyond billing, from social science to data-driven development. Recently, CDRs have been increasingly used to study human mobility, whose understanding is crucial e.g. for planning efficient transportation infrastructure. A major difficulty in analyzing human mobility using CDR data is that the location of a cell phone user is not recorded continuously but typically only when a call is initiated or a text message is sent. In this paper we address this problem, and develop a method for estimating travel times between cities based on CDRs that relies not on individual trajectories of people, but their collective statistical properties. We apply our method to data from Senegal, released by Sonatel and Orange for the 2014 Data for Development Challenge. We turn CDR mobility traces to estimates on travel times between Senegalese cities, filling an existing gap in knowledge. Moreover, the proposed method is shown to be highly valuable for monitoring travel conditions and their changes in near real-time, as demonstrated by measuring the decrease in travel times due to the opening of the Dakar-Diamniadio highway. Overall, our results indicate that it is possible to extract reliable de facto information on typical travel times that is useful for a variety of audiences ranging from casual travelers to transport infrastructure planners. © 2016, Kujala et al."
"10.1140/epjds/s13688-016-0066-4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84959457887&doi=10.1140%2fepjds%2fs13688-016-0066-4&partnerID=40&md5=b7e4afdbea845279bbda766e8215365e","Contributing to the writing of history has never been as easy as it is today thanks to Wikipedia, a community-created encyclopedia that aims to document the world’s knowledge from a neutral point of view. Though everyone can participate it is well known that the editor community has a narrow diversity, with a majority of white male editors. While this participatory gender gap has been studied extensively in the literature, this work sets out to assess potential gender inequalities in Wikipedia articles along different dimensions: notability, topical focus, linguistic bias, structural properties, and meta-data presentation. We find that (i) women in Wikipedia are more notable than men, which we interpret as the outcome of a subtle glass ceiling effect; (ii) family-, gender-, and relationship-related topics are more present in biographies about women; (iii) linguistic bias manifests in Wikipedia since abstract terms tend to be used to describe positive aspects in the biographies of men and negative aspects in the biographies of women; and (iv) there are structural differences in terms of meta-data and hyperlinks, which have consequences for information-seeking activities. While some differences are expected, due to historical and social contexts, other differences are attributable to Wikipedia editors. The implications of such differences are discussed having Wikipedia contribution policies in mind. We hope that the present work will contribute to increased awareness about, first, gender issues in the content of Wikipedia, and second, the different levels on which gender biases can manifest on the Web. © 2016, Wagner et al."
"10.1140/epjds/s13688-016-0065-5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84959321348&doi=10.1140%2fepjds%2fs13688-016-0065-5&partnerID=40&md5=0dcc08c3b13ed181efecfd03d37b6d2d","Segregation is widespread in all realms of human society. Several influential studies have argued that intolerance is not a prerequisite for a segregated society, and that segregation can arise even when people generally prefer diversity. We investigated this paradox experimentally, by letting groups of high-school students play four different real-time interactive games. Incentives for neighbor similarity produced segregation, but incentives for neighbor dissimilarity and neighborhood diversity prevented it. The participants continued to move while their game scores were below optimal, but their individual moves did not consistently take them to the best alternative position. These small differences between human and simulated agents produced different segregation patterns than previously predicted, thus challenging conclusions about segregation arising from these models. © 2016, Tsvetkova et al."
"10.1140/epjds/s13688-016-0064-6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84958951237&doi=10.1140%2fepjds%2fs13688-016-0064-6&partnerID=40&md5=35e5c9808810e4c709573a75194c6d9f","The exploration of people’s everyday life has long been of interest to social scientists. Recent years have witnessed a growing interest in analyzing human behavioral data generated by technology (e.g. mobile phones). To date, a few large-scale studies have been designed to measure human behaviors and interactions using multiple sources of data. A common characteristic of these studies is the population under investigation: students having similar daily routines and needs. This choice constraints the range of behaviors, of places and the generalization of the results. In order to widen this line of studies, we focus on a different target group: parents with young children aged 0 through 10 years. Children influence multiple aspects of their parents’ lives, from the satisfaction of basic human needs and the fulfillment of social roles to their financial status and sleep quality. In this paper, we describe the Mobile Territorial Lab (MTL) project, a longitudinal living lab which has been sensing by means of technology (mobile phones) the lives of more than 100 parents in different areas of the Trentino region in Northern Italy. We present the preliminary results after two years of experimentation of, to the best of our knowledge, the most complete picture of parents’ daily lives. Through the collection and analysis of the collected data, we created a multi-layered view of the participants’ lives, tracking social interactions, mobility routines, spending patterns, and personality characteristics. Overall, our results prove the relevance of living lab approaches to measure human behaviors and interactions, which can pave the way to new studies exploiting a richer number of behavioral indicators. Moreover, we believe that the proposed methodology and the collected data could be very valuable for researchers from different disciplines such as social psychology, sociology, computer science, economy, etc., which are interested in understanding human behaviour. © 2016, Centellegher et al."
"10.1186/s40537-016-0051-6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013979641&doi=10.1186%2fs40537-016-0051-6&partnerID=40&md5=08c3d55cc6a4681a06f0ca210dd3b6cf","Big Data is the term used for larger data sets that are very complex and not easily processed by the traditional devices. Today is the need of the new technology for processing these large data sets. Apache Hadoop is the good option and it has many components that worked together to make the hadoop ecosystem robust and efficient. Apache Pig is the core component of hadoop ecosystem and it accepts the tasks in the form of scripts. To run these scripts Apache Pig may use MapReduce or Apache Tez framework. In our previous paper we analyze how these two frameworks different from each other on the basis of some parameters chosen. We compare both the frameworks in theoretical and empirical way on the single node cluster. Here, in this paper we try to perform the analysis on multinode cluster which is installed at Amazon cloud. © 2016, The Author(s)."
"10.1186/s40537-016-0041-8","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013977458&doi=10.1186%2fs40537-016-0041-8&partnerID=40&md5=863be78453ba992473c16850abcff324","For getting up-to-date insight into online services, extracted data has to be processed in near real time. For example, major big data companies (Facebook, LinkedIn, Twitter) analyse streaming data for development of new services. Several technologies have been developed, which could be selected for implementation of stream processing functionalities. The contribution of this paper is feasibility analysis of technologies for stream-based processing of semi-structured data. Particularly, feasibility of a Big Data management system for semi-structured data (AsterixDB) will be compared to Spark streaming, which has been integrated with Cassandra NoSQL database for persistence. The study focuses on stream processing in a simulated social media use case (tweet analysis), which has been implemented to Eucalyptus cloud computing environment on a distributed shared memory multiprocessor platform. The results indicate that AsterixDB is able to provide significantly better performance both in terms of throughput and latency, when data feed functionality of AsterixDB is used, and stream processing has been implemented with Java. AsterixDB also scaled on the same level or better, when the amount of nodes on the cloud platform was increased. However, stream processing in AsterixDB was delayed by batching of data, when tweets were streamed into the database with data feeds. © 2016, Pääkkönen."
"10.1186/s40537-016-0053-4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013970526&doi=10.1186%2fs40537-016-0053-4&partnerID=40&md5=723c23eed5e85c7c5a696a2ffb4e6163","A very important area of financial risk management is systemic risk modelling, which concerns the estimation of the interrelationships between financial institutions, with the aim of establishing which of them are more central and, therefore, more contagious/subject to contagion. The aim of this paper is to develop a novel systemic risk model. A model that, differently from existing ones, employs not only the information contained in financial market prices, but also big data coming from financial tweets. From a methodological viewpoint, the novelty of our paper is the estimation of systemic risk models using two different data sources: financial markets and financial tweets, and a proposal to combine them, using a Bayesian approach. From an applied viewpoint, we present the first systemic risk model based on big data, and show that such a model can shed further light on the interrelationships between financial institutions. © 2016, The Author(s)."
"10.1186/s40537-016-0049-0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013968843&doi=10.1186%2fs40537-016-0049-0&partnerID=40&md5=50fbbf7fe90356878bb7bbdd1f1d4237","Patient wellness and preventative care are increasingly becoming a concern for many patients, employers, and healthcare professionals. The federal government has increased spending for wellness alongside new legislation which gives employers and insurance providers some new tools for encouraging preventative care. Not all preventative care and wellness programs have a net positive savings however. Our research attempts to create a patient wellness score which integrates many lifestyle components and a holistic patient prospective. Using a large comprehensive survey conducted by the Centers for Disease Control and Prevention, models are built combining both medical professional input and machine learning algorithms. Models are compared and 8 out of 9 models are shown to have a statistically significant (p = 0.05) increase in area under the receiver operating characteristic when using the hybrid approach when compared to expert-only models. Models are then aggregated and linearly transformed for patient-friendly output. The resulting predictive models provide patients and healthcare providers a comprehensive numerical assessment of a patient’s health, which may be used to track patient wellness so at to help maintain or improve their current condition. © 2016, The Author(s)."
"10.1186/s40537-016-0044-5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013958643&doi=10.1186%2fs40537-016-0044-5&partnerID=40&md5=e4c862063780ebbf70dde56e8eade2e2","Road accident data analysis plays an important role in identifying key factors associated with road accidents. These associated factors help in taking preventive measures to overcome the road accidents. Various studies have been done on road accident data analysis using traditional statistical techniques and data mining techniques. All these studies focused on identifying key factors associated with road accidents in different countries. Road accident is uncertain and unpredictable events which can occur in any circumstances. Also, road accidents do not have similar impacts in every region of the districts. There are chances that road accident rate is increasing in a certain district but it has some lower impact in other districts. Hence, the more focus on road safety should be on those regions or districts where road accident trend is increasing. Time series analysis is an important area of study which can be helpful in identifying the increasing or decreasing trends in different districts. In this paper, we have proposed a framework to analyze road accident time series data that takes 39 time series data of 39 districts of Gujrat and Uttarakhand state of India. This framework segments the time series data into different clusters. A time series merging algorithm is proposed to find the representative time series (RTS) for each cluster. This RTS is further used for trend analysis of different clusters. The results reveals that road accident trend is going to increase in certain clusters and those districts should be the prime concern to take preventive measure to overcome the road accidents. © 2016, The Author(s)."
"10.1186/s40537-016-0045-4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013956400&doi=10.1186%2fs40537-016-0045-4&partnerID=40&md5=7a352b31b117e98ae309db72a6d5b404","The big data phenomenon is becoming a fact. Continuous increase of digitization and connecting devices to Internet are making current solutions and services smarter, richer and more personalized. The emergence of the NoSQL databases, like Cassandra, with their massive scalability and high availability encourages us to investigate the management of the stored data within such storage system. In our present work, we harness the geohashing technique to enable spatial queries as extension to Cassandra query language capabilities while preserving the native syntax. The developed framework showed the feasibility of this approach where basic spatial queries are underpinned and the query response time is reduced by up to 70 times for a fairly large area. © 2016, The Author(s)."
"10.1186/s40537-016-0055-2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013954197&doi=10.1186%2fs40537-016-0055-2&partnerID=40&md5=39f1bdba376454cf43f127e5fe69bd7d","Life Sciences have been established and widely accepted as a foremost Big Data discipline; as such they are a constant source of the most computationally challenging problems. In order to provide efficient solutions, the community is turning towards scalable approaches such as the utilization of cloud resources in addition to any existing local computational infrastructures. Although bioinformatics workflows are generally amenable to parallelization, the challenges involved are however not only computationally, but also data intensive. In this paper we propose a data management methodology for achieving parallelism in bioinformatics workflows, while simultaneously minimizing data-interdependent file transfers. We combine our methodology with a novel two-stage scheduling approach capable of performing load estimation and balancing across and within heterogeneous distributed computational resources. Beyond an exhaustive experimentation regime to validate the scalability and speed-up of our approach, we compare it against a state-of-the-art high performance computing framework and showcase its time and cost advantages. © 2016, The Author(s)."
"10.1186/s40537-016-0056-1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013952196&doi=10.1186%2fs40537-016-0056-1&partnerID=40&md5=2321455047bd5da2bf41304a519caffa","In recent years, there has been an increasing amount of data being produced and stored, which is known as Big Data. The social networks, internet of things, scientific experiments and commercial services play a significant role in generating a vast amount of data. Three main factors are important in Big Data; Volume, Velocity and Variety. One needs to consider all three factors when designing a platform to support Big Data. The Large Hadron Collider (LHC) particle accelerator at CERN consists of a number of data-intensive experiments, which are estimated to produce a volume of about 30 PB of data, annually. The velocity of these data that are propagated will be extremely fast. Traditional methods of collecting, storing and analysing data have become insufficient in managing the rapidly growing volume of data. Therefore, it is essential to have an efficient strategy to capture these data as they are produced. In this paper, a number of models are explored to understand what should be the best approach for collecting and storing Big Data for analytics. An evaluation of the performance of full execution cycles of these approaches on the monitoring of the Worldwide LHC Computing Grid (WLCG) infrastructure for collecting, storing and analysing data is presented. Moreover, the models discussed are applied to a community driven software solution, Apache Flume, to show how they can be integrated, seamlessly. © 2016, The Author(s)."
"10.1186/s40537-016-0047-2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013947883&doi=10.1186%2fs40537-016-0047-2&partnerID=40&md5=a4b3c08b48c2a373827bf20009bb3cdb","This paper proposes an algorithm called optimized relativity search to reduce the number of nodes in a graph when attempting to decrease the running time for personalized page rank (PPR) estimation. Even though similar estimations have been done, this method significantly increases the speed of computation, making it a feasible candidate for large graph solutions, such as search engines and friend recommendation techniques used in social media. In this study, the weighted page rank method was combined with the Monte-Carlo technique and a local update algorithm over a reduced map space; this algorithm was developed to achieve a more accurate and faster search method than FAST PPR. The experimental results showed that for nodes with a high degree of incoming nodes, the speed of estimation was twice as fast compared to FAST PPR, at the expense of a little accuracy. © 2016, The Author(s)."
"10.1186/s40537-016-0046-3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013939101&doi=10.1186%2fs40537-016-0046-3&partnerID=40&md5=cae32b6832237f8d0a40c74866931bc1","Road and traffic accidents are an important concern around the world. Road accidents not only affects the public health with different level of injury but also results in property damage. Data analysis has the capability to identify the different reasons behind road accidents i.e. traffic characteristics, weather characteristics, road characteristics and etc. A variety of research on road accident data analysis has already proves its importance. Some studies focused on identifying factors associated with accident severity while others focused on identifying the associated factors behind accident occurrence. These research analyses used traditional statistical methods as well as data mining methods. Data mining is frequently used method for analyzing road accident data in present research. Trend analysis is another important research area in road accident domain. Trend analysis can assist in identifying the increasing or decreasing accidents rate in different reasons. In this study, we have proposed a method to analyze hourly road accident data using Cophenetic correlation coefficient from Gujarat state in India. The motive of this study is to provide an efficient way to choose the best suitable distance metric to cluster the series of counts data that provide a better clustering result. The result shows that the proposed method is capable of efficiently group the different districts with similar road accident patterns into single cluster or group which can be further used for trend analysis or similar tasks. © 2016, The Author(s)."
"10.1186/s40537-015-0037-9","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013937930&doi=10.1186%2fs40537-015-0037-9&partnerID=40&md5=6c815c4b127cf6fbd573d166bb3b10d1","Analysis of online user-generated content is receiving attention for its wide applications from both academic researchers and industry stakeholders. In this pilot study, we address common Big Data problems of time constraints and memory costs involved with using standard single-machine hardware and software. A novel Big Data processing framework is proposed to investigate a niche subset of user-generated popular culture content on Douban, a well-known Chinese-language online social network. Huge data samples are harvested via an asynchronous scraping crawler. We also discuss how to manipulate heterogeneous features from raw samples to facilitate analysis of various film details, review comments, and user profiles on Douban with specific regard to a wave of South Korean films (2003–2014), which have increased in popularity among Chinese film fans. In addition, an improved Apriori algorithm based on MapReduce is proposed for content-mining functions. An exploratory simulation of results demonstrates the flexibility and applicability of the proposed framework for extracting relevant information from complex social media data, knowledge which can in turn be extended beyond this niche dataset and used to inform producers and distributors of films, television shows, and other digital media content. © 2016, Yang and Yecies."
"10.1186/s40537-015-0038-8","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013930334&doi=10.1186%2fs40537-015-0038-8&partnerID=40&md5=e69f767ed2c64738273310d71832a5fc","This paper describes the vision behind and the mission of the Maxeler Application Gallery (AppGallery.Maxeler.com) project. First, it concentrates on the essence and performance advantages of the Maxeler dataflow approach. Second, it reviews the support technologies that enable the dataflow approach to achieve its maximum. Third, selected examples of the Maxeler Application Gallery are presented; these examples are treated as the final achievement made possible when all the support technologies are put to work together (internal infrastructure of the AppGallery.Maxeler.com is given in a follow-up paper). As last, the possible impact of the Application Gallery is presented and the major conclusions are drawn. © 2016, Trifunovic et al."
"10.1186/s40537-016-0052-5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013925990&doi=10.1186%2fs40537-016-0052-5&partnerID=40&md5=d2907bc720641ef64f67fc4aadbd247c","We introduce d2o, a Python module for cluster-distributed multi-dimensional numerical arrays. It acts as a layer of abstraction between the algorithm code and the data-distribution logic. The main goal is to achieve usability without losing numerical performance and scalability. d2o’s global interface is similar to the one of a numpy.ndarray, whereas the cluster node’s local data is directly accessible for use in customized high-performance modules. d2o is written in pure Python which makes it portable and easy to use and modify. Expensive operations are carried out by dedicated external libraries like numpy and mpi4py. The performance of d2o is on a par with numpy for serial applications and scales well when moving to an MPI cluster. d2o is open-source software available under the GNU General Public License v3 (GPL-3) at https://gitlab.mpcdf.mpg.de/ift/D2O. © 2016, The Author(s)."
"10.1186/s40537-015-0036-x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013925486&doi=10.1186%2fs40537-015-0036-x&partnerID=40&md5=c9a0666688287089f9a413852ef34a3a","Recently, many researchers have focused on data stream processing as an efficient method for extracting knowledge from big data. Data stream clustering is an unsupervised approach that is employed for huge data. The continuous effort on data stream clustering method has one common goal which is to achieve an accurate clustering algorithm. However, there are some issues that are overlooked by the previous works in proposing data stream clustering solutions; (1) clustering dataset including big segments of repetitive data, (2) monitoring clustering structure for ordinal data streams and (3) determining important parameters such as k number of exact clusters in stream of data. In this paper, DCSTREAM method is proposed with regard to the mentioned issues to cluster big datasets using the vector model and k-Means divide and conquer approach. Experimental results show that DCSTREAM can achieve superior quality and performance as compare to STREAM and ConStream methods for abrupt and gradual real world datasets. Results show that the usage of batch processing in DCSTREAM and ConStream is time consuming compared to STREAM but it avoids further analysis for detecting outliers and novel micro-clusters. © 2016, Khalilian et al."
"10.1186/s40537-016-0058-z","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013921019&doi=10.1186%2fs40537-016-0058-z&partnerID=40&md5=c708d5a3117d0d58b2c8a42c9658d1a5","Community structures and relation patterns, and ranking them for social networks provide us with great knowledge about network. Such knowledge can be utilized for target marketing or grouping similar, yet distinct, nodes. The ever-growing variety of social networks necessitates detection of minute and scattered communities, which are important problems across different research fields including biology, social studies, physics, etc. Existing community detection algorithms such as fast and folding or modularity based are either incapable of finding graph anomalies or too slow and impractical for large graphs. The main contributions of this work are twofold: (i) we optimize the Attractor algorithm, speeding it up by a factor depending on complexity of the graph; i.e. the more complex a social graph is, the better result the algorithm will achieve, and (ii) we propose a community ranker algorithm for the first time. The former is achieved by amalgamating loops and incorporating breadth-first search (BFS) algorithm for edge alignments and to fill in the missing cache, preserving a constant of time equal to the number of edges in the graph. For the latter, we make the first attempt to enumerate how influential each community is in a given graph, ranking them based on their normalized impact factor. © 2016, The Author(s)."
"10.1186/s40537-016-0057-0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013918176&doi=10.1186%2fs40537-016-0057-0&partnerID=40&md5=e0b80c85bc0d4490984b3f862e35bed3","Nowadays, big data is a key component in (bio)medical research. However, the meaning of the term is subject to a wide array of opinions, without a formal definition. This hampers communication and leads to missed opportunities. For example, in the (bio)medical field we have observed many different interpretations, some of which have a negative connotation, impeding exploitation of big data approaches. In this paper we pursue a better understanding of the term big data through a data-driven systematic approach using text analysis of scientific (bio)medical literature. We attempt to find how existing big data definitions are expressed within the chosen application domain. We build upon findings of previous qualitative research by De Mauro et al. (Lib Rev 65: 122–135, 14), which analysed fifteen definitions and identified four key big data themes (i.e., information, methods, technology, and impact). We have revisited these and other definitions of big data, and consolidated them into eight additional themes, resulting in a total of twelve themes. The corpus was composed of paper abstracts extracted from (bio)medical literature databases, searching for ‘big data’. After text pre-processing and parameter selection, topic modelling was applied with 25 topics. The resulting top-20 words per topic were annotated with the twelve big data themes by seven observers. The analysis of these annotations show that the themes proposed by De Mauro et al. are strongly expressed in the corpus. Furthermore, several of the most popular big data V’s (i.e., volume, velocity, and value) also have a relatively high presence. Other V’s introduced more recently (e.g. variability) were however hardly found in the 25 topics. These findings show that the current understanding of big data within the (bio)medical domain is in agreement with more general definitions of the term. © 2016, The Author(s)."
"10.1186/s40537-016-0054-3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013910339&doi=10.1186%2fs40537-016-0054-3&partnerID=40&md5=7ca142c8ac17f7423360f70f95a415b7","Human behavior is essentially social and humans start their daily routines by interacting with others. There are many forms of social interactions and we have used mobile phone based social interaction features and social surveys for finding human stress behavior. For this, we gathered mobile phone call logs data set containing 111,444 voice calls of 131 adult members of a living community for a period of more than 5 months. And we identified that top 5 social network measures like hierarchy, density, farness, reachability and eigenvector of individuals have profound influence on individuals’ stress levels in a social network. If an ego lies in the shortest path of all other alters then the ego receives more information and hence is more stressed. In this paper, we have used TreeNet machine learning algorithm for its speed and immunity to outliers. We have tested our results with another Random Forest classifier as well and yet, we found TreeNet to be more efficient. This research can be of vital importance to economists, professionals, analysts, and policy makers. © 2016, The Author(s)."
"10.1186/s40537-016-0048-1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013907160&doi=10.1186%2fs40537-016-0048-1&partnerID=40&md5=df1f6da770fd77be062a6cdeeb0087f2","We collect big data use cases for a representative sample of telecom companies worldwide and observe a wide and skewed distribution of big data returns, with a few companies reporting large impact for a long tail of telecom companies with limited returns. Using a joint model of adoption and returns to adoption, we find that the skewness of the distribution arises from a few telecom companies being able to follow key big data managerial and organizational practices. We also find that big data returns exhibit economies of scope, decreasing returns to scale, while big data talents are complementary to big data capex investments. © 2016, The Author(s)."
"10.1186/s40537-016-0060-5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013904782&doi=10.1186%2fs40537-016-0060-5&partnerID=40&md5=67c8ff5e6dcfbca4883e5d98c24d9b19","Graph clustering is an important technique to understand the relationships between the vertices in a big graph. In this paper, we propose a novel random-walk-based graph clustering method. The proposed method restricts the reach of the walking agent using an inflation function and a normalization function. We analyze the behavior of the limited random walk procedure and propose a novel algorithm for both global and local graph clustering problems. Previous random-walk-based algorithms depend on the chosen fitness function to find the clusters around a seed vertex. The proposed algorithm tackles the problem in an entirely different manner. We use the limited random walk procedure to find attractor vertices in a graph and use them as features to cluster the vertices. According to the experimental results on the simulated graph data and the real-world big graph data, the proposed method is superior to the state-of-the-art methods in solving graph clustering problems. Since the proposed method uses the embarrassingly parallel paradigm, it can be efficiently implemented and embedded in any parallel computing environment such as a MapReduce framework. Given enough computing resources, we are capable of clustering graphs with millions of vertices and hundreds millions of edges in a reasonable time. © 2016, The Author(s)."
"10.1186/s40537-016-0040-9","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013904761&doi=10.1186%2fs40537-016-0040-9&partnerID=40&md5=086f49429ef4e3f008e9908c11f2c98c","“Data streams” is defined as class of data generated over “text, audio and video” channel in continuous form. The streams are of infinite length and may comprise of structured or unstructured data. With these features, it is difficult to store and process data streams with simple and static strategies. The processing of data stream poses four main challenges to researchers. These are infinite length, concept-evolution, concept-drift and feature evolution. Infinite-length is because the amount of data has no bounds. Concept-drift is due to slow changes in the concept of stream. Concept-evolution occurs due to presence of unknown classes in data. Feature-evolution is due to progression new features and regression of old features. To perform any analytics data streams, the conversion to knowledgable form is essential. The researcher in past have proposed various strategies, most of the research is focussed on problem of infinite-length and concept-drift. The research work presented in the paper describes a efficient string based methodology to process “data streams” and control the challenges of infinite-length, concept-evolution and concept-drift. Subject areas Data mining, Machine learning © 2016, Chandak."
"10.1186/s40537-016-0039-2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013900908&doi=10.1186%2fs40537-016-0039-2&partnerID=40&md5=864dae19dba377cca9777ca273e10de2","Finding topics from a collection of documents, such as research publications, patents, and technical reports, is helpful for summarizing large scale text collections and the world wide web. It can also help forecast topic trends in the future. This can be beneficial for many applications, such as modeling the evolution of the direction of research and forecasting future trends of the IT industry. In this paper, we propose using association analysis and ensemble forecasting to automatically discover topics from a set of text documents and forecast their evolving trend in a near future. In order to discover meaningful topics, we collect publications from a particular research area, data mining and machine learning, as our data domain. An association analysis process is applied to the collected data to first identify a set of topics, followed by a temporal correlation analysis to help discover correlations between topics, and identify a network of topics and communities. After that, an ensemble forecasting approach is proposed to predict the popularity of research topics in the future. Our experiments and validations on data with 9 years of publication records validate the effectiveness of the proposed design. © 2016, Hurtado et al."
"10.1186/s40537-016-0042-7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013895773&doi=10.1186%2fs40537-016-0042-7&partnerID=40&md5=645ba7fc4437c92b1297c39dbb803c4d","The use of Big Data in today’s world has become a necessity due to the massive number of technologies developed recently that keeps on providing us with data such as sensors, surveillance system and even smart phones and smart wearable devices they all tend to produce a lot of information that need to be analyzed and studied in details to provide us with some insight to what these data represent. In this paper we focus on the application of the techniques of data reduction based on data nodes in large networks datasets by computing data similarity computation, maximum similarity clique (MSC) and then finding the shortest path in a quick manner due to the data reduction in the graph. As the number of vertices and edges tend to increase on large networks the aim of this article is to make the reduction of the network that will cause an impact on calculating the shortest path for a faster analysis in a shortest time. © 2016, The Author(s)."
"10.1186/s40537-016-0050-7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013888400&doi=10.1186%2fs40537-016-0050-7&partnerID=40&md5=de1773d6cf970ac7c30e44884012a324","Because communities are the fundamental component of big data/large data network graphs, community detection in large-scale graphs is an important area to study. Communities are a collection of a set of nodes with similar features. In a given graph there can be many features for clustering the nodes to form communities. Varying the features of interest can form several communities. Out of all communities that are formed, only a few communities are dominant and most influential for a given network graph. This might well contain influential nodes; i.e., for each possible feature of clustering, there will be only a few influential communities in the graph. Identification of such communities is a salient subject for research. This paper present a technique to identify the top-K communities, based on the average Katz centrality of all the communities in a network of communities and the distinctive nature of the communities. One can use these top-K communities to spread information efficiently into the network, as these communities are capable of influencing neighboring communities and thus spreading the information into the network efficiently. © 2016, The Author(s)."
"10.1186/s40537-016-0043-6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013880960&doi=10.1186%2fs40537-016-0043-6&partnerID=40&md5=291433db134501920a9adcf3cf65a9fa","Machine learning and data mining techniques have been used in numerous real-world applications. An assumption of traditional machine learning methodologies is the training data and testing data are taken from the same domain, such that the input feature space and data distribution characteristics are the same. However, in some real-world machine learning scenarios, this assumption does not hold. There are cases where training data is expensive or difficult to collect. Therefore, there is a need to create high-performance learners trained with more easily obtained data from different domains. This methodology is referred to as transfer learning. This survey paper formally defines transfer learning, presents information on current solutions, and reviews applications applied to transfer learning. Lastly, there is information listed on software downloads for various transfer learning solutions and a discussion of possible future research work. The transfer learning solutions surveyed are independent of data size and can be applied to big data environments. © 2016, The Author(s)."
"10.1186/s40537-016-0059-y","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013878522&doi=10.1186%2fs40537-016-0059-y&partnerID=40&md5=cbe6fc410a4572b40bb5cccc8dae2141","Big data is a term used for very large data sets that have more varied and complex structure. These characteristics usually correlate with additional difficulties in storing, analyzing and applying further procedures or extracting results. Big data analytics is the term used to describe the process of researching massive amounts of complex data in order to reveal hidden patterns or identify secret correlations. However, there is an obvious contradiction between the security and privacy of big data and the widespread use of big data. This paper focuses on privacy and security concerns in big data, differentiates between privacy and security and privacy requirements in big data. This paper covers uses of privacy by taking existing methods such as HybrEx, k-anonymity, T-closeness and L-diversity and its implementation in business. There have been a number of privacy-preserving mechanisms developed for privacy protection at different stages (for example, data generation, data storage, and data processing) of a big data life cycle. The goal of this paper is to provide a major review of the privacy preservation mechanisms in big data and present the challenges for existing mechanisms. This paper also presents recent techniques of privacy preserving in big data like hiding a needle in a haystack, identity based anonymization, differential privacy, privacy-preserving big data publishing and fast anonymization of big data streams. This paper refer privacy and security aspects healthcare in big data. Comparative study between various recent techniques of big data privacy is also done as well. © 2016, The Author(s)."
"10.1186/s40537-015-0014-3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013871861&doi=10.1186%2fs40537-015-0014-3&partnerID=40&md5=47833db21ec6579cce7ee31c6552f276","Using a random sample consisting of hundreds of companies worldwide, we are testing the impact on company performance of investing in big data projects targeted on three major business domains (namely, customer interface, company supply chain and competitors). The performance test relies on a so-called trans-logarithmic production function, allowing for a more direct test of the complementarity between big data capital and big data labour investments; further, we have used a Heckman correction to adjust for the fact that companies investing in big data are generally more productive than their peers. We confirm and extend early results of a productivity impact from big data. We find that for the average of our sample, more productive firms are also faster adopters of big data than their industry peers (this explains 2.5% of productivity difference). Big data investments in labour and IT architecture are complements, with a total productivity growth effect of about 5.9%. Big data projects targeting customers and competitive intelligence domains bring slightly more performance than big data projects aimed at supply chain improvements. © 2015, Bughin."
"10.1007/s41019-016-0026-9","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065014031&doi=10.1007%2fs41019-016-0026-9&partnerID=40&md5=06516352d7aa48a061495a9a446f3fdb","With the proliferation of graph-based applications, such as social network management and Web structure mining, update-intensive graph databases have become an important component of today’s data management platforms. Several techniques have been recently proposed to exploit locality on both data organization and computational model in graph databases. However, little investigation has been conducted on buffer management of graph databases. To the best of our knowledge, current buffer managers of graph databases suffer performance loss caused by unnecessary random I/O access. To solve this problem, we develop a novel batch replacement policy for buffer management. This policy enables us to maximally exploit sequential I/O to improve the performance of graph database. However, trivial solution produces impractical maintenance for replacement plan with maximal sequential I/O. To enable the policy, we first devise a segment tree-based buffer manager to efficiently maintain a optimal replacement plan. Unfortunately, segment tree-based solution becomes bottleneck in multi-core environment. To remedy this weakness, a B-tree-based buffer manager is further proposed. Extensive experiments on real-world and synthetic datasets demonstrate the superiority of our method. © 2017, The Author(s)."
"10.1007/s41019-016-0027-8","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064245210&doi=10.1007%2fs41019-016-0027-8&partnerID=40&md5=99c3de57cb9fa3a33445cf7de48be22a","Given a set of objects and a query q, a point p is q’s Reverse k Nearest Neighbour (RkNN) if q is one of p’s k-closest objects. RkNN queries have received significant research attention in the past few years. However, we realize that the state-of-the-art algorithm, SLICE, accesses many objects that do not contribute to its RkNN results when running the filtering phase, which deteriorates the query performance. In this paper, we propose a novel RkNN algorithm with pre-computation by partitioning the data space into disjoint rectangular regions and constructing the guardian set for each region R. We guarantee that, for each q that lies in R, its RkNN results are only affected by the objects in R’s guardian set. The advantage of this approach is that the results of a query q∈ R can be computed by using SLICE on only the objects in its guardian set instead of using the whole dataset. Besides, we raise two new useful variants of RkNN and propose algorithms. Our comprehensive experimental study on synthetic and real the proposed approaches are the most efficient algorithms for RkNN and its variants. © 2017, The Author(s)."
"10.1007/s41019-016-0028-7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057236911&doi=10.1007%2fs41019-016-0028-7&partnerID=40&md5=2e139f24ca521012516bc4d6bcd84a09","Finding interesting tree patterns hidden in large datasets is a central topic in data mining with many practical applications. Unfortunately, previous contributions have focused almost exclusively on mining-induced patterns from a set of small trees. The problem of mining homomorphic patterns from a large data tree has been neglected. This is mainly due to the challenging unbounded redundancy that homomorphic tree patterns can display. However, mining homomorphic patterns allows for discovering large patterns which cannot be extracted when mining induced or embedded patterns. Large patterns better characterize big trees which are important for many modern applications in particular with the explosion of big data. In this paper, we address the problem of mining frequent homomorphic tree patterns from a single large tree. We propose a novel approach that extracts non-redundant maximal homomorphic patterns. Our approach employs an incremental frequency computation method that avoids the costly enumeration of all pattern matchings required by previous approaches. Matching information of already computed patterns is materialized as bitmaps, a technique that not only minimizes the memory consumption, but also the CPU time. Our contribution also includes an optimization technique which can further reduce the search space of homomorphic patterns. We conducted detailed experiments to test the performance and scalability of our approach. The experimental evaluation shows that our approach mines larger patterns and extracts maximal homomorphic patterns from real and synthetic datasets outperforming state-of-the-art embedded tree mining algorithms applied to a large data tree. © 2017, The Author(s)."
"10.1007/s41019-017-0032-6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046573817&doi=10.1007%2fs41019-017-0032-6&partnerID=40&md5=ed27725bfebcfc133d17a209b86a2da2","Item recommendation helps people to discover their potentially interested items among large numbers of items. One most common application is to recommend top-n items on implicit feedback datasets (e.g., listening history, watching history or visiting history). In this paper, we assume that the implicit feedback matrix has local property, where the original matrix is not globally low rank but some sub-matrices are low rank. In this paper, we propose Local Weighted Matrix Factorization (LWMF) for top-n recommendation by employing the kernel function to intensify local property and the weight function to model user preferences. The problem of sparsity can also be relieved by sub-matrix factorization in LWMF, since the density of sub-matrices is much higher than the original matrix. We propose a heuristic method to select sub-matrices which approximate the original matrix well. The greedy algorithm has approximation guarantee of factor 1-1e to get a near-optimal solution. The experimental results on two real datasets show that the recommendation precision and recall of LWMF are both improved about 30% comparing with the best case of weighted matrix factorization (WMF). © 2017, The Author(s)."
"10.1007/s41019-017-0033-5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044459849&doi=10.1007%2fs41019-017-0033-5&partnerID=40&md5=b538abbbf3aabe56ac876879cb6d8b29","In a wide variety of emerging data-intensive applications, such as social network analysis, Web document clustering, entity resolution, and detection of consistently co-expressed genes in systems biology, the detection of dense subgraphs (cliques) is an essential component. Unfortunately, this problem is NP-Complete and thus computationally intensive at scale—hence there is a need for efficient processing, as well as the techniques for distributing the computation across multiple machines such that the computation, which is too time-consuming on a single machine, can be efficiently performed on a machine cluster given that it is large enough. In this paper, we propose a new algorithm (called GP) for maximal clique enumeration. It identifies cliques by the operation of binary graph partitioning, which iteratively divides a graph until each task is sufficiently small to be processed in parallel. Given a connected graph G= (V, E) , the GP algorithm has a space complexity of O(|E|) and a time complexity of O(| E| μ(G)) , where μ(G) represents the number of different cliques existing in G. We also present a hybrid algorithm, which can effectively leverage the advantages of both the GP algorithm and the classical Bron-and-Kerbosch (BK) algorithm. Then, we develop corresponding parallel solutions based on the GP and hybrid algorithms. Finally, we evaluate the performance of the proposed solutions on real and synthetic graph data. Our extensive experiments show that in both centralized and parallel setting, our proposed GP and hybrid approaches achieve considerably better performance than the state-of-the-art BK approach. Our parallel solutions are implemented and evaluated on MapReduce, a popular shared-nothing parallel framework, but can easily generalize to other shared-nothing or shared-memory parallel frameworks. © 2017, The Author(s)."
"10.1007/s41019-016-0022-0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029474107&doi=10.1007%2fs41019-016-0022-0&partnerID=40&md5=5081ddc37ba2727c2cf1fd4097f8a6b9","Research on big data analytics is entering in the new phase called fast data where multiple gigabytes of data arrive in the big data systems every second. Modern big data systems collect inherently complex data streams due to the volume, velocity, value, variety, variability, and veracity in the acquired data and consequently give rise to the 6Vs of big data. The reduced and relevant data streams are perceived to be more useful than collecting raw, redundant, inconsistent, and noisy data. Another perspective for big data reduction is that the million variables big datasets cause the curse of dimensionality which requires unbounded computational resources to uncover actionable knowledge patterns. This article presents a review of methods that are used for big data reduction. It also presents a detailed taxonomic discussion of big data reduction methods including the network theory, big data compression, dimension reduction, redundancy elimination, data mining, and machine learning methods. In addition, the open research issues pertinent to the big data reduction are also highlighted. © 2016, The Author(s)."
"10.1057/s41270-016-0009-8","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031507563&doi=10.1057%2fs41270-016-0009-8&partnerID=40&md5=23e154012c9f08cc57e686fa5e5da0e2","The paper aims to broaden the knowledge regarding consumer attitudes towards global brands and to determine whether these attitudes can be ‘used’ as segmentation criteria. The paper, firstly, identifies a number of dimensions that have been used to analyse consumer perceptions of global brands; secondly, a cross-country analysis carried out in three countries examines the selected factors related to both national and cultural consumer context and to consumer brand equity. Through a multiple correspondence analysis and a cluster analysis, the paper identifies distinct and fresh groups of consumers which are good indicators of existing market trends. © 2016 Macmillan Publishers Ltd."
"10.1057/s41270-016-0007-x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031504656&doi=10.1057%2fs41270-016-0007-x&partnerID=40&md5=5c97c167c4cb67e6e9a8d9c6c2e9680c","In the challenging environment of the transparent electronic marketplace in which competitors are only a click away, Web retailers are particularly vulnerable to customer attrition. Central to business growth and survival, customer retention is an important issue that every business strives to understand and harness. While some studies have attempted to determine the factors that influence customer retention, few measure it quantitatively. However, businesses have long been eager to have quantitative information concerning their customer base: How many of their customers they can consider retained at any given time? What time lapse should trigger an alert that the customer may have defected? Based on real purchasing data from a Web retailer, and using 80 percentage of assurance as an example, this paper proposes a customer retention assessment method by calculating the aggregate 80th percentile of maximum inter-purchase times and confirms the validity of this method by showing that the assessment successfully sets apart valuable customers, in terms of number of orders, average spending per order, and total spending. This research not only enables researchers to undertake longitudinal studies of customer repatronage behavior, but also helps practitioners monitor customer retention effectively. © 2016 Macmillan Publishers Ltd."
"10.1057/s41270-016-0008-9","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031497571&doi=10.1057%2fs41270-016-0008-9&partnerID=40&md5=53b0274e308eb3e1841beb05d85140c7","The area of latent need revelation and activation can lead to enhanced product/ brand differentiation and to a strong receptivity to advertising messages. This study thus focuses on approaches for latent need revelation with a particular emphasis on consumer complaining behavior through problem detection analysis (PDA). The work reveals insights into decision modes favoring PDA, methodologies, successes, and issues with implementation, especially the need for trade-off analyses. It also addresses the relationship between benefits and problems, as well as problems and their frequency of occurrence, including the ability of solved problems to address brand loyalty issues. Overall, the study provides unique observations on the tools and techniques that are especially suited for latent need revelation. © 2016 Macmillan Publishers Ltd."
"10.1057/s41270-016-0005-z","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031493593&doi=10.1057%2fs41270-016-0005-z&partnerID=40&md5=9997eafced7cf82b7621829647b6641f","Market coverage is an important attribute for determining the success of a product. The larger the market covered by a product is, the higher the amount of sales for that product will be. Market coverage strategies thus contribute to the success of a product in tapping the market. In this study, we emphasize the impact of market coverage on the rate of adoption in determining product sales. New product diffusion models based on the market covered are proposed. A methodological approach of weighted criteria is implemented to evaluate and rank the proposed models. The analysis is conducted on real-life sales datasets. © 2016 Macmillan Publishers Ltd."
"10.1080/23270012.2016.1229140","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052834877&doi=10.1080%2f23270012.2016.1229140&partnerID=40&md5=48da9663e89c63305e4410b7df77c089","In this paper we develop a framework to support the introduction of renewable energy generation and carbon emission constraints into a defined electric power network, and the key operational decisions regarding its configuration. We describe and model the major components of a hybrid renewable energy system (HRES), including renewable energy sources (solar and wind), fossil fuel generators, transmission/distribution, power storage, energy markets, and end-customer demand. Our methodology involves a conceptual diagram notation for power network topology, combined with a formal mathematical model that describes the HRES optimization framework. We introduce environmental goals as constraints to the model, based on emissions restrictions dictated by a policy-maker extraneous to the model. We then proceed to implement the HRES optimization problem solution through a mixed-integer linear programming (MILP) model by leveraging IBM Optimization Programming Language (OPL) CPLEX Studio. Lastly, we develop a proof-of-concept to demonstrate the feasibility of the model. © 2016, © 2016 Antai College of Economics and Management, Shanghai Jiao Tong University."
"10.1080/23270012.2016.1239228","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042075172&doi=10.1080%2f23270012.2016.1239228&partnerID=40&md5=eee53d17cecd04a40af53d1dd7351461","Today businesses are facing radical transformations through digitalization of services and products. Accordingly, their ability to innovate is increasingly linked to the capacity to innovate through information and communication technologies (ICTs). This article investigates the role of information systems (IS) as a key factor for innovation capacity. To this end, the article discusses an interpretive framework for understanding the degree of capacity of innovation through information systems (IS) reached by a given company and the contradictions that bound its evolution. An interpretive study is also presented, where the framework has been applied to seven French companies from various industries. Consistently with the framework, the interviews address process areas and practices related to three core categories: management, innovation engineering and support. The study reveals seven fundamental contradictions that can explain the main tendencies observed across the companies. © 2016, © 2016 Antai College of Economics and Management, Shanghai Jiao Tong University."
"10.4018/IJBAN.2016100101","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046252325&doi=10.4018%2fIJBAN.2016100101&partnerID=40&md5=211355c06849127319eca9b01fa85378","The number of educational courses offered online is growing, with students often having no choice for alternative formats. However, personal characteristics may affect online academic performance. In this study, the authors apply two business analytics methods - multiple linear/polynomial regression and generalized additive modeling (GAM) - to predict online student performance based on six personal characteristics. These characteristics are: communication aptitude, desire to learn, escapism, hours studied, gender, and English as a Second Language. Survey data from 168 students were partitioned into training/validation sets and the best fit models from the training data were tested on the validation data. While the regression method outdid the GAM at predicting student performance overall, the GAM explained the performance behavior better over various predictor intervals using natural splines. The study confirms the usefulness of business analytics methods and presents implications for college administrators and faculty to optimize individual student online learning. Copyright © 2016, IGI Global."
"10.4018/IJBAN.2016100102","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85037342886&doi=10.4018%2fIJBAN.2016100102&partnerID=40&md5=3fb6ca10d014433516ce658253ae8122","Clustering techniques typically group similar instances underlying individual attributes by supposing that similar instances have similar attributes characteristic. On contrary, clustering similar instances given a specific behavior is framed through supervised learning. For instance, which fashion products have similar behavior in term of sales. Unfortunately, conventional clustering methods cannot tackle this case, since they handle attributes by a same manner. In fact, conventional clustering approaches do not consider any response, and moreover they assume attributes act by the same importance. However, clustering instances with respect to responses leads to a better data analytics. In this research, the authors introduce an approach for the goal supervised clustering and show its advantage in terms of data analytics as well as prediction. To verify the feasibility and the performance of this approach the authors conducted several experiments on a real dataset derived from an apparel industry. Copyright © 2016, IGI Global."
"10.1089/big.2016.0015","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988663636&doi=10.1089%2fbig.2016.0015&partnerID=40&md5=88c3d98cde329fbf98c5497e1dbb4e29","Synthetic data are becoming increasingly important mechanisms for sharing data among collaborators and with the public. Multiple methods for the generation of synthetic data have been proposed, but many have short comings with respect to maintaining the statistical properties of the original data. We propose a new method for fully synthetic data generation that leverages linear and integer mathematical programming models in order to match the moments of the original data in the synthetic data. This method has no inherent disclosure risk and does not require parametric or distributional assumptions. We demonstrate this methodology using the Framingham Heart Study. Existing synthetic data methods that use chained equations were compared with our approach. We fit Cox proportional hazards, logistic regression, and nonparametric models to synthetic data and compared with models fitted to the original data. True coverage, the proportion of synthetic data parameter confidence intervals that include the original data's parameter estimate, was 100% for parametric models when up to four moments were matched, and consistently outperformed the chained equations approach. The area under the curve and accuracy of the nonparametric models trained on synthetic data marginally differed when tested on the full original data. Models were also trained on synthetic data and a partition of original data and were tested on a held-out portion of original data. Fourth-order moment matched synthetic data outperformed others with respect to fitted parametric models but did not always outperform other methods with fitted nonparametric models. No single synthetic data method consistently outperformed others when assessing the performance of nonparametric models. The performance of fourth-order moment matched synthetic data in fitting parametric models suggests its use in these cases. Our empirical results also suggest that the performance of synthetic data generation techniques, including the moment matching approach, is less stable for use with nonparametric models. The benefits of the moment matching approach should be weighed against additional computational costs. In summary, our results demonstrate that the introduced moment matching approach may be considered as an alternative to existing synthetic data generation methods. © 2016, Mary Ann Liebert, Inc. 2016."
"10.1089/big.2016.0026","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988662288&doi=10.1089%2fbig.2016.0026&partnerID=40&md5=34700317194d42f0d203f39eff662875","Multiaspect data are ubiquitous in modern Big Data applications. For instance, different aspects of a social network are the different types of communication between people, the time stamp of each interaction, and the location associated to each individual. How can we jointly model all those aspects and leverage the additional information that they introduce to our analysis? Tensors, which are multidimensional extensions of matrices, are a principled and mathematically sound way of modeling such multiaspect data. In this article, our goal is to popularize tensors and tensor decompositions to Big Data practitioners by demonstrating their effectiveness, outlining challenges that pertain to their application in Big Data scenarios, and presenting our recent work that tackles those challenges. We view this work as a step toward a fully automated, unsupervised tensor mining tool that can be easily and broadly adopted by practitioners in academia and industry. © 2016, Mary Ann Liebert, Inc. 2016."
"10.1089/big.2016.0017","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988660867&doi=10.1089%2fbig.2016.0017&partnerID=40&md5=e34b3c865bfe7ba65385c5d6da08f433","The availability of electronic health records creates fertile ground for developing computational models of various medical conditions. We present a new approach for detecting and analyzing patients with unexpected responses to treatment, building on machine learning and statistical methodology. Given a specific patient, we compute a statistical score for the deviation of the patient's response from responses observed in other patients having similar characteristics and medication regimens. These scores are used to define cohorts of patients showing deviant responses. Statistical tests are then applied to identify clinical features that correlate with these cohorts. We implement this methodology in a tool that is designed to assist researchers in the pharmaceutical field to uncover new features associated with reduced response to a treatment. It can also aid physicians by flagging patients who are not responding to treatment as expected and hence deserve more attention. The tool provides comprehensive visualizations of the analysis results and the supporting data, both at the cohort level and at the level of individual patients. We demonstrate the utility of our methodology and tool in a population of type II diabetic patients, treated with antidiabetic drugs, and monitored by the HbA1C test. © 2016, Mary Ann Liebert, Inc. 2016."
"10.1016/j.bdr.2016.08.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988411270&doi=10.1016%2fj.bdr.2016.08.001&partnerID=40&md5=947fc16db4cf1fa232464da45bf57d19","We demonstrate an Efficient Location-Aware aNalytics system (ELAN), aiming to provide users with location-aware data analytics services. For each user-selected spatial region, ELAN can instantly identify the most important functionality features of the region (e.g., business zones and residential areas) by efficiently analyzing the user-generated content (UGC) within the region. For each feature, ELAN can efficiently calculate the spatial boundary of the functional zone (denoted by a convex hull) in order to help users better understand the feature and furthermore we can identify the influential range of a certain feature. ELAN has many real-world applications, e.g., choosing business locations and popular regions discovery. There are two main challenges in designing a location-aware data analytics system. The first is to achieve high performance, as the region may contain a large amount of location-based UGC data. The second is to support continuous queries as users may continuously change the region by zooming in, zooming out, and panning the map. To address these challenges, we propose effective spatio-textual indexes and efficient incremental algorithms to support instant location-aware data analytics. We have implemented and deployed a system, which has been commonly used and widely accepted. © 2016"
"10.1016/j.bdr.2016.02.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84975451590&doi=10.1016%2fj.bdr.2016.02.002&partnerID=40&md5=8a6fff7c2c9122b57077aa108606b713","Machine Learning (ML) is a powerful tool that can be used to make predictions on the future nature of data based on the past history. ML algorithms operate by building a model from input examples to make data-driven predictions or decisions for the future. The growing concept “Big Data” has brought much success in the field of data science; it provides data scalability in a variety of ways that empower data science. ML can also be used in conjunction with Big Data to build effective predictive systems or to solve complex data analytic problems. In this work, we propose an electricity generation forecasting system that could predict the amount of power required at a rate close to the electricity consumption for the United States. The proposed scheme uses Big Data analytics to process the data collected on power management in the past 20 years. Then, it applies a ML model to train the system for the prediction stage. The model can forecast future power generation based on the collected data, and our test results show that the proposed system can predict the required power generation close to 99% of the actual usage. Our results indicate that the ML with Big Data can be integrated in forecasting techniques to improve the efficiency and solve complex data analytic problems existing in the power management systems. © 2016 Elsevier Inc."
"10.1016/j.bdr.2015.12.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84957837140&doi=10.1016%2fj.bdr.2015.12.001&partnerID=40&md5=ce7c6d0790493f37bd44814daa4f3e52","Classification with imbalanced class distributions is a major problem in machine learning. Researchers have given considerable attention to the applications in many real-world scenarios. Although several works have utilized the area under the receiver operating characteristic (ROC) curve to select potentially optimal classifiers in imbalanced classifications, limited studies have been devoted to finding the classification threshold for testing or unknown datasets. In general, the classification threshold is simply set to 0.5, which is usually unsuitable for an imbalanced classification. In this study, we analyze the drawbacks of using ROC as the sole measure of imbalance in data classification problems. In addition, a novel framework for finding the best classification threshold is proposed. Experiments with SCOP v.1.53 data reveal that, with the default threshold set to 0.5, our proposed framework demonstrated a 20.63% improvement in terms of F-score compared with that of more commonly used methods. The findings suggest that the proposed framework is both effective and efficient. A web server and software tools are available via http://datamining.xmu.edu.cn/prht/ or http://prht.sinaapp.com/. © 2016 Elsevier Inc."
"10.1007/s41019-016-0021-1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051473314&doi=10.1007%2fs41019-016-0021-1&partnerID=40&md5=ebaf52a4c28de3761917a31c3ab2e5cd",[No abstract available]
"10.1007/s41019-016-0016-y","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049178356&doi=10.1007%2fs41019-016-0016-y&partnerID=40&md5=d602ed7cad7018bd11342f057a6591c1","Data Integration Systems (DIS) are concerned with integrating data from multiple data sources to resolve user queries. Typically, organisations providing data sources specify security policies that impose stringent requirements on the collection, processing, and disclosure of personal and sensitive data. If the security policies were not correctly enforced by the integration component of DIS, the data is exposed to data leakage threats, e.g. unauthorised disclosure or secondary use of the data. SecureDIS is a framework that helps system designers to mitigate data leakage threats during the early phases of DIS development. SecureDIS provides designers with a set of informal guidelines written in natural language to specify and enforce security policies that capture confidentiality, privacy, and trust properties. In this paper, we apply a formal approach to model a DIS with the SecureDIS security policies and verify the correctness and consistency of the model. The model can be used as a basis to perform security policies analysis or automatically generate a Java code to enforce those policies within DIS. © 2016, The Author(s)."
"10.1007/s41019-016-0015-z","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048860113&doi=10.1007%2fs41019-016-0015-z&partnerID=40&md5=f5dcd5ec50551e3b417dddd22d1d33bf","The recent years have seen the birth of several NoSQL datastores, which are getting more and more popularity for their ability to handle high volumes of heterogeneous and unstructured data in a very efficient way. In several cases, NoSQL databases proved to outclass in terms of performance, scalability, and ease of use relational database management systems, meeting the requirements of a variety of today ICT applications. However, recent surveys reveal that, despite their undoubted popularity, NoSQL datastores suffer from some weaknesses, among which the lack of effective support for data protection appears among the most serious ones. Proper data protection mechanisms are therefore required to fill this void. In this work, we start to address this issue by focusing on access control and discussing the definition of a fine-grained access control framework for document-oriented NoSQL datastores. More precisely, we first focus on issues and challenges related to the definition of such a framework, considering theoretical, implementation, and integration aspects. Then, we discuss the reasons for which state-of-the-art fine-grained access control solutions proposed for relational database management systems cannot be used within the NoSQL scenario. We then introduce possible strategies to address the identified issues, which are at the basis of the framework development. Finally, we shortly report the outcome of an experience where the proposed framework has been used to enhance the data protection features of a popular NoSQL database. © 2016, The Author(s)."
"10.1007/s41019-016-0018-9","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85034094623&doi=10.1007%2fs41019-016-0018-9&partnerID=40&md5=4242338803f56c040c122d4acb5ef899","With the growing popularity of cloud computing, more and more enterprises and individuals tend to store their sensitive data on the cloud in order to reduce the cost of data management. However, new security and privacy challenges arise when the data stored in the cloud due to the loss of data control by the data owner. This paper focuses on the techniques of verifiable data storage and secure data deduplication. We firstly summarize and classify the state-of-the-art research on cloud data storage mechanism. Then, we present some potential research directions for secure data outsourcing. © 2016, The Author(s)."
"10.1007/s41019-016-0020-2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027276871&doi=10.1007%2fs41019-016-0020-2&partnerID=40&md5=5e1cd9c9f6399ca6faaa49c569daeceb","The main goal of a personalized recommender system is to provide useful recommendations on various items to the users. In order to generate recommendations, the service needs to access various types of user data such as previous product purchasing history, demographic and biographical information. However, users are sensitive to disclosure of personal information as it can be easily misused by malicious third parties. Consequently, there are unavoidable security concerns which will become known through attempted unauthorized access while providing the recommendation services. In order to protect against breaches of personal information, it is necessary to obfuscate the user information by means of an efficient encryption technique while simultaneously generating the recommendation by making true information inaccessible to the system. To address these challenges, we propose a privacy-preserving recommender system using homomorphic encryption, by which the system can provide recommendations without knowing the actual ratings. Our approach is based on the ElGamal cryptosystem by which both addition and multiplication of plaintexts can be performed. The performance of the proposed scheme shows significantly high accuracy in-terms of computation and communication costs as well as outperforming other existing solutions. © 2016, The Author(s)."
"10.1007/s41019-016-0014-0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021987233&doi=10.1007%2fs41019-016-0014-0&partnerID=40&md5=fdf4656e967bb246fdbafd0bffce53d0","Cloud storage services have become ubiquitous. A large number of individuals and organizations are using them to store and share data, taking the benefits of mobility and affordability offered by these services. However, secure management of data in cloud storage services, more specifically supporting multi-party sharing in the context of a collaboration, is a challenging problem. The problem is further exacerbated if the data owner does not have any trust on the cloud storage providers and the data need regular updates from collaborating parties. A number of cryptographically enforced secure cloud storage solutions have been proposed to address this problem. One of the key issues with these solutions is the revocation of access to data for invalid users without moving the data (in the era of big data) and relying on the cloud service providers. In this paper, we introduce a cloud storage system that offers cryptographically enforced security. In contrast to other cryptographically protected cloud storage systems, our system supports a fine-grained access control mechanism and allows flexible revocations of invalid users without moving the data and relying on the cloud service providers. Our system employs an attribute-based encryption technique to support a complex access structure that allows a user to define human readable access policies to the data in the cloud storage. In addition, our system supports a flexible revocation scheme that can revoke invalid users directly by updating the revoked users’ list or indirectly by updating an epoch counter. The system administrator can choose one of these options flexibly depending on the needs. Our system also allows authorized users to update the encrypted data, and any users accessing such updated data in future can verify whether the data are modified by authorized users. © 2016, The Author(s)."
"10.1007/s41019-016-0017-x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020182359&doi=10.1007%2fs41019-016-0017-x&partnerID=40&md5=187b58cacd88a809296cdabc18f7a267","In wireless sensor networks (WSNs), provenance records the data source, forwarding, and aggregating information of data packets on their way to the base station. Provenance is critical for assessing the trustworthiness of the received data, diagnosing network failures, detecting early signs of attacks, etc. However, because the provenance size expands rapidly with the increase in packet transmission hops, the provenance schemes developed for use in wired computer networks are not generally applicable to WSNs. Therefore, specific provenance techniques have been developed for WSNs that take into account the constrained resources of sensor nodes. In this paper, we survey such techniques. Special focus in the paper is devoted to a systematic and comprehensive classification of the solutions proposed in the literature. We review each solution by highlighting its pros and cons. Finally, we discuss recent trends in provenance encoding schemes for WSNs. © 2016, The Author(s)."
"10.1080/23270012.2016.1183237","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013257716&doi=10.1080%2f23270012.2016.1183237&partnerID=40&md5=d67bd83bc6d7646bd8e095beca4c90ef","This paper explores a fuzzy analytic network process (FANP) approach in identifying the content of manufacturing strategy infrastructural decisions that integrates sustainability and classical manufacturing strategy framework with firm size component. The findings are as follows: (1) firms must ensure high-quality product requirements with a make-to-order production system; (2) large firms must prioritize decentralization of functional areas but they must also provide higher systems integration of all production-related information to mobilize better communication channels; (3) large firms must employ highly skilled human resources in order to provide strong support on functional areas; and (4) small and medium enterprises must focus on product introduction to the market as a strategy priority area. The main contribution of this paper is the decision-making framework under uncertainty that holistically identifies the decisions that comprise a sustainable manufacturing strategy which may serve as guidelines in improving existing sustainable manufacturing practices or in creating new ones. © 2016, © 2016 Antai College of Economics and Management, Shanghai Jiao Tong University."
"10.1080/23270012.2016.1197052","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85010535982&doi=10.1080%2f23270012.2016.1197052&partnerID=40&md5=14e4a953baac82c943fed7a633a74301","Strategic innovation diffusion converts newly created knowledge into increasing a firm’s value primarily through innovative product offerings. In this paper, we present a time-based adoption pattern with pricing and promotional expenditure as a three-dimensional innovation diffusion model (3D-IDM). In our proposed 3D-IDM, we assume that value of the product plays a crucial role of being the major driver of diffusion, and is classified into the following three main factors: (1) continuation time of the product in the market–representing goodwill of the product; (2) price of the product–indicating consumers’ buying behaviour; and (3) marketing efforts of the firm. A special form of the Cobb–Douglas production function is used to design the three-dimensional framework. An empirical study is performed on number of consumer-durable sales data to validate and compare the proposed model. Various performance measures are treated uniquely using the Mahalanobis distance-based approach (DBA) to determine the relative strength of each model. © 2016, © 2016 Antai College of Economics and Management, Shanghai Jiao Tong University."
"10.1057/s41270-016-0002-2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031507234&doi=10.1057%2fs41270-016-0002-2&partnerID=40&md5=689fe528558152a441f9abc0ebe9357c","Balancing customization or standardization of service is one of the major concerns of today’s service industry. A high level of customization might delight the customer but is usually associated with higher cost, longer waiting time with higher customer involvement in the service delivery process unlike standardization. The study attempts to develop an instrument to measure the level of customization–standardization of service and its relationship with loyalty. For this purpose, the measurement with 29 five Likert scale questions related to customization–standardization continuum scale and loyalty has been developed. Three industries; education, hospitality, and health care, have been selected. Overall 350 questionnaires were distributed based on non-probability convenience sampling. The result of the EFA categorized the scale items into three factors namely: Service Resource, Service Process, and Service Setup of which will be the focus measurement for both the level of customization and standardization. © 2016 Macmillan Publishers Ltd."
"10.1057/s41270-016-0004-0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031505476&doi=10.1057%2fs41270-016-0004-0&partnerID=40&md5=7fa13aa6867802da7a1c5c611ce96025","This study provides an understanding towards linkage between consumers’ purchase behavior attributes and store attractiveness. It adds significance to store location decision strategy by undertaking the study in two agglomerations of street shopping area and a shopping mall. Both agglomerations involve clustering of similar stores but have different geographic locations. The study in its first step identifies attributes which influence an individual to purchase from a particular agglomeration. Secondly, the impact of these identified attributes was studied on store attractiveness of both agglomerations. Store attractiveness was measured in terms of time being spent and intention to revisit an agglomeration. The results suggested that in addition to depth and variety of stores, merchandise characteristics such as price of merchandise and accessibility influence an individuals’ store selection decision. It was found that the degree of influence varies with type of agglomeration. © 2016 Macmillan Publishers Ltd."
"10.1057/s41270-016-0003-1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031496914&doi=10.1057%2fs41270-016-0003-1&partnerID=40&md5=e1e27a5ac9eeebcc77ec3f7511812a61","This paper explores the use of multilevel modelling to provide a statistical framework for geodemographic analysis. It argues that combining a neighbourhood classification with a modelling approach allows the levels of the geodemographic hierarchy to be considered simultaneously, identifying those that are most appropriate to the analysis and allowing the apparent differences between neighbourhood types to be considered in regard to their statistical significance, and to the uncertainty of the estimates. The paper shows how the model can be extended to create a cross-classified multiscale model that makes better use of the locational information available to improve the efficiency of the neighbourhood targeting. The ideas are illustrated with a case study using a sample of data and the freely available London Output Area Classification to predict which neighbourhoods in London have the highest percentages of Asian school pupils. The multiscale model is shown to outperform the predictions made using geodemographics alone. © 2016 Macmillan Publishers Ltd."
"10.1057/s41270-016-0006-y","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029077395&doi=10.1057%2fs41270-016-0006-y&partnerID=40&md5=95e7dae177c9c56bbeb3f1ee32b0cb14","Prior research involving promotional response models has largely overlooked the impact of time limits on customer response. We address this issue by developing a pricepromotion model in which time limits are conceptualized not only in terms of their effects on customer awareness but also customer urgency. Further, the proposed model incorporates recent insights from behavioral research which indicates that time limits can either enhance or diminish the effects of discount incentives. A key advantage of our approach is that it places minimal demands on managers’ resources, while also leveraging their unique knowledge of the market. In collaboration with an online retailer who frequently uses price promotions, we examine a real-world promotion and illustrate the insights facilitated by the model. In this empirical context, urgency declines with longer time limits but it does so less steeply when accompanied by larger discounts. Additionally, the managers learned that they tend to use time limits that are too long and discounts that are too small, resulting in foregone profits. We conclude by demonstrating the value of sensitivity analyses for understanding how profits can vary with changes in the environment and with differences in the accuracy of managers’ knowledge about customer responsiveness. © 2016 Macmillan Publishers Ltd."
"10.4018/IJBAN.2016070101","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046284857&doi=10.4018%2fIJBAN.2016070101&partnerID=40&md5=1bac86cf3dbd19ef557b6787355ea541","Strategic decision-making is one of the most important criteria for organizational success. Therefore, it is vital to have a well-developed decision making process in place. However, one of the greatest challenges facing organizations today is making important and timely business decisions. The focus of this study was to examine critical business intelligence input factors that influenced the decision making process. The business intelligence input factors considered were accessibility, reliability, quality of information, frequency of use, relevance, security, and quality of decisions. The results of this study show that the decision making process is very complex. Based on the analyses of the data, the findings indicate that these factors help determine reasons why managers use business intelligence technology in the decision making process. These findings will help organizations decision makers make better. This can improve the operational and strategic decision making process, thereby creating a competitive advantage for the organization.Analytics, Business Intelligence, Logistics, Management, Technology Copyright © 2016, IGI Global."
"10.4018/IJBAN.2016070103","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046269764&doi=10.4018%2fIJBAN.2016070103&partnerID=40&md5=655d3faf21894bcc6d06f896db606b1a","All textbooks and articles dealing with classical tests in the context of linear models stress the implications of a significantly large F-ratio since it indicates that the mean square for whatever effect is being evaluated contains significantly more than just error variation. In general, though, with one minor exception, all texts and articles, to the authors’ knowledge, ignore the implications of an F-ratio that is significantly smaller than one would expect due to chance alone. Why this is so difficult to explain since such an occurrence is similar to a range value falling below the lower limit on a control chart for variation or a p-value falling below the lower limit on a control chart for proportion defective. In both of those cases the small value represents an unusual and significant occurrence and, if valid, a process change that indicates an improvement. Therefore, it behooves the quality manager to determine what that change is in order to have it continue. In the case of a significantly small F-ratio some problem may be indicated that requires the designer of the experiment to identify it, and to take “corrective action.” While graphical procedures are available for helping to identify some of the possible problems that are discussed they are somewhat subjective when deciding if one is looking at an actual effect; e.g., interaction, or whether the result is merely due to random variation. A significantly small F-ratio can be used to support conclusions based on the graphical procedures by providing a level of statistical significance as well as serving as a warning flag or warning that problems may exist in the design and/or analysis. Copyright © 2016, IGI Global."
"10.4018/IJBAN.2016070102","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046263583&doi=10.4018%2fIJBAN.2016070102&partnerID=40&md5=8fef9c4092e0f855550b7c6fbb831bdb","Stock trading, used to predict the direction of future stock prices, is a dynamic business primarily based on human intuition. This involves analyzing some non-linear fundamental and technical stock variables which are recorded periodically. This study presents the development of an ANN-based prediction model for forecasting closing price in the stock markets. The major steps taken are identification of technical variables used for prediction of stock prices, collection and pre-processing of stock data, and formulation of the ANN-based predictive model. Stock data of periods between 2010 and 2014 were collected from the Nigerian Stock Exchange (NSE) and stored in a database. The data collected were classified into training and test data, where the training data was used to learn non-linear patterns that exist in the dataset; and test data was used to validate the prediction accuracy of the model. Evaluation results obtained from WEKA shows that discrepancies between actual and predicted values are insignificant. © 2016, IGI Global."
"10.4018/IJBAN.2016070104","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85011312684&doi=10.4018%2fIJBAN.2016070104&partnerID=40&md5=7f4b84e6335696702f1335bf107cd416","Today, mobile phone is an essential part of the lives of most people – a reason why, banking sector and mobile phone service providers have teamed up to provide banking services to customers via mobile phones. However, customers often are critical of the quality of such alternative financial delivery services provided by banks. The aim of this paper is to examine the underlying factors and service qualities that can influence customers’ behavioral intentions towards the use of mobile banking services in Bangladesh. The hybrid model in this study identifies service qualities like reliability, privacy, information quality, responsiveness and empathy that capture customers’ overall perceptions of the performance of mobile banking services. In addition, other factors like performance expectancy, effort expectancy and facilitating conditions significantly influence customers’ intention to use such services. Implications of these findings provide practical recommendations to banking industry, and directions for further work from the perspective of Bangladesh. Copyright © 2016, IGI Global."
"10.1089/big.2015.0059","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84991833472&doi=10.1089%2fbig.2015.0059&partnerID=40&md5=2252dc369a17fff05e7ab21c31844500","The digitization of a patient's health record has profoundly impacted medicine and healthcare. The compilation and accessibility of medical history has provided clinicians an unprecedented, holistic account of a patient's conditions, procedures, medications, family history, and social situation. In addition to the bedside benefits, this level of information has opened the door for population-level monitoring and research, the results of which can be used to guide initiatives that are aimed at improving quality of care. Cerner Corporation partners with health systems to help guide population management and quality improvement projects. With such an enormous and diverse client base - varying in geography, size, organizational structure, and analytic needs - discerning meaning in the data and how they fit with that particular hospital's goals is a slow, difficult task that requires clinical, statistical, and technical literacy. This article describes the development of dashboards for efficient data visualization at the healthcare facility level. Focusing on two areas with broad clinical importance, sepsis patient outcomes and 30-day hospital readmissions, dashboards were developed with the goal of aggregating data and providing meaningful summary statistics, highlighting critical performance metrics, and providing easily digestible visuals that can be understood by a wide range of personnel with varying levels of skill and areas of expertise. These internal-use dashboards have allowed associates in multiple roles to perform a quick and thorough assessment on a hospital of interest by providing the data to answer necessary questions and to identify important trends or opportunities. This automation of a previously manual process has greatly increased efficiency, saving hours of work time per hospital analyzed. Additionally, the dashboards have standardized the analysis process, ensuring use of the same metrics and processes so that overall themes can be compared across hospitals and health systems. © 2016, Mary Ann Liebert, Inc."
"10.1089/big.2015.0045","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84991777431&doi=10.1089%2fbig.2015.0045&partnerID=40&md5=6f1c0207626bd2332a1403ed9ca8c4ca","The availability of copious data about many human, social, and economic phenomena is considered an opportunity for the production of official statistics. National statistical organizations and other institutions are more and more involved in new projects for developing what is sometimes seen as a possible change of paradigm in the way statistical figures are produced. Nevertheless, there are hardly any systems in production using Big Data sources. Arguments of confidentiality, data ownership, representativeness, and others make it a difficult task to get results in the short term. Using Call Detail Records from Ivory Coast as an illustration, this article shows some of the issues that must be dealt with when producing statistical indicators from Big Data sources. A proposal of a graphical method to evaluate one specific aspect of the quality of the computed figures is also presented, demonstrating that the visual insight provided improves the results obtained using other traditional procedures. © 2016, Mary Ann Liebert, Inc."
"10.1089/big.2015.0056","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84991773817&doi=10.1089%2fbig.2015.0056&partnerID=40&md5=4fec07e736e7aca6022599b510b3c828","This work presents a systemic top-down visualization of Bitcoin transaction activity to explore dynamically generated patterns of algorithmic behavior. Bitcoin dominates the cryptocurrency markets and presents researchers with a rich source of real-time transactional data. The pseudonymous yet public nature of the data presents opportunities for the discovery of human and algorithmic behavioral patterns of interest to many parties such as financial regulators, protocol designers, and security analysts. However, retaining visual fidelity to the underlying data to retain a fuller understanding of activity within the network remains challenging, particularly in real time. We expose an effective force-directed graph visualization employed in our large-scale data observation facility to accelerate this data exploration and derive useful insight among domain experts and the general public alike. The high-fidelity visualizations demonstrated in this article allowed for collaborative discovery of unexpected high frequency transaction patterns, including automated laundering operations, and the evolution of multiple distinct algorithmic denial of service attacks on the Bitcoin network. © Dan McGinn et al. 2016; Published by Mary Ann Liebert, Inc."
"10.1089/big.2016.0007","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84991769817&doi=10.1089%2fbig.2016.0007&partnerID=40&md5=478bd8babb5cb243677af063dcd66aa7","Comprehensibility in modeling is the ability of stakeholders to understand relevant aspects of the modeling process. In this article, we provide a framework to help guide exploration of the space of comprehensibility challenges. We consider facets organized around key questions: Who is comprehending? Why are they trying to comprehend? Where in the process are they trying to comprehend? How can we help them comprehend? How do we measure their comprehension? With each facet we consider the broad range of options. We discuss why taking a broad view of comprehensibility in modeling is useful in identifying challenges and opportunities for solutions. © 2016, Mary Ann Liebert, Inc."
"10.1089/big.2015.0055","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84991753381&doi=10.1089%2fbig.2015.0055&partnerID=40&md5=bd75800ccb7084c343c8a6a3333a0218","Sunspots, colder areas that are visible as dark spots on the surface of the Sun, have been observed for centuries. Their number varies with a period of ∼11 years, a phenomenon closely related to the solar activity cycle. Recently, observation records dating back to 1749 have been reassessed, resulting in the release of a time series of sunspot numbers covering 266 years of observations. This series is analyzed using circular analysis to determine the periodicity of the occurrence of solar maxima. The circular analysis is combined with spiral graphs to provide a single visualization, simultaneously showing the periodicity of the series, the degree to which individual cycle lengths deviate from the average period, and differences in levels reached during the different maxima. This type of visualization of cyclic time series with varying cycle lengths in which significant events occur periodically is broadly applicable. It is aimed particularly at science communication, education, and public outreach. © 2016, Mary Ann Liebert, Inc."
"10.1089/big.2015.0057","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84987718199&doi=10.1089%2fbig.2015.0057&partnerID=40&md5=1b9bf4f812ac6dccc7413e560041437d","Translational medicine is a domain turning results of basic life science research into new tools and methods in a clinical environment, for example, as new diagnostics or therapies. Nowadays, the process of translation is supported by large amounts of heterogeneous data ranging from medical data to a whole range of -omics data. It is not only a great opportunity but also a great challenge, as translational medicine big data is difficult to integrate and analyze, and requires the involvement of biomedical experts for the data processing. We show here that visualization and interoperable workflows, combining multiple complex steps, can address at least parts of the challenge. In this article, we present an integrated workflow for exploring, analysis, and interpretation of translational medicine data in the context of human health. Three Web services - tranSMART, a Galaxy Server, and a MINERVA platform - are combined into one big data pipeline. Native visualization capabilities enable the biomedical experts to get a comprehensive overview and control over separate steps of the workflow. The capabilities of tranSMART enable a flexible filtering of multidimensional integrated data sets to create subsets suitable for downstream processing. A Galaxy Server offers visually aided construction of analytical pipelines, with the use of existing or custom components. A MINERVA platform supports the exploration of health and disease-related mechanisms in a contextualized analytical visualization system. We demonstrate the utility of our workflow by illustrating its subsequent steps using an existing data set, for which we propose a filtering scheme, an analytical pipeline, and a corresponding visualization of analytical results. The workflow is available as a sandbox environment, where readers can work with the described setup themselves. Overall, our work shows how visualization and interfacing of big data processing services facilitate exploration, analysis, and interpretation of translational medicine data. © Venkata Satagopam et al. 2016; Published by Mary Ann Liebert, Inc."
"10.1016/j.bdr.2016.05.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84989911362&doi=10.1016%2fj.bdr.2016.05.002&partnerID=40&md5=28230ef34a1ad6d4781d9fcc79260597","With the increasing volumes of information gathered via patient monitoring systems, physicians have been put on increasing pressure for making sophisticated analytical decisions that exploit the various types of data that is being gathered per patient. This phenomenon of continuously growing datasets is arising and gaining momentum in several application domains to what is now recognized in the business community as the Big Data challenge. In this article, we define and discuss some of the major challenges in the healthcare systems which can be effectively tackled by the recent advancement in ICT technologies. In particular, we focus on sensing technologies, cloud of computing, internet-of-things and big data analytics systems as emerging technologies which are made possible by the remarkable progress in various aspects including network communication speed, computational capabilities and data storage capacities that provide various advantages and characteristics that can contribute towards improving the efficiency and effectiveness of healthcare services. In addition, we describe the architectural components of our proposed framework, SmartHealth, for big data analytics services and describe its various applications in the healthcare domain. © 2016 Elsevier Inc."
"10.1016/j.bdr.2016.05.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84989875263&doi=10.1016%2fj.bdr.2016.05.001&partnerID=40&md5=243e5700ec6ff08a35b84c7fb3b12ffe","The stock market is influenced by several factors, such as macroeconomics, regulatory, purely speculative ones, and many others. However, one of the most relevant and meaningful is the general opinion and the overall investors’ sentiment, i.e., what investors think about a certain firm and, as a consequence, about the relative stock. This investors’ sentiment is here proxied by the Twitter content, and the study sums up to the recent outbreak of works that exploit sentiment analysis and Twitter data for stock market predictions. The sample analyzed concerns three major technology companies over a two-months period, on a minute basis. Using microblogging activities and a scoring algorithm for each tweet, it was possible to formulate interesting forecasting models identifying a new set of variables and indicators of the stock market future movements. A selection model has been used to implement the study, and the evidences found were encouraging, since it has been possible to draw the conclusion that this new source of data may increase the explanatory power of financial forecasting models. More in detail, it looks like that the average sentiment associated to any tweet is not so relevant as expected in prediction terms, while the posting volume has a greater forecasting power and it could be used to augment the models. Although this kind of analysis are becoming mainstream and quite common, this work represents an interesting case study for the technological sector rather than advancing fundamental new techniques in the field. © 2016 Elsevier Inc."
"10.1016/j.bdr.2016.04.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84989859380&doi=10.1016%2fj.bdr.2016.04.002&partnerID=40&md5=b224ac952dbb614752c178c784df9ecb","A large number of specialized graph processing systems have been developed to cope with the increasing demand of graph analytics. Most of them require users to deploy a new framework in the cluster for graph processing and switch to other systems to execute non-graph algorithms. This increases the complexity of cluster management and results in unnecessary data movement and duplication. In this paper, we propose our graph processing engine, named epiCG, which is built on top of epiC, an elastic data processing system. The core of epiCG is a new unit called GraphUnit, which is able to not only perform iterative graph processing efficiently, but also collaborate with other types of units to accomplish any complex/multi-stage data analytics. epiCG supports both edge-cut and vertex-cut partitioning methods, and for the latter method, we propose a novel light-weight greedy strategy that enables all the GraphUnits to generate vertex-cut partitioning in parallel. Furthermore, unlike existing graph processing systems, failure recovery in epiCG is completely automatic. We compare epiCG with several prevalent graph processing systems via extensive experiments with real-life dataset and applications. The results show that epiCG possesses high efficiency and scalability, and performs exceptionally well in large dataset settings, showcasing its suitability for large-scale graph processing. © 2016 Elsevier Inc."
"10.1016/j.bdr.2016.04.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84973489463&doi=10.1016%2fj.bdr.2016.04.001&partnerID=40&md5=e937938612c11cdc2921d70daffbef90","Adverse drug events (ADEs) contribute significantly to morbidity and mortality in the healthcare system. The availability of digitalised hospitals' narrative clinical data offers a potentially rich resource to enhance pharmacovigilance efforts to manage potential safety issues arising from real-world use of drugs. The goal of this paper was to establish a foundation for creating an evaluation corpus by developing a set of annotation guidelines to achieve high inter-annotator agreement (IAA) and to evaluate the performance of basic entity identification tools for drugs, adverse events (AEs) and drug-AE relationships from 100 discharge summaries of a tertiary hospital in Singapore. Two teams of three annotators worked independently on text annotation using Knowtator. Three-way IAA of 86%, 70% and 49% were achieved for drugs, AEs and drug-AE relationships respectively. The performance of the machine algorithm was evaluated against annotations made by at least two annotators, with a recall of 84% and precision of 73% for drugs and a recall of 67% and precision of 53% for AEs. The high recall and precision for drug entity extraction suggests that machine pre-annotation of drugs followed by human annotation of AEs and drug-AE relationships could be a feasible approach in expediting the process of creating a larger evaluation corpus. Non-matches between machine and human annotations were examined to identify ways to further refine the algorithm. When successfully implemented, the identification of ADEs could greatly support pharmacovigilance work in characterising the magnitude and scope of ADEs and prioritising interventions to improve the drug safety. © 2016 Elsevier Inc."
"10.1016/j.bdr.2015.11.003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84958279681&doi=10.1016%2fj.bdr.2015.11.003&partnerID=40&md5=f6143c928f9b9c617ce464e3c21565e7","Decades of research and experiences on managing large databases and current world's strong interests in massive data information conveyed many indexing methods to a new extent. From extensive experiments, FB+-tree has displayed its excellent potential for big data in-memory management. FB+-tree is an idea that builds fast indexing structure using multi-level key ranges, which is explained based on exploiting the B+-tree in this article. With FB+-tree, point searches and range searches are helped by early termination of searches for non-existent data. Range searches can be processed depth-first or breath-first. One group of multiple searches can be processed with one pass on the indexing structure to minimize total cost. Implementation options and strategies are explained to show the flexibility of this technology for easy adaption and high efficiency. FB+-tree can be tuned to speed up queries directed at popular ranges of index or index ranges of particular interest to the user. Extended experiments are presented particularly for testing its adaptability and performance for big data. © 2016 Elsevier Inc."
"10.1016/j.bdr.2015.10.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84947584456&doi=10.1016%2fj.bdr.2015.10.001&partnerID=40&md5=b05fb3699104c2b454a5d10752b246ac","Many industries are riding the wave of big data as the new era of data-driven decision making is unveiling. The field of big data analytics is gaining fast traction in industry, academia and the government; the healthcare arena is no different. In this paper, big data analytics are applied to healthcare data that is collected from multiple sources to gain quality insights and apprehend best practices of the field (using new healthcare specific big data tools). The US states are unceasingly pursuing potential improvements to their healthcare's Quality of Service (QoS). Recent changes in data sharing provisions, such as the disposition of the recent Affordable Health Care Act (ACA), changed the rules of the game, and provided the US states with a new set of measurable health quality variables that they couldn't overlook anymore. Individuals in those states without health insurance tend to ignore visiting the clinic even if they feel symptoms of a disease; healthy young individuals with insurance can also have the same behavior. Health experts constantly recommend closer immersion in one's health and more engagement with preventive healthcare. In three experimental studies, this multidisciplinary paper examines historical health data from all over the country, assesses the medical QoS for multiple US states using a new healthcare-specific analytical infrastructure, delivers forecasts and correlations for future healthcare activities, and provides data-driven © 2015 Elsevier Inc."
"10.1007/s41019-016-0008-y","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065010958&doi=10.1007%2fs41019-016-0008-y&partnerID=40&md5=3f8d28404cc21d84feb14d43387bb69b","Hydroclimatic research requires highly intensive resources in terms of computation and data to perform simulations. Setting up complex experiment environment and configurations to submit jobs in computational clusters as well as managing user’s limited storage spaces by transferring big size data into the secondary storage are complicated and time-consuming. As a possible answer to address such issues in hydroclimatic research, new technologies, software-defined storage and containers have been introduced. When the two technologies are combined to support hydroclimatic simulations, we discuss how the software-defined storage data infrastructure strengthens containers in terms of flexibility of data handling and storage scalability. © 2016, The Author(s)."
"10.1007/s41019-016-0012-2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053378117&doi=10.1007%2fs41019-016-0012-2&partnerID=40&md5=79fd04cc7fbcd2c384214159abf6e9f4","In this paper we address the difficulty of clipping articles from mobile apps. We propose a service called UniClip that allows a user to save the full content of an article by snapping a screenshot part of it. UniClip leverages a huge amount of indexed web data to mine the article by starting with a snapped screenshot. We propose approaches to solve three challenges: (1) how to represent a screenshot; (2) how to formulate effective queries for retrieving a full article; and (3) how to rank the best URL at the top from multiple search result lists. Experimental results indicate that our approach is effective in achieving as high an F 1 measure as 0.905, which outperforms the best of three baseline methods by 18 points. © 2016, The Author(s)."
"10.1007/s41019-016-0013-1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048039626&doi=10.1007%2fs41019-016-0013-1&partnerID=40&md5=564c77df94f8e0b065c85eae89bf9f04","Route recommendation is one of the most widely used location-based services nowadays, as it is vital for nice-driving experience and smooth public traffic. Given a pair of user-specified origin and destination, a route recommendation service aims to provide users with the routes of the best travelling experience according to given criteria. However, even the routes recommended by the big-thumb service providers can deviate significantly from the ones travelled by experienced drivers, which motivates the previous research that leverages crowds’ knowledge to improve the recommendation quality. Since route recommendation is normally an online task, low-latency response to drivers’ queries is required in this kind of systems. Unfortunately, latency of crowdsourced systems is usually high, because they need to generate tasks and wait for workers’ feedbacks before answering queries. To address this issue, we extend our previous system—CrowdPlanner—by proposing some strategies to reuse existing answers (truths) to deal with newly coming queries more efficiently. A prototype system has been deployed to many voluntary mobile clients and extensive tests on real-scenario queries have shown the superiority of our system in comparison with the results given by map services and popular route-mining algorithms. © 2016, The Author(s)."
"10.1007/s41019-016-0009-x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85043364044&doi=10.1007%2fs41019-016-0009-x&partnerID=40&md5=a59c959335fd8a44100a18ee31849fb8","Insiders are often legal users who are authorized to access system and data. If they misuse their privileges, it would bring great threat to system security. In practice, we could not have any knowledge about fraud pattern in advance, and most malicious behaviors are often in accordance with security rules; thus, it is difficult to predefine regulations for preventing all kinds of frauds. In this paper, we propose a data-driven evaluation model to detect malicious insiders, which audits user behaviors from both parallel and incremental aspects. Users are grouped together according to their positions and responsibilities, based on which the normal pattern is learned. For each user, a routine behavior pattern is also learned for historical assessment. Then, users are evaluated against both group patterns and routine patterns by probabilistic methods. The deviation degree is adopted as an evidence to justify an anomaly. We also recognize the abnormal activities that often make a user behavior much deviate, which can help an administrator revisit security policies or update activity weights in assessment. At last, experiments are performed on several real dataset. © 2016, The Author(s)."
"10.1007/s41019-016-0007-z","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85043257259&doi=10.1007%2fs41019-016-0007-z&partnerID=40&md5=ea227d9dea5274408817dd6446b129c6","This paper aims at introducing a new dissemination framework for cultural heritage (CH) making possible affordable solutions for small and medium museums to cooperate/collaborate in the creation of exhibitions. The framework also makes possible new data-based communication strategies able to combine content belonging to different cultural archives and accessed through an ontology-based integration and discovery mechanism, and fosters new data sharing and distribution policies that preserve intellectual property rights. The proposed dissemination model redefines the concept of digital storytelling with the aim of increasing the participation of domain experts in the dissemination of CH. The framework is designed around a graph-based architecture for creating attractive and engaging multimedia narratives that will be transformed in real experiences personalized according to the user’s profile, interests and context of use. Recommender and digital right management services are provided to authors and users for helping them in the creation, personalization and navigation of stories and for guaranteeing the adoption of suitable sharing and distribution policies. © 2016, The Author(s)."
"10.1080/23270012.2015.1063466","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057186011&doi=10.1080%2f23270012.2015.1063466&partnerID=40&md5=bed2c9e3f0bc9c7548754e346764d6f6","In this article, the authors propose a modified version of S.L. Chen and Liu's model with a two-stage production system. Assume that the retailer's order quantity is concerned with the manufacturer's selling price and the warranty period of product. The used cost of the customer is measured under the Taguchi's quadratic quality loss function and concluded in the retailer's profit function. The quality of the lot for the manufacturer is determined by adopting a two-stage single sampling rectifying inspection plan. The modified economic manufacturing quantity (EMQ) model is addressed in formulating the manufacturer's expected profit. The retailer's order quantity, manufacturer's wholesale price, production run length, process mean, and warranty period of product will be jointly determined by maximizing the total expected profit of the supply chain system including the manufacturer and the retailer. Finally, the quality investment policy is introduced to illustrate the profit improvement for the supply chain system. © 2015, © 2015 Antai College of Economics and Management, Shanghai Jiao Tong University."
"10.1080/23270012.2016.1152170","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020931672&doi=10.1080%2f23270012.2016.1152170&partnerID=40&md5=e1295361f6d1620cbeaf933ad1b93469","About two thirds of the population in India lives in villages. There is an acute shortage of health centers in rural areas. Hospitals are not located uniformly across different regions of country. Rural areas are also not well connected with cities due to a lack of infrastructure. Therefore, the demand for super specialty hospitals is greater in rural areas. This paper has analyzed the health requirement in a prominent Indian state, Bihar, in terms of population density. The purpose of this study is to illustrate the hospital site-selection problem by using the fuzzy extended elimination and choice expressing reality (ELECTRE) approach. Different attributes considered for site selection in this paper are cost, proximity, population characteristics, availability of human resources, accessibility, environment, etc. The findings of the study will be of great value to the health ministry and policy makers in taking judicious decision s while selecting the site for a new hospital or health center. © 2016, © 2016 Antai College of Economics and Management, Shanghai Jiao Tong University."
"10.1080/23270012.2016.1141331","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017285701&doi=10.1080%2f23270012.2016.1141331&partnerID=40&md5=8a24983e0df8894891668adf8dbc18ec","The data in electronic medical records (EMR) are complex in structure. They are independent, yet related to each other. In order to improve information access through the use of EMR, annotating work on these data is necessary. The annotation on metadata, the resource data which contain a meta-model of the database, is the basis of the annotating work if a semi-automated or an automated annotating approach which aims at making the database more accessible is expected. In this study, a method has been proposed to transform the terms which cannot be matched directly by changing them literally but maintaining their semantics, and then annotating them indirectly. After the transforming work, a refinement method which is reducible to phrase sense disambiguation (PSD) is employed to ensure accuracy. A pilot study on a hospital database has been conducted to test the accuracy and effectiveness of the proposed method. © 2016, © 2016 Antai College of Economic and Management, Shanghai Jiao Tong University."
"10.1080/23270012.2016.1145078","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84987970687&doi=10.1080%2f23270012.2016.1145078&partnerID=40&md5=edd6c94db90ae715668a1ecb3ce51934","In this paper, a two-warehouse economic order quantity (EOQ) model for non-instantaneously deteriorating items with stock-dependent demand under the effects of inflation and the time value of money is presented. Also in this model, shortages are allowed and partially backlogged. The backlogging rate is dependent on the waiting time for the next replenishment. The objective of this model is to minimize the total inventory cost of the retailer by finding the optimal intervals and the optimal order quantity. An algorithm is designed to find the optimum solution of the proposed model. Numerical examples are given to demonstrate the results. Sensitivity analysis of the model with respect to several system parameters has been carried out and some managerial inferences are obtained. © 2016, © 2016 Antai College of Economics and Management, Shanghai Jiao Tong University."
"10.1016/j.bdr.2015.11.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84963778899&doi=10.1016%2fj.bdr.2015.11.001&partnerID=40&md5=ba136959f117710519c2bbd0dfcd1d0e","In emerging big data era, mobile social networking (MSN) is an important data source, which provides an attractive proximity-based communication platform for mobile users with similar interests, attributes, or background to communicate with each other. In this kind of proximity-based MSN, profile matching protocol, which enables a mobile user to break the ice and start a conversation with someone attractive, is one of important components for its success. However, profile matching may occasionally leak the sensitive information, hence privacy concerns often hinder users from enabling this functionality. Aiming at this problem, in this paper, we present a new secure and fine-grained privacy-preserving matching protocol, called SFPM. Differently from those previously reported private profile matching schemes, our proposed SFPM can fine-grainedly differentiate users with the same value of matching metrics by two phases of profile matching. In addition to the personal privacy preservation through secure and efficient cryptographic algorithm, SFPM also achieves the flexibility of profiles changing at the same time. Extensive performance evaluations via smartphones with android system are conducted, and experimental results demonstrate the effectiveness of the SFPM protocol. © 2015 Elsevier Inc."
"10.1016/j.bdr.2016.02.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84961794765&doi=10.1016%2fj.bdr.2016.02.001&partnerID=40&md5=510101cf342569cfea5c3047f9163fc6","In the past few years, Big Data analytics have changed the way computing services and resources are being used. New users are getting into the cloud services provided by data centers on a daily basis, and the number of applications hosted on the clouds is rapidly increasing. However, existing data center architectures cannot cope with the black-box nature of the cloud nor with Big Data analytics. Current data center networks using electrical packet switches have many disadvantages. Electrical links consume too much power to handle the increased bandwidth demanded by emerging applications. They are often not suitable for providing high bandwidth at low latency and have bottleneck issues. A promising solution involves adding optical interconnections to the infrastructure in data centers offering high throughput, low latency, and incredibly low energy consumption compared to the current data center networks based on commodity switches. In this work, we propose a hybrid electrical and optical networking architecture for data centers hosting Cloud Computing and Big Data applications. Furthermore, we analyze and compare application behaviors with current and proposed hybrid architectures. The results show that hybrid architecture provides a viable solution to data center issues and easily outperforms existing data center architectures. The results reflect that hybrid architecture reduces the delay by nearly 39% and decreases the cost of cooling systems from 49.57% to 27% of the total costs, while reducing the overall long-term costs of data centers. © 2016 Elsevier Inc."
"10.1016/j.bdr.2015.12.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84957050411&doi=10.1016%2fj.bdr.2015.12.002&partnerID=40&md5=75327e3473a66ff12b52f01eaa932c5b","We live in a world increasingly driven by data with more information about individuals, companies and governments available than ever before. Now, every business is powered by Information Technology and generating Big data. Future Business Intelligence can be extracted from the big data. NoSQL [1] and Map-Reduce [2] technologies find an efficient way to store, organize and process the big data using Virtualization and Linux Container (a.k.a. Container) [3] technologies.Provisioning containers on top of virtual machines is a better model for high resource utilization. As the more containers share the same CPU, the context switch latency for each container increases significantly. Such increase leads to a negative impact on the network IO throughput and creates a bottleneck in the big data environments.As part of this paper, we studied container networking and various factors of context switch latency. We evaluate Hadoop benchmarks [5] against the number of containers and virtual machines. We observed a bottleneck where Hadoop [4] cluster throughput is not linear with the number of nodes sharing the same CPU. This bottleneck is due to virtual network layers which adds a significant delay to Round Trip Time (RTT) of data packets. Future work of this paper can be extended to analyze the practical implications of virtual network layers and a solution to improve the performance of big data environments based on containers. © 2016 Elsevier Inc."
"10.1016/j.bdr.2015.11.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84949671050&doi=10.1016%2fj.bdr.2015.11.002&partnerID=40&md5=1b272cfa470f268ed0c92d7140b2bc0f","Critical infrastructure systems perform functions and missions that are essential for our national economy, health, and security. These functions are vital to commerce, government, and society and are closely interrelated with people's lives. To provide highly secured critical infrastructure systems, a scalable, reliable and robust threat monitoring and detection system should be developed to efficiently mitigate cyber threats. In addition, big data from threat monitoring systems pose serious challenges for cyber operations because an ever growing number of devices in the system and the amount of complex monitoring data collected from critical infrastructure systems require scalable methods to capture, store, manage, and process the big data. To address these challenges, in this paper, we propose a cloud computing based network monitoring and threat detection system to make critical infrastructure systems secure. Our proposed system consists of three main components: monitoring agents, cloud infrastructure, and an operation center. To build our proposed system, we use both Hadoop MapReduce and Spark to speed up data processing by separating and processing data streams concurrently. With a real-world data set, we conducted real-world experiments to evaluate the effectiveness of our developed network monitoring and threat detection system in terms of network monitoring, threat detection, and system performance. Our empirical data indicates that the proposed system can efficiently monitor network activities, find abnormal behaviors, and detect network threats to protect critical infrastructure systems. © 2015 Elsevier Inc."
"10.4018/IJBAN.2016040103","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046276828&doi=10.4018%2fIJBAN.2016040103&partnerID=40&md5=20dd9751965ee301e0528988c046a249","Authors of this paper were directly involved in several data warehousing and business intelligence projects that were carried out in companies in Croatia. Based on the experience from those projects, this paper presents how those solutions were implemented — and more importantly, how they were used by managers at different levels of management. Three different companies are described, including three different implemented solutions. One company was small, one was mid-sized and one was large. All had differences in ownership and education level of managers. Implementation (technical) details are omitted, but this paper shows that managers do not always see what business intelligence can do. Furthermore, it is shown that such projects do not always have to be expensive. © 2016, IGI Global."
"10.4018/IJBAN.2016040101","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020478144&doi=10.4018%2fIJBAN.2016040101&partnerID=40&md5=968a76e3de5157b662ccb64690c604f0","“Big Data” is an emerging term used with business, engineering, and other domains. Although Big Data is a popular term used today, it is not a new concept. However, the means in which data can be collected is more readily available than ever, which makes Big Data more relevant than ever because it can be used to improve decisions and insights within the domains it is used. The term Big Data can be loosely defined as data that is too large for traditional analysis methods and techniques. In this article, varieties of prominent but loose definitions for Big Data are shared. In addition, a comprehensive overview of issues related to Big Data is summarized. For example, this paper examines the forms, locations, methods of analyzing and exploiting Big Data, and current research on Big Data. Big Data also concerns a myriad of tangential issues, from privacy to analysis methods that will also be overviewed. Best practices will further be considered. Additionally, the epistemology of Big Data and its history will be examined, as well as technical and societal problems existing with Big Data. © 2016,IGI Global."
"10.4018/IJBAN.2016040102","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016485178&doi=10.4018%2fIJBAN.2016040102&partnerID=40&md5=7cab2adb3d8c38caf3cd8d43c47f22a1","This paper reports on the results of extracting useful information from text notes captured within a Customer Relationship Management (CRM) system to segment and thus target groups of customers likely to respond to cross-selling campaigns. These notes often contain text that is indicative of customer intentions. The results indicate that the notes are meaningful in classifying customers who are likely to respond to purchase multiple communication devices. A Naïve Bayes classifier outperformed a Support Vector Machine classifier for this task. When combined with structured information, the classifier performed only marginally better. Thus, customer service notes can be an important source of predictive data in CRM systems. © 2016, IGI Global."
"10.4018/IJBAN.2016040104","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85014861825&doi=10.4018%2fIJBAN.2016040104&partnerID=40&md5=8f667c91faea4db538edde966a9eb832","Cloud computing is a new paradigm model that offers different services to its customers. The increasing number of users for cloud services i.e. software, platform or infrastructure is one of the major reasons for security threats for customers’ data. Some major security issues are highlighted in data storage service in the literature. Data of thousands of users are stored on a single centralized place where the possibility of data threat is high. There are many techniques discussed in the literature to keep data secure in the cloud, such as data encryption, private cloud and multiple clouds concepts. Data encryption is used to encrypt the data or change the format of the data into the unreadable format that unauthorized users could not understand even if they succeed to get access of the data. Data encryption is very expensive technique, it takes time to encrypt and decrypt the data. Deciding the security approach for data security without understanding the security needs of the data is a technically not a valid approach. It is a basic requirement that one should understand the security level of data before applying data encryption security approach. To discover the data security level of the data, the authors used machine learning approach in the cloud. In this paper, a data classification approach is proposed for the cloud and is implemented in a virtual machine named as Master Virtual Machine (Vmm). Other Vms are the slave virtual machines which will receive from Vmm the classified information for further processing in cloud. In this study the authors used three (3) virtual machines, one master Vmm and two slaves Vms. The master Vmm is responsible for finding the classes of the data based on its confidentiality level. The data is classified into two classes, confidential (sensitive) and non-confidential (non-sensitive/public) data using K-NN algorithm. After classification phase, the security phase (encryption phase) shall encrypt only the confidential (sensitive) data. The confidentiality based data classification is using K-NN in cloud virtual environment as the method to encrypt efficiently the only confidential data. The proposed approach is efficient and memory space friendly and these are the major findings of this work. © 2016, IGI Global."
"10.1089/big.2014.0064","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84991821750&doi=10.1089%2fbig.2014.0064&partnerID=40&md5=ff8266b9e70f8a3886b8728f0702f094","Aerial imagery captured via unmanned aerial vehicles (UAVs) is playing an increasingly important role in disaster response. Unlike satellite imagery, aerial imagery can be captured and processed within hours rather than days. In addition, the spatial resolution of aerial imagery is an order of magnitude higher than the imagery produced by the most sophisticated commercial satellites today. Both the United States Federal Emergency Management Agency (FEMA) and the European Commission's Joint Research Center (JRC) have noted that aerial imagery will inevitably present a big data challenge. The purpose of this article is to get ahead of this future challenge by proposing a hybrid crowdsourcing and real-time machine learning solution to rapidly process large volumes of aerial data for disaster response in a time-sensitive manner. Crowdsourcing can be used to annotate features of interest in aerial images (such as damaged shelters and roads blocked by debris). These human-annotated features can then be used to train a supervised machine learning system to learn to recognize such features in new unseen images. In this article, we describe how this hybrid solution for image analysis can be implemented as a module (i.e., Aerial Clicker) to extend an existing platform called Artificial Intelligence for Disaster Response (AIDR), which has already been deployed to classify microblog messages during disasters using its Text Clicker module and in response to Cyclone Pam, a category 5 cyclone that devastated Vanuatu in March 2015. The hybrid solution we present can be applied to both aerial and satellite imagery and has applications beyond disaster response such as wildlife protection, human rights, and archeological exploration. As a proof of concept, we recently piloted this solution using very high-resolution aerial photographs of a wildlife reserve in Namibia to support rangers with their wildlife conservation efforts (SAVMAP project, http://lasig.epfl.ch/savmap). The results suggest that the platform we have developed to combine crowdsourcing and machine learning to make sense of large volumes of aerial images can be used for disaster response. © Mary Ann Liebert, Inc. 2016."
"10.1089/big.2015.0043","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84991798406&doi=10.1089%2fbig.2015.0043&partnerID=40&md5=52404b4ee77023a021ef2383816e503f","This case study evaluates and tracks vitality of a city (Seattle), based on a data-driven approach, using strategic, robust, and sustainable metrics. This case study was collaboratively conducted by the Downtown Seattle Association (DSA) and CDO Analytics teams. The DSA is a nonprofit organization focused on making the city of Seattle and its Downtown a healthy and vibrant place to Live, Work, Shop, and Play. DSA primarily operates through public policy advocacy, community and business development, and marketing. In 2010, the organization turned to CDO Analytics (cdoanalytics.org) to develop a process that can guide and strategically focus DSA efforts and resources for maximal benefit to the city of Seattle and its Downtown. CDO Analytics was asked to develop clear, easily understood, and robust metrics for a baseline evaluation of the health of the city, as well as for ongoing monitoring and comparisons of the vitality, sustainability, and growth. The DSA and CDO Analytics teams strategized on how to effectively assess and track the vitality of Seattle and its Downtown. The two teams filtered a variety of data sources, and evaluated the veracity of multiple diverse metrics. This iterative process resulted in the development of a small number of strategic, simple, reliable, and sustainable metrics across four pillars of activity: Live, Work, Shop, and Play. Data during the 5 years before 2010 were used for the development of the metrics and model and its training, and data during the 5 years from 2010 and on were used for testing and validation. This work enabled DSA to routinely track these strategic metrics, use them to monitor the vitality of Downtown Seattle, prioritize improvements, and identify new value-added programs. As a result, the four-pillar approach became an integral part of the data-driven decision-making and execution of the Seattle community's improvement activities. The approach described in this case study is actionable, robust, inexpensive, and easy to adopt and sustain. It can be applied to cities, districts, counties, regions, states, or countries, enabling cross-comparisons and improvements of vitality, sustainability, and growth. © Mary Ann Liebert, Inc. 2016."
"10.1089/big.2016.0010","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84991780639&doi=10.1089%2fbig.2016.0010&partnerID=40&md5=3830fd139c8fdac1c585940c8ff14d35","The number of data-based research articles focusing on patient sociodemographic profiling and experience with healthcare practices is still relatively small. One of the reasons for this relative lack of research is that categorizing patients into different demographic groups can lead to significant reductions in sample numbers for homogeneous subgroups. The aim of this article is to identify problems and issues when dealing with big data that contains information at two levels: patient experience of their general practice, and scores received by practices. The Practice Accreditation and Improvement Survey (PAIS) consisting of 27 five-point Likert items and 11 sociodemographic questions is a Royal Australian College of General Practitioners (RACGP)-endorsed instrument for seeking patient views as part of the accreditation of Australian general practices. The data were collected during the 3-year period May 2011-July 2014, during which time PAIS was completed for 3734 individual general practices throughout Australia involving 312,334 anonymous patients. This represents over 60% of practices in Australia, and ∼75% of practices that undergo voluntary accreditation. The sampling method for each general practice was convenience sampling. The results of our analysis show how sociodemographic profiles of Australian patients can affect their ratings of practices and also how the location of the practice (State/Territory, remote access area) can affect patient experience. These preliminary findings can act as an initial set of results against which future studies in patient experience trends can be developed and measured in Australia. Also, the methods used in this article provide a methodological framework for future patient experience researchers to use when dealing with data that contain information at two levels, such as the patient and practice. Finally, the outcomes demonstrate that different subgroups can experience healthcare provision differently, especially indigenous patients and young patients. The implications of these findings for healthcare policy and priorities will need to be further investigated. © Mary Ann Liebert, Inc. 2016."
"10.1089/big.2016.0012","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84991777589&doi=10.1089%2fbig.2016.0012&partnerID=40&md5=f09ea7987ce2a755a44ab515e2cfa96c","We report on an ongoing project to develop data-driven tools to help individuals make better choices about health insurance and to better understand the range of costs to which they are exposed under different health plans. We describe a simulation tool that we developed to evaluate the likely usage and costs for an individual and family under a wide range of health service usage outcomes, but that can be tailored to specific physicians and the needs of the user and to reflect the demographics and other special attributes of the family. The simulator can accommodate, for example, specific known physician visits or planned procedures, while also generating statistically reasonable ""unexpected"" events like ER visits or catastrophic diagnoses. On the other hand, if a user provides only a small amount of information (e.g., just information about the family members), the simulator makes a number of generic assumptions regarding physician usage, etc., based on the age, gender, and other features of the family. Data to parameterize all of these events is informed by a combination of the information provided by the user and a series of specialized databases that we have compiled based on publicly available government data and commercial data as well as our own analysis of this initially very coarse and rigid data. To demonstrate both the subtlety of choosing a healthcare plan and the degree to which the simulator can aid in such evaluations, we present sample results using real insurance plans and two example policy shoppers with different demographics and healthcare needs. © Mary Ann Liebert, Inc. 2016."
"10.1089/big.2015.0029","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84991759483&doi=10.1089%2fbig.2015.0029&partnerID=40&md5=bec9bb58618adc37abccc7873a82eeb0","Disease progression models, statistical models that assess a patient's risk of diabetes progression, are popular tools in clinical practice for prevention and management of chronic conditions. Most, if not all, models currently in use are based on gold standard clinical trial data. The relatively small sample size available from clinical trial limits these models only considering the patient's state at the time of the assessment and ignoring the trajectory, the sequence of events, that led up to the state. Recent advances in the adoption of electronic health record (EHR) systems and the large sample size they contain have paved the way to build disease progression models that can take trajectories into account, leading to increasingly accurate and personalized assessment. To address these problems, we present a novel method to observe trajectories directly. We demonstrate the effectiveness of the proposed method by studying type 2 diabetes mellitus (T2DM) trajectories. Specifically, using EHR data for a large population-based cohort, we identified a typical trajectory that most people follow, which is a sequence of diseases from hyperlipidemia (HLD) to hypertension (HTN), impaired fasting glucose (IFG), and T2DM. In addition, we also show that patients who follow different trajectories can face significantly increased or decreased risk. © Mary Ann Liebert, Inc. 2016."
"10.1007/s41019-015-0001-x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065033409&doi=10.1007%2fs41019-015-0001-x&partnerID=40&md5=eb51247619647278a0ea9fb9918d3581","This paper explores the challenges raised by big data in privacy-preserving data management. First, we examine the conflicts raised by big data with respect to preexisting concepts of private data management, such as consent, purpose limitation, transparency and individual rights of access, rectification and erasure. Anonymization appears as the best tool to mitigate such conflicts, and it is best implemented by adhering to a privacy model with precise privacy guarantees. For this reason, we evaluate how well the two main privacy models used in anonymization (k-anonymity and ε-differential privacy) meet the requirements of big data, namely composability, low computational cost and linkability. © 2015, The author(s)."
"10.1007/s41019-015-0003-8","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85037354252&doi=10.1007%2fs41019-015-0003-8&partnerID=40&md5=86b32368d8260e6d46d4c04a6f6df58f","Nowadays, the message diffusion links among users or Web sites drive the development of countless innovative applications. However, in reality, it is easier for us to observe the time stamps when different nodes in the network react on a message, while the connections empowering the diffusion of the message remain hidden. This motivates recent extensive studies on the network inference problem: unveiling the edges from the records of messages disseminated through them. Existing solutions are computationally expensive, which motivates us to develop an efficient two-step general framework, Clustering Embedded Network Inference (CENI). CENI integrates clustering strategies to improve the efficiency of network inference. By clustering nodes directly on the time lines of messages, we propose two naive implementations of CENI: Infection-centric CENI and Cascade-centric CENI. Additionally, we point out the critical dimension problem of CENI: Instead of one-dimensional time lines, we need to first project the nodes to an Euclidean space of certain dimension before clustering. A CENI adopting clustering method on the projected space can better preserve the structure hidden in the cascades and generate more accurately inferred links. By addressing the critical dimension problem, we propose the third implementation of the CENI framework: Projection-based CENI. Through extensive experiments on two real datasets, we show that the three CENI models only need around 20–50 % of the running time of state-of-the-art methods. Moreover, the inferred edges of Projection-based CENI preserve or even outperform the effectiveness of state-of-the-art methods. © 2015, The author(s)."
"10.1007/s41019-015-0004-7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85010343594&doi=10.1007%2fs41019-015-0004-7&partnerID=40&md5=949d154bdd4a9721d1bd50898747dc99","In this paper, we discuss the application of concept of data quality to big data by highlighting how much complex is to define it in a general way. Already data quality is a multidimensional concept, difficult to characterize in precise definitions even in the case of well-structured data. Big data add two further dimensions of complexity: (i) being “very” source specific, and for this we adopt the interesting UNECE classification, and (ii) being highly unstructured and schema-less, often without golden standards to refer to or very difficult to access. After providing a tutorial on data quality in traditional contexts, we analyze big data by providing insights into the UNECE classification, and then, for each type of data source, we choose a specific instance of such a type (notably deep Web data, sensor-generated data, and Twitters/short texts) and discuss how quality dimensions can be defined in these cases. The overall aim of the paper is therefore to identify further research directions in the area of big data quality, by providing at the same time an up-to-date state of the art on data quality. © 2015, The Author(s)."
"10.1007/s41019-015-0002-9","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84997478360&doi=10.1007%2fs41019-015-0002-9&partnerID=40&md5=981bcfefc3111a7592c729409d2c3a7b","Real-time stream computing becomes increasingly important due to the sheer amount of content continually generated in various kinds of social networks and e-commerce websites. Many distributed real-time computing systems have been built for different applications, and Storm is one of the most prominent systems with high-performance, fault-tolerance and low-latency features. However, the Storm programming paradigm is low level and leaves programmers’ codes hard to maintain and reuse. In this paper, we present a high-level abstraction system on Storm, called POS. The POS system provides a Pig Latin-like language on top of the Storm execution engine. Programmers can write POS program, and the system compiles the program into physical plans which are executed over Storm. We discuss the challenges in developing POS system and elaborate on its implementation details. Our experiments show that POS yields satisfactory performance compared with raw Storm. © 2015, The Author(s)."
"10.1057/jma.2016.4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031503505&doi=10.1057%2fjma.2016.4&partnerID=40&md5=45862634a463dc2ee4008d7d71f01706","In order to broaden the set of methods for developing better multi-item marketing scales, this article proposes an item response theory (IRT) procedure that consists of two successive steps. The first exploratory step uses a non-parametric IRT model for dimensionality detection and scale purification. The second confirmatory step employs a parametric IRT model for evaluating, and likely refining, the resulting scale. This two-step IRT procedure also involves a wide range of new parametric and non-parametric IRT-based interpretive tools (that is, goodness-of-fit indices, scale-score reliability coefficients and item response/information functions) that allows scale developers and users to work in a highly informed manner. To illustrate the benefits of the proposed procedure, the article presents an extended application to the topical construct of emotional attachment. © 2016 Macmillan Publishers Ltd."
"10.1057/jma.2016.3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031502740&doi=10.1057%2fjma.2016.3&partnerID=40&md5=026dc84efd5424c12b50300cce53f6bd","Brand Personality as a metaphor has been topic of interest for scholars especially after Aaker’s influential work. This study is an empirical comment on generalization of brand personality scale for bikes in India. The study revealed that BPS is not applicable in India in its present form. Ten items (‘down to earth’, ‘family oriented’, ‘small town’, ‘feminine’, ‘smooth’, ‘sincere’, ‘western’, ‘successful’, ‘sentimental’ and ‘independent’) from BPS were found to be not applicable for brand personality of bikes in India. The study argues that culture, social setting, non-anthropomorphism for certain traits and product misfits to be the potential reasons for non-applicability. © 2016 Macmillan Publishers Ltd."
"10.1057/jma.2016.1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031502135&doi=10.1057%2fjma.2016.1&partnerID=40&md5=c7fe25d761204115466183a6ec65cec2","Marketing data are multifaceted. Measures of marketing constructs have multiple legitimate sources of variance such as firms, customers and brands. So the invoked causality between a marketing concept and its measures should account for covariance at more than one level. Depending on a marketing construct’s conceptual domain, different sources of variance are focal to the theoretical relationships being investigated, and the construct’s relationship with its indicators can be either formative or reflective for each source. The authors show the vanishing tetrad test can be used to determine whether marketing data are formative along their multiple facets (for example, consumers versus brands). © 2016 Macmillan Publishers Ltd."
"10.1057/jma.2016.5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031499512&doi=10.1057%2fjma.2016.5&partnerID=40&md5=62f7e2fae6c9a1ba9dad6b336f3fc4db","This article presents my educational and career paths on the way to being a ‘Marketing Analytics Person’. It also notes several ‘Core Lessons’ that I believe are useful to anyone who is aspiring toward an academic career in marketing analytics. Some of these pertain to educational choices, while others to choices during one’s professorial career. The article spans a time period of more than a half-century. © 2016 Macmillan Publishers Ltd."
"10.1080/23270012.2015.1062735","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056805790&doi=10.1080%2f23270012.2015.1062735&partnerID=40&md5=018894082397e507d4b36fda99ceeda3","In Chen and Liu's optimum profit model with a traditional production system, they did not consider the effect of product quality on the customer's demand order quantity, and also ignored the used cost of customers for product. In fact, the customer's demand quantity is always seriously related to product quality. Hence, in the present paper, we modify Chen and Liu's model to address the determination of the optimal process parameters by employing the idea of quality loss and single sampling rectifying inspection plan. Assuming that the quality characteristic of the product is normally distributed, Taguchi's symmetric quadratic quality loss function is applied in evaluating the product quality. Three decision variables, i.e., the mean of the process characteristic, the production run length of the product and the retailer's order quantity, are jointly determined in our modified model to maximize the expected total profit of society, which includes both the manufacturer and the retailer. A heuristic solution procedure is developed for this optimization problem, and a numerical example is provided for illustration. From the numerical results, it can be seen that both the sale price per unit and the intercept of the mean demand of the customer are two major (or significant) parameters in the model and should be more accurately estimated in practice. Finally, the quality investment policy is provided to compare its effect on the optimum profit model with quality improvement. © 2015, © 2015 Antai College of Economics and Management, Shanghai Jiao Tong University."
"10.1080/23270012.2016.1140597","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049116553&doi=10.1080%2f23270012.2016.1140597&partnerID=40&md5=6965649ff2420697266e4a3304a5a1ca",[No abstract available]
"10.1080/23270012.2015.1121118","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026423999&doi=10.1080%2f23270012.2015.1121118&partnerID=40&md5=0c7c6d0963b31151ff929eadf3a255e3","Most manufacturing firms consider product pricing as a key strategic decision. Production planning and scheduling, on the other hand, are mostly treated as a non-strategic decisions. Most businesses, therefore, follow some sequential decision-making process; where product prices are determined first and then operational plans are made to fulfill the resulting demand at the lowest possible cost. In this paper, we present a coordinated decision model approach for multi-product pricing and lot sizing decisions for a manufacturer who has limited production capacity. Although the presented model is specific to demands that follow constant elasticity of the price, it can easily be extended to other convex demand functions. We show that a coordinated decision-making process where price and production plans are determined simultaneously may lead to substantially higher profits. We propose an efficient solution methodology for finding optimal prices and product quantities and use real-world data to demonstrate the applicability of this research. © 2016, © 2016 Antai College of Economics and Management, Shanghai Jiao Tong University."
"10.1080/23270012.2015.1113895","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017050720&doi=10.1080%2f23270012.2015.1113895&partnerID=40&md5=2a3b258ae0245c6d756caabb4d7ad472","India's unorganized labour force contributes about one third of the total labour sector. The scenario is even worse in the Indian automotive industry which employs a little over 7% on a permanent basis. Problems get exaggerated due to the outdated labour laws, ironically established to support and protect workers. The disappointing areas in the labour contract act and labour laws have led to unfair wage practices and a hostile work environment, giving way to labour discord. This research paper discusses the key issues of labour dissonance in the Indian automobile industry using a Bayesian network analysis. Real-life case-study examples from the Indian automobile industry were considered to identify the rationale behind labour unrest. Bayesian analysis of a set of 250 responses helped us to understand the associations among key attributes of labour dissatisfaction. © 2016, © 2016 Antai College of Economics and Management, Shanghai Jiao Tong University."
"10.1080/23270012.2016.1141332","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85006920071&doi=10.1080%2f23270012.2016.1141332&partnerID=40&md5=feb52ca6a33d6518819672ca73ca060f","Big data has attracted much attention from academia and industry. But the discussion of big data is disparate, fragmented and distributed among different outlets. This paper conducts a systematic and extensive review on 186 journal publications about big data from 2011 to 2015 in the Science Citation Index (SCI) and the Social Science Citation Index (SSCI) database aiming to provide scholars and practitioners with a comprehensive overview and big picture about research on big data. The selected papers are grouped into 20 research categories. The contents of the paper(s) in each research category are summarized. Research directions for each category are outlined as well. The results in this study indicate that the selected papers were mainly published between 2013 and 2015 and focus on technological issues regarding big data. Diverse new approaches, methods, frameworks and systems are proposed for data collection, storage, transport, processing and analysis in the selected papers. Possible directions for future research on big data are discussed. © 2016, © 2016 Antai College of Economics and Management, Shanghai Jiao Tong University."
"10.5334/dsj-2016-015","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85011890605&doi=10.5334%2fdsj-2016-015&partnerID=40&md5=623e89c10ef910a89654e215f6044341","Data quality is an important guarantee for scientific research. The Geomagnetic Network of China (GNC) controls data quality for all observatories under the jurisdiction of China Earthquake Administration. This paper presents the quality control content and methods; quality evaluation and feedback procedures; and the process for publishing data sets via the GNC website. Technical challenges and proposed quality assurance procedures for future GNC data sets are described. © 2016 The Author(s)."
"10.5334/dsj-2016-017","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85011890580&doi=10.5334%2fdsj-2016-017&partnerID=40&md5=e254d6148a8a0dc91b076c613ebd16c8","The Homer Multitext project (HMT) is documenting the language and structure of Greek epic poetry, and the ancient tradition of commentary on it. The project's primary data consist of editions of Greek texts; automated and manually created readings analyze the texts across historical and thematic axes. This paper describes an abstract model we follow in documenting an open-ended body of diverse analyses. The analyses apply to passages of texts at different levels of granularity; they may refer to overlapping or mutually exclusive passages of text; and they may apply to non-contiguous passages of text. All are recorded in with explicit, concise, machine-actionable canonical citation of both text passage and analysis in a scheme aligning all analyses to a common notional text. We cite our texts with urns that capture a passage's position in an Ordered Hierarchy of Citation Objects (OHCO2). Analyses are modeled as data-objects with five properties. We create collections of 'analytical objects', each uniquely identified by its own URN and each aligned to a particular edition of a text by a URN citation. We can view these analytical objects as an extension of the edition's citation hierarchy; since they are explicitly ordered by their alignment with the edition they analyze, each collection of analyses meets satisfies the (OHCO2) model of a citable text. We call these texts that are derived from and aligned to an edition 'analytical exemplars'. © 2016 The Author(s)."
"10.5334/dsj-2016-013","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85011848778&doi=10.5334%2fdsj-2016-013&partnerID=40&md5=a4ebc8858ba4118bf5cc67be0cee118e","Coptic SCRIPTORIUM is a platform for interdisciplinary and computational research in Coptic texts and linguistics. The purpose of this project was to research and implement a system of stable identification for the texts and linguistic data objects in Coptic SCRIPTORIUM to facilitate their citation and reuse. We began the project with a preferred solution, the Canonical Text Services URN model, which we validated for suitability for the corpus and compared it to other approaches, including HTTP URLs and Handles. The process of applying the CTS model to Coptic SCRIPTORIUM required an in-depth analysis that took into account the domain-specific scholarly research and citation practices, the structure of the textual data, and the data management workflow. © 2016 The Author(s)."
"10.5334/dsj-2016-018","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85011832595&doi=10.5334%2fdsj-2016-018&partnerID=40&md5=3475510f08758277dc726db977c5594f","The continuous growth of geophysical observations requires adequate methods for their processing and analysis. This becomes one of the most important and widely discussed issues in the data science community. The system analysis methods and data mining techniques are able to sustain the solution of this problem. This paper presents an innovative holistic hardware/software system (HSS) developed for efficient management and intellectual analysis of geomagnetic data, registered by Russian geomagnetic observatories and international satellites. Geomagnetic observatories that comprise the International Real-time Magnetic Observatory Network (INTERMAGNET) produce preliminary (raw) and definitive (corrected) geomagnetic data of the highest quality. The designed system automates and accelerates routine production of definitive data from the preliminary magnetograms, obtained by Russian observatories, due to implemented algorithms that involve artificial intelligence elements. The HSS is the first system that provides sophisticated automatic detection and multi-criteria classification of extreme geomagnetic conditions, which may be hazardous for technological infrastructure and economic activity in Russia. It enables the online access to digital geomagnetic data, its processing results and modelling calculations along with their visualization on conventional and spherical screens. The concept of the presented system agrees with the accepted 'four Vs' paradigm of Big Data. The HSS can increase significantly the 'velocity' and 'veracity' features of the INTERMAGNET system. It also provides fusion of large sets of ground-based and satellite geomagnetic data, thus facilitating the 'volume' and 'variety' of handled data. © 2016 The Author(s)."
"10.5334/dsj-2016-016","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85011826513&doi=10.5334%2fdsj-2016-016&partnerID=40&md5=3ee3874e26868ddd408ba2754cf142f4","The discrete mathematical analysis (DMA) is a series of algorithms aimed at the solution of basic problems of data analysis: clustering and tracing in multidimensional arrays, morphological analysis of reliefs, search for anomalies and trends in records etc. All the DMA algorithms are of universal nature, joined by the same formal foundation, based, in its turn, on fuzzy logic (FL) and fuzzy mathematics (FM). The current study finalizes the search for the anomalies in one-dimensional time series within the scope of DMA: here the initial concept of an interpreter's logic gets its additional development. First, the formal expert's opinions are more fully expressed, and this is realized with the more complex measures of activity (the concept of straightenings (Gvishiani et al. 2003; Gvishiani et al. 2004; Zlotnicki et al. 2005) is replaced by the measures of activity which come to the fore): second, for the junction of anomalies, a recently created DPS (Discrete Perfect Sets) algorithm is used DPS (Discrete Perfect Sets) (Agayan et al. 2011; Agayan et al. 2014). © 2016 The Author(s)."
"10.1140/epjds/s13688-015-0062-0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85007212152&doi=10.1140%2fepjds%2fs13688-015-0062-0&partnerID=40&md5=ea8616c2b8329f7dc2d6d7c9424fdf35","Link prediction appears as a central problem of network science, as it calls for unfolding the mechanisms that govern the micro-dynamics of the network. In this work, we are interested in ego-networks, that is the mere information of interactions of a node to its neighbors, in the context of social relationships. As the structural information is very poor, we rely on another source of information to predict links among egos’ neighbors: the timing of interactions. We define several features to capture different kinds of temporal information and apply machine learning methods to combine these various features and improve the quality of the prediction. We demonstrate the efficiency of this temporal approach on a cellphone interaction dataset, pointing out features which prove themselves to perform well in this context, in particular the temporal profile of interactions and elapsed time between contacts. © 2016 Tabourier et al."
"10.1140/epjds/s13688-016-0063-7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85007180338&doi=10.1140%2fepjds%2fs13688-016-0063-7&partnerID=40&md5=998ec165d26f224f048bc4e99d4e19c7","We investigate the structure of global inter-firm linkages using a dataset that contains information on business partners for about 400,000 firms worldwide, including all the firms listed on the major stock exchanges. Among the firms, we examine three networks, which are based on customer-supplier, licensee-licensor, and strategic alliance relationships. First, we show that these networks all have scale-free topology and that the degree distribution for each follows a power law with an exponent of 1.5. The shortest path length is around six for all three networks. Second, we show through community structure analysis that the firms comprise a community with those firms that belong to the same industry but different home countries, indicating the globalization of firms’ production activities. Finally, we discuss what such production globalization implies for the proliferation of conflict minerals (i.e., minerals extracted from conflict zones and sold to firms in other countries to perpetuate fighting) through global buyer-supplier linkages. We show that a limited number of firms belonging to some specific industries and countries plays an important role in the global proliferation of conflict minerals. Our numerical simulation shows that regulations on the purchases of conflict minerals by those firms would substantially reduce their worldwide use. © 2016 Mizuno et al."
"10.5334/dsj-2016-009","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84995603576&doi=10.5334%2fdsj-2016-009&partnerID=40&md5=4466e40c017d3fc55b63878da192ce4e","3D modelling can be used for a variety of purposes, including biomedical modelling for orthopaedic or anatomical applications. Low back pain is prevalent in society yet few validated 3D models of the lumbar spine exist to facilitate assessment. We therefore created a 3D surface data set for lumbar vertebrae from human vertebrae. Models from 86 lumbar vertebrae were constructed using an inexpensive method involving image capture by digital camera and reconstruction of 3D models via an image-based technique. The reconstruction method was validated using a laser-based arm scanner and measurements derived from real vertebrae using electronic callipers. Results show a mean relative error of 5.2% between image-based models and real vertebrae, a mean relative error of 4.7% between image-based and arm scanning models and 95% of vertices' errors are less than 3.5 millimetres with a median of 1.1 millimetres. The accuracy of the method indicates that the generated models could be useful for biomechanical modelling or 3D visualisation of the spine."
"10.5334/dsj-2016-008","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84995578399&doi=10.5334%2fdsj-2016-008&partnerID=40&md5=88cf68751baaaf520eefdb343833aea2","Understanding the earth as a system requires integrating many forms of data from multiple fields. Builders and funders of the cyberinfrastructure designed to enable open data sharing in the geosciences risk a key failure mode: What if geoscientists do not use the cyberinfrastructure to share, discover and reuse data? In this study, we report a baseline assessment of engagement with the NSF EarthCube initiative, an open cyberinfrastructure effort for the geosciences. We find scientists perceive the need for cross-disciplinary engagement and engage where there is organizational or institutional support. However, we also find a possibly imbalanced involvement between cyber and geoscience communities at the outset, with the former showing more interest than the latter. This analysis highlights the importance of examining fields and disciplines as stakeholders to investments in the cyberinfrastructure supporting science. © 2016 The Author(s)."
"10.5334/dsj-2016-012","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84995566952&doi=10.5334%2fdsj-2016-012&partnerID=40&md5=87ff82d7d7448af3dd2bde734cab46e1","The olog approach of Spivak and Kent (PLoS ONE 7, 1 (2012) p e24274) is applied to the practical development of data transfer frameworks, yielding simple rules for construction and assessment of data transfer standards. The simplicity, extensibility and modularity of such descriptions allows discipline experts unfamiliar with complex ontological constructs or toolsets to synthesise multiple pre-existing standards, potentially including a variety of file formats, into a single overarching ontology. These ontologies nevertheless capture all scientifically-relevant prior knowledge, and when expressed in machine-readable form are sufficiently expressive to mediate translation between legacy and modern data formats. A format-independent programming interface informed by this ontology consists of six functions, of which only two handle data. Demonstration software implementing this interface is used to translate between two common diffraction image formats using such an ontology in place of an intermediate format. © 2016 The Author(s)."
"10.5334/dsj-2016-007","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84995562499&doi=10.5334%2fdsj-2016-007&partnerID=40&md5=bd86c68e0c388f8932ea06bdf89c8b0e","Nowadays, materials scientific data come from lab experiments, simulations, individual archives, enterprise and internet in all scales and formats. The data flood has outpaced our capability to process, manage, analyze, and provide intelligent services. Extracting valuable information from the huge data ocean is necessary for improving the quality of domain services. The most acute information management challenges today stem from organizations relying on amounts of diverse, interrelated data sources, but having no way to manage the dataspaces in an integrated, user-demand driven and services convenient way. Thus, we proposed the model of Virtual Data-Space (VDS) in materials science field to organize multi-source and heterogeneous data resources and offer services on the data in place without losing context information. First, the concept and theoretical analysis are described for the model. Then the methods for construction of the model is proposed based on users' interests. Furthermore, the dynamic evolution algorithm of VDS is analyzed using the user feedback mechanism. Finally, we showed its efficiency for intelligent, real-time, on-demand services in the field of materials engineering."
"10.5334/dsj-2016-011","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84995545716&doi=10.5334%2fdsj-2016-011&partnerID=40&md5=f722ec6322471ddd0d48606a2188cb08","The EarthCollab project is using the VIVO Semantic Web software suite to support the discovery of information, data, and potential collaborators within the geodesy and polar science communities. This paper discusses the ontology selection, consolidation, and reuse efforts of EarthCollab. EarthCollab's ontology design approach heavily emphasizes ontology reuse, bringing together existing ontologies to support diverse use cases related to the discovery of geoscience information and resources. We developed a small local ontology to tie these existing ontologies together and to build appropriate geoscience-relevant connections. Five key ontology decision drivers are presented to outline EarthCollab's ontology design process and decision points: use cases, existing systems and metadata, semantic application dependencies, external ontology characteristics, and community recommendations for good ontological modeling practices. © 2016 The Author(s)."
"10.5334/dsj-2016-002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84975883387&doi=10.5334%2fdsj-2016-002&partnerID=40&md5=c4ed69c5aef89124d010414a4e2b984e","The Institut Pierre Simon Laplace (IPSL) encompasses a wide diversity of projects that focus on the Arctic. From these observations the IPSL has generated a large number of datasets gathering Arctic observations. These observations include measurements on atmospheric chemical composition, snow micro-physical properties or ocean measurements. However, some of these datasets remain locally stored and there is a lack of public awareness regarding these resources, which has hindered their visualisation and sharing. This motivated the creation of the LABEX L-IPSL Arctic metadata Portal (http://climserv.ipsl.polytechnique.fr/arcticportal/), presented here, which improves the visibility of the variety of observations collected within the institute as well as the evaluation of numerical models. The LABEX L-IPSL Arctic metadata Portal will also promote new avenues in Arctic research within the IPSL and with other collaborating institutions. © 2016 by the authors."
"10.5334/dsj-2016-005","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84975873995&doi=10.5334%2fdsj-2016-005&partnerID=40&md5=20b98294a5562256e75a17140d73241c","The discipline of data science has emerged over the past decade as a convergence of high-power computing, data visualization and analysis, and data-driven application domains. Prominent research institutions and private sector industry have embraced data science, but foundations for effective tertiary-level data science education remain absent. This is nothing new, however, as the university has an established tradition of developing its educational mission hand-in-hand with the development of novel methods for human understanding (Feingold, 1991). Thus, it is natural that universities ""figure out"" data science concurrent with the development of needed pedagogy. We consider data science education with respect to recent trends in interdisciplinary and experiential educational methodologies. The first iteration of the Berkeley Institute for Data Science (BIDS) Collaborative, which took place at the University of California, Berkeley in the Spring of 2015, is used as a case study. From this, we draw lessons learned regarding the necessary components of effective tertiary data science education, which range from a complete end-to-end workflow, technological tools for development and team communications, and appropriate motivation and incentives. Our findings will be tested and revised in subsequent iterations of the BIDS Collaborative as we continue our study of data science education, research, and social impact. © 2016 by the authors."
"10.5334/dsj-2016-004","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84975832621&doi=10.5334%2fdsj-2016-004&partnerID=40&md5=1f5b739d67b2142ee325f5a93ce0845e","Scalability is an increasingly important target for distributed real-time databases. Replication is widely applied to improve the scalability and availability of data. With full replication, database systems cannot scale well, since all updates must be replicated to all nodes, whether or not they are needed there. With virtual Full Replication, all nodes have an image of a fully replicated database and the system manages the knowledge of what is needed for each node to adapt to the actual needs, so that the system can be more scalable. This work proposes a scalable and consistent replication protocol using an adaptive clustering technique that dynamically detects the new data requirements. Because time is critical in such systems, the clustering technique must take into account both the communication time cost and the timing properties of the data. The proposed protocol also proposes a new updated method for addressing the temporal inconsistency problem by skipping unnecessary operations. It allows many database nodes to update their data concurrently, without any need for distributed synchronization. It uses state-transfer propagation with on-demand integration techniques to reduce the temporal inconsistency. The experimental results show the ability of the proposed protocol to reduce the system resources consumed and improves system scalability while maintaining consistency. © 2016 by the authors."
"10.5334/dsj-2016-001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84975824100&doi=10.5334%2fdsj-2016-001&partnerID=40&md5=0382662b69596b3d9164d27172806471","The majority of restoration strategies in the wake of large-scale disasters have focused on short-term emergency response solutions. Few consider medium- to long-term restoration strategies to reconnect urban areas to national supply chain interdependent critical infrastructure systems (SCICI). These SCICI promote the effective flow of goods, services, and information vital to the economic vitality of an urban environment. To re-establish the connectivity that has been broken during a disaster between the different SCICI, relationships between these systems must be identified, formulated, and added to a common framework to form a system-level restoration plan. To accomplish this goal, a considerable collection of SCICI data is necessary. The aim of this paper is to review what data are required for model construction, the accessibility of these data, and their integration with each other. While a review of publically available data reveals a dearth of real-time data to assist modeling long-term recovery following an extreme event, a significant amount of static data does exist and these data can be used to model the complex interdependencies needed. For the sake of illustration, a particular SCICI (transportation) is used to highlight the challenges of determining the interdependencies and creating models capable of describing the complexity of an urban environment with the data publically available. Integration of such data as is derived from public domain sources is readily achieved in a geospatial environment, after all geospatial infrastructure data are the most abundant data source and while significant quantities of data can be acquired through public sources, a significant effort is still required to gather, develop, and integrate these data from multiple sources to build a complete model. Therefore, while continued availability of high quality, public information is essential for modeling efforts in academic as well as government communities, a more streamlined approach to a real-time acquisition and integration of these data is essential. © 2016 by the authors."
"10.5334/dsj-2016-003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84975801822&doi=10.5334%2fdsj-2016-003&partnerID=40&md5=792a4473004b8c1d215f2a24929ca9d1","The global application of the Crystallographic Information Framework (CIF) to the molecular structure domain has been successful over the past 20 years. It is used widely by molecular science journals and databases for submission, validation and deposition. This paper will give an overview of the CIF implementation, highlighting its particular successes and occasional failures. It will also recommend criteria for the application of an ontology-based data management system to any information domain. The paper will conclude with some details of the latest STAR data definition language and the importance of methods to the preservation of derivative data items. © 2016 by the authors."
"10.1140/epjds/s13688-016-0080-6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84971254258&doi=10.1140%2fepjds%2fs13688-016-0080-6&partnerID=40&md5=622b92f2d8066db405f6ec08b84a7ade","Gender homophily, or the preference for interaction with individuals of the same gender, has been observed in many contexts, especially during childhood and adolescence. In this study we investigate such phenomenon by analyzing the interactions of the ∼10 million users of Tuenti, a Spanish social networking service popular among teenagers. In dyadic relationships we find evidence of higher gender homophily for women. We also observe a preference of users with more friends to connect to the opposite gender. A particularly marked gender difference emerges in signing up for the social networking service and adding the first friends, and in the interactions by means of wall messages. In these contexts we find evidence of a strong homophily for women, and little or no homophily for men. By examining the gender composition of triangle motifs, we observe a marked tendency of users to group into gender homogeneous clusters, with a particularly high number of male-only triangles. We show that age plays an important role in this context, with a tendency to higher homophily for young teenagers in both dyadic and triadic relationships. Our findings have implications for addressing gender gap issues, understanding adolescent online behavior and technology adoption, and modeling social networks. © 2016, Laniado et al."
"10.1504/IJBIDM.2016.082214","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013113195&doi=10.1504%2fIJBIDM.2016.082214&partnerID=40&md5=f2e26b1173143bbe3ace9fe0d07420ae","Time series is a sequence of continuous and unbounded group of data observations recorded from many applications. Time series motif discovery is an essential and important task in time series mining. Discovering motifs in time series has attracted the researcher's attention for efficient time series classification problems and several algorithms have been proposed to solve the problem. However, these algorithms depend on predefined parameters like support, confidence, and length of the motif and they are sensitive to the parameters. To overcome the challenge, this paper proposes a multi-objective genetic algorithm to discover a good trade-off between representative and interesting motif. The discovered motifs are validated for their potential interest in time series classification using nearest neighbour classifier. Extensive experiments show that the proposed approach can efficiently discover motifs with different lengths and more accurate than state-of-the-art time series techniques. The paper also demonstrates the efficiency of motif discovery in classifying the large time series medical data from MIT-BIH-arrhythmia database. Copyright © 2016 Inderscience Enterprises Ltd."
"10.1504/IJBIDM.2016.082213","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013104658&doi=10.1504%2fIJBIDM.2016.082213&partnerID=40&md5=706d76a68d970c5008e672662d8965c2","We present a framework that we are currently developing, that allows one to extract knowledge from the knowledge discovery in database (KDD) dataset. Data mining is a very active and space growing research area. Knowledge discovery in databases (KDD) is very useful in scientific domains. In simple terms, association rule mining is one of the most well-known methods for such knowledge discovery. Initially, database are divided into training and testing for the aid of fuzzy generating the rules using fuzzy rules generation the set of rules are generated from the given dataset. From the generated rules, we are extracting the significant rules by using the improved artificial bee colony algorithm and cuckoo search algorithm (IABCCS). After extracting optimal knowledge from the dataset via rules, the data will be classified using fuzzy classifier with the aid of this finally we will classify the attack and normal. Copyright © 2016 Inderscience Enterprises Ltd."
"10.1504/IJBIDM.2016.082212","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013092899&doi=10.1504%2fIJBIDM.2016.082212&partnerID=40&md5=3f2b72ce94074b796e9f3e587e758e7d","The overall process of the classification method is divided into two main steps, such as: 1) feature selection using RS-MKFCM algorithm; 2) classification using CGA-based neural network classifier. At first, the multiple kernel fuzzy c-means clustering with rough set theory (RS-MKFCM) algorithm is applied on the high dimensional micro array dataset to select the important features. After that, the classification is done through CGA-NN classifier. Selected features from micro array dataset are collected and fed to the neural network for training. In neural network, we utilise scaled conjugate gradient algorithm for training. It provides faster training with excellent test efficiency. To improve the classification performance, hybridisation of cuckoo search and genetic algorithm (CGA) is utilised with neural network for weight optimisation process. At last, the experimentation is performed by means of the five different micro array dataset. The experimentation result proves that the CGA-NN classifier outperformed the existing approach by attaining the maximum accuracy of 98.93% for ovarian cancer dataset when compared existing NN only achieved 51.87% and also support vector machine classifier achieves the 88.23%. Copyright © 2016 Inderscience Enterprises Ltd."
"10.1504/IJBIDM.2016.082215","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013076913&doi=10.1504%2fIJBIDM.2016.082215&partnerID=40&md5=138bc2d99999e1707645cef76941e7c7","The proposed research aims to integrate contextual information with tourist spot recommender system. Ubiquitous crowdsourcing model has been used for collecting the contextual information using fuzzy linguistic values in current time. The existing tourist spot recommender systems give list tourist spots along with their ratings. Proposed model takes this list as an input and the current fuzzy contextual information collected from crowd present at the spot and computes a new score for each spot/location. This new score not only conveys the popularity of a spot in vicinity but also advices whether this spot is worth visiting at given point in time. A prototype system has been implemented using java and MATLAB and is evaluated by 104 real users. Copyright © 2016 Inderscience Enterprises Ltd."
"10.1504/IJBIDM.2016.082216","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013046207&doi=10.1504%2fIJBIDM.2016.082216&partnerID=40&md5=430e8598774666ac03b5352b4f5f261e","In information technology, one of the modern and rising trends is cloud computing, the foremost problems in cloud computing is task scheduling. A good scheduling algorithm has to reduce the execution time and cost together with QoS requirements of the users. In this paper, a task scheduling scheme with the aid of a hybridisation of BAT and artificial bee colony (BABC) on diverse computing systems is proposed. In BABC, each dimension of a solution represents a task and a solution as a whole signifies all tasks priorities. The vital issue is how to allocate users tasks to exploit the income of infrastructure as a service (IaaS) provider while promising quality-of-service (QoS). The generated solution is proficient to quality of service (QoS) and improves IaaS providers' credibility and economic benefit. According to the evolved results, it has been found that our algorithm always outperforms the traditional algorithms. Copyright © 2016 Inderscience Enterprises Ltd."
"10.1504/IJBIDM.2016.081866","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85011310864&doi=10.1504%2fIJBIDM.2016.081866&partnerID=40&md5=f250b76de8035a31cecfe7b9859c9d78","Feature subset selection is the process of identifying and removing many irrelevant and redundant features. Initially, the input micro array dataset is selected from the medical database. Then, preprocessing step is done in the input dataset. The resultant output is fed to the second step; here, the features are optimally selected using clustering and tree generation process. In our proposed technique, the modified kernel-based fuzzy c-means clustering algorithm with optimal minimum spanning tree algorithm is applied on the high dimensional dataset to select the important features, in which the optimal features are selected by means of binary cuckoo search. Next, the classification is done through neuro fuzzy classifier. At last, the experimentation is performed by means of different micro array dataset. The result proves that neuro fuzzy classifier outperformed the existing approach by attaining maximum accuracy of 89% for GLA-BRA-180 dataset when compared existing NN only achieved 68.2%, fuzzy classifier attains 63.1% and KNN classifier attains 67.3%. © 2016 Inderscience Enterprises Ltd."
"10.1504/IJBIDM.2016.081867","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85011295567&doi=10.1504%2fIJBIDM.2016.081867&partnerID=40&md5=bfdc30b9db9ab1a2ff8230e80ace7d02","The data mining, nowadays, has surfaced as an investigative procedure devoted to the exploration of the data in the hunt for the reliable patterns or methodical associations between the variables. In the current investigation, an earnest effort made to employ an effective hybrid clustering approach to cluster the record and regain the record in accordance with the pattern mining. The novel technique consists of two vital steps such as the training and testing stages. In the training stage, the closed itemsets of each record are extorted by means of the support values, paving the way for the incredible decrease in the error making items. In the testing stage, the records having identical or approximately identical weights are clustered by means of the hybrid K-means-GSO clustering algorithm. Consequently, records at the apex of rank list are regained from the testing stage. The epoch-making technique is performed in the Java platform. © 2016 Inderscience Enterprises Ltd."
"10.1504/IJBIDM.2016.081864","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85011290457&doi=10.1504%2fIJBIDM.2016.081864&partnerID=40&md5=ea118f9f80de4091886fcd8648d28c8c","Periodic patterns occur in a wide variety of datasets like time series, temporal, spatiotemporal and biological datasets. Asynchronous periodic pattern mining discovers the patterns that appear significantly with strict periodicity in one or more subsequences called valid segments with tolerable disturbance between the valid segments. The state-of-the-art asynchronous periodic pattern mining algorithms proposed in the literature mine many redundant asynchronous periodic patterns. This paper proposes an algorithm to mine non-redundant asynchronous periodic patterns. It is a variation of a four-phase algorithm named SMCA. The proposed algorithm mines precise and concise set of non-redundant patterns whose number is in orders of magnitude smaller than that of previous methods. The proposed algorithm is explained using a hypothetical dataset and its performance is evaluated using three real data sets which are stock market data from Bombay Stock Exchange, Weather data from Cambridge University and Migratory Zebras' data from MoveBank. © 2016 Inderscience Enterprises Ltd."
"10.1504/IJBIDM.2016.081865","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85011265930&doi=10.1504%2fIJBIDM.2016.081865&partnerID=40&md5=5f1dde14b5c10ebab3e64005e21b59b2","In impulsive shopping, proper packaging of a product attracts the consumer and makes that particular product outstanding among its counter parts on the same shelf. Colour is one of the major elements in packaging design. In food products packages, colour convey subconscious information regarding quality, taste, pleasure and even the price to the consumer. The objective here is to determine which one of the colours or their combination, on food products like as cakes, biscuits, and milk would lead to an increase in sales of the products' categories. Here for each of the food products, some brands with similar quality, price and commercialisation are selected. Then, the proper colours for packaging of each of the products are extracted using data mining. The results indicate that the extracted colours can contribute to distinction between slow-seller and best-seller brands by 21%; therefore, the use of such colours would increase the products' sales. © 2016 Inderscience Enterprises Ltd."
"10.1504/IJBIDM.2016.081869","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85011260303&doi=10.1504%2fIJBIDM.2016.081869&partnerID=40&md5=15b49d8a73fd453be60906c9c7f6199d","Breast cancer is the most dangerous cancer among women and second mortality among them. Mammography is the efficient methodology used in early finding of breast cancer. However, mammograms requires high amount of skill and there is a possibility of radiologist to misunderstand it. Hence, computer aided diagnosis are used for finding the abnormalities in mammograms. Automated classification of mass and microcalcification system is proposed in this work using NSCT and SVM. The classification of abnormalities is achieved by extracting the microcalcification and mass features from the contourlet coefficients of the image and the results are used as an input to the SVM. The proposed automated system classifies the mammogram as normal or abnormal and result is abnormal, then it classifies the abnormal severity as benign or malignant. The evaluation of the proposed system is conceded on MIAS database. The experimentation result shows that the proposed system contributes improved classification rate. © 2016 Inderscience Enterprises Ltd."
"10.1504/IJBIDM.2016.081608","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85010825704&doi=10.1504%2fIJBIDM.2016.081608&partnerID=40&md5=973848c1789cecf3faf4b90202f59804","Association rule mining is an important task in data mining which discovers hidden associations between items in the database based on user-specified support and confidence thresholds. To find the relevant associations, an appropriate threshold has to be specified. The support threshold plays a vital role in the quantity and quality of the rules found. The challenge is that one should not miss the rare associations and on the other hand uninteresting associations should not be generated. This paper proposes an approach to obtain the appropriate support thresholds at each level of the level-wise mining approach. It sets the support threshold by analysing the frequency of items and their associations in the database at each level. It uses the central measure of tendency and measure of dispersion to analyse the database and sets the thresholds accordingly. The performance of the proposed approach has been evaluated against multiple sparse and dense datasets. Experimental results show that this approach produces the interesting rules without specifying the user specified support threshold. © 2016 Inderscience Enterprises Ltd."
"10.1504/IJBIDM.2016.081605","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85010748382&doi=10.1504%2fIJBIDM.2016.081605&partnerID=40&md5=cd422663ee445265ca73e324615e2041","The cloud computing has elegantly emerged as a unique category of computing which is invariably dependent on sharing computing resources reasonably possessing the local servers or personal devices to successfully address the applications. For the purpose of effectively overcoming the related issues, an optimal blowfish algorithm-based approach is envisioned for the effective encryption and decryption. In the relative step, an innovative signcryption algorithm is elegantly employed for signature authentication. The key manager is entrusted with the task of authenticating the policy related to the file. If the policy precisely corresponds to the file name, then an identical public key is created. If not, a new public key is generated and with the help of public key and private key, the file is encrypted and uploaded into the cloud. The proposed method provides higher security in terms of the data usage for individual persons. © 2016 Inderscience Enterprises Ltd."
"10.1504/IJBIDM.2016.081604","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85010737685&doi=10.1504%2fIJBIDM.2016.081604&partnerID=40&md5=eb021d781873e9ab3dbc374903f3990b","The need for evaluation models capable of returning 'slender' and reliable mass appraisals of properties belonging to different market segments has been made mandatory by the events that are covering the global real estate finance, because of the emergence of non-performing loans in the banks' balance sheets. In Italy, the non-performing loans have been estimated by the Italian Banking Association equal to about 300 billion euro in 2014. In the present paper, three approaches of data-driven techniques (hedonic price model, artificial neural networks and evolutionary polynomial regression) have been applied to a sample of residential apartments recently sold in a district of the city of Bari (Italy), in order to test the respective performance for mass appraisals. The models obtained by the implementation of the three procedures have been compared in terms of statistical accuracy, empirical compliance of the results and complexity of the functional relationships. © 2016 Inderscience Enterprises Ltd."
"10.1504/IJBIDM.2016.081612","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85010735777&doi=10.1504%2fIJBIDM.2016.081612&partnerID=40&md5=3fd0dc038690aaec375c9af4f42d82e0","The paper presents a novel application to extract biomedical entities automatically using machine learning techniques from large volumes of biomedical text. The data in large quantities are accumulating day by day and requires automatic extraction of information. Data mining is the science of extracting information from large data. Biomedical Named entity recognition (BioNER) is the task of data mining that extracts named entities from biological texts. In this paper, we focus on developing a BioNER system for extraction of biological target, disease and chemical entities from biomedical texts. We developed the system using graphical based machine learning technique the CRFs. We have applied a set of diverse features containing standard lexical, syntactic and orthographic features combined with novel and biologically inspired features, action terms and process verbs. The system was evaluated with three widely recognised datasets. The results demonstrated the portability and the potency of the system. © 2016 Inderscience Enterprises Ltd."
"10.1504/IJBIDM.2016.076426","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84969872681&doi=10.1504%2fIJBIDM.2016.076426&partnerID=40&md5=36ecba2e7babc12589f0fa919b5f29bb","A new method to fill in, or impute, missing prices in retail price time series datasets is proposed, called retail price time series imputation (RPTSI). It is constructed from an ensemble of three existing methods: namely, price change lookup, central moving average, and polynomial interpolation. Four extended variations of RPTSI are also proposed by considering historical prices for similar products sold by the same retailer and equivalent products sold by competing retailers. Crowdsourced datasets from four North American cities over a year and a half period were used in experiments to evaluate the five RPTSI-based methods and to compare the results against those obtained using last value carried forward, mean imputation, moving average, polynomial interpolation, and multiple imputation. Accuracy was measured by using mean absolute imputation error. Experimental results showed that the RPTSI-based methods had significantly higher accuracy than the other methods on both univariate and multivariate time series datasets. © Copyright 2016 Inderscience Enterprises Ltd."
"10.1504/IJBIDM.2016.076435","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84969835208&doi=10.1504%2fIJBIDM.2016.076435&partnerID=40&md5=a2e4e8b4ff84f341450eef86e32dc7c6","With the recent growth of bibliographic data, many research fields work on defining new techniques for their analysis. In this context, data could be represented as heterogeneous networks. In order to analyse information networks in a multidimensional way, online analytical processing (OLAP) may be a relevant solution but it must be adapted for networked data by considering nodes and edges. A first approach that has been proposed in the literature consists in building cubes of graphs. In a different and complementary way, our proposal consists in enriching graphs with cubes. Indeed, the nodes or/and edges of the considered network are described by a cube. It allows interesting analyses for the user who can navigate within a graph enriched by cubes according to different granularity levels, with dedicated operators. We implemented our approach and performed an experimental study on a real dataset to show the interest of our proposal. © Copyright 2016 Inderscience Enterprises Ltd."
"10.1504/IJBIDM.2016.076433","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84969820348&doi=10.1504%2fIJBIDM.2016.076433&partnerID=40&md5=62fa979603d6c307f538c76e57011445","The internet era has created highly favourable circumstances for business and online services. The need of getting personified recommendations from the increasing pile of information had forced researchers to build recommendation system. In most of the e-commerce websites, recommendation preferences are made by analysing user's behaviour, existing relevant information, browsing patterns, transactional data, item rating and by extracting information from social network profiles. Recommendations predicted from this kind of information can also go futile. Since many of the users gives irrelevant information and many change their views with changing in timely trends. In our work, we are concentrating on generating recommendations based on user's locality. Initially, offline clusterisation is done based on existing location data using CNM-algorithm. The cluster that corresponds to the user request is analysed, and fuzzy preference-based tree is constructed to generate recommendations. © Copyright 2016 Inderscience Enterprises Ltd."
"10.1504/IJBIDM.2016.076425","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84969751516&doi=10.1504%2fIJBIDM.2016.076425&partnerID=40&md5=31164c0f2ee3d4c2fb9eca900d5284ac","Data warehousing and online analytical processing (OLAP) are essential elements to decision support. In the case of textual data, decision support requires new tools, mainly textual aggregation functions, for better and faster high level analysis and decision making. Such tools will provide textual measures to users who wish to analyse documents online. In this paper, we propose a new aggregation function for textual data in an OLAP context based on the K-means method. This approach will highlight aggregates semantically richer than those provided by classical OLAP operators. The distance used in K-means is replaced by the Google similarity distance which takes into account the semantic similarity of keywords for their aggregation. The performance of our approach is analysed and compared to other methods such as Topkeywords, TOPIC, TuBE and BienCube. The experimental study shows that our approach achieves better performances in terms of recall, precision, F-measure complexity and runtime. © Copyright 2016 Inderscience Enterprises Ltd."
"10.1504/IJBIDM.2016.076413","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84969751454&doi=10.1504%2fIJBIDM.2016.076413&partnerID=40&md5=76fbb5694b1e8e2e280ce76c3f1db0d0","Analysis of web server logs of e-business organisations is critical to provide insight into users' web usage behaviour which can assist in designing most attractive websites. In this article, a mountain density function (MDF)-based fuzzy clustering framework to discover user session clusters from web logs is proposed. Major steps in this framework include web log preprocessing, MDF-based discovery of user session clusters and their validation. To deal with high dimensionality of user sessions, a fuzzy approach for assigning weights to user sessions has been proposed. For the discovery of user session clusters, fuzzy c-means (FCM) and fuzzy c-medoids (FCMed) algorithms are explored. Since the selection of suitable initial cluster centres is a big challenge, MDF-based fuzzy c-means (MDFCM) and fuzzy c-medoids (MDFCMed) algorithms are proposed to overcome this problem. Our results show that quality of clusters formed using MDFCM/MDFCMed is much better than FCM and FCMed. © Copyright 2016 Inderscience Enterprises Ltd."
"10.1504/IJBIDM.2016.076418","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84969749347&doi=10.1504%2fIJBIDM.2016.076418&partnerID=40&md5=d006dfdc89cb3fe4351697bd51680ed6","Businesses and customers are interested in predicting the retail prices of products. In a competitive environment, the price of a product at a given target outlet is typically related to the price of the same or similar products at nearby competing outlets. This research predicts the start of day and current prices of a specific product at every outlet in a given city using four vector autoregression models that incorporate the historical retail prices of the product at a target outlet and at competing outlets. The models also include the estimated wholesale price of the product. Three ways of identifying local competitors are considered. The wholesale supplier is that with similar pricing patterns to a target outlet. The proposed models outperform a simple autoregression approach that does not include local competitors or wholesale prices in experiments carried out using data obtained from outlets in five North American cities. © Copyright 2016 Inderscience Enterprises Ltd."
"10.4018/IJBAN.2016010102","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045863739&doi=10.4018%2fIJBAN.2016010102&partnerID=40&md5=f27d2c28e3e5b514b47e4122df6727bc","One way to enhance the likelihood that more university students will graduate within the specific major that they begin with is to attract the type of students who have typically (historically) done well in that field of study. This paper expands upon a study that utilizes data mining techniques to analyze the characteristics of students who enroll as actuarial students and then either drop out of the major or graduate as actuarial students. Several predictive models including logistic regression, neural networks and decision trees are obtained using input variables describing academic attributes of the students. The models are then compared and the best fitting model is determined. The regression model turns out to be the best predictor. Since this is a very well understood method, it can easily be explained. The decision tree, although its underpinnings are somewhat difficult to explain, gives a clear and well understood output. In addition, the non-predictive method of cluster analysis is applied in order to group these students into distinct classifications based on the values of the input variables. Finally, a new approach to modeling in SAS®, called Rapid Predictive Modeler (RPM), is described and utilized. The results of the RPM also select the regression model as the best predictor. Copyright © 2016,"
"10.4018/IJBAN.2016010103","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045697147&doi=10.4018%2fIJBAN.2016010103&partnerID=40&md5=3b4da32252dffc7055d191b6a8c7e08c","Temporal association rule mining is a data mining technique in which relationships between items which satisfy certain timing constraints can be discovered. This paper presents the concept of temporal association rules in order to solve the problem of classification of inventories by including time expressions into association rules. Firstly, loss profit of frequent items is calculated by using temporal association rule mining algorithm. Then, the frequent items in particular time-periods are ranked according to descending order of loss profits. The manager can easily recognize most profitable items with the help of ranking found in the paper. An example is illustrated to validate the results. Copyright © 2016, IGI Global."
"10.1089/big.2015.0020","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84991830734&doi=10.1089%2fbig.2015.0020&partnerID=40&md5=2b1c6c332a8f65c84584c02d374a86e7","We present a new approach to population health, in which data-driven predictive models are learned for outcomes such as type 2 diabetes. Our approach enables risk assessment from readily available electronic claims data on large populations, without additional screening cost. Proposed model uncovers early and late-stage risk factors. Using administrative claims, pharmacy records, healthcare utilization, and laboratory results of 4.1 million individuals between 2005 and 2009, an initial set of 42,000 variables were derived that together describe the full health status and history of every individual. Machine learning was then used to methodically enhance predictive variable set and fit models predicting onset of type 2 diabetes in 2009-2011, 2010-2012, and 2011-2013. We compared the enhanced model with a parsimonious model consisting of known diabetes risk factors in a real-world environment, where missing values are common and prevalent. Furthermore, we analyzed novel and known risk factors emerging from the model at different age groups at different stages before the onset. Parsimonious model using 21 classic diabetes risk factors resulted in area under ROC curve (AUC) of 0.75 for diabetes prediction within a 2-year window following the baseline. The enhanced model increased the AUC to 0.80, with about 900 variables selected as predictive (p < 0.0001 for differences between AUCs). Similar improvements were observed for models predicting diabetes onset 1-3 years and 2-4 years after baseline. The enhanced model improved positive predictive value by at least 50% and identified novel surrogate risk factors for type 2 diabetes, such as chronic liver disease (odds ratio [OR] 3.71), high alanine aminotransferase (OR 2.26), esophageal reflux (OR 1.85), and history of acute bronchitis (OR 1.45). Liver risk factors emerge later in the process of diabetes development compared with obesity-related factors such as hypertension and high hemoglobin A1c. In conclusion, population-level risk prediction for type 2 diabetes using readily available administrative data is feasible and has better prediction performance than classical diabetes risk prediction algorithms on very large populations with missing data. The new model enables intervention allocation at national scale quickly and accurately and recovers potentially novel risk factors at different stages before the disease onset. © Mary Ann Liebert, Inc. 2015."
"10.1089/big.2015.0033","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84991829574&doi=10.1089%2fbig.2015.0033&partnerID=40&md5=250c5e7d0c1c30ccdda350eef942ee88","We present a Bayesian method for building scoring systems, which are linear models with coefficients that have very few significant digits. Usually the construction of scoring systems involve manual effort - humans invent the full scoring system without using data, or they choose how logistic regression coefficients should be scaled and rounded to produce a scoring system. These kinds of heuristics lead to suboptimal solutions. Our approach is different in that humans need only specify the prior over what the coefficients should look like, and the scoring system is learned from data. For this approach, we provide a Metropolis-Hastings sampler that tends to pull the coefficient values toward their ""natural scale."" Empirically, the proposed method achieves a high degree of interpretability of the models while maintaining competitive generalization performances. © Mary Ann Liebert, Inc. 2015."
"10.1089/big.2015.0038","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84991781053&doi=10.1089%2fbig.2015.0038&partnerID=40&md5=6b4ddab01070f75b302560725ec67598","Electronic Healthcare Records (EHRs) have the potential to improve healthcare quality and to decrease costs by providing quality metrics, discovering actionable insights, and supporting decision-making to improve future outcomes. Within the United States Medicaid Program, rates of recidivism among emergency department (ED) patients serve as metrics of hospital performance that help ensure efficient and effective treatment within the ED. We analyze ED Medicaid patient data from 1,149,738 EHRs provided by a hospital over a 2-year period to understand the characteristics of the ED return visits within a 72-hour time frame. Frequent flyer patients with multiple revisits account for 47% of Medicaid patient revisits over this period. ED encounters by frequent flyer patients with prior 72-hour revisits in the last 6 months are thrice more likely to result in a readmit than those of infrequent patients. Statistical L1-logistic regression and random forest analyses reveal distinct patterns of ED usage and patient diagnoses between frequent and infrequent patient encounters, suggesting distinct opportunities for interventions to improve efficacy of care and streamline ED workflow. This work forms a foundation for future development of predictive models, which could flag patients at high risk of revisiting. © Mary Ann Liebert, Inc. 2015."
"10.1089/big.2015.0019","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84991753476&doi=10.1089%2fbig.2015.0019&partnerID=40&md5=f9b636641490ac4e669519c3389e43d4","Pathogen distribution models that predict spatial variation in disease occurrence require data from a large number of geographic locations to generate disease risk maps. Traditionally, this process has used data from public health reporting systems; however, using online reports of new infections could speed up the process dramatically. Data from both public health systems and online sources must be validated before they can be used, but no mechanisms exist to validate data from online media reports. We have developed a supervised learning process to validate geolocated disease outbreak data in a timely manner. The process uses three input features, the data source and two metrics derived from the location of each disease occurrence. The location of disease occurrence provides information on the probability of disease occurrence at that location based on environmental and socioeconomic factors and the distance within or outside the current known disease extent. The process also uses validation scores, generated by disease experts who review a subset of the data, to build a training data set. The aim of the supervised learning process is to generate validation scores that can be used as weights going into the pathogen distribution model. After analyzing the three input features and testing the performance of alternative processes, we selected a cascade of ensembles comprising logistic regressors. Parameter values for the training data subset size, number of predictors, and number of layers in the cascade were tested before the process was deployed. The final configuration was tested using data for two contrasting diseases (dengue and cholera), and 66%-79% of data points were assigned a validation score. The remaining data points are scored by the experts, and the results inform the training data set for the next set of predictors, as well as going to the pathogen distribution model. The new supervised learning process has been implemented within our live site and is being used to validate the data that our system uses to produce updated predictive disease maps on a weekly basis. © Helena M.M. Patching et al. 2016; Published by Mary Ann Liebert, Inc. 2015."
"10.1089/big.2015.0049","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84991746002&doi=10.1089%2fbig.2015.0049&partnerID=40&md5=558a2d89171c47c87c290cdbf6287b37","The last several years have seen an explosion of interest in wearable computing, personal tracking devices, and the so-called quantified self (QS) movement. Quantified self involves ordinary people recording and analyzing numerous aspects of their lives to understand and improve themselves. This is now a mainstream phenomenon, attracting a great deal of attention, participation, and funding. As more people are attracted to the movement, companies are offering various new platforms (hardware and software) that allow ever more aspects of daily life to be tracked. Nearly every aspect of the QS ecosystem is advancing rapidly, except for analytic capabilities, which remain surprisingly primitive. With increasing numbers of qualified self participants collecting ever greater amounts and types of data, many people literally have more data than they know what to do with. This article reviews the opportunities and challenges posed by the QS movement. Data science provides well-tested techniques for knowledge discovery. But making these useful for the QS domain poses unique challenges that derive from the characteristics of the data collected as well as the specific types of actionable insights that people want from the data. Using a small sample of QS time series data containing information about personal health we provide a formulation of the QS problem that connects data to the decisions of interest to the user. © Mary Ann Liebert, Inc. 2015."
"10.1016/j.bdr.2015.04.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84947033929&doi=10.1016%2fj.bdr.2015.04.002&partnerID=40&md5=bcde997813658988acc31133de7f8efc","The problem of collective classification (CC) for large-scale network data has received considerable attention in the last decade. Enabling CC usually increases accuracy when given a fully-labeled network with a large amount of labeled data. However, such labels can be difficult to obtain and learning a CC model with only a few such labels in large-scale sparsely labeled networks can lead to poor performance. In this paper, we show that leveraging the unlabeled portion of the data through semi-supervised collective classification (SSCC) is essential to achieving high performance. First, we describe a novel data-generating algorithm, called generative model with network regularization (GMNR), to exploit both labeled and unlabeled data in large-scale sparsely labeled networks. In GMNR, a network regularizer is constructed to encode the network structure information, and we apply the network regularizer to smooth the probability density functions of the generative model. Second, we extend our proposed GMNR algorithm to handle network data consisting of multi-label instances. This approach, called the multi-label regularized generative model (MRGM), includes an additional label regularizer to encode the label correlation, and we show how these smoothing regularizers can be incorporated into the objective function of the model to improve the performance of CC in multi-label setting. We then develop an optimization scheme to solve the objective function based on EM algorithm. Empirical results on several real-world network data classification tasks show that our proposed methods are better than the compared collective classification algorithms especially when labeled data is scarce. © 2015 Elsevier Inc.."
"10.1016/j.bdr.2015.02.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84946837818&doi=10.1016%2fj.bdr.2015.02.001&partnerID=40&md5=1389cb3e90fca4e55fd26b08c29f4cf3","Big data analytics is the key research subject for future data driven decision making applications. Due to the large amount of data, progressive analytics could provide an efficient way for querying big data clusters. Each cluster contains only a piece of the examined data. Continuous queries over these data sources require intelligent mechanisms to result the final outcome (query response) in the minimum time with the maximum performance. A Query Controller (QC) is responsible to manage continuous/sequential queries and return the final outcome to users or applications. In this paper, we propose a mechanism that can be adopted by the QC. The proposed mechanism is capable of managing partial results retrieved by a number of processors each one responsible for each cluster. Each processor executes a query over a specific cluster of data. Our mechanism adopts two sequential decision making models for handling the incoming partial results. The first model is based on a finite horizon time-optimized model and the second one is based on an infinite horizon optimally scheduled model. We provide mathematical formulations for solving the discussed problem and present simulation results. Through a large number of experiments, we reveal the advantages of the proposed models and give numerical results comparing them with a deterministic model. These results indicate that the proposed models can efficiently reduce the required time for returning the final outcome to the user/application while keeping the quality of the aggregated result at high levels. © 2015 Elsevier Inc.."
"10.1016/j.bdr.2015.09.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84946826631&doi=10.1016%2fj.bdr.2015.09.001&partnerID=40&md5=aa8b835641a18c9747fad72c7bb12f78","During periods of high volume, big data stream applications may not have enough resources to process all incoming tuples. To maximize the production of the most critical results under such resource shortages, a recent solution, PR (short for Preferential Result), utilizes both static criteria (defined at compile-time) and dynamic criteria (identified online at run-time) to prioritize the processing of tuples throughout the query pipeline. Unfortunately, locating the optimal criteria placement (i.e., where in the query pipeline to evaluate each prioritization criteria) is extremely compute-intensive and runs in exponential time. This makes PR impractical for complex big data stream systems. Our proposed criteria selection and placement approach, PR-Prune (short for Preferential Result-Pruning), is practical. PR-Prune prunes ineffective dynamic criteria and combines multiple criteria along the same pipeline. To achieve this, PR-Prune seeks to expand the duration in the query pipeline that tuples identified as critical are pulled forward. Our experiments use a real data stream from the S&P 500 stocks, synthetic data streams, and a diverse set of queries. The results substantiate that PR-Prune increases the production of the most critical results compared to the state-of-the-art approaches. In addition, PR-Prune significantly lowers the optimization search time compared to PR. © 2015 Elsevier Inc."
"10.1140/epjds/s13688-015-0045-1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84958778477&doi=10.1140%2fepjds%2fs13688-015-0045-1&partnerID=40&md5=44bc7a0513f3f5effd72d2e6cf6392bc","Managerial decision making is likely to be a dominant determinant of performance of teams in team sports. Here we use Japanese and German football data to investigate correlates between temporal patterns of formation changes across matches and match results. We found that individual teams and managers both showed win-stay lose-shift behavior, a type of reinforcement learning. In other words, they tended to stick to the current formation after a win and switch to a different formation after a loss. In addition, formation changes did not statistically improve the results of succeeding matches. The results indicate that a swift implementation of a new formation in the win-stay lose-shift manner may not be a successful managerial rule of thumb. © 2015, Tamura and Masuda."
"10.1140/epjds/s13688-015-0052-2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84958777153&doi=10.1140%2fepjds%2fs13688-015-0052-2&partnerID=40&md5=61b5c8174f333d22a100b83fc1f2ed4e","The hypothesis of preferential attachment (PA) - whereby better connected individuals make more connections - is hotly debated, particularly in the context of epidemiological networks. The simplest models of PA, for example, are incompatible with the eradication of any disease through population-level control measures such as random vaccination. Typically, evidence has been sought for the presence or absence of preferential attachment via asymptotic power-law behaviour. Here, we present a general statistical method to test directly for evidence of PA in count data and apply this to data for contacts relevant to the spread of respiratory diseases. We find that while standard methods for model selection prefer a form of PA, careful analysis of the best fitting PA models allows for a level of contact heterogeneity that in fact allows control of respiratory diseases. Our approach is based on a flexible but numerically cheap likelihood-based model that could in principle be applied to other integer data where the hypothesis of PA is of interest. © 2015, House et al."
"10.1140/epjds/s13688-015-0057-x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84958777093&doi=10.1140%2fepjds%2fs13688-015-0057-x&partnerID=40&md5=049010782a3ec9ca450d131da9ba8d21","The spreading of information is of crucial importance for the modern information society. While we still receive information from mass media and other non-personalized sources, online social networks and influence of friends have become important personalized sources of information. This calls for metrics to measure the influence of users on the behavior of their friends. We demonstrate that the currently existing metrics of friends’ influence are biased by the presence of highly popular items in the data, and as a result can lead to an illusion of friends influence where there is none. We correct for this bias and develop three metrics that allow to distinguish the influence of friends from the effects of item popularity, and apply the metrics on real datasets. We use a simple network model based on the influence of friends and preferential attachment to illustrate the performance of our metrics at different levels of friends’ influence. © 2015, Vidmer et al."
"10.1140/epjds/s13688-015-0059-8","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84958776733&doi=10.1140%2fepjds%2fs13688-015-0059-8&partnerID=40&md5=4db1f4f9f7470fbc85963f00734cb4d6","In recent years, we have seen scientists attempt to model and explain human dynamics and in particular human movement. Many aspects of our complex life are affected by human movement such as disease spread and epidemics modeling, city planning, wireless network development, and disaster relief, to name a few. Given the myriad of applications, it is clear that a complete understanding of how people move in space can lead to considerable benefits to our society. In most of the recent works, scientists have focused on the idea that people movements are biased towards frequently-visited locations. According to them, human movement is based on a exploration/exploitation dichotomy in which individuals choose new locations (exploration) or return to frequently-visited locations (exploitation). In this work we focus on the concept of recency. We propose a model in which exploitation in human movement also considers recently-visited locations and not solely frequently-visited locations. We test our hypothesis against different empirical data of human mobility and show that our proposed model replicates the characteristic patterns of the recency bias. © 2015, Barbosa et al."
"10.1140/epjds/s13688-015-0044-2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84958776603&doi=10.1140%2fepjds%2fs13688-015-0044-2&partnerID=40&md5=8099712eea5f02f6330fa3a30feb8765","The high population density in cities confers many advantages, including improved social interaction and information exchange. However, it is often argued that urban living comes at the expense of reducing happiness. The goal of this research is to shed light on the relationship between urban communication and urban happiness. We analyze geo-located social media posts (tweets) within a major urban center (Milan) to produce a detailed spatial map of urban sentiments. We combine this data with high-resolution mobile communication intensity data among different urban areas. Our results reveal that happy (respectively unhappy) areas preferentially communicate with other areas of their type. This observation constitutes evidence of homophilous communities at the scale of an entire city (Milan), and has implications on interventions that aim to improve urban well-being. © 2015, Alshamsi et al."
"10.1140/epjds/s13688-015-0048-y","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84958775333&doi=10.1140%2fepjds%2fs13688-015-0048-y&partnerID=40&md5=3437365e4f8318498495df8372f26d3d","The Internet is composed of routing devices connected between them and organized into independent administrative entities: the Autonomous Systems. The existence of different types of Autonomous Systems (like large connectivity providers, Internet Service Providers or universities) together with geographical and economical constraints, turns the Internet into a complex modular and hierarchical network. This organization is reflected in many properties of the Internet topology, like its high degree of clustering and its robustness. In this work we study the modular structure of the Internet router-level graph in order to assess to what extent the Autonomous Systems satisfy some of the known notions of community structure. We observe that most of the classical community detection methods fail to detect the Autonomous Systems as communities, mainly because the modular structure of the Internet (as that of many complex networks) is much richer than what can be captured by optimizing a global functional: Autonomous Systems have largely variable sizes, structures and functions. Classical methods are severely affected by resolution limits and by the heterogeneity of the communities; even when using multiresolution methods, there is no single resolution at which most of the communities can be captured. However, we show that multiresolution methods do find the community structure of the Autonomous Systems, but each of them has to be observed at the correct resolution level. Then we develop a low-complexity multiresolution modularity optimization algorithm that finds communities at different resolution levels in a continuous scale, in one single run. Using this method, we show that with a scarce knowledge of the node affiliations, multiresolution methods can be adjusted to retrieve the Autonomous Systems, significantly improving the results of classical single-resolution methods. Finally, in the light of our results, we discuss recent work concerning the use of a priori information to find community structure in complex networks. © 2015, Beiró et al."
"10.1140/epjds/s13688-015-0047-z","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84958772881&doi=10.1140%2fepjds%2fs13688-015-0047-z&partnerID=40&md5=d0e1a00dd448e9cb90cfbb22df0913b0","In this paper we apply techniques of complex network analysis to data sources representing public funding programs and discuss the importance of the considered indicators for program evaluation. Starting from the Open Data repository of the 2007-2013 Italian Program Programma Operativo Nazionale ‘Ricerca e Competitività’ (PON R&C), we build a set of data models and perform network analysis over them. We discuss the obtained experimental results outlining interesting new perspectives that emerge from the application of the proposed methods to the socio-economical evaluation of funded programs. © 2015, Nicotri et al."
"10.1140/epjds/s13688-015-0061-1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84958768154&doi=10.1140%2fepjds%2fs13688-015-0061-1&partnerID=40&md5=63b8bcde4030c7de1a9414693285f05a","Smartphones and wearables have become an indispensable part of our daily life. Their improved sensing and computing capabilities bring new opportunities for human behavior monitoring and analysis. Most work so far has been focused on detecting correlation rather than causation among features extracted from smartphone data. However, pure correlation analysis does not offer sufficient understanding of human behavior. Moreover, causation analysis could allow scientists to identify factors that have a causal effect on health and well-being issues, such as obesity, stress, depression and so on and suggest actions to deal with them. Finally, detecting causal relationships in this kind of observational data is challenging since, in general, subjects cannot be randomly exposed to an event. In this article, we discuss the design, implementation and evaluation of a generic quasi-experimental framework for conducting causation studies on human behavior from smartphone data. We demonstrate the effectiveness of our approach by investigating the causal impact of several factors such as exercise, social interactions and work on stress level. Our results indicate that exercising and spending time outside home and working environment have a positive effect on participants stress level while reduced working hours only slightly impact stress. © 2015, Tsapeli and Musolesi."
"10.1140/epjds/s13688-015-0049-x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84958766475&doi=10.1140%2fepjds%2fs13688-015-0049-x&partnerID=40&md5=136dfc39042898c18b099a6ba0b90939","One of the greatest concerns related to the popularity of GPS-enabled devices and applications is the increasing availability of the personal location information generated by them and shared with application and service providers. Moreover, people tend to have regular routines and be characterized by a set of “significant places”, thus making it possible to identify a user from his/her mobility data. In this paper we present a series of techniques for identifying individuals from their GPS movements. More specifically, we study the uniqueness of GPS information for three popular datasets, and we provide a detailed analysis of the discriminatory power of speed, direction and distance of travel. Most importantly, we present a simple yet effective technique for the identification of users from location information that are not included in the original dataset used for training, thus raising important privacy concerns for the management of location datasets. © 2015, Rossi et al."
"10.1140/epjds/s13688-015-0054-0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84958762492&doi=10.1140%2fepjds%2fs13688-015-0054-0&partnerID=40&md5=80b3ea9f82049e74a21a891db8905d26","Novel data streams (NDS), such as web search data or social media updates, hold promise for enhancing the capabilities of public health surveillance. In this paper, we outline a conceptual framework for integrating NDS into current public health surveillance. Our approach focuses on two key questions: What are the opportunities for using NDS and what are the minimal tests of validity and utility that must be applied when using NDS? Identifying these opportunities will necessitate the involvement of public health authorities and an appreciation of the diversity of objectives and scales across agencies at different levels (local, state, national, international). We present the case that clearly articulating surveillance objectives and systematically evaluating NDS and comparing the performance of NDS to existing surveillance data and alternative NDS data is critical and has not sufficiently been addressed in many applications of NDS currently in the literature. © 2015, Althouse et al."
"10.1140/epjds/s13688-015-0058-9","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84958754340&doi=10.1140%2fepjds%2fs13688-015-0058-9&partnerID=40&md5=49f1214b65e431ed64ebf3ad4497e9ee","Given the ever increasing amount of publicly available social media data, there is growing interest in using online data to study and quantify phenomena in the offline “real” world. As social media data can be obtained in near real-time and at low cost, it is often used for “now-casting” indices such as levels of flu activity or unemployment. The term “social sensing” is often used in this context to describe the idea that users act as “sensors”, publicly reporting their health status or job losses. Sensor activity during a time period is then typically aggregated in a “one tweet, one vote” fashion by simply counting. At the same time, researchers readily admit that social media users are not a perfect representation of the actual population. Additionally, users differ in the amount of details of their personal lives that they reveal. Intuitively, it should be possible to improve now-casting by assigning different weights to different user groups. In this paper, we ask “How does social sensing actually work?” or, more precisely, “Whom should we sense-and whom not-for optimal results?”. We investigate how different sampling strategies affect the performance of now-casting of two common offline indices: flu activity and unemployment rate. We show that now-casting can be improved by (1) applying user filtering techniques and (2) selecting users with complete profiles. We also find that, using the right type of user groups, now-casting performance does not degrade, even when drastically reducing the size of the dataset. More fundamentally, we describe which type of users contribute most to the accuracy by asking if “babblers are better”. We conclude the paper by providing guidance on how to select better user groups for more accurate now-casting. © 2015, An and Weber."
"10.1140/epjds/s13688-015-0060-2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84958753907&doi=10.1140%2fepjds%2fs13688-015-0060-2&partnerID=40&md5=c0364dbb457e8cd63f6236b264268fea","Uber has recently been introducing novel practices in urban taxi transport. Journey prices can change dynamically in almost real time and also vary geographically from one area to another in a city, a strategy known as surge pricing. In this paper, we explore the power of the new generation of open datasets towards understanding the impact of the new disruption technologies that emerge in the area of public transport. With our primary goal being a more transparent economic landscape for urban commuters, we provide a direct price comparison between Uber and the Yellow Cab company in New York. We discover that Uber, despite its lower standard pricing rates, effectively charges higher fares on average, especially during short in length, but frequent in occurrence, taxi journeys. Building on this insight, we develop a smartphone application, OpenStreetCab, that offers a personalized consultation to mobile users on which taxi provider is cheaper for their journey. Almost five months after its launch, the app has attracted more than three thousand users in a single city. Their journey queries have provided additional insights on the potential savings similar technologies can have for urban commuters, with a highlight being that on average, a user in New York saves 6 U.S. Dollars per taxi journey if they pick the cheapest taxi provider. We run extensive experiments to show how Uber’s surge pricing is the driving factor of higher journey prices and therefore higher potential savings for our application’s users. Finally, motivated by the observation that Uber’s surge pricing is occurring more frequently that intuitively expected, we formulate a prediction task where the aim becomes to predict a geographic area’s tendency to surge. Using exogenous to Uber data, in particular Yellow Cab and Foursquare data, we show how it is possible to estimate customer demand within an area, and by extension surge pricing, with high accuracy. © 2015, Noulas et al."
"10.1140/epjds/s13688-015-0055-z","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84958752638&doi=10.1140%2fepjds%2fs13688-015-0055-z&partnerID=40&md5=9c852141ec26ed91ef948d22dd7db1de","Transportation planning is strongly influenced by the assumption that every individual has a constant daily budget of ≈1 hour for his daily mobility. However, recent experimental results are proving this assumption as wrong. Here, we study the differences in daily travel-time expenditures among 24 Italian cities, extracted from a large set of GPS data on vehicles mobility. To understand these variations at the level of individual behaviour, we introduce a trip duration model that allows for a description of the distribution of travel-time expenditures in a given city using two parameters. The first parameter reflects the accessibility of desired destinations, whereas the second one can be associated to a travel-time budget and represents physiological limits due to stress and fatigue. Within the same city, we observe variations in the distributions according to home position, number of mobility days and a driver’s average number of daily trips. These results can be interpreted by a stochastic time-consumption model, where the generalised cost of travel times is given by a logarithmic-like function, in agreement with the Weber-Fechner law. Our experimental results show a significant variability in the travel-time budgets in different cities, and for different categories of drivers within the same city. This explicitly clashes with the idea of the existence of a constant travel-time budget and opens new perspectives for the modelling and governance of urban mobility. © 2015, Gallotti et al."
"10.1140/epjds/s13688-015-0051-3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84958743444&doi=10.1140%2fepjds%2fs13688-015-0051-3&partnerID=40&md5=1ef86ccd0f5041ccd59443ff5133ef0e","Customers mobility is dependent on the sophistication of their needs: sophisticated customers need to travel more to fulfill their needs. In this paper, we provide more detailed evidence of this phenomenon, providing an empirical validation of the Central Place Theory. For each customer, we detect what is her favorite shop, where she purchases most products. We can study the relationship between the favorite shop and the closest one, by recording the influence of the shop’s size and the customer’s sophistication in the discordance cases, i.e. the cases in which the favorite shop is not the closest one. We show that larger shops are able to retain most of their closest customers and they are able to catch large portions of customers from smaller shops around them. We connect this observation with the shop’s larger sophistication, and not with its other characteristics, as the phenomenon is especially noticeable when customers want to satisfy their sophisticated needs. This is a confirmation of the recent extensions of the Central Place Theory, where the original assumptions of homogeneity in customer purchase power and needs are challenged. Different types of shops have also different survival logics. The largest shops get closed if they are unable to catch customers from the smaller shops, while medium size shops get closed if they cannot retain their closest customers. All analysis are performed on a large real-world dataset recording all purchases from millions of customers across the west coast of Italy. © 2015, Coscia et al."
"10.1140/epjds/s13688-015-0056-y","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84958740290&doi=10.1140%2fepjds%2fs13688-015-0056-y&partnerID=40&md5=42304db611ff614e5e5b9fff157bdb6d","Large-scale protests occur frequently and sometimes overthrow entire political systems. Meanwhile, online social networks have become an increasingly common component of people’s lives. We present a large-scale longitudinal study that connects online social media behaviors to offline protest. Using almost 14 million geolocated tweets and data on protests from 16 countries during the Arab Spring, we show that increased coordination of messages on Twitter using specific hashtags is associated with increased protests the following day. The results also show that traditional actors like the media and elites are not driving the results. These results indicate social media activity correlates with subsequent large-scale decentralized coordination of protests, with important implications for the future balance of power between citizens and their states. © 2015, Steinert-Threlkeld et al."
"10.1140/epjds/s13688-015-0053-1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84958739588&doi=10.1140%2fepjds%2fs13688-015-0053-1&partnerID=40&md5=6cb6dbc0bc01dbe79bc957d51f72e2f1","The widespread adoption of mobile devices that record the communications, social relations, and movements of billions of individuals in great detail presents unique opportunities for the study of social structures and human dynamics at very large scales. This is particularly the case for developing countries where social and economic data can be hard to obtain and is often too sparse for real-time analytics. Here we leverage mobile call log data from Côte d’Ivoire to analyze the relations between its nation-wide communications network and the socio-economic dynamics of its regional economies. We introduce the CallRank indicator to quantify the relative importance of an area on the basis of call records, and show that a region’s ratio of in- and out-going calls can predict its income level. We detect a communication divide between rich and poor regions of Côte d’Ivoire, which corresponds to existing socio-economic data. Our results demonstrate the potential of mobile communication data to monitor the economic development and social dynamics of low-income developing countries in the absence of extensive econometric and social data. Our work may support efforts to stimulate sustainable economic development and to reduce poverty and inequality. © 2015, Mao et al."
"10.1186/s40537-015-0023-2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013991168&doi=10.1186%2fs40537-015-0023-2&partnerID=40&md5=06ef4342398c88fa9b23521d8783dca6","Knowledge-as-a service is an emerging research trend that constitutes a promising path for organizations aiming to achieve better customer support and decision-making across a wide range of content providers. However, only few studies have explored how traditional knowledge management processes and practices are challenged and evolve in relation to social and technological transformation stemming from the new age of cloud-computing environments. This research paper attempts to answer this gap by introducing a new framework for the management of knowledge in the cloud. The objective of this study is twofold. First, it aims to develop a knowledge-based decision framework using recent knowledge-as-as-service paradigms for creating, retrieving and reusing technical knowledge in cloud environments. Second, and in relation to this view, the current state of data analysis (business intelligence, big data analytics) is assessed in light of cloud computing environments and the simultaneous emergence of new knowledge management paradigms. An overview of cloud computing challenges is given as well as a representation of supporting knowledge management processes, followed by the description of related applications of the AKAAS (Actionable Knowledge As A Service) operating framework. The first part of the research develops upon literature on Knowledge Management Systems (KMS) frameworks. The second part of this study examines and assesses the nature of data and analytics required to provide on-demand delivery of knowledge in the cloud. Finally, the third part of the paper explores practical applications of the framework to guide the development of knowledge management processes and practices towards internal and external users adoption. We conclude by suggesting future research directions, including case studies examining differential knowledge cloud-based designs. © 2015, Depeige and Doyencourt."
"10.1186/s40537-014-0011-y","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013989875&doi=10.1186%2fs40537-014-0011-y&partnerID=40&md5=348bb7820b296c885046523656449bc2","The ability to detect and process anomalies for Big Data in real-time is a difficult task. The volume and velocity of the data within many systems makes it difficult for typical algorithms to scale and retain their real-time characteristics. The pervasiveness of data combined with the problem that many existing algorithms only consider the content of the data source; e.g. a sensor reading itself without concern for its context, leaves room for potential improvement. The proposed work defines a contextual anomaly detection framework. It is composed of two distinct steps: content detection and context detection. The content detector is used to determine anomalies in real-time, while possibly, and likely, identifying false positives. The context detector is used to prune the output of the content detector, identifying those anomalies which are considered both content and contextually anomalous. The context detector utilizes the concept of profiles, which are groups of similarly grouped data points generated by a multivariate clustering algorithm. The research has been evaluated against two real-world sensor datasets provided by a local company in Brampton, Canada. Additionally, the framework has been evaluated against the open-source Dodgers dataset, available at the UCI machine learning repository, and against the R statistical toolbox. © 2015, Hayes and Capretz; licensee Springer."
"10.1186/s40537-015-0024-1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013981862&doi=10.1186%2fs40537-015-0024-1&partnerID=40&md5=d4f428f8589c368b214350208ae3dd82","Big data application has many data resources and data. In the first stage of software engineering, a service overview or a system overview cannot be seen. In this paper, we propose that two processes of “Big data analytics” and “Implementation of data modeling” should be collaborated with Model-driven architecture (MDA). Data modeling with those two process in MDA should be repeated fast in order to verify the data model and to find a new data resource for a service. Our first research goal of big data application is to predict side effect of drug which is one of screening methods in drug discovery. This prediction model is constructed with data mining methods at the intersection of statistics, machine learning and database system. Moreover, a new service for drug discovery by new uses for old drugs can be found in data modeling and developed. We demonstrate that the prediction model and the data model for drug discovery are implemented as a prototype system to verify those models and their practicality. © 2015, Etani."
"10.1186/s40537-015-0015-2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013978976&doi=10.1186%2fs40537-015-0015-2&partnerID=40&md5=7f583108a859184c3001e7c128b2d597","Sentiment analysis or opinion mining is one of the major tasks of NLP (Natural Language Processing). Sentiment analysis has gain much attention in recent years. In this paper, we aim to tackle the problem of sentiment polarity categorization, which is one of the fundamental problems of sentiment analysis. A general process for sentiment polarity categorization is proposed with detailed process descriptions. Data used in this study are online product reviews collected from Amazon.com. Experiments for both sentence-level categorization and review-level categorization are performed with promising outcomes. At last, we also give insight into our future work on sentiment analysis. © 2015, Fang and Zhan; licensee Springer."
"10.1186/s40537-014-0007-7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013977220&doi=10.1186%2fs40537-014-0007-7&partnerID=40&md5=c1817bbbcb6e985acea50051e2e9445d","Big Data Analytics and Deep Learning are two high-focus of data science. Big Data has become important as many organizations both public and private have been collecting massive amounts of domain-specific information, which can contain useful information about problems such as national intelligence, cyber security, fraud detection, marketing, and medical informatics. Companies such as Google and Microsoft are analyzing large volumes of data for business analysis and decisions, impacting existing and future technology. Deep Learning algorithms extract high-level, complex abstractions as data representations through a hierarchical learning process. Complex abstractions are learnt at a given level based on relatively simpler abstractions formulated in the preceding level in the hierarchy. A key benefit of Deep Learning is the analysis and learning of massive amounts of unsupervised data, making it a valuable tool for Big Data Analytics where raw data is largely unlabeled and un-categorized. In the present study, we explore how Deep Learning can be utilized for addressing some important problems in Big Data Analytics, including extracting complex patterns from massive volumes of data, semantic indexing, data tagging, fast information retrieval, and simplifying discriminative tasks. We also investigate some aspects of Deep Learning research that need further exploration to incorporate specific challenges introduced by Big Data Analytics, including streaming data, high-dimensional data, scalability of models, and distributed computing. We conclude by presenting insights into relevant future works by posing some questions, including defining data sampling criteria, domain adaptation modeling, defining criteria for obtaining useful data abstractions, improving semantic indexing, semi-supervised learning, and active learning. © 2015, Najafabadi et al.; licensee Springer."
"10.1186/s40537-015-0021-4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013975049&doi=10.1186%2fs40537-015-0021-4&partnerID=40&md5=143cf4f83d91c6734260bfbb79500816","W e h a v e e n t e r e d t h e b i g data age. Knowledge extraction from massive data is becoming more and more urgent. MapReduce provides a feasible framework for programming machine learning algorithms in Map and Reduce functions. The relatively simple programming interface has helped to solve machine learning algorithms’ scalability problems. However, this framework suffers from an obvious weakness: it does not support iterations. This makes it difficult for algorithms requiring iterations to fully explore the efficiency of MapReduce. In this paper, we propose to apply Meta-learning programmed with MapReduce to avoid parallelizing machine learning algorithms while also improving their scalability to big datasets. The experiments conducted on Hadoop’s fully distributed mode on Amazon EC2 demonstrate that our algorithm Meta-MapReduce (MMR) reduces the training computational complexity significantly when the number of computing nodes increases while obtaining smaller error rates than those on a single node. The comparison of MMR with the contemporary parallelized Ad a B oost algorithm, AdaBoost.PL, shows that MMR obtains lower error rates. © 2015, Liu et al."
"10.1186/s40537-015-0032-1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013974691&doi=10.1186%2fs40537-015-0032-1&partnerID=40&md5=a257aecdd96c8ea16435354e892711ae","With an ever-increasing amount of options, the task of selecting machine learning tools for big data can be difficult. The available tools have advantages and drawbacks, and many have overlapping uses. The world’s data is growing rapidly, and traditional tools for machine learning are becoming insufficient as we move towards distributed and real-time processing. This paper is intended to aid the researcher or professional who understands machine learning but is inexperienced with big data. In order to evaluate tools, one should have a thorough understanding of what to look for. To that end, this paper provides a list of criteria for making selections along with an analysis of the advantages and drawbacks of each. We do this by starting from the beginning, and looking at what exactly the term “big data” means. From there, we go on to the Hadoop ecosystem for a look at many of the projects that are part of a typical machine learning architecture and an understanding of how everything might fit together. We discuss the advantages and disadvantages of three different processing paradigms along with a comparison of engines that implement them, including MapReduce, Spark, Flink, Storm, and H 2 O. We then look at machine learning libraries and frameworks including Mahout, MLlib, SAMOA, and evaluate them based on criteria such as scalability, ease of use, and extensibility. There is no single toolkit that truly embodies a one-size-fits-all solution, so this paper aims to help make decisions smoother by providing as much information as possible and quantifying what the tradeoffs will be. Additionally, throughout this paper, we review recent research in the field using these tools and talk about possible future directions for toolkit-based learning. © 2015, Landset et al."
"10.1186/s40537-015-0031-2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013964229&doi=10.1186%2fs40537-015-0031-2&partnerID=40&md5=22d6f4d569e137eb2a19a4ea9468587b","This paper provides a multi-disciplinary overview of the research issues and achievements in the field of Big Data and its visualization techniques and tools. The main aim is to summarize challenges in visualization methods for existing Big Data, as well as to offer novel solutions for issues related to the current state of Big Data Visualization. This paper provides a classification of existing data types, analytical methods, visualization techniques and tools, with a particular emphasis placed on surveying the evolution of visualization methodology over the past years. Based on the results, we reveal disadvantages of existing visualization methods. Despite the technological development of the modern world, human involvement (interaction), judgment and logical thinking are necessary while working with Big Data. Therefore, the role of human perceptional limitations involving large amounts of information is evaluated. Based on the results, a non-traditional approach is proposed: we discuss how the capabilities of Augmented Reality and Virtual Reality could be applied to the field of Big Data Visualization. We discuss the promising utility of Mixed Reality technology integration with applications in Big Data Visualization. Placing the most essential data in the central area of the human visual field in Mixed Reality would allow one to obtain the presented information in a short period of time without significant data losses due to human perceptual issues. Furthermore, we discuss the impacts of new technologies, such as Virtual Reality displays and Augmented Reality helmets on the Big Data visualization as well as to the classification of the main challenges of integrating the technology. © 2015, Olshannikova et al."
"10.1186/s40537-015-0022-3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013961399&doi=10.1186%2fs40537-015-0022-3&partnerID=40&md5=0c4312069b8e5046f9be4c6473a58b63","Treemaps are well-known for visualizing hierarchical data. Most related approaches have been focused on layout algorithms and paid little attention to other display properties and interactions. Furthermore, the structural information in conventional Treemaps is too implicit for viewers to perceive. This paper presents Cabinet Tree, an approach that: i) draws branches explicitly to show relational structures, ii) adapts a space-optimized layout for leaves and maximizes the space utilization, iii) uses coloring and labeling strategies to clearly reveal patterns and contrast different attributes intuitively. We also apply the continuous node selection and detail window techniques to support user interaction with different levels of the hierarchies. Our quantitative evaluations demonstrate that Cabinet Tree achieves good scalability for increased resolutions and big datasets. © 2015, Yang et al."
"10.1186/s40537-015-0034-z","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013959166&doi=10.1186%2fs40537-015-0034-z&partnerID=40&md5=a0b2de9f9f8935a96ddb9b98f9078dba","The term smart manufacturing refers to a future-state of manufacturing, where the real-time transmission and analysis of data from across the factory creates manufacturing intelligence, which can be used to have a positive impact across all aspects of operations. In recent years, many initiatives and groups have been formed to advance smart manufacturing, with the most prominent being the Smart Manufacturing Leadership Coalition (SMLC), Industry 4.0, and the Industrial Internet Consortium. These initiatives comprise industry, academic and government partners, and contribute to the development of strategic policies, guidelines, and roadmaps relating to smart manufacturing adoption. In turn, many of these recommendations may be implemented using data-centric technologies, such as Big Data, Machine Learning, Simulation, Internet of Things and Cyber Physical Systems, to realise smart operations in the factory. Given the importance of machine uptime and availability in smart manufacturing, this research centres on the application of data-driven analytics to industrial equipment maintenance. The main contributions of this research are a set of data and system requirements for implementing equipment maintenance applications in industrial environments, and an information system model that provides a scalable and fault tolerant big data pipeline for integrating, processing and analysing industrial equipment data. These contributions are considered in the context of highly regulated large-scale manufacturing environments, where legacy (e.g. automation controllers) and emerging instrumentation (e.g. internet-aware smart sensors) must be supported to facilitate initial smart manufacturing efforts. © 2015, O’Donovan et al."
"10.1186/s40537-015-0033-0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013958514&doi=10.1186%2fs40537-015-0033-0&partnerID=40&md5=7915f2738d9288562718a1243b70ade4","The Internet of things promises a continuous flow of data where traditional database and data-mining methods cannot be applied. This paper presents improvements on the Ubiquitous Self-Organized Map (UbiSOM), a novel variant of the well-known Self-Organized Map (SOM), tailored for streaming environments. This approach allows ambient intelligence solutions using multidimensional clustering over a continuous data stream to provide continuous exploratory data analysis. The average quantization error and average neuron utility over time are proposed and used to estimating the learning parameters, allowing the model to retain an indefinite plasticity and to cope with changes within a multidimensional data stream. We perform parameter sensitivity analysis and our experiments show that UbiSOM outperforms existing proposals in continuously modeling possibly non-stationary data streams, converging faster to stable models when the underlying distribution is stationary and reacting accordingly to the nature of the change in continuous real world data streams. © 2015, Silva and Marques."
"10.1186/s40537-015-0035-y","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013948770&doi=10.1186%2fs40537-015-0035-y&partnerID=40&md5=e89cd388738ae1786a93b4e4ebdf17a6","One of the key objectives in accident data analysis to identify the main factors associated with a road and traffic accident. However, heterogeneous nature of road accident data makes the analysis task difficult. Data segmentation has been used widely to overcome this heterogeneity of the accident data. In this paper, we proposed a framework that used K-modes clustering technique as a preliminary task for segmentation of 11,574 road accidents on road network of Dehradun (India) between 2009 and 2014 (both included). Next, association rule mining are used to identify the various circumstances that are associated with the occurrence of an accident for both the entire data set (EDS) and the clusters identified by K-modes clustering algorithm. The findings of cluster based analysis and entire data set analysis are then compared. The results reveal that the combination of k mode clustering and association rule mining is very inspiring as it produces important information that would remain hidden if no segmentation has been performed prior to generate association rules. Further a trend analysis have also been performed for each clusters and EDS accidents which finds different trends in different cluster whereas a positive trend is shown by EDS. Trend analysis also shows that prior segmentation of accident data is very important before analysis. © 2015, Kumar and Toshniwal."
"10.1186/s40537-015-0013-4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013947969&doi=10.1186%2fs40537-015-0013-4&partnerID=40&md5=a3e7aa86cc05f8a09dac1db7442b235e","Intrusion Detection has been heavily studied in both industry and academia, but cybersecurity analysts still desire much more alert accuracy and overall threat analysis in order to secure their systems within cyberspace. Improvements to Intrusion Detection could be achieved by embracing a more comprehensive approach in monitoring security events from many different heterogeneous sources. Correlating security events from heterogeneous sources can grant a more holistic view and greater situational awareness of cyber threats. One problem with this approach is that currently, even a single event source (e.g., network traffic) can experience Big Data challenges when considered alone. Attempts to use more heterogeneous data sources pose an even greater Big Data challenge. Big Data technologies for Intrusion Detection can help solve these Big Heterogeneous Data challenges. In this paper, we review the scope of works considering the problem of heterogeneous data and in particular Big Heterogeneous Data. We discuss the specific issues of Data Fusion, Heterogeneous Intrusion Detection Architectures, and Security Information and Event Management (SIEM) systems, as well as presenting areas where more research opportunities exist. Overall, both cyber threat analysis and cyber intelligence could be enhanced by correlating security events across many diverse heterogeneous sources. © 2015, Zuech et al.; licensee Springer."
"10.1186/s40537-015-0020-5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013947210&doi=10.1186%2fs40537-015-0020-5&partnerID=40&md5=d1f91ba0af7dcdda9c39d04271ba4c52","Document summarization provides an instrument for faster understanding the collection of text documents and has a number of real life applications. Semantic similarity and clustering can be utilized efficiently for generating effective summary of large text collections. Summarizing large volume of text is a challenging and time consuming problem particularly while considering the semantic similarity computation in summarization process. Summarization of text collection involves intensive text processing and computations to generate the summary. MapReduce is proven state of art technology for handling Big Data. In this paper, a novel framework based on MapReduce technology is proposed for summarizing large text collection. The proposed technique is designed using semantic similarity based clustering and topic modeling using Latent Dirichlet Allocation (LDA) for summarizing the large text collection over MapReduce framework. The summarization task is performed in four stages and provides a modular implementation of multiple documents summarization. The presented technique is evaluated in terms of scalability and various text summarization parameters namely, compression ratio, retention ratio, ROUGE and Pyramid score are also measured. The advantages of MapReduce framework are clearly visible from the experiments and it is also demonstrated that MapReduce provides a faster implementation of summarizing large text collections and is a powerful tool in Big Text Data analysis. © 2015, Nagwani."
"10.1186/s40537-015-0028-x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013945182&doi=10.1186%2fs40537-015-0028-x&partnerID=40&md5=7d03aad9f9970098e5f288b8923f69ee","The manufacturing industry is currently in the midst of a data-driven revolution, which promises to transform traditional manufacturing facilities in to highly optimised smart manufacturing facilities. These smart facilities are focused on creating manufacturing intelligence from real-time data to support accurate and timely decision-making that can have a positive impact across the entire organisation. To realise these efficiencies emerging technologies such as Internet of Things (IoT) and Cyber Physical Systems (CPS) will be embedded in physical processes to measure and monitor real-time data from across the factory, which will ultimately give rise to unprecedented levels of data production. Therefore, manufacturing facilities must be able to manage the demands of exponential increase in data production, as well as possessing the analytical techniques needed to extract meaning from these large datasets. More specifically, organisations must be able to work with big data technologies to meet the demands of smart manufacturing. However, as big data is a relatively new phenomenon and potential applications to manufacturing activities are wide-reaching and diverse, there has been an obvious lack of secondary research undertaken in the area. Without secondary research, it is difficult for researchers to identify gaps in the field, as well as aligning their work with other researchers to develop strong research themes. In this study, we use the formal research methodology of systematic mapping to provide a breadth-first review of big data technologies in manufacturing. © 2015, O’Donovan et al."
"10.1186/s40537-015-0017-0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013942592&doi=10.1186%2fs40537-015-0017-0&partnerID=40&md5=597214c2f9138d09d916373fbb7cf5b0","Automated workflows are the key concept of big data pipelines in science, engineering and enterprise applications. The performance analysis of automated workflows is an important topic of the continuous improvement process and the foundation of designing new workflows. This paper introduces the concept of process evolution functions and event reduction policies, which allow for the time resolved visualization of an unlimited number of concurrent workflows by means of aggregated task views. The visualization allows for an intuitive approach to the performance analysis of concurrent workflows. The theoretical foundation of this approach is applicable for workflows represented by directed acyclic graphs. It is explained on the basis of a simple IO-workflow model, which is typically found for distributed resource management systems utilized for many-task computing. AMS subject classification68Mxx © 2015, Kempa-Liehr; licensee Springer."
"10.1186/s40537-015-0025-0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013940863&doi=10.1186%2fs40537-015-0025-0&partnerID=40&md5=7dec0f6b329ddf27797719aeeaaaf4da","For over forty years, relational databases have been the leading model for data storage, retrieval and management. However, due to increasing needs for scalability and performance, alternative systems have emerged, namely NoSQL technology. The rising interest in NoSQL technology, as well as the growth in the number of use case scenarios, over the last few years resulted in an increasing number of evaluations and comparisons among competing NoSQL technologies. While most research work mostly focuses on performance evaluation using standard benchmarks, it is important to notice that the architecture of real world systems is not only driven by performance requirements, but has to comprehensively include many other quality attribute requirements. Software quality attributes form the basis from which software engineers and architects develop software and make design decisions. Yet, there has been no quality attribute focused survey or classification of NoSQL databases where databases are compared with regards to their suitability for quality attributes common on the design of enterprise systems. To fill this gap, and aid software engineers and architects, in this article, we survey and create a concise and up-to-date comparison of NoSQL engines, identifying their most beneficial use case scenarios from the software engineer point of view and the quality attributes that each of them is most suited to. © 2015, Lourenço et al.; licensee Springer."
"10.1186/s40537-014-0010-z","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013933689&doi=10.1186%2fs40537-014-0010-z&partnerID=40&md5=8a11623670985de0c4ed06ab6bdad424","The paper discusses the shift in the computing paradigm and the programming model for Big Data problems and applications. We compare DataFlow and ControlFlow programming models through their quantity and quality aspects. Big Data problems and applications that are suitable for implementation on DataFlow computers should not be measured using the same measures as ControlFlow computers. We propose a new methodology for benchmarking, which takes into account not only the execution time, but also the power and space, needed to complete the task. Recent research shows that if the TOP500 ranking was based on the new performance measures, DataFlow machines would outperform ControlFlow machines. To support the above claims, we present eight recent implementations of various algorithms using the DataFlow paradigm, which show considerable speed-ups, power reductions and space savings over their implementation using the ControlFlow paradigm. © 2015, Trifunovic et al.; licensee Springer."
"10.1186/s40537-015-0018-z","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013924704&doi=10.1186%2fs40537-015-0018-z&partnerID=40&md5=9a8ca9bbd85f1d62183a6003f02ce218","Introduction: Intuitive formulation of informative and computationally-efficient queries on big and complex datasets present a number of challenges. As data collection is increasingly streamlined and ubiquitous, data exploration, discovery and analytics get considerably harder. Exploratory querying of heterogeneous and multi-source information is both difficult and necessary to advance our knowledge about the world around us. Research design: We developed a mechanism to integrate dispersed multi-source data and service the mashed information via human and machine interfaces in a secure, scalable manner. This process facilitates the exploration of subtle associations between variables, population strata, or clusters of data elements, which may be opaque to standard independent inspection of the individual sources. This a new platform includes a device agnostic tool (Dashboard webapp, http://socr.umich.edu/HTML5/Dashboard/) for graphical querying, navigating and exploring the multivariate associations in complex heterogeneous datasets. Results: The paper illustrates this core functionality and serviceoriented infrastructure using healthcare data (e.g., US data from the 2010 Census, Demographic and Economic surveys, Bureau of Labor Statistics, and Center for Medicare Services) as well as Parkinson’s Disease neuroimaging data. Both the back-end data archive and the front-end dashboard interfaces are continuously expanded to include additional data elements and new ways to customize the human and machine interactions. Conclusions: A client-side data import utility allows for easy and intuitive integration of user-supplied datasets. This completely open-science framework may be used for exploratory analytics, confirmatory analyses, meta-analyses, and education and training purposes in a wide variety of fields. © 2015, Husain et al."
"10.1186/s40537-014-0008-6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013916411&doi=10.1186%2fs40537-014-0008-6&partnerID=40&md5=7d033775a6522cce7f73975df64d50f4","The primary purpose of this paper is to provide an in-depth analysis of different platforms available for performing big data analytics. This paper surveys different hardware platforms available for big data analytics and assesses the advantages and drawbacks of each of these platforms based on various metrics such as scalability, data I/O rate, fault tolerance, real-time processing, data size supported and iterative task support. In addition to the hardware, a detailed description of the software frameworks used within each of these platforms is also discussed along with their strengths and drawbacks. Some of the critical characteristics described here can potentially aid the readers in making an informed decision about the right choice of platforms depending on their computational needs. Using a star ratings table, a rigorous qualitative comparison between different platforms is also discussed for each of the six characteristics that are critical for the algorithms of big data analytics. In order to provide more insights into the effectiveness of each of the platform in the context of big data analytics, specific implementation level details of the widely used k-means clustering algorithm on various platforms are also described in the form pseudocode. © 2014, Singh and Reddy; licensee Springer."
"10.1186/s40537-015-0027-y","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013910051&doi=10.1186%2fs40537-015-0027-y&partnerID=40&md5=1f9d1300cb2abe2dc5a327a82e9ce94c","Rapid growth of high dimensional datasets in recent years has created an emergent need to extract the knowledge underlying them. Clustering is the process of automatically finding groups of similar data points in the space of the dimensions or attributes of a dataset. Finding clusters in the high dimensional datasets is an important and challenging data mining problem. Data group together differently under different subsets of dimensions, called subspaces. Quite often a dataset can be better understood by clustering it in its subspaces, a process called subspace clustering. But the exponential growth in the number of these subspaces with the dimensionality of data makes the whole process of subspace clustering computationally very expensive. There is a growing demand for efficient and scalable subspace clustering solutions in many Big data application domains like biology, computer vision, astronomy and social networking. Apriori based hierarchical clustering is a promising approach to find all possible higher dimensional subspace clusters from the lower dimensional clusters using a bottom-up process. However, the performance of the existing algorithms based on this approach deteriorates drastically with the increase in the number of dimensions. Most of these algorithms require multiple database scans and generate a large number of redundant subspace clusters, either implicitly or explicitly, during the clustering process. In this paper, we present SUBSCALE, a novel clustering algorithm to find non-trivial subspace clusters with minimal cost and it requires only k database scans for a k-dimensional data set. Our algorithm scales very well with the dimensionality of the dataset and is highly parallelizable. We present the details of the SUBSCALE algorithm and its evaluation in this paper. © 2015, Kaur and Datta."
"10.1186/s40537-015-0029-9","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013904891&doi=10.1186%2fs40537-015-0029-9&partnerID=40&md5=972979518ecb016f6233316fb1b33536","Online reviews are often the primary factor in a customer’s decision to purchase a product or service, and are a valuable source of information that can be used to determine public opinion on these products or services. Because of their impact, manufacturers and retailers are highly concerned with customer feedback and reviews. Reliance on online reviews gives rise to the potential concern that wrongdoers may create false reviews to artificially promote or devalue products and services. This practice is known as Opinion (Review) Spam, where spammers manipulate and poison reviews (i.e., making fake, untruthful, or deceptive reviews) for profit or gain. Since not all online reviews are truthful and trustworthy, it is important to develop techniques for detecting review spam. By extracting meaningful features from the text using Natural Language Processing (NLP), it is possible to conduct review spam detection using various machine learning techniques. Additionally, reviewer information, apart from the text itself, can be used to aid in this process. In this paper, we survey the prominent machine learning techniques that have been proposed to solve the problem of review spam detection and the performance of different approaches for classification and detection of review spam. The majority of current research has focused on supervised learning methods, which require labeled data, a scarcity when it comes to online review spam. Research on methods for Big Data are of interest, since there are millions of online reviews, with many more being generated daily. To date, we have not found any papers that study the effects of Big Data analytics for review spam detection. The primary goal of this paper is to provide a strong and comprehensive comparative study of current research on detecting review spam using various machine learning techniques and to devise methodology for conducting further investigation. © 2015, Crawford et al."
"10.1186/s40537-014-0009-5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013901597&doi=10.1186%2fs40537-014-0009-5&partnerID=40&md5=b03d19a818587b4a957decb31cf0c667","Molecular Simulation (MS) is a powerful tool for studying physical/chemical features of large systems and has seen applications in many scientific and engineering domains. During the simulation process, the experiments generate a very large number of atoms and intend to observe their spatial and temporal relationships for scientific analysis. The sheer data volumes and their intensive interactions impose significant challenges for data accessing, managing, and analysis. To date, existing MS software systems fall short on storage and handling of MS data, mainly because of the missing of a platform to support applications that involve intensive data access and analytical process. In this paper, we present the database-centric molecular simulation (DCMS) system our team developed in the past few years. The main idea behind DCMS is to store MS data in a relational database management system (DBMS) to take advantage of the declarative query interface (i.e., SQL), data access methods, query processing, and optimization mechanisms of modern DBMSs. A unique challenge is to handle the analytical queries that are often compute-intensive. For that, we developed novel indexing and query processing strategies (including algorithms running on modern co-processors) as integrated components of the DBMS. As a result, researchers can upload and analyze their data using efficient functions implemented inside the DBMS. Index structures are generated to store analysis results that may be interesting to other users, so that the results are readily available without duplicating the analysis. We have developed a prototype of DCMS based on the PostgreSQL system and experiments using real MS data and workload show that DCMS significantly outperforms existing MS software systems. We also used it as a platform to test other data management issues such as security and compression. © 2014, Kumar et al.; licensee Springer."
"10.1186/s40537-015-0030-3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013880602&doi=10.1186%2fs40537-015-0030-3&partnerID=40&md5=5d2fc3ed00498765472b8e0dd5e67f3c","The age of big data is now coming. But the traditional data analytics may not be able to handle such large quantities of data. The question that arises now is, how to develop a high performance platform to efficiently analyze big data and how to design an appropriate mining algorithm to find the useful things from big data. To deeply discuss this issue, this paper begins with a brief introduction to data analytics, followed by the discussions of big data analytics. Some important open issues and further research directions will also be presented for the next step of big data analytics. © 2015, Tsai et al."
"10.1186/s40537-015-0016-1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013875756&doi=10.1186%2fs40537-015-0016-1&partnerID=40&md5=6e4a843e2abea64341d14e26df9de1ec","Background: The promise of Big Biomedical Data may be offset by the enormous challenges in handling, analyzing, and sharing it. In this paper, we provide a framework for developing practical and reasonable data sharing policies that incorporate the sociological, financial, technical and scientific requirements of a sustainable Big Data dependent scientific community. Findings: Many biomedical and healthcare studies may be significantly impacted by using large, heterogeneous and incongruent datasets; however there are significant technical, social, regulatory, and institutional barriers that need to be overcome to ensure the power of Big Data overcomes these detrimental factors. Conclusions: Pragmatic policies that demand extensive sharing of data, promotion of data fusion, provenance, interoperability and balance security and protection of personal information are critical for the long term impact of translational Big Data analytics. © 2015, Toga and Dinov."
"10.1186/s40537-015-0019-y","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013874009&doi=10.1186%2fs40537-015-0019-y&partnerID=40&md5=62ee7ad2ed4a18179cc8e36dce80c782","Community structure is thought to be one of the main organizing principles in most complex networks. Big data and complex networks represent an area which researchers are analyzing worldwide. Of special interest are groups of vertices within which connections are dense. In this paper we begin with discussing community dynamics and exploring complex network structural parameters. We put forward structural and functional models for analyzing complex networks under situations of perturbations. We introduce modified adjacency and modified Laplacian matrices. We further introduce network or degree centrality (weighted Laplacian centrality) based on modified Laplacian, weighted micro-community centrality. We discuss its robustness and importance for micro-community detection for social and technological complex networks with overlapping communities. We also introduce ’k-clique sub-community’ overlapping community detection based on degree and weighted micro-community centrality. The proposed algorithms use optimal partition of k-clique sub-community for modularity optimization. We establish relationship between degree centrality and modularity. This proposed method with modified adjacency matrix helps us solve NP-hard problem. © 2015, Chopade and Zhan; licensee Springer."
"10.1057/jma.2015.15","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031499339&doi=10.1057%2fjma.2015.15&partnerID=40&md5=aa958a7d4caf3df31d588e586101cdff","Online social networks have revolutionized the way we communicate, and many companies are already taking full advantage of these tools to improve productivity. The objectives of our study are to measure the presence of large Spanish companies in these online social networks and the importance companies give to the use of such systems and to determine the link between the attitudes of companies toward technology and the adoption of online social networking in their businesses. To do this, we surveyed 99 Spanish companies whose turnover was among the highest in their respective economic sectors, and we carried out a descriptive analysis of the association of variables using the χ2 independence test. The results show that the presence of large Spanish companies in online social networks is scant; however, the results emphasize the importance that such enterprises give to information technology, information systems and a future presence on online social networks. © 2015 Macmillan Publishers Ltd."
"10.1057/jma.2015.18","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031494306&doi=10.1057%2fjma.2015.18&partnerID=40&md5=1c7caf600bf28703feb04b8b0fd39fcf","Tradition pricing efforts without specifying time span often attempt a shortterm solution. A price optimization model factored in a customer lifetime value (LTV) study recently becomes a keen interest to many businesses striving for a long-term success. This article will discuss how to advance pricing models from a tactic focus to a strategic win. As such, two modeling steps will be covered thereby: (i) an intrinsic link between profit and price is first established with a contribution of LTV, particularly in direct marketing (DM) arena; (ii) a rigorous price equation is then migrated into a tabular simulation so that practitioners can alternatively resolve an optimization in either way. © 2015 Macmillan Publishers Ltd."
"10.1057/jma.2015.16","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031309758&doi=10.1057%2fjma.2015.16&partnerID=40&md5=dc4f4180c5f458eef466ece79816ff5c","When building campaign management systems, two main aspects are important from an algorithmic point of view. First, many customers need to be classified with respect to their (bank) product portfolios, which means ranking the potential products for advertisement such as direct mailings, e-mails, website or mobile app advertisement. Second, campaigns for different customer segments need to be planned in advance to balance the costs and the success of marketing activities. For these tasks, we propose a process, starting with rule-based classification, including an evolutionary algorithm combined with a nearest-neighbor technique to obtain understandable, optimized rules. The classification results lead to product rankings for each customer. We cluster the rankings with p-means, using a modified Canberra distance, so that we obtain customer segments. Then, we design a new index for measuring the assignments of campaigns with products to the clusters. With this campaign clustering index (CACLUX), we gain the potential to plan the campaign design for customer segments in advance, comparing poor assignments with good ones. Overall, we present a line of action that generates on the basis of rule-based customer classification and product rankings quantified assignments of campaigns to clusters. © 2015 Macmillan Publishers Ltd."
"10.1057/jma.2015.17","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029485257&doi=10.1057%2fjma.2015.17&partnerID=40&md5=ce0d9226c15364079dc0a69cd369cc83","In today’s competitive business scenario, the entire retail industry is facing a tough challenge in attracting shoppers. Besides reduced prices and sales promotions, sellers have recognized many other factors that entice consumers to visit retail stores like, pleasant experiences, eye-catching displays and store aroma, and so on. Retailers need to understand the buying behaviour of the consumers and the variables associated with it. The purpose of the present study is to scrutinize the inter-relationship among various antecedent factors that define impulsive buying behaviour among shoppers. From past researches, this study extracted antecedent factors constituting impulsive buying. Through extensive discussions among experts from industry and academia, a qualitative assessment of the relationship between these factors was undertaken. With mathematical computations, as suggested under Interpretive Structural Modelling (ISM), a conceptual model is proposed to explain impulsive buying. The model reflects that all the six congregating factors do not trigger impulsive buying directly. It follows a hierarchy-of-effects model, with merchandising display initially sparking the urge for impulsive tendency. Merchandising displays increase the effectiveness of both – promotional schemes and incentives, and retailer-induced instore efforts. These two factors further enhance the effect of socialization and crowd in malls, which cumulatively contribute to arousal of hedonism that eventually leads to impulsive buying. The retail industry, especially the mall developers and supermarket/hypermarket may use the findings in strategizing to enhance their ability to engage, arouse and achieve impulivse buying among shoppers. The proposed hierarchical model with interconnection of these component factors will be of immense interest to retail/mall managers and researchers. This can be a valuable guide for resource allocation. © 2015 Macmillan Publishers Ltd."
"10.1080/23270012.2015.1086704","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057038821&doi=10.1080%2f23270012.2015.1086704&partnerID=40&md5=d824f7c4f824e1e519edfd783b834f22","Technological advancements such as the industrial Internet of Things now allow companies to continuously monitor the operating conditions of expensive equipment using sensors. With the tremendous amount of sensor data flowing in continuously, equipment makers are seeking innovative analytical solutions to turn operational data to help guide their tactical and strategic decisions. Using sensor data on wind turbine operations and service records from a top Fortune 100 company in the energy industry, we showcase techniques to map out operational-level data for analysis, and develop several analytical models (a sequence analysis, a logistic regression and a survival model) to help predict and evaluate equipment failure risks. Our analyses highlight the significant value propositions of sensor data in the big data era. Practical implications as well as extensions of the proposed predictive models are discussed. © 2015, © 2015 Antai College of Economics and Management, Shanghai Jiao Tong University."
"10.1080/23270012.2015.1116414","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051586932&doi=10.1080%2f23270012.2015.1116414&partnerID=40&md5=346ae61e8d9635e81ebdbb92afddad08",[No abstract available]
"10.1080/23270012.2015.1100969","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85006894457&doi=10.1080%2f23270012.2015.1100969&partnerID=40&md5=08368a9bdc2846cf992f567cbf945a88","Public libraries are increasingly using social media to connect their users in innovative ways. Librarians make use of social media as a tool for ‘being part of their communities’, promoting library services and events, and creating participatory services. However, little empirical investigation has explored the success of such social media use by libraries. In this paper, we study the role of Twitter for engaging users via a focus on public libraries. We use topic-modeling techniques to classify library user engagement strategies into four categories: literature exhibits, engaging topics, community building and library showcasing. These four engagement strategies are re-examined via a sentiment analysis of tweets collected from 10 public libraries over 3 months. Through data mining of tweets, we explore how user engagement strategies are used by libraries on Twitter, and suggest the best practices for libraries interested in pursuing social media initiatives to use to engage their users effectively. © 2015, © 2015 Antai College of Economics and Management, Shanghai Jiao Tong University."
"10.1089/big.2014.0054","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84991832961&doi=10.1089%2fbig.2014.0054&partnerID=40&md5=82105d3a5c5d4e326675eed438a99679","The wealth of information provided by real-time streams of data has paved the way for life-changing technological advancements, improving the quality of life of people in many ways, from facilitating knowledge exchange to self-understanding and self-monitoring. Moreover, the analysis of anonymized and aggregated large-scale human behavioral data offers new possibilities to understand global patterns of human behavior and helps decision makers tackle problems of societal importance. In this article, we highlight the potential societal benefits derived from big data applications with a focus on citizen safety and crime prevention. First, we introduce the emergent new research area of big data for social good. Next, we detail a case study tackling the problem of crime hotspot classification, that is, the classification of which areas in a city are more likely to witness crimes based on past data. In the proposed approach we use demographic information along with human mobility characteristics as derived from anonymized and aggregated mobile network data. The hypothesis that aggregated human behavioral data captured from the mobile network infrastructure, in combination with basic demographic information, can be used to predict crime is supported by our findings. Our models, built on and evaluated against real crime data from London, obtain accuracy of almost 70% when classifying whether a specific area in the city will be a crime hotspot or not in the following month. © Mary Ann Liebert, Inc. 2015."
"10.1089/big.2015.0015","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84991822022&doi=10.1089%2fbig.2015.0015&partnerID=40&md5=754c1fd1948237cb4f28327de1636078","The Kavli HUMAN Project (KHP) will provide groundbreaking insights into how biological, medical, and social factors interact and impact the risks for cognitive decline from birth through older age. It will richly measure the effect of cognitive decline on the ability to perform key activities of daily living. In addition, due to its family focus, the KHP will measure the impact on family members, including the amount of time that family members spend providing care to older adults with dementia. It will also clarify the division of caregiving duties among family members and the effects on caregivers' work, family life, and balance thereof. At the same time, for care that the family cannot provide, it will clarify the extent to which cognitive decline impacts healthcare utilization and end-of-life decision making. © Mary Ann Liebert, Inc. 2015."
"10.1089/big.2015.0014","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84991821851&doi=10.1089%2fbig.2015.0014&partnerID=40&md5=06332641fd423c490a936e143a40cd75","Health is shaped by both personal choices and features of the food environment. Food-choice decisions depend on complex interactions between biology and behavior, and are further modulated by the built environment and community structure. That lower-income families have lower-quality diets is well established. Yet, diet quality also varies across small geographic neighborhoods and can be influenced by transportation, retail, and ease of access to healthy foods, as well as by attitudes, beliefs, and social interactions. The learnings from the Seattle Obesity Study (SOS II) can be usefully applied to the much larger, more complex, and far more socially and ethnically diverse urban environment of New York City. The Kavli HUMAN Project (KHP) is ideally positioned to advance the understanding of health disparities by exploring the multiple underpinnings of food decision making. By combining geo-localized food shopping and consumption data with health behaviors, diet quality measures, and biomarkers, also coded by geographic location, the KHP will create the first-of-its-kind bio-behavioral, economic, and cultural atlas of diet quality and health for New York City. © Mary Ann Liebert, Inc. 2015."
"10.1089/big.2015.0017","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84991783972&doi=10.1089%2fbig.2015.0017&partnerID=40&md5=cb14725f0dfb75090fc0a871ae32cc2c","This article describes how the fine-grained data being collected by Internet labor market intermediaries, such as employment websites, online labor markets, and knowledge discussion boards, are providing new research opportunities and directions for the empirical analysis of labor market activity. After discussing these data sources, we examine some of the research opportunities they have created, highlight some examples of existing work that already use these new data sources, and enumerate the challenges associated with the use of these corporate data sources. © Mary Ann Liebert, Inc. 2015."
"10.1089/big.2015.0016","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84991781007&doi=10.1089%2fbig.2015.0016&partnerID=40&md5=2a9887427ccbe43d5601d25a502e5ba7","The next frontier in medicine involves better quantifying human traits, known as ""phenotypes."" Biological markers have been directly associated with disease risks, but poor measurement of behaviors such as diet and exercise limits our understanding of preventive measures. By joining together an uncommonly wide range of disciplines and expertise, the Kavli HUMAN Project will advance measurement of behavioral phenotypes, as well as environmental factors that impact behavior. By following the same individuals over time, KHP will liberate new understanding of dynamic links between behavioral phenotypes, disease, and the broader environment. As KHP advances understanding of the bio-behavioral complex, it will seed new approaches to the diagnosis, prevention, and treatment of human disease. © Mary Ann Liebert, Inc. 2015."
"10.1089/big.2015.0002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84991768859&doi=10.1089%2fbig.2015.0002&partnerID=40&md5=3902fdb43f5e1d11f102adb9fd7a882a","Solving the missing-value (MV) problem with small estimation errors in large-scale data environments is a notoriously resource-demanding task. The most widely used MV imputation approaches are computationally expensive because they explicitly depend on the volume and the dimension of the data. Moreover, as datasets and their user community continuously grow, the problem can only be exacerbated. In an attempt to deal with such a problem, in our previous work, we introduced a novel framework coined Pythia, which employs a number of distributed data nodes (cohorts), each of which contains a partition of the original dataset. To perform MV imputation, the Pythia, based on specific machine and statistical learning structures (signatures), selects the most appropriate subset of cohorts to perform locally a missing value substitution algorithm (MVA). This selection relies on the principle that particular subset of cohorts maintains the most relevant partition of the dataset. In addition to this, as Pythia uses only part of the dataset for imputation and accesses different cohorts in parallel, it improves efficiency, scalability, and accuracy compared to a single machine (coined Godzilla), which uses the entire massive dataset to compute imputation requests. Although this article is an extension of our previous work, we particularly investigate the robustness of the Pythia framework and show that the Pythia is independent from any MVA and signature construction algorithms. In order to facilitate our research, we considered two well-known MVAs (namely K-nearest neighbor and expectation-maximization imputation algorithms), as well as two machine and neural computational learning signature construction algorithms based on adaptive vector quantization and competitive learning. We prove comprehensive experiments to assess the performance of the Pythia against Godzilla and showcase the benefits stemmed from this framework. © Mary Ann Liebert, Inc. 2015."
"10.1089/big.2015.0008","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84991747002&doi=10.1089%2fbig.2015.0008&partnerID=40&md5=2c98ef1434cb74302dc978b34c6cc915","The days of surprise about actual election outcomes in the big data world are likely to be fewer in the years ahead, at least to those who may have access to such data. In this paper we highlight the potential for forecasting the Unites States presidential election outcomes at the state and county levels based solely on the data about viewership of television programs. A key consideration for relevance is that given the infrequent nature of elections, such models are useful only if they can be trained using recent data on viewership. However, the target variable (election outcome) is usually not known until the election is over. Related to this, we show here that such models may be trained with the television viewership data in the ""safe"" states (the ones where the outcome can be assumed even in the days preceding elections) to potentially forecast the outcomes in the swing states. In addition to their potential to forecast, these models could also help campaigns target programs for advertisements. Nearly two billion dollars were spent on television advertising in the 2012 presidential race, suggesting potential for big data-driven optimization of campaign spending. © Mary Ann Liebert, Inc. 2015."
"10.1089/big.2015.0013","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84991746064&doi=10.1089%2fbig.2015.0013&partnerID=40&md5=76fd15343efd62a368ce4a84221af70c","Despite clear links between genes and smoking, effective public policy requires far richer measurement of the feedback between biological, behavioral, and environmental factors. The Kavli HUMAN Project (KHP) plans to exploit the plummeting costs of data gathering and to make creative use of new technologies to construct a longitudinal panel data set that would compare favorably to existing longitudinal surveys, both in terms of the richness of the behavioral measures and the cost-effectiveness of the data collection. By developing a more comprehensive approach to characterizing behavior than traditional methods, KHP will allow researchers to paint a much richer picture of an individual's life-cycle trajectory of smoking, alcohol, and drug use, and interactions with other choices and environmental factors. The longitudinal nature of KHP will be particularly valuable in light of the increasing evidence for how smoking behavior affects physiology and health. The KHP could have a transformative impact on the understanding of the biology of addictive behaviors such as smoking, and of a rich range of prevention and amelioration policies. © Mary Ann Liebert, Inc. 2015."
"10.1089/big.2015.0012","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84991744743&doi=10.1089%2fbig.2015.0012&partnerID=40&md5=3e7235e67d466433ed319dd9a8be23f5","Until now, most large-scale studies of humans have either focused on very specific domains of inquiry or have relied on between-subjects approaches. While these previous studies have been invaluable for revealing important biological factors in cardiac health or social factors in retirement choices, no single repository contains anything like a complete record of the health, education, genetics, environmental, and lifestyle profiles of a large group of individuals at the within-subject level. This seems critical today because emerging evidence about the dynamic interplay between biology, behavior, and the environment point to a pressing need for just the kind of large-scale, long-term synoptic dataset that does not yet exist at the within-subject level. At the same time that the need for such a dataset is becoming clear, there is also growing evidence that just such a synoptic dataset may now be obtainable - at least at moderate scale - using contemporary big data approaches. To this end, we introduce the Kavli HUMAN Project (KHP), an effort to aggregate data from 2,500 New York City households in all five boroughs (roughly 10,000 individuals) whose biology and behavior will be measured using an unprecedented array of modalities over 20 years. It will also richly measure environmental conditions and events that KHP members experience using a geographic information system database of unparalleled scale, currently under construction in New York. In this manner, KHP will offer both synoptic and granular views of how human health and behavior coevolve over the life cycle and why they evolve differently for different people. In turn, we argue that this will allow for new discovery-based scientific approaches, rooted in big data analytics, to improving the health and quality of human life, particularly in urban contexts. © Mary Ann Liebert, Inc. 2015."
"10.1016/j.bdr.2015.03.004","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84955403484&doi=10.1016%2fj.bdr.2015.03.004&partnerID=40&md5=52012fdcd6680d6235c237a79cc6617a","A futures trading evaluation system is used to help investors analyze their trading history and find out the root cause of profit and loss, so that investors can learn from their past and make better decisions in the future. To analyze trading history of investors, the system processes a large volume of transaction data to calculate key performance indicators (KPI) as well as time series behavior patterns, and concludes some recommendations with the help of an expert knowledge base. This work is based on our early work of parallel techniques for large data analysis for futures trading evaluation service. In our early work, we have used the query rewriting technique to avoid joining between fact table and dimension table for OLAP aggregation queries, and used a data driven shared scanning of data method to compute KPIs for one customer. However, the query rewriting technique cannot eliminate joining for queries which aggregate on an intermediate level of the hierarchy of a dimensional table, so we propose a segmented bit encoding of dimensional table method which can eliminate the joining operation when the query aggregates on any level of the hierarchy of any dimensional table. Furthermore, our previous method perform badly when concurrency is high, so we propose an inter customer data scan sharing scheme to improve system performance in highly concurrent situations. We present our new experimental results. © 2015 Elsevier Inc."
"10.1016/j.bdr.2015.03.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84955272582&doi=10.1016%2fj.bdr.2015.03.001&partnerID=40&md5=fb1143d404e3d62142e3e7015436d8c3","As distributed computing systems are used more widely, driven by trends such as 'big data' and cloud computing, they are being used for an increasingly wide range of applications. With this massive increase in application heterogeneity, the ability to have a general purpose resource management technique that performs well in heterogeneous environments is becoming increasingly important.In this paper, we present Multi-Tier Resource Allocation (MTRA) as a novel fine-grained resource management technique for distributed systems. The core idea is based on allocating resources to individual tasks in a tiered or layered approach. To account for heterogeneity, we propose a dynamic resource allocation method that adjusts resource allocations to individual tasks on a cluster node based on resource utilisation levels. We demonstrate the efficacy of this technique in a data-intensive computing environment, MapReduce data processing framework in Hadoop YARN. Our results demonstrate that MTRA is an effective general purpose resource management technique particularly for data-intensive computing environments. On a range of MapReduce benchmarks in a Hadoop YARN environment, our MTRA technique improves performance by up to 18%. In a Facebook workload model it improves job execution times by 10% on average, and up to 56% for individual jobs. © 2015 Elsevier Inc."
"10.1016/j.bdr.2015.01.007","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84955250490&doi=10.1016%2fj.bdr.2015.01.007&partnerID=40&md5=93f3fd2f8915165368b4013ab0768856","The non-contiguous access pattern of many scientific applications results in a large number of I/O requests, which can seriously limit the data-access performance. Collective I/O has been widely used to address this issue. However, the performance of collective I/O could be dramatically degraded in today's high-performance computing systems due to the increasing shuffle cost caused by highly concurrent data accesses. This situation tends to be even worse as many applications become more and more data intensive. Previous research has primarily focused on optimizing I/O access cost in collective I/O but largely ignored the shuffle cost involved. Previous works assume that the lowest average response time leads to the best QoS and performance, while that is not always true for collective requests when considering the additional shuffle cost. In this study, we propose a new hierarchical I/O scheduling (HIO) algorithm to address the increasing shuffle cost in collective I/O. The fundamental idea is to schedule applications' I/O requests based on a shuffle cost analysis to achieve the optimal overall performance, instead of achieving optimal I/O accesses only. The algorithm is currently evaluated with the MPICH3 and PVFS2. Both theoretical analysis and experimental tests show that the proposed hierarchical I/O scheduling has a potential in addressing the degraded performance issue of collective I/O with highly concurrent accesses. © 2015 Elsevier Inc."
"10.1016/j.bdr.2015.03.003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84945309893&doi=10.1016%2fj.bdr.2015.03.003&partnerID=40&md5=2c7db47594b9602029c38652124f5c42","The smart electricity grid enables a two-way flow of power and data between suppliers and consumers in order to facilitate the power flow optimization in terms of economic efficiency, reliability and sustainability. This infrastructure permits the consumers and the micro-energy producers to take a more active role in the electricity market and the dynamic energy management (DEM). The most important challenge in a smart grid (SG) is how to take advantage of the users' participation in order to reduce the cost of power. However, effective DEM depends critically on load and renewable production forecasting. This calls for intelligent methods and solutions for the real-time exploitation of large volumes of data generated by the vast amount of smart meters. Hence, robust data analytics, high performance computing, efficient data network management, and cloud computing techniques are critical towards the optimized operation of SGs. This research aims to highlight the big data issues and challenges faced by the DEM employed in SG networks. It also provides a brief description of the most commonly used data processing methods in the literature, and proposes a promising direction for future research in the field. © 2015 Elsevier Inc."
"10.1057/jma.2015.9","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031500647&doi=10.1057%2fjma.2015.9&partnerID=40&md5=77b1dfabc75f4f9b1c515966925ae8b4","Using Experian Simmons Market Research Bureau’s National Consumer Survey data, this study explores how users of social media use various types of traditional media. The study found that, in general, magazines were used most heavily among social media users, followed by newspapers, radio and television. Second, for the purpose of getting information, social media users relied on television most, followed by newspapers, radio and magazines. Last, the degree of reliance on traditional media for information varied significantly across the types of social media users. Implications of the research for advertising practitioners are discussed, especially in light of media planning. © 2015 Macmillan Publishers Ltd."
"10.1057/jma.2015.13","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031492708&doi=10.1057%2fjma.2015.13&partnerID=40&md5=3dd64b38c8e220b79be825bc65d8886a","A new metric of the quality of a savings product, from the perspective of a bank, is introduced. This is the stickiness of a dollar, the average time a random deposited dollar remains with the bank. A practical algorithm for calculating stickiness is deduced and theoretical properties of the metric are derived. Stickiness is a metric that can be applied both at the aggregate and at the individual level. Examples are given from a bank with branches in different countries. With these examples it is shown that stickiness has some desirable properties: (1) stickiness at the individual level positively correlates with stickiness at the aggregate level; (2) stickiness increases with time, but slower than at a linear rate and (3) at the individual level stickiness in the future can be predicted by stickiness in the past. As a consequence, stickiness can be regarded as a stable customer variable. It is, therefore sensible to characterize demographic groups, e.g. stickiness increases with age, and to use it as active or descriptive variable for customer segments. An example is given of a new product that is relatively sticky. © 2015 Macmillan Publishers Ltd."
"10.1057/jma.2015.11","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85024486120&doi=10.1057%2fjma.2015.11&partnerID=40&md5=7a19adbf7ce62d08b6956a1dd8d486c9","To determine spending on acquisition and retention of customers, a company can focus on maximizing a variety of indicators such as customer equity, profit or sales growth. In this article we use standard techniques from multivariable calculus to maximize market share through analysis of retention and switching ratios both in the absence and presence of a budget constraint. In the process, we identify firm theoretical foundations for some common intuitions, and give new insights on optimal competitive strategy. We include practical numerical examples that show marketing practitioners how to apply our analysis. © 2015 Macmillan Publishers Ltd."
"10.1057/jma.2015.12","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019583273&doi=10.1057%2fjma.2015.12&partnerID=40&md5=a363b8ccdb9fdf3e9797644f843e220b","As the concept of Customer Lifetime Value (CLV) is gaining a growing acceptance to achieve superior efficiency and effectiveness of marketing plans, development of more accurate models to measure CLV is becoming an important issue. Most CLV models cannot consider the management flexibility in uncertain situations. Real options valuation is the approach in such situations. This article aims to provide models to measure CLV using real options valuation. The research firstly aims to identify major options a seller has in relationship with its customers. Then, we focus on development of models to measure options’ values for better estimation of CLV. In this research, we develop three models based on real options. Each model is useful when the following options are available: option to market penetration, option to customer development through add-on selling and option of stage investment. The applicability of the models is demonstrated by numerical illustrations. Results show the classical models of CLV measurement ignore the value of options despite the fact that sometimes these options have considerable values and are decisive in managers’ decision making. © 2015 Macmillan Publishers Ltd."
"10.1057/jma.2015.10","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85009147613&doi=10.1057%2fjma.2015.10&partnerID=40&md5=9f7b5941a487e03a52b4c655438133ab","In today’s fast moving world of marketing from product-orientation to customer-orientation, the management of customer treatment can be seen as a key to achieve revenue growth and profitability. Knowledge of customer behavior can help marketing managers re-evaluate their strategies with the customers and plan to improve and expand their application of the most effective strategies. B2B or business customers are more complex, their buying process is more complicated and their sales value is greater. The business marketers usually prefer to cooperate with fewer but larger buyers than the final consumer marketer. As a business transaction requires more decision makings and more professional buying effort than the consumer market does, the efficient relationship with business customers is of paramount importance. Most customer segmentation approaches based on customer value fail to account for the factor of time and the trend of value changes in their analysis. In this article, we classify customers based on their value using the RFM model and K-means clustering method. Then, an assessment of changes over several periods of time is carried out. The originality of this research lies in its incorporation of time and trend of customer value changes in improving the accuracy of predictions based on the past behavior of customers. For this purpose, we used the POS customer transactions. © 2015 Macmillan Publishers Ltd."
"10.1080/23270012.2015.1067154","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042114327&doi=10.1080%2f23270012.2015.1067154&partnerID=40&md5=23dcaf4b263cb035cefdba288bf868e2","Web 3.0 technology will revolutionize the learning process, enabling data linking to connect learning resources and create ontologies for different areas of knowledge that enable ‘smart searches.’ Smart or semantic searches perceive relationships among various pieces of information and present them to the learner. Connectivism has been proposed as a theory to guide learning in this new Web 3.0 environment. This paper discusses the relevance of connectivism and then develops an ontology for learning resources. The authors propose a hybrid similarity measure to evaluate the similarity among different learning resources. The paper presents a case study that was conducted to evaluate the proposed similarity measure on education data sets and demonstrates the effectiveness of the proposed methods. © 2015, © 2015 Antai College of Economics and Management, Shanghai Jiao Tong University."
"10.1080/23270012.2015.1090889","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030213571&doi=10.1080%2f23270012.2015.1090889&partnerID=40&md5=c707605f29c628da98edd472d14359ae","Rule induction (RI) produces classifiers containing simple yet effective ‘If–Then' rules for decision makers. RI algorithms normally based on PRISM suffer from a few drawbacks mainly related to rule pruning and rule-sharing items (attribute values) in the training data instances. In response to the above two issues, a new dynamic rule induction (DRI) method is proposed. Whenever a rule is produced and its related training data instances are discarded, DRI updates the frequency of attribute values that are used to make the next in-line rule to reflect the data deletion. Therefore, the attribute value frequencies are dynamically adjusted each time a rule is generated rather statically as in PRISM. This enables DRI to generate near perfect rules and realistic classifiers. Experimental results using different University of California Irvine data sets show competitive performance in regards to error rate and classifier size of DRI when compared to other RI algorithms. © 2015, © 2015 Antai College of Economics and Management, Shanghai Jiao Tong University."
"10.1080/23270012.2015.1039612","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013886136&doi=10.1080%2f23270012.2015.1039612&partnerID=40&md5=d5c3f9b5da364943ac3d09d88bb887f0","This paper develops an inventory model for deteriorating items with finite replenishment rate under a progressive payment scheme within the cycle time. In this model, the deterioration function follows a probability distribution such as a (1) uniform distribution, (2) triangular distribution or (3) beta distribution. Here, the retailer is allowed a trade-credit offer by the supplier to buy more items. This model aids in minimizing the total inventory cost of the retailer by finding the optimal cycle length, the optimal time length of replenishment and the optimal order quantity. Some theorems have been framed to characterize the optimal solutions. The necessary and sufficient conditions of the existence and uniqueness of the optimal solutions are also provided. The optimal solution of the model is illustrated with the help of numerical examples, and numerical comparisons between the three models are also given. Finally, sensitivity analysis and graphical representations are given to demonstrate the model. © 2015, © 2015 Antai College of Economics and Management, Shanghai Jiao Tong University."
"10.1080/23270012.2015.1077481","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85011605047&doi=10.1080%2f23270012.2015.1077481&partnerID=40&md5=eb4727047471690ec270ecd7ecde3aae","The location selection of the logistics distribution center for a supply and demand network (SDN) enterprises directly affected the efficiency of the logistics system operation and the customer service level. In this paper, we present a location selection model of the logistics distribution center for SDN enterprises. In order to improve the optimization effectiveness of the traditional methods in solving the location selection problem, an improved firefly algorithm was presented. By introducing a coordination factor, the search step can be automatically adjusted, and the accuracy of the algorithm can be improved. By introducing a chaotic search strategy, the diversity of firefly populations and the global optimization ability of the algorithm can be improved. The simulation experiments showed that the improved firefly algorithm achieved a more favorable effectiveness than three other algorithms we tested. © 2015, © 2015 Antai College of Economics and Management, Shanghai Jiao Tong University."
"10.1080/23270012.2015.1082449","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978845796&doi=10.1080%2f23270012.2015.1082449&partnerID=40&md5=a62ad48df118ac74e76579729b21992a","With more and more data generated, it has become a big challenge for traditional architectures and infrastructures to process large amounts of data within an acceptable time and resources. In order to efficiently extract value from these data, organizations need to find new tools and methods specialized for big data processing. For this reason, big data analytics has become a key factor for companies to reveal hidden information and achieve competitive advantages in the market. Currently, enormous publications of big data analytics make it difficult for practitioners and researchers to find topics they are interested in and track up to date. This paper aims to present an overview of big data analytics’ content, scope and findings as well as opportunities provided by the application of big data analytics. © 2015, © 2015 Antai College of Economics and Management, Shanghai Jiao Tong University."
"10.4018/IJBAN.2015070101","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046267117&doi=10.4018%2fIJBAN.2015070101&partnerID=40&md5=02f616d4251b4fa224c79f2cd9d08330","Lean is a systematic approach to identify and eliminate non-value-added activities or waste through continuous improvement process. While traditional lean manufacturing focuses on the activities within a single organization, lean supply chain consists of the same processes, but it views these processes over multiple organizations. This research addresses an important yet under-studied area – lean supply chain management in small organizations, especially small manufacturing firms. The study examines driving factors of lean supply chain management, focus of lean supply chain practices, and major supply chain and information technology solutions applied in these companies. Through a research survey, the study has provided important insights into the current status of lean supply chain practices and related implementation issues in small businesses. Copyright © 2015, IGI Global."
"10.1089/big.2015.0006","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84991834384&doi=10.1089%2fbig.2015.0006&partnerID=40&md5=c005756ad2c7ee9245319778df2f42f4","Online systems promise to improve advertisement targeting via the massive and detailed data available. However, there often is too few data on exactly the outcome of interest, such as purchases, for accurate campaign evaluation and optimization (due to low conversion rates, cold start periods, lack of instrumentation of offline purchases, and long purchase cycles). This paper presents a detailed treatment of proxy modeling, which is based on the identification of a suitable alternative (proxy) target variable when data on the true objective is in short supply (or even completely nonexistent). The paper has a two-fold contribution. First, the potential of proxy modeling is demonstrated clearly, based on a massive-scale experiment across 58 real online advertising campaigns. Second, we assess the value of different specific proxies for evaluating and optimizing online display advertising, showing striking results. The results include bad news and good news. The most commonly cited and used proxy is a click on an ad. The bad news is that across a large number of campaigns, clicks are not good proxies for evaluation or for optimization: clickers do not resemble buyers. The good news is that an alternative sort of proxy performs remarkably well: observed visits to the brand's website. Specifically, predictive models built based on brand site visits - which are much more common than purchases - do a remarkably good job of predicting which browsers will make a purchase. The practical bottom line: evaluating and optimizing campaigns using clicks seems wrongheaded; however, there is an easy and attractive alternative - use a well-chosen site-visit proxy instead. © 2015, Mary Ann Liebert, Inc."
"10.1089/big.2014.0060","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84991829790&doi=10.1089%2fbig.2014.0060&partnerID=40&md5=2bc2d6eacac2898a0f15c88d750052b3","On April 2nd, 2014, the Department of Health and Human Services (HHS) announced a historic policy in its effort to increase the transparency in the American healthcare system. The Center for Medicare and Medicaid Service (CMS) would publicly release a dataset containing information about the types of Medicare services, requested charges, and payments issued by providers across the country. In its release, HHS stated that the data would shed light on ""Medicare fraud, waste, and abuse."" While this is most certainly true, we believe that it can provide so much more. Beyond the purely financial aspects of procedure charges and payments, the procedures themselves may provide us with additional information, not only about the Medicare population, but also about the physicians themselves. The procedures a physician performs are for the most part not novel, but rather recommended, observed, and studied. However, whether a physician decides on advocating a procedure is somewhat discretionary. Some patients require a clear course of action, while others may benefit from a variety of options. This article poses the following question: How does a physician's past experience in medical school shape his or her practicing decisions? This article aims to open the analysis into how data, such as the CMS Medicare release, can help further our understanding of knowledge transfer and how experiences during education can shape a physician's decision's over the course of his or her career. This work begins with an evaluation into similarities between medical school charges, procedures, and payments. It then details how schools' procedure choices may link them in other, more interesting ways. Finally, the article includes a geographic analysis of how medical school procedure payments and charges are distributed nationally, highlighting potential deviations. © Copyright 2015, Mary Ann Liebert, Inc. 2015."
"10.1089/big.2014.0034","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84991807876&doi=10.1089%2fbig.2014.0034&partnerID=40&md5=f2c34b68de574235858348a58eace1c3","Most healthcare data warehouses include big data such as health plan, medical, and pharmacy claims information for many thousands and sometimes millions of insured individuals. This makes it possible to identify those with multiple chronic conditions who may benefit from participation in care coordination programs meant to improve their health. The objective of this article is to describe how large databases, including individual and claims data, and other, smaller types of data from surveys and personal interviews, are used to support a care coordination program. The program described in this study was implemented for adults who are generally 65 years of age or older and have an AARP® Medicare Supplement Insurance Plan (i.e., a Medigap plan) insured by UnitedHealthcare Insurance Company (or, for New York residents, UnitedHealthcare Insurance Company of New York). Individual and claims data were used first to calculate risk scores that were then utilized to identify the majority of individuals who were qualified for program participation. For efficient use of time and resources, propensity to succeed modeling was used to prioritize referrals based upon their predicted probabilities of (1) engaging in the care coordination program, (2) saving money once engaged, and (3) receiving higher quality of care. To date, program evaluations have reported positive returns on investment and improved quality of healthcare among program participants. In conclusion, the use of data sources big and small can help guide program operations and determine if care coordination programs are working to help older adults live healthier lives. © Copyright 2015, Mary Ann Liebert, Inc. 2015."
"10.1089/big.2014.0069","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84991781038&doi=10.1089%2fbig.2014.0069&partnerID=40&md5=481f98627e130225ab67c1a7b881f67e","Physicians are being challenged to obtain data for outcomes research and measures of quality practice in medicine. We developed a prospective data collection system (registry) that provides data points across all elements of a neurosurgical stereotactic radiosurgery practice. The registry architecture is scalable and suitable for any aspect of neurosurgical practice. Our purpose was to outline the challenges in creating systems for high quality data acquisition and describe experiences in initial testing and use. Over a two year period, a multicenter team working with software engineers developed a comprehensive radiosurgery registry based on a MS-Sequel® server platform. Three neurosurgeons at one center were responsible for final editing. Alpha testing began in September 2012 and server-based beta testing began in February 2013. The major elements included demographics, disease-based items (47 categories for different brain tumors, vascular malformations, and functional disorders) with relevant clinical grading systems, treatment-based items (imaging, physics, clinical), and follow-up data (clinical, imaging, subsequent therapeutics). Nine hundred patients were entered into the registry at one test center, with new entries and follow-up data entered daily at the point of contact. With experience, the mean time for one new entry was 6 minutes. Mean time for one follow-up entry was 45 seconds. The system was made secure for individual use and amenable for both data entry and research. Analytics used different filters to create customized outcomes charts as selected by the user (e.g., survival, neurologic function, complications). A local or multicenter prospective data collection registry was created for use across 47 clinical indications for stereotactic cranial radiosurgery. Further refinement of fields and logic is ongoing. The system is reliable, robust, and allows use of rapid analytical tools. Large medical registries will become widely used for collection and analysis of large data sets and should have broad applicability to many other elements of neurosurgical and medical practice. © Copyright 2015, Mary Ann Liebert, Inc. 2015."
"10.1089/big.2014.0055","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84991780653&doi=10.1089%2fbig.2014.0055&partnerID=40&md5=6d13ad9cf1a0842aacc144460eb50dfd","This report describes a groundbreaking military-civilian collaboration that benefits from an Army and Department of Defense (DoD) big data business intelligence platform called the Person-Event Data Environment (PDE). The PDE is a consolidated data repository that contains unclassified but sensitive manpower, training, financial, health, and medical records covering U.S. Army personnel (Active Duty, Reserve, and National Guard), civilian contractors, and military dependents. These unique data assets provide a veridical timeline capturing each soldier's military experience from entry to separation from the armed forces. The PDE was designed to afford unprecedented cost-efficiencies by bringing researchers and military scientists to a single computerized repository rather than porting vast data resources to individual laboratories. With funding from the Robert Wood Johnson Foundation, researchers from the University of Pennsylvania Positive Psychology Center joined forces with the U.S. Army Research Facilitation Laboratory, forming the scientific backbone of the military-civilian collaboration. This unparalleled opportunity was necessitated by a growing need to learn more about relations between psychological and health assets and health outcomes, including healthcare utilization and costs - issues of major importance for both military and civilian population health. The PDE represents more than 100 times the population size and many times the number of linked variables covered by the nation's leading sources of population health data (e.g., the National Health and Nutrition Examination Survey). Following extensive Army vetting procedures, civilian researchers can mine the PDE's trove of information using a suite of statistical packages made available in a Citrix Virtual Desktop. A SharePoint collaboration and governance management environment ensures user compliance with federal and DoD regulations concerning human subjects' protections and also provides a secure portal for multisite collaborations. Taking similarities and differences between military and civilian populations into account, PDE studies can provide much more detailed insight into health-related questions of broad societal concern. Finding ways to make the rich repository of digitized information in the PDE available through military-civilian collaboration can help solve critical medical and behavioral issues affecting the health and well-being of our nations' military and civilian populations. © Copyright 2015, Mary Ann Liebert, Inc. 2015."
"10.1016/j.bdr.2015.01.004","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84929179512&doi=10.1016%2fj.bdr.2015.01.004&partnerID=40&md5=ad840c6b0641f9567e4337115d726e3c","Tensors, or multi dimensional arrays, are receiving significant attention due to the various types of data that can be modeled by them; examples include call graphs (sender, receiver, time), knowledge bases (subject, verb, object), 3-dimensional web graphs augmented with anchor texts, to name a few. Scalable tensor mining aims to extract important patterns and anomalies from a large amount of tensor data. In this paper, we provide an overview of scalable tensor mining. We first present main algorithms for tensor mining, and their scalable versions. Next, we describe success stories of using tensors for interesting data mining problems including higher order web analysis, knowledge base mining, network traffic analysis, citation analysis, and sensor data analysis. Finally, we discuss interesting future research directions for scalable tensor mining. © 2015 Elsevier Inc."
"10.1016/j.bdr.2015.01.006","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84929170243&doi=10.1016%2fj.bdr.2015.01.006&partnerID=40&md5=af4d099c6d2818058a1844f13788a294","In recent years, the rapid development of Internet, Internet of Things, and Cloud Computing have led to the explosive growth of data in almost every industry and business area. Big data has rapidly developed into a hot topic that attracts extensive attention from academia, industry, and governments around the world. In this position paper, we first briefly introduce the concept of big data, including its definition, features, and value. We then identify from different perspectives the significance and opportunities that big data brings to us. Next, we present representative big data initiatives all over the world. We describe the grand challenges (namely, data complexity, computational complexity, and system complexity), as well as possible solutions to address these challenges. Finally, we conclude the paper by presenting several suggestions on carrying out big data projects. © 2015 Elsevier Inc."
"10.1016/j.bdr.2015.01.008","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84929166422&doi=10.1016%2fj.bdr.2015.01.008&partnerID=40&md5=ddb9e4b3fe15042d66a286ce471981c3","Digital technologies have fundamentally altered the nature of organizing innovation and production leading to open collaboration ecosystems. Individuals self-organize in open, voluntary technology-enabled collectives to share their enhancements to the data or collaborate on analyzing, disseminating, or leveraging the data for many applications, from enterprise computing to mobile, consumer oriented applications. 'Big data' is an increasingly important 'engine' to better understand the complex 'nervous system' of open collaboration. However, we need to equip open collaboration researchers with new datasets that span different contexts, as well as novel computational models and analytical techniques. In this paper, we will elaborate on research questions concerning open digital collaboration and derive the data analytical challenges that need to be addressed to answer these research questions. © 2015 Elsevier Inc."
"10.1016/j.bdr.2015.01.003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84929159979&doi=10.1016%2fj.bdr.2015.01.003&partnerID=40&md5=94ca2c4dc0502a477cb8cb393a925edd","Geospatial big data refers to spatial data sets exceeding capacity of current computing systems. A significant portion of big data is actually geospatial data, and the size of such data is growing rapidly at least by 20% every year. In this paper, we explore the challenges and opportunities which geospatial big data brought us. Several case studies are introduced to show the importance and benefits of the analytics of geospatial big data, including fuel and time saving, revenue increase, urban planning, and health care. Then, we introduce new emerging platforms for sharing the collected geospatial big data and for tracking human mobility via mobile devices. The researchers in academia and industry have spent a lot of efforts to improve the value of geospatial big data as well as take advantage of its value. Along the same line, we present our current research activities toward the analytics of geospatial big data, especially on interactive analytics of real-time or dynamic data. © 2015 Elsevier Inc."
"10.1057/jma.2015.4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031504674&doi=10.1057%2fjma.2015.4&partnerID=40&md5=d6831f4ce6e51a4cea124994c1c32bd1","Rating scales are measuring instruments that are widely used in social science research. However, many different rating scale formats are used in the literature, differing specifically in the number of response alternatives offered. Previous studies on the optimal number of response alternatives have focused exclusively on the participants’ final response results, rather than on the participants’ information processing. We used an eye-tracking study to explore this issue from an information processing perspective. We analyzed the information processing in six scales with different response alternatives. We compared the reaction times, net acquiescence response styles, extreme response styles and proportional changes in the response alternatives of the six scales. Our results suggest that the optimal number of response alternatives is five. © 2015 Macmillan Publishers Ltd."
"10.1057/jma.2015.5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031499546&doi=10.1057%2fjma.2015.5&partnerID=40&md5=fb1a62e7c39750381b95b4bbb1ffce86","Uplift modeling, a predictive modeling technique, empowers marketers or other researchers to identify the ‘true’ treatment responders who would be most positively influenced by the treatment or intervention through uncovering their characteristics separately from the characteristics of baseline or control responders (that is, those who would have responded anyway). This article briefly reviews the concept of uplift modeling and extends the current work to multiple treatment situations (where at least two treatments are available as options). It discusses the mathematical problem of optimizing treatment at the individual level, and proposes a practical heuristic solution. Finally, it presents a framework accounting for the variability in estimates when handling multiple assignments. An example from an online retailer is used to illustrate the methodologies. © 2015 Macmillan Publishers Ltd."
"10.1057/jma.2015.8","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031491052&doi=10.1057%2fjma.2015.8&partnerID=40&md5=587bd4ffd847bb5ba9e31ec6bf332341","Marketing scales are valuable in measuring multi-dimensional constructs used for analytical marketing studies. One such type of scales is consumer innovativeness. Previous research on conceptualising and measuring consumer innovativeness has focused on new products about which respondents have attitudinal or behavioural repertoire to draw upon and report, and seem to have paid insufficient attention to the characteristics of innovators and their cognitive styles in scenarios that involve radical and really new innovations. The present research aims to fill the gap by addressing the radical and really new innovations in their respective new markets, in which most consumers have no attitudinal or behavioural repertoire to reflect upon. Following the suggested procedure for marketing scales, qualitative research was used to generate initial items. In the second stage, large-scale questionnaire surveys were conducted in three Middle Eastern countries. The structural equation modelling technique was applied to the survey data in order to valid the final items for the new consumer innovativeness scale. The final scale items were then statistically cross-validated across countries in order to find out the cultural transferability of the new scale. © 2015 Macmillan Publishers Ltd."
"10.1057/jma.2015.6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031490164&doi=10.1057%2fjma.2015.6&partnerID=40&md5=e8e1cc2692c7958179bd329afee2f0d3","Because careless responding to questionnaire items can be quite common, and even low rates of careless responding can substantially impact some analyzes, researchers have been advised to remove careless respondents before data analysis. However, identifying these respondents is a non-trivial task. In the context of multi-item, unidimensional scales, it has been suggested that response variance may hold information about careless responding. This notion was tested in the current research with two studies. Data collected from respondents indicates that people responding carelessly display more variance in their responses than people responding with more effort. Given this finding, a procedure which uses measures of response variance and cluster analysis to identify careless respondents was developed. The effectiveness of the procedure with different specifications was tested with simulated data and validated with data from actual respondents. On the basis of the results, we advocate using the procedure with specifications that conservatively identify careless respondents. Such an approach will identify the most extreme careless respondents, while maximizing the retention of careful, honest respondents. We discuss the advantages the developed procedure has over existing procedures for identifying careless respondents. © 2015 Macmillan Publishers Ltd."
"10.1080/23270012.2015.1048923","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052892193&doi=10.1080%2f23270012.2015.1048923&partnerID=40&md5=983cd9d5270c1d1f86b58f4b0c75f766","This paper proposes a practical and framework-based approach to design an architecture transformation strategy and roadmap aiming to transform or modernize critical legacy enterprise systems. The approach is business value driven with IT supportability in terms of lower application operational and support costs, higher business value and shorter time to market of application delivery. The approach introduces a robust enterprise application architecture assessment framework with an emphasis on technical (internal) and strategic (external) perspectives to guide the application assessment and also a finance self-support transformation strategy to aid its transformation roadmap design. The approach was applied in multiple large enterprises successfully and received endorsements and positive feedback from the sponsors. The paper also presents a case study detailing the successful application of the approach to modernize an enterprise logistics transportation management system. © 2015, © 2015 Antai College of Economics and Management, Shanghai Jiao Tong University."
"10.1080/23270012.2015.1043965","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052828958&doi=10.1080%2f23270012.2015.1043965&partnerID=40&md5=d8dfa7b8d15280f8a869642a2a9aff1d","Existing approaches suggest that IT strategy should be a reflection of business strategy. However, actually organizations do not often follow business strategy even if it is formally declared. In these conditions, IT strategy can be viewed not as a plan, but as an organizational shared view on the role of information systems. This approach generally reflects only a top-down perspective of IT strategy. So it can be supplemented by a strategic behavior pattern (i.e. more or less standard response to a change that is formed as result of previous experience) to implement a bottom-up approach. Two critical regular components that establish the behavioral pattern regarding IT are proposed here: a model of IT-related decision making, and an efficiency measurement metric to estimate the maturity of business processes and appropriate IT. Usage of proposed tools is demonstrated in practical cases. © 2015, © 2015 Antai College of Economics and Management, Shanghai Jiao Tong University."
"10.1080/23270012.2015.1047904","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027415322&doi=10.1080%2f23270012.2015.1047904&partnerID=40&md5=5a28ffe941a32ff5790b9dabdcc05f0e","This paper explains the mechanism of R&D network formation based on a game model of network embeddedness. To describe the game relationship between a new member and an original R&D network, this game model is set up by introducing three factors: R&D efficiency of the new member, asymmetric information, and trust. By solving the game model, this paper analyzes their impact on the level of embeddedness and transaction cost within the Network. Study results show that, during the formation of an R&D network, low efficiency and asymmetric information will do harm to the level of embeddedness and raise the transaction cost, while trust will have a complicated impact on them because of the probability of misplaced-trust. © 2015, © 2015 Antai College of Economics and Management, Shanghai Jiao Tong University."
"10.1080/23270012.2015.1048753","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85005984995&doi=10.1080%2f23270012.2015.1048753&partnerID=40&md5=4a487d86ad60fde1cc30d22b31c9c59a","The growing interest in the material flow (MF) theory has invoked much interesting research in recent years. Although the MF theory is relatively new, a review of the related literature from a historical perspective shows that MF theory represents a new stage of the evolutionary development of interrelated subjects such as Physical Distribution (PD), Logistics, and Supply Chain Management (SCM). The purpose of this paper is to provide a summative review of the evolution of the subjects of PD, Logistics, and SCM, and their new development, MF theory. The paper aims at tracing how concepts and findings in PD, Logistics, SCM, and MF have been developed and have evolved. The study shows that PD evolved to Logistics in middle of the 1980s; starting from the late 1990s, Logistics has evolved to SCM; and today PD, Logistics, and SCM can be considered to be under the umbrella provided by a new theory called MF theory. This paper points out that MF theory is a necessity to deal with the overwhelming complexity of material flow systems in the global economy of the twenty-first century. © 2015, © 2015 Antai College of Economics and Management, Shanghai Jiao Tong University."
"10.1080/23270012.2015.1029550","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84981702062&doi=10.1080%2f23270012.2015.1029550&partnerID=40&md5=5dfbe447d4d74f0584b7ac093c7185cc","Monitoring health conditions over a human body to detect anomalies is a multidisciplinary task, which involves anatomy, artificial intelligence, and sensing and computing networks. A wearable wireless sensor network (WWSN) turns into an emerging technology, which is capable of acquiring dynamic data related to a human body's physiological conditions. The collected data can be applied to detect anomalies in a patient, so that he or she can receive an early alert about the adverse trend of the health condition, and doctors can take preventive actions accordingly. In this paper, a new WWSN for anomaly detections of health conditions has been proposed, system architecture and network has been discussed, the detecting model has been established and a set of algorithms have been developed to support the operation of the WWSN. The novelty of the detected model lies in its relevance to chronobiology. Anomalies of health conditions are contextual and assessed not only based on the time and spatial correlation of the collected data, but also based on mutual relations of the data streams from different sources of sensors. A new algorithm is proposed to identify anomalies using the following procedure: (1) collected raw data is preprocessed and transferred into a set of directed graphs to represent the correlations of data streams from different sensors; (2) the directed graphs are further analyzed to identify dissimilarities and frequency patterns; (3) health conditions are quantified by a coefficient number, which depends on the identified dissimilarities and patterns. The effectiveness and reliability of the proposed WWSN has been validated by experiments in detecting health anomalies including tachycardia, arrhythmia and myocardial infarction. © 2015, © 2015 Antai College of Economics and Management, Shanghai Jiao Tong University."
"10.4018/IJBAN.2015040104","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046774371&doi=10.4018%2fIJBAN.2015040104&partnerID=40&md5=e08493abb0aa273f1ae814c8467763fc","This paper studies the holistic issue of the combination of analyst projection accuracy, diversity, and distribution. It first tracks the earnings per share and sales projections of a number of equity analysts for 17 representative U.S. companies using data from 1978 to 2012. It also compares the analysts' consensus of earnings per share, sales, rating, and price target zone to identify the degree of concentration among the opinions. It finds that the magnitudes and coefficient variations of projection errors are small, and the analysts' opinions are insignificantly diversified. Analysts' projections are not significantly skewed, but pessimistic opinions dominate. The results suggest that the projections issued by equity analysts are generally precise, and following their opinions is technically feasible. Analysts' mistakes are more due to inappropriate investment timing and less due to their abilities to pick firms with robust future performance. Copyright © 2015, IGI Global."
"10.4018/IJBAN.2015040103","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046757913&doi=10.4018%2fIJBAN.2015040103&partnerID=40&md5=0822b3a6cbebd45ed3a38518d484defe","In data analysis, when data are unattainable, it is common to select a closely related attribute as a proxy. But sometimes substitution of one attribute for another is not sufficient to satisfy the needs of the analysis. In these cases, a classification model based on one dataset can be investigated as a possible proxy for another closely related domain's dataset. If the model's structure is sufficient to classify data from the related domain, the model can be used as a proxy tree. Such a proxy tree also provides an alternative characterization of the related domain. Just as important, if the original model does not successfully classify the related domain data the domains are not as closely related as believed. This paper presents a methodology for evaluating datasets as proxies along with three cases that demonstrate the methodology and the three types of results. Copyright © 2015, IGI Global."
"10.4018/IJBAN.2015040101","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046278701&doi=10.4018%2fIJBAN.2015040101&partnerID=40&md5=671f4af1c7ecba7df1f92177b74b4e05","There are many research papers talking about building various machine learning models to predict the market index. However, very few attention has been paid to effectively validating or calibrating the prediction results. The focus of this paper is to present a dynamic modeling and validation framework for the market direction prediction. The central idea is to calibrate the probabilistic prediction by estimating two conditional probabilities of correct forecast from the dynamic validation data set. The calibration method can be combined with any predictive model that generates probabilistic prediction of the market direction. Copyright © 2015, IGI Global."
"10.4018/ijban.2015040105","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046246504&doi=10.4018%2fijban.2015040105&partnerID=40&md5=6e49b9e2c4749b7275db46a3dc26e775","Due to the recent trends of chip-miniaturization, the performance of a single-core is plateauing and hence, improving the performance of serial-execution based legacy code has become challenging. Since the expansion in power system operation continues to increase the number of contingencies to be examined, serial execution platforms present a crucial bottleneck in analyzing sufficiently large number of contingencies within a reasonably small time for performing power-system stability analysis. This paper presents an approach to parallelize power system contingency analysis over multicore processors using Chapel language. To achieve load-balancing for avoiding wastage of computation resources, the authors use efficient work-stealing scheduling. They discuss the important features of Chapel and design choices which enable us to achieve high performance gains. The approach is evaluated using hundreds of contingencies of a large 13029-bus power system. The authors compare the performance of serial-execution with that obtained using 2, 4, 8 and 16 threads. The simulation results have shown that the approach outperforms a conventional scheduling technique, namely master-slave scheduling and also scales effectively with increasing number of threads. It is believed that the performance gains obtained from the approach would be highly useful for control center operators in analyzing a large number of contingencies and thus taking suitable corrective and preventive action against catastrophic events. Also, the insights gained from these experiments will be useful in several business enterprises. Copyright © 2015, IGI Global."
"10.4018/IJBAN.2015040102","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84969976848&doi=10.4018%2fIJBAN.2015040102&partnerID=40&md5=d6be0022d83cb57a74c1bb802498cbdd","Cloud computing has been proven to have numerous benefits for organizations, especially in supply chain management. More and more organizations have adopted cloud-based solutions to be their primary sourcing applications. Nevertheless, this technology is not without challenges, and in order to deploy and implement cloud-based solutions successfully with minimal risks organizations need to have practical guidance on this emerging technology. Despite the rapid growth of cloud computing in supply chain areas, the existing literature is still conceptual, inadequate, and mainly focused on the pre-adoption stage of this technology. This research examines the post-adoption stage of cloud-based supply chain solutions from the decision science perspective and intends to provide organizations with practical guidance on how to ensure the efficiency of a cloud-based supply chain system and control risks associated with cloud-based solutions. Copyright © 2015, IGI Global."
"10.1089/big.2014.0066","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84991823737&doi=10.1089%2fbig.2014.0066&partnerID=40&md5=57244ca21413649c40674ca0afa432a4","Methods from machine learning and data science are becoming increasingly important in the social sciences, providing powerful new ways of identifying statistical relationships in large data sets. However, these relationships do not necessarily offer an understanding of the processes underlying the data. To address this problem, we have developed a method for fitting nonlinear dynamical systems models to data related to social change. Here, we use this method to investigate how countries become trapped at low levels of socioeconomic development. We identify two types of traps. The first is a democracy trap, where countries with low levels of economic growth and/or citizen education fail to develop democracy. The second trap is in terms of cultural values, where countries with low levels of democracy and/or life expectancy fail to develop emancipative values. We show that many key developing countries, including India and Egypt, lie near the border of these development traps, and we investigate the time taken for these nations to transition toward higher democracy and socioeconomic well-being. ï¿½ Shyam Ranganathan et al. 2015; Published by Mary Ann Liebert, Inc."
"10.1089/big.2014.0061","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84991783905&doi=10.1089%2fbig.2014.0061&partnerID=40&md5=e9a02b298ed743accedac1e78bda2407","Satellite imagery is a form of big data that can be harnessed for many social good applications, especially those focusing on rural areas. In this article, we describe the common problem of selecting sites for and planning rural development activities as informed by remote sensing and satellite image analysis. Effective planning in poor rural areas benefits from information that is not available and is difficult to obtain at any appreciable scale by any means other than algorithms for estimation and inference from remotely sensed images. We discuss two cases in depth: the targeting of unconditional cash transfers to extremely poor villages in sub-Saharan Africa and the siting and planning of solar-powered microgrids in remote villages in India. From these cases, we draw out some common lessons broadly applicable to informed rural development. © Kush R. Varshney et al. 2015; Published by Mary Ann Liebert, Inc."
"10.1089/big.2014.0021","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84991772205&doi=10.1089%2fbig.2014.0021&partnerID=40&md5=d8bbee57da1c6e3b0c1e80400229fdf0","One of the most challenging problems facing crime analysts is that of identifying crime series, which are sets of crimes committed by the same individual or group. Detecting crime series can be an important step in predictive policing, as knowledge of a pattern can be of paramount importance toward finding the offenders or stopping the pattern. Currently, crime analysts detect crime series manually; our goal is to assist them by providing automated tools for discovering crime series from within a database of crimes. Our approach relies on a key hypothesis that each crime series possesses at least one core of crimes that are very similar to each other, which can be used to characterize the modus operandi (M.O.) of the criminal. Based on this assumption, as long as we find all of the cores in the database, we have found a piece of each crime series. We propose a subspace clustering method, where the subspace is the M.O. of the series. The method has three steps: We first construct a similarity graph to link crimes that are generally similar, second we find cores of crime using an integer linear programming approach, and third we construct the rest of the crime series by merging cores to form the full crime series. To judge whether a set of crimes is indeed a core, we consider both pattern-general similarity, which can be learned from past crime series, and pattern-specific similarity, which is specific to the M.O. of the series and cannot be learned. Our method can be used for general pattern detection beyond crime series detection, as cores exist for patterns in many domains. © Tong Wang et al. 2015; Published by Mary Ann Liebert, Inc."
"10.1089/big.2014.0072","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84991759882&doi=10.1089%2fbig.2014.0072&partnerID=40&md5=3ed4b9e512edde19d0a9587b64e8eb0c","Human rights organizations are increasingly monitoring social media for identification, verification, and documentation of human rights violations. Since manual extraction of events from the massive amount of online social network data is difficult and time-consuming, we propose an approach for automated, large-scale discovery and analysis of human rights-related events. We apply our recently developed Non-Parametric Heterogeneous Graph Scan (NPHGS), which models social media data such as Twitter as a heterogeneous network (with multiple different node types, features, and relationships) and detects emerging patterns in the network, to identify and characterize human rights events. NPHGS efficiently maximizes a nonparametric scan statistic (an aggregate measure of anomalousness) over connected subgraphs of the heterogeneous network to identify the most anomalous network clusters. It summarizes each event with information such as type of event, geographical locations, time, and participants, and provides documentation such as links to videos and news reports. Building on our previous work that demonstrates the utility of NPHGS for civil unrest prediction and rare disease outbreak detection, we present an analysis of human rights events detected by NPHGS using two years of Twitter data from Mexico. NPHGS was able to accurately detect relevant clusters of human rights-related tweets prior to international news sources, and in some cases, prior to local news reports. Analysis of social media using NPHGS could enhance the information-gathering missions of human rights organizations by pinpointing specific abuses, revealing events and details that may be blocked from traditional media sources, and providing evidence of emerging patterns of human rights violations. This could lead to more timely, targeted, and effective advocacy, as well as other potential interventions. © Feng Chen et al. 2015; Published by Mary Ann Liebert, Inc."
"10.1016/j.bdr.2015.02.004","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84925591673&doi=10.1016%2fj.bdr.2015.02.004&partnerID=40&md5=da7d1be456f77556dc4b7695067669cb","Healthcare network information growth follows an exponential pattern, and current database management systems cannot adequately manage this huge amount of data. It is necessary to use a ""big data"" solution for healthcare problems. One of the most important problems in healthcare is finding Patient Similarity (PaSi). Current methods for finding PaSi are not adaptive and do not support all data sources, nor can they fulfill user requirements for a query tool. In this paper, we propose a scalable and distributable method to solve PaSi problems over MapReduce architecture. ScaDiPaSi, supports storage and retrieval of all kinds of data sources in a timely manner. The dynamic nature of the proposed method helps users to define conditions on all entered fields. Our evaluation shows that we can use this method with high confidence and low execution time. © 2015 Elsevier Inc."
"10.1057/jma.2015.3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031508365&doi=10.1057%2fjma.2015.3&partnerID=40&md5=564b6bc6437421d412a6e7a17a0c4c32","Information systems aid managers in their marketing decision making as they enable marketers to understand their customers better and come up with effective solutions to satisfy their needs, and provide the computer-based tools, models and methods to analyze the data. In addition, information systems can also be central to the provision of customer-relationship management tools. Thus, we suggest that the design and configuration of these systems must reflect assessments of cost-benefit tradeoffs beside satisfying technical and functional requirements. In this study, we develop a framework for an economics-driven assessment of alternatives for designing marketing information systems. The framework defines different design strategies that consider uncertainties in utilization, cost and performance differentials between technologies, and penalties resulting from delaying implementation. Our evaluation highlights conditions under which one design strategy will outperform others. © 2015 Macmillan Publishers Ltd."
"10.1057/jma.2015.1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85009195771&doi=10.1057%2fjma.2015.1&partnerID=40&md5=feca03700fe6f62dcc8e01feabb77402","If Big Data has been widely discussed, only a few marketing researchers have actually paid attention to it. However the development of Big Data, even in the abstract, provides researchers with the opportunity to rethink our approach to gathering and applying knowledge. Many countries, companies and universities are investing millions of dollars in the development of Big Data. Some believe this era of data driven computational social science has the same potential as the emergence of cognitive science in the 1960s and should not be left to private companies or government agencies. Given the entrance of Big Data techniques to companies and the university setting, it is our role, as consumer researchers, to identify issues that are relevant to our field and to suggest a consumer research method adapted to Big Data. The first section of this article addresses ethical and epistemic issues to consider when conducting marketing research with Big Data. The second section suggests the use of abductive reasoning as a first step in the research process in order to bring context to consumers’ digital traces and make new theories emerge. Finally, we present the archetype-based analysis as an example of what researchers can do with Big Data when they adopt abductive, inductive and deductive approaches in the research process. © 2015 Macmillan Publishers Ltd."
"10.2481/dsj.14-047","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047286029&doi=10.2481%2fdsj.14-047&partnerID=40&md5=72e117a56278525095fa33a00a654b09","With the growing importance of data to the scholarly record and the critical role journals play in facilitating data sharing, the complex landscape of scholarly journal data publication policies has become an obstacle for research. This paper outlines Data-PE, a framework for evaluating these policies. It takes the form of a conceptual foundation, comprising twelve criteria for evaluation, operationalized through an evaluation tool. Its objective is to function as a flexible means for a variety of stakeholders to appraise individual policies. Examples of the use of the framework are provided and means for the validation of the tool are discussed. © 2015, Committee on Data for Science and Technology. All rights reserved."
"10.2481/dsj.14-020","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84927562264&doi=10.2481%2fdsj.14-020&partnerID=40&md5=ddf85a64fe8e4f627d70176923a4e786","IBAMar is a regional database that puts together all the physical and biochemical data provided by multiparametric probes and water sample analysis taken during the cruises managed by the Balearic Oceanographic Center of the Instituto Español de Oceanografía (COB-IEO) during the last four decades. Initially, it integrated data from hydrographic profiles obtained from CTDs (conductivity, temperature, depth) equipped with several sensors, but it has been recently extended to incorporate data obtained with hydrocasts using oceanographic Niskin or Nansen bottles. The result is an extensive regional resource database that includies physical hydrographic data such as temperature (T), salinity (S), dissolved oxygen (DO), fluorescence, and turbidity, as well as biochemical data, specifically dissolved inorganic nutrients (phosphate, nitrate, nitrite, and silicate) and chlorophyll-a. Different technologies and methodologies were used by independent teams during the four decades of data sampling. However in the IBAMar database, data have been reprocessed using the same protocols and a standard quality control (QC) methodology has been applied to each variable. The result is a homogeneous and quality-controlled data. IBAMar database at standard levels is freely available for exploration and download from http://www.ba.ieo.es/ibamar/. © 2015, Committee on Data for Science and Technology. All rights reserved."
"10.2481/dsj.14-043","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84927135786&doi=10.2481%2fdsj.14-043&partnerID=40&md5=bd98ae990b196bd5387ce5b7b310b8d3","Making scientific data openly accessible and available for re-use is desirable to encourage validation of research results and/or economic development. Understanding what users may, or may not, do with data in online data repositories is key to maximizing the benefits of scientific data re-use. Many online repositories that allow access to scientific data indicate that data is ""open,"" yet specific usage conditions reviewed on 40 ""open"" sites suggest that there is no agreed upon understanding of what ""open"" means with respect to data. This inconsistency can be an impediment to data re-use by researchers and the public. © 2015, Committee on Data for Science and Technology. All rights reserved."
,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84926637226&partnerID=40&md5=c3449069ad5ce9d1069e9fbffd589d6a","This paper presents a stewardship maturity assessment model in the form of a matrix for digital environmental datasets. Nine key components are identified based on requirements imposed on digital environmental data and information that are cared for and disseminated by U.S. Federal agencies by U.S. law, i.e., Information Quality Act of 2001, agencies' guidance, expert bodies' recommendations, and users. These components include: preservability, accessibility, usability, production sustainability, data quality assurance, data quality control/monitoring, data quality assessment, transparency/traceability, and data integrity. A five-level progressive maturity scale is then defined for each component associated with measurable practices applied to individual datasets, representing Ad Hoc, Minimal, Intermediate, Advanced, and Optimal stages. The rationale for each key component and its maturity levels is described. This maturity model, leveraging community best practices and standards, provides a unified framework for assessing scientific data stewardship. It can be used to create a stewardship maturity scoreboard of dataset(s) and a roadmap for scientific data stewardship improvement or to provide data quality and usability information to users, stakeholders, and decision makers. © 2015, Committee on Data for Science and Technology. All rights reserved."
"10.1080/23270012.2014.1003151","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057051387&doi=10.1080%2f23270012.2014.1003151&partnerID=40&md5=06c4d09402ed89030493cdf44eeec313","Decision-making is one of the critical activities of management in business. However, decision support systems that support management decision-making activities lack the semantics involved in responding to semantic queries involving reasoning. We consider an ontology-based knowledge base that covers linked data about organizations, people and activities in a supply chain. In this paper, we explore how to effectively answer semantic queries to support management decision-making, where custom rules are employed for answering qualitative queries. We explore the ontological data representation and a similarity measure for data integration. We present a hybrid reasoning algorithm for answering qualitative queries. This algorithm adapts the reasoner such that, when proving a goal, it does a simple retrieval when it encounters trusted items, and backward-chaining over untrusted items. We provide a case study and evaluate the hybrid reasoning algorithm on scalability, query processing time and support for management decision-making. © 2015, © 2015 Antai College of Economics and Management, Shanghai Jiao Tong University."
"10.1080/23270012.2015.1008593","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016636355&doi=10.1080%2f23270012.2015.1008593&partnerID=40&md5=f24da23ed563e5af6a62a2debee98b0d","Intelligent manufacturing design of a complex production-inventory system becomes a key issue for the organization of responsiveness to uncertainties. This paper addresses a two-echelon production-inventory model for a non-repairable product where the system consists of single manufacturer and single retailer. The manufacturer procures raw material (which also contains imperfect raw materials) from an outside supplier then proceeds to convert perfect-quality raw material as a finished product, and finally delivers to the retailer. In this study we assume that the demand is sensitive to promotional efforts/sales teams' initiatives and the production rate is uncertain but possible to describe with a triangular fuzzy number. Then we use the signed distance method to defuzzify the fuzzy joint total cost and an analytical method is employed to achieve the optimal solutions so that the total costs of both manufacturer and retailer are minimized. An efficient algorithm is developed to design an intelligent manufacturing strategy such as optimal production lot-size, backlogging and the initiatives of sales teams. A numerical example and sensitivity analysis are given to demonstrate the application of the proposed model. © 2015, © 2015 Antai College of Economics and Management, Shanghai Jiao Tong University."
"10.1080/23270012.2015.1019582","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84992745400&doi=10.1080%2f23270012.2015.1019582&partnerID=40&md5=24d8a67e2545c70d16094b078ddfd871","To estimate the fuel consumption of a civil aircraft, we propose to use the receiver operating characteristic (ROC) curve to optimize a support vector machine (SVM) model. The new method and procedure has been developed to build, train, validate, and apply an SVM model. A conceptual support vector network is proposed to model fuel consumption, and the flight data collected from routes are used as the inputs to train an SVM model. During the training phase, an ROC curve is defined to evaluate the performance of the model. To validate the applicability of the trained model, a case study is developed to compare the data from an aircraft performance manual and from the implemented simulation model. The investigated aircraft in the case study is a Boeing 737-800 powered by CFM-56 engines. The comparison has shown that the trained SVM model from the proposed procedure is capable of representing a complex fuel consumption function accurately for all phases during the flight. The proposed methodology is generic, and can be extended to reliably model the fuel consumption of other types of aircraft, such as piston engine aircraft or turboprop engine aircraft. © 2015, © 2015 Antai College of Economics and Management, Shanghai Jiao Tong University."
"10.1080/23270012.2015.1020891","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84979665325&doi=10.1080%2f23270012.2015.1020891&partnerID=40&md5=f00e1fa0cfa4f15058e0260921e501a3","Over the past few decades, with the development of automatic identification, data capture and storage technologies, people generate data much faster and collect data much bigger than ever before in business, science, engineering, education and other areas. Big data has emerged as an important area of study for both practitioners and researchers. It has huge impacts on data-related problems. In this paper, we identify the key issues related to big data analytics and then investigate its applications specifically related to business problems. ©, This work was authored as part of the Contributor's official duties as an Employee of the United States Government and is therefore a work of the United States Government. In accordance with 17 USC. 105, no copyright protection is available for such works under US Law."
"10.1080/23270012.2015.1012232","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84954312268&doi=10.1080%2f23270012.2015.1012232&partnerID=40&md5=f88cf5b214f2e342c063026fb8596ef6","The consumerization of information technology (IT) has created a growing population of mobile knowledge workers that requires organizations to implement appropriate strategies of mobile knowledge management to support their business-related activities. This research presents and investigates an analytical model of mobile knowledge management by capturing the mutual effects of a central knowledge base and individual mobile devices for a mobile platform. We investigate the best technology level of the mobile platform for both the homogeneous and heterogeneous settings and explore how they change with several critical factors including the central knowledge base, environmental mobility and knowledge volatility. Our model-based framework and analytical results provide valuable insights for practitioners to effectively design mobile platforms and manage mobile knowledge assets. © 2015, © 2015 Antai College of Economics and Management, Shanghai Jiao Tong University."
"10.1016/j.bdr.2015.08.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84947043463&doi=10.1016%2fj.bdr.2015.08.001&partnerID=40&md5=c804abbb891c79be479d70fd013a2e78","Big Data is an emerging phenomenon that is rapidly changing business models and work styles [1]. Big Data platforms allow the storage and analysis of high volumes of data with heterogeneous format from different sources. This integrated analysis allows the derivation of properties and correlations among data that can then be used for a variety of purposes, such as making predictions that can profitably affect decision processes. As a matter of fact, nowadays Big Data analytics are generally considered an asset for making business decisions. Big Data platforms have been specifically designed to support advanced form of analytics satisfying strict performance and scalability requirements. However, no proper consideration has been devoted so far to data protection. Indeed, although the analyzed data often include personal and sensitive information, with relevant threats to privacy implied by the analysis, so far Big Data platforms integrate quite basic form of access control, and no support for privacy policies. Although the potential benefits of data analysis are manifold, the lack of proper data protection mechanisms may prevent the adoption of Big Data analytics by several companies. This motivates the fundamental need to integrate privacy and security awareness into Big Data platforms. In this paper, we do a first step to achieve this ambitious goal, discussing research issues related to the definition of a framework that supports the integration of privacy aware access control features into existing Big Data platforms. © 2015 Elsevier Inc.."
"10.1140/epjds/s13688-015-0041-5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84933532710&doi=10.1140%2fepjds%2fs13688-015-0041-5&partnerID=40&md5=e85a84add8c5e349636a542d7e4063a6","We analyse a large mobile phone activity dataset provided by Telecom Italia for the TelecomBig Data Challenge contest. The dataset reports the international country codes of every call/SMS made and received by mobile phone users in Milan, Italy, between November and December 2013, with a spatial resolution of about 200 meters. We first show that the observed spatial distribution of international codes well matches the distribution of international communities reported by official statistics, confirming the value of mobile phone data for demographic research. Next, we define an entropy function to measure the heterogeneity of the international phone activity in space and time. By comparing the entropy function to empirical data, we show that it can be used to identify the city’s hotspots, defined by the presence of points of interests. Eventually, we use the entropy function to characterize the spatial distribution of international communities in the city. Adopting a topological data analysis approach, we find that international mobile phone users exhibit some robust clustering patterns that correlate with basic socio-economic variables. Our results suggest that mobile phone records can be used in conjunction with topological data analysis tools to study the geography of migrant communities in a global city. © 2015 Bajardi et al."
"10.1140/epjds/s13688-015-0038-0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84933528043&doi=10.1140%2fepjds%2fs13688-015-0038-0&partnerID=40&md5=bcee67aa2ee53bb9556294d58d1aea23","Human mobility in a city represents a fascinating complex system that combines social interactions, daily constraints and random explorations. New collections of data that capture human mobility not only help us to understand their underlying patterns but also to design intelligent systems. Bringing us the opportunity to reduce traffic and to develop other applications that make cities more adaptable to human needs. In this paper, we propose an adaptive routing strategy which accounts for individual constraints to recommend personalized routes and, at the same time, for constraints imposed by the collectivity as a whole. Using big data sets recently released during the Telecom Italia Big Data Challenge, we show that our algorithm allows us to reduce the overall traffic in a smart city thanks to synergetic effects, with the participation of individuals in the system, playing a crucial role. © 2015 De Domenico et al."
"10.1140/epjds/s13688-015-0042-4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84933519053&doi=10.1140%2fepjds%2fs13688-015-0042-4&partnerID=40&md5=790e7ff01d47917e368e4591b1819cc0","Recent grassroots movements have suggested that online social networks might play a key role in their organization, as adherents have a fast, many-to-many, communication channel to help coordinate their mobilization. The structure and dynamics of the networks constructed from the digital traces of protesters have been analyzed to some extent recently. However, less effort has been devoted to the analysis of the semantic content of messages exchanged during the protest. Using the data obtained from a microblogging service during the brewing and active phases of the 15M movement in Spain, we perform the first large scale test of theories on collective emotions and social interaction in collective actions. Our findings show that activity and information cascades in the movement are larger in the presence of negative collective emotions and when users express themselves in terms related to social content. At the level of individual participants, our results show that their social integration in the movement, as measured through social network metrics, increases with their level of engagement and of expression of negativity. Our findings show that non-rational factors play a role in the formation and activity of social movements through online media, having important consequences for viral spreading. © 2015 Alvarez et al."
"10.1140/epjds/s13688-015-0043-3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84933512701&doi=10.1140%2fepjds%2fs13688-015-0043-3&partnerID=40&md5=4fc049a3280725e8c082e31c06d96098","There is an increasing trend of people leaving digital traces through social media. This reality opens new horizons for urban studies. With this kind of data, researchers and urban planners can detect many aspects of how people live in cities and can also suggest how to transform cities into more efficient and smarter places to live in. In particular, their digital trails can be used to investigate tastes of individuals, and what attracts them to live in a particular city or to spend their vacation there. In this paper we propose an unconventional way to study how people experience the city, using information from geotagged photographs that people take at different locations. We compare the spatial behavior of residents and tourists in 10 most photographed cities all around the world. The study was conducted on both a global and local level. On the global scale we analyze the 10 most photographed cities and measure how attractive each city is for people visiting it from other cities within the same country or from abroad. For the purpose of our analysis we construct the users’ mobility network and measure the strength of the links between each pair of cities as a level of attraction of people living in one city (i.e., origin) to the other city (i.e., destination). On the local level we study the spatial distribution of user activity and identify the photographed hotspots inside each city. The proposed methodology and the results of our study are a low cost mean to characterize touristic activity within a certain location and can help cities strengthening their touristic potential. © 2015 Paldino et al."
"10.1140/epjds/s13688-015-0039-z","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84933510273&doi=10.1140%2fepjds%2fs13688-015-0039-z&partnerID=40&md5=3f95d1eb81d454065fb6f32364c546b9","The expanding availability of high-quality, large-scale data from the realm of culture and the arts promises novel opportunities for understanding and harnessing the dynamics of the creation, collaboration, and dissemination processes – fundamentally network phenomena - of artistic works and styles. To this end, in this paper we explore the complex network of western classical composers constructed from a comprehensive CD (Compact Disc) recordings data that represent the centuries-old musical tradition using modern data analysis and modeling techniques. We start with the fundamental properties of the network such as the degree distribution and various centralities, and find how they correlate with composer attributes such as artistic styles and active periods, indicating their significance in the formation and evolution of the network. We also investigate the growth dynamics of the network, identifying superlinear preferential attachment as a major growth mechanism that implies a future of the musical landscape where an increasing concentration of recordings onto highly-recorded composers coexists with the diversity represented by the growth in the sheer number of recorded composers. Our work shows how the network framework married with data can be utilized to advance our understanding of the underlying principles of complexities in cultural systems. © 2015 Park et al"
"10.1504/IJBIDM.2015.072210","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84943574744&doi=10.1504%2fIJBIDM.2015.072210&partnerID=40&md5=5dc6dfd35d5f31c5efee7cd95df94672","Data warehouses are an important element of business intelligence and decision support in many companies and inter-organisational data infrastructures. However, when personal information of individuals is concerned, it is critical to provide sufficient protection mechanisms in order to preserve privacy. In addition to classical access control, database anonymisation is an important element of an encompassing strategy for privacy-preserving data storage. This article gives an overview on selected anonymisation concepts and techniques and investigates if they are suitable for a data warehouse context. Furthermore, a process of privacy-preserving data integration and provisioning is presented and the impact of architecture, privacy criteria, and further parameter choices is discussed. Finally, we experimentally compare the impact of these parameters on data utility after anonymisation in several experiments on multiple datasets and derive corresponding recommendations. Copyright © 2015 Inderscience Enterprises Ltd."
"10.1504/IJBIDM.2015.072212","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84943563923&doi=10.1504%2fIJBIDM.2015.072212&partnerID=40&md5=e7bcb38b7c604b435469ced89b499c0b","Motifs and anomalies are two important representative patterns in a time series. Existing approaches usually handle motif discovery and anomaly detection in time series separately. In this paper, we propose a new efficient clustering-based method for discovering motif and detecting anomaly at the same time in large time series data. Our method first extracts motif/anomaly candidates from a time series by using significant extreme points and then clusters the candidates by using BIRCH algorithm. The proposed method computes anomaly scores for all sub-clusters and discovers the top motif based on the sub-cluster with the smallest anomaly score and detects the top anomaly based on the sub-cluster with the largest anomaly score. Experimental results on several benchmark datasets show that our proposed method can discover precise motif and anomaly with high time efficiency on large time series data. Copyright © 2015 Inderscience Enterprises Ltd."
"10.1504/IJBIDM.2015.072213","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84943553508&doi=10.1504%2fIJBIDM.2015.072213&partnerID=40&md5=b217a1ed624444f68e018fd797d2ffab","This work considers extracting delta in a distributed environment where the collaboration from highly autonomous operational database management systems is limited to granting read only access on a set of selected relational tables. Because of inherently huge volume of data in data warehouse system, it is critical to minimise communication costs as much as possible. Based on the observation that usually, two consecutive snapshots are not very different, a statistical-based group hash method is developed to minimise the volumes of data required to complete the data extraction. In addition, to relax the assumption that the changes to remote data are only caused by random events, we define a progression pattern to describe data changes with temporal regularities and also propose a method for progression pattern discovery. Copyright © 2015 Inderscience Enterprises Ltd."
"10.1504/IJBIDM.2015.072211","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84943539868&doi=10.1504%2fIJBIDM.2015.072211&partnerID=40&md5=6756c6c28b4fc2cec3afe7b7fce19c9a","The aim of the paper is to present a critical review of analytics and visualisation technology for big data, and propose future directions to overcome the shortcomings of the current technologies. The current machine learning and data-mining algorithms are operating mostly on predefined scales of aggregation, while in the vast amounts of data the problem arises at the level of aggregation which cannot be defined ahead of time. We therefore identify a novel and extended architecture to operate on flexible multi-resolution hypothesis space. With such architecture framework the goal is to open a space of possibly discovered models towards classes of data, which are by today's approaches discovered only for special cases. Furthermore, the multi-resolution approach to big-data analytics could allow scenarios like semi-supervised and unsupervised anomaly detection, detecting complex relationships from the heterogeneous data sources, and providing ground for visualisation of complex processes. Copyright © 2015 Inderscience Enterprises Ltd."
"10.1504/IJBIDM.2015.071327","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84940054274&doi=10.1504%2fIJBIDM.2015.071327&partnerID=40&md5=38f6e07e2c28223e97eb992294aeb553","Information retrieval problem occurs when the target information is not available 'literally' into the set of documents. In problems in which the goal is to find 'hidden' information, it is important to develop hybrid methodologies or improve and design a new one. In this work the authors are dealing with identifying the most informative piece of data on a collection of documents, in order to obtain the best result on a posterior fuzzy clustering stage. The aim is to find similarities between the documents and a reference target, to establish relationships related to a non-literal feature. We propose to apply the well-known entropy term weighting scheme and then show a posterior different procedures to the right election of the interest data. This procedure brings the biggest amount of information within the smallest amount of data. Applying a specific selection procedure for a group of words, gives more information to differentiate and separate the documents after using the entropy weighting. This returns considerable results on the processing time and the right fuzzy clustering of the documents collection. Copyright © 2015 Inderscience Enterprises Ltd."
"10.1504/IJBIDM.2015.071325","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84940044662&doi=10.1504%2fIJBIDM.2015.071325&partnerID=40&md5=0138d225802d19c47a972fb582c4c7c1","The economic evaluations conducted by the public operator require to estimate not only the financial effects generated by the investment project, but also the social, cultural and environmental results. This study proposes a model that, starting from the collection and processing of data which characterise the thermodynamic building's behaviour, allows the correct selection of energetic requalification interventions, both under technical and financial issue. Moreover the model allows to take into account in monetary terms the extent of the lower CO2 emissions as a result of the proposed design solution. The criterion for the quantification of the value corresponding to the avoided emissions of CO2, follows analysis of the main models proposed in the literature. We examined the results of the protocol applied to a real case. Copyright © 2015 Inderscience Enterprises Ltd."
"10.1504/IJBIDM.2015.071326","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84940042993&doi=10.1504%2fIJBIDM.2015.071326&partnerID=40&md5=63bf5edd8bb3d5da6638df2b05d2462f","Time-series prediction is an intensively researched area, yet most studies in this field have focused on predicting movements of a single series only, whilst prediction of multiple time-series based on patterns of interaction between multiple time-series has received very little attention. On the other hand, findings in various studies show that given a multiple time-series data there exist patterns of relationship between the observed variables, and being able to model them would lead to the possibility of building a more accurate model to predict their future values. Nevertheless, as real-world systems change dynamically over time, having a single model to explain simultaneous movement of multiple time-series will not be sufficient. To address this problem, the paper presents an algorithm that is capable of building a new decision model on-the-fly based on the state of relationships, between observed variables at a particular time-point. The proposed algorithm utilises non-parametric regression analysis to extract profiles of relationship between observed variables and then employs the nearest neighbour approach to find appropriate conditions from the past. Experimental results on a real-world dataset suggest that the implementation of kernel regression merged with nearest neighbour approach shows that it outperforms established methods such as multiple linear regression and multi-layer perceptron. Copyright © 2015 Inderscience Enterprises Ltd."
"10.1504/IJBIDM.2015.071311","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84940034041&doi=10.1504%2fIJBIDM.2015.071311&partnerID=40&md5=6dd6d5242c665028678bc36e0ffbf083","Time series clustering is one of the crucial tasks in time series data mining. The most popular method in time series clustering is k-means algorithm due to its simplicity and flexibility. So far, k-means for time series clustering has been most used with Euclidean distance. Dynamic time warping (DTW) distance measure has increasingly been used as a similarity measurement for various data mining tasks in place of traditional Euclidean distance due to its superiority in sequence-alignment flexibility. However, there exist some difficulties in clustering with DTW distance, for example, the problem of shape averaging in DTW or the problem of speeding up DTW distance calculation. In this paper, we compare the performance of the three shape averaging methods in DTW: nonlinear alignment and averaging filter (NLAAF), prioritised shape averaging (PSA) and DTW barycenter averaging (DBA) and propose an efficient method to implement k-means clustering for time series data with DTW distance. In our method, we choose to use DBA method for shape-based time series averaging, apply early abandoning method for speeding up DTW distance calculation and median-based method for determining initial centroids for k-means clustering. The experimental results on benchmark datasets validate our proposed implementation method for time series k-means clustering with DTW. Copyright © 2015 Inderscience Enterprises Ltd."
"10.1504/IJBIDM.2015.071324","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84940026192&doi=10.1504%2fIJBIDM.2015.071324&partnerID=40&md5=d4cac9733c415a5bfefd41112f93add8","Online analytical processing (OLAP) is considered as a database prototype that gives a platform for the rich analysis of multidimensional data. A logical data structure known as the data cube often supports the OLAP. However, mining association rules from multidimensional data using OLAP techniques with data mining facilities is an issue of substantial complexity. In practice, the complexity is excited by the existence of dimension hierarchies that subdivide dimensions into aggregation layers of various granularity. Discovery of hierarchy-sensitive association rules can be very costly on large cubes. In this paper, we present an OLAP hierarchy-sensitive framework that supports the efficient and transparent manipulation of dimension hierarchies for extracting association rules from data cube. The experimental results show that, when compared to the alternatives, very slight overhead is required to handle streams of inter-dimensional association rules requests. Copyright © 2015 Inderscience Enterprises Ltd."
"10.1504/IJBIDM.2015.069269","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84929254183&doi=10.1504%2fIJBIDM.2015.069269&partnerID=40&md5=ba89392d8a7113fed6fb54469cfd5e59","The problems related to the management of information increase everyday in several contexts. This process needs of more and more effective and efficient knowledge management techniques. We chose as case study the issue of urban planning and design due to the cultural and scientific innovations, which led in recent years to a completely new interpretation of a city as a system of elements each other connected. In the idea that the urban phenomena are manly driven by the topological structure of the urban space, the science of complex networks is flourishing as a novel reference to include a quantitative modelling approach in territorial planning and design. The paper aim is to give a conceptual framework to integrate the knowledge of the elements composing the city, using an ontology to define, organise and manage them, with the functional potential of the urban space that connects them to each other, stressing the concept of topological structure in terms of configuration. The functional aspects revealed by the configurational analysis of the city-network will be improved and better contextualised by the implementation of an urban ontology-based model. The obtained ontologies are shared using system based on peer-to-peer (P2P) paradigm exposed by a web service. © 2015 Inderscience Enterprises Ltd."
"10.1504/IJBIDM.2015.069271","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84929250765&doi=10.1504%2fIJBIDM.2015.069271&partnerID=40&md5=6bcd809e5780fce0b32f68d736fc1018","Ortigia, the historic centre of Syracuse, is a complex urban entity characterised by high outer homogeneity and inner heterogeneity. The evolution of its real estate market during the last decade is somehow related to the global property market one. In addition its events are connected with the evolution of the exploiting policies still ongoing. The critical observations of its features aim at providing tools able to support the decisions about subsidies and local property taxes. This study continues the observations we have carried out for five years, this time involving clustering analysis, a data mining technique able to recognise different submarkets, and suitable to make the valuation pattern fit to the different market areas. For each of the latter significant characteristics have been recognised with reference to the 'monetary declination' of these particular capital assets. © 2015 Inderscience Enterprises Ltd."
"10.1504/IJBIDM.2015.069268","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84929238642&doi=10.1504%2fIJBIDM.2015.069268&partnerID=40&md5=5e27d55883e1945dcdf88b90e1814711","User behaviour can be extracted from the web pages accessed by the end user while browsing the internet. This extracted behaviour helps in providing recommendations to the end user. This paper presents mining of frequent itemsets and refinement of usage clusters for web page recommendation. The not so interesting recommendations obtained from a cluster are in abundance due to large number of sessions in a cluster. To solve such a problem we intend to refine clusters on the basis of weighted frequent itemsets that in turn help to generate improved quality refined clusters. After getting refined clusters, the same can be used for a number of applications such as personalisation on the basis of interests of the end user, improvement in website structure and improving the accuracy of a recommender system. The accuracy of a centroid-based recommender system is evaluated using original and refined clusters. © 2015 Inderscience Enterprises Ltd."
"10.1504/IJBIDM.2015.069022","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84928822667&doi=10.1504%2fIJBIDM.2015.069022&partnerID=40&md5=e5224e986a4219e86b96938b471f2f52","This paper describes an effective solution for recommending textual OLAP over data warehousing environments. The recommendation process is based on text semantics and query personalisation to improve the relevance of the retrieved results. In order to aggregate and recommend documents, we need a measure of semantic similarity. The first issue we addressed was the meaning of similarity between two concepts. For this, we used ontologies and the distance between terms over the ontology based on the least common ancestor. The second issue we dealt with was the meaning of similarity between two documents. For that, we calculated the statistical metric frequency. The purpose of query personalisation is to offer to the user an interactive way for obtaining relevant aggregation of documents based on adjustable parameters. We implemented the solution for multidimensional analysis over PubMed database. In the case study, we used the Medical Subject Headings provided by the US National Library of Medicine. At the end, we present the results of some experiments that show that good recommendations are possible. The results are discussed based on the evaluation metrics: precision, recall and F1-measure. Copyright © 2015 Inderscience Enterprises Ltd."
"10.1504/IJBIDM.2015.069041","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84928819187&doi=10.1504%2fIJBIDM.2015.069041&partnerID=40&md5=e230a6bd322b51bfb3f8d1ae96da5890","In the decision-making process of urban regeneration projects, the lack of transparency is one of the most frequent causes of the deceleration and the stopping of initiatives. In the present work, with reference to a multi-criteria model recently developed for the municipality of Rome (Italy), a solution to this issue has been proposed. Borrowing the formal logic of fuzzy systems, the possibility to assign a different importance to the parameters which explain the objectives of the redevelopment, to compare the projects submitted by different operators, as well as to make it clear the phase in which the public administration evaluates the priorities of the projects, has been introduced. A measure of the coherence of the projects submitted by private operators with the importance of the objectives of redevelopment set by the public administration has been computed. The result is a flexible structure of the algorithm developed, able to adapt to the specificities of the case study and to the changes that, over time, could arise in the evaluation mechanism of the decision maker. Copyright © 2015 Inderscience Enterprises Ltd."
"10.1504/IJBIDM.2015.069040","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84928796347&doi=10.1504%2fIJBIDM.2015.069040&partnerID=40&md5=1b2b93b00ddb79f6c0ef6eacf5de800a","The numerous concepts of socio-economic hardship are, furthermore, attributable to a traditional distinction between absolute and relative conditions of hardship. The options of scientific research were therefore oriented towards the establishment of a multi-dimensional approach, sometimes abandoning dichotomous logic in order to arrive at fuzzy classifications in which each unit belongs and, at the same time, does not belong, to a category. A multidimensional index that considers hardship as the overall condition of being disadvantaged and deprived seems the most appropriate in view of the socio-economic differential analysis of demographic phenomena. The approach used in this work to synthesise and measure the conditions of the hardship of a population is based on a clustering procedure (fuzzy c-means) aimed at outlining various not defined a priori profiles, which should be assigned to each family with different socio-economic behaviours. In comparison with conventional methods, this clustering method allows a set of data to belong not only to a main cluster but also to two or more clusters with 'fuzzy profiles'. Copyright © 2015 Inderscience Enterprises Ltd."
"10.1504/IJBIDM.2015.069039","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84928795334&doi=10.1504%2fIJBIDM.2015.069039&partnerID=40&md5=171e0457d1e2e768fb6ed645073fa5b8","Reasoning about spatio-temporal phenomena requires the adoption of common granularities that facilitate and enhance the comprehension of a particular phenomenon. In our day-to-day activities, spatial granules like state, province or country, and temporal granules like day, month or year, are used to index facts and to allow reasoning adopting the level of detail considered appropriate in a particular analytical context. In an era where huge amounts of spatio-temporal data are collected every day, it is crucial to model the spatio-temporal phenomena expressed in such data sets having in mind that different levels of detail can be useful in the analysis of such phenomena and that different levels of detail are related, for instance, through a spatial or temporal hierarchy. As the size and level of details of the data sets increase, the need to use multiple levels of detail that enhance our capability to achieve useful insights from data also increases. This paper presents a granularity theory devised to model spatio-temporal phenomena at different levels of detail. This granularity theory is more general than the existing granularities proposals. In fact, we relate those proposals with the presented granularity theory. Copyright © 2015 Inderscience Enterprises Ltd."
"10.1504/IJBIDM.2015.069038","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84928783786&doi=10.1504%2fIJBIDM.2015.069038&partnerID=40&md5=61470f9cbcef0847fe48408628745cf3","As far as the common problem of countries fiscal equalisation is concerned, the specific topic of equalisation of real estate tax is of considerable importance, especially if the real property tax base is a function of the cadastral income and, at the same time, in cases the cadastral income does not express the real market value of the asset. Such a circumstance - pending a cadastral reform which aligns the cadastral value of assets with the corresponding market values - political and economic instruments, as well as adequate estimation algorithms, should be arranged in order to correlate the tax on real estate with the relative market value of the asset. In this regard, it is proposed a model able to consider the effect of the prevalent intrinsic features of the asset on its market value. Among the variables considered, is taken into account the energy class of the asset, which is increasingly important for the legislator in matter of politics of energy saving of a country. The effectiveness of the model, which requires the definition of a value function according to the evaluation logic of typical values procedure, is tested through an application to an urban area on municipal scale. Copyright © 2015 Inderscience Enterprises Ltd."
"10.4018/ijban.2015010104","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046286919&doi=10.4018%2fijban.2015010104&partnerID=40&md5=41090863b1d5883320169553d2e630be","This paper presents findings ofan online questionnaire survey on the perceivedimportance ofchartist/technical and fundamental analysis and the usage of Chartist Methods and Services and Valuation Techniques among stock brokers of Bombay Stock Exchange, India. Stock brokers rely more on fundamental analysis vis-à-vis technical analysis at longer forecasting horizons and rely more on technical analysis at shorter forecasting horizons. Among Chartist Methods and Services, Sentiment Indicators were most used and Chart Company or Analyst was least used by brokers. Among Valuation Techniques, Earnings Multiple Methods were most used and Dividend Discount Models were least used by brokers. Stock brokers’age correlates with usage of sentiment indicators and their gender correlates with the usage of computer graphics and services. Regarding the use of chartist / technical and fundamental analysis on seven forecasting horizons, four distinct forecasting styles among stock brokers could be identified through cluster analysis. Copyright © 2015, IGI Global."
"10.4018/ijban.2015010101","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046259184&doi=10.4018%2fijban.2015010101&partnerID=40&md5=3518036ebba3c8a305ead9e004b0a8c3","This paper examines the Data Envelopment Analysis (DEA) methodology from a cognitive perspective. Specifically, it analyzes (a) the role of DEA scores as an overall efficiency measure and (b) to what extent the presence of DEA scores for a non-financial performance appraisal influences a posterior financial performance assessment. The study confirms that the efficiency score acts as a strong performance marker when deciding on which decision making units (DMUs) should be awarded for their non-financial performance. Furthermore, it shows that the results of the non-financial performance evaluation may act as an anchor which significantly influences a posterior financial assessment. These insights have practical consequences for planning, reporting, and controlling processes that incorporate DEA efficiency scores. © 2015,IGI Global."
"10.4018/ijban.2015010103","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019370623&doi=10.4018%2fijban.2015010103&partnerID=40&md5=4566b289f806c59e651e4e3a6a6effc2","In order to meet the needs of different customer segments, manufacturers use multiple distribution channels. This paper will examine two of the most common types of multi-channel structures. Under Structure 1, a supply chain includes a manufacturer, its online store and its own retail store, like GAP’s business model. A profit maximization model is used to obtain optimal strategies in terms of optimal retail price and level of value-added services provided by manufacturer-owned retailer. Under Structure 2, a supply chain includes a manufacturer, its online store and an independent retail store, like Dell’s business model. Stackelberg game is applied to obtain the optimal retail price, wholesale price, and level of value-added services provided by an independent retailer. Furthermore, comparisons between these two business structures are discussed and managerial guidelines are proposed. Finally, numerical examples are provided and real business examples are discussed to illustrate and justify the theoretical results. © 2015,IGI Global."
"10.4018/ijban.2015010102","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84940109862&doi=10.4018%2fijban.2015010102&partnerID=40&md5=435f3a46425bbb2a0b67b13c674dddfc","Data clustering has found significant applications in various domains like bioinformatics, medical data, imaging, marketing study and crime analysis. There are several types of data clustering such as partitional, hierarchical, spectral, density-based, mixture-modeling to name a few. Among these, partitional clustering is well suited for most of the applications due to the less computational requirement. An analysis of various literatures available on partitional clustering will not only provide good knowledge, but will also lead to find the recent problems in partitional clustering domain. Accordingly, it is planned to do a comprehensive study with the literature of partitional data clustering techniques. In this paper, thirty three research articles have been taken for survey from the standard publishers from 2005 to 2013 under two different aspects namely the technical aspect and the application aspect. The technical aspect is further classified based on partitional clustering,constraint-basedpartitionalclusteringandevolutionaryprogramming-basedclusteringtechniques. Furthermore, an analysis is carried out, to find out the importance of the different approaches that can be adopted, so that any new development in partitional data clustering can be made easier to be carried out by researchers. © 2015,IGI Global."
"10.1089/big.2014.0068","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84991834377&doi=10.1089%2fbig.2014.0068&partnerID=40&md5=4e2fb27d19c5c0b2001c1a0d37d4dac3","More and more, the needs of data analysts are requiring the use of data outside the control of their own organizations. The increasing amount of data available on the Web, the new technologies for linking data across datasets, and the increasing need to integrate structured and unstructured data are all driving this trend. In this article, we provide a technical overview of the emerging ""broad data"" area, in which the variety of heterogeneous data being used, rather than the scale of the data being analyzed, is the limiting factor in data analysis efforts. The article explores some of the emerging themes in data discovery, data integration, linked data, and the combination of structured and unstructured data. © Mary Ann Liebert, Inc. 2014."
"10.1089/big.2014.0046","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84991810691&doi=10.1089%2fbig.2014.0046&partnerID=40&md5=6f27cf3f17df7e66ba5ef900bc952c58","Developed under the Intelligence Advanced Research Project Activity Open Source Indicators program, Early Model Based Event Recognition using Surrogates (EMBERS) is a large-scale big data analytics system for forecasting significant societal events, such as civil unrest events on the basis of continuous, automated analysis of large volumes of publicly available data. It has been operational since November 2012 and delivers approximately 50 predictions each day for countries of Latin America. EMBERS is built on a streaming, scalable, loosely coupled, shared-nothing architecture using ZeroMQ as its messaging backbone and JSON as its wire data format. It is deployed on Amazon Web Services using an entirely automated deployment process. We describe the architecture of the system, some of the design tradeoffs encountered during development, and specifics of the machine learning models underlying EMBERS. We also present a detailed prospective evaluation of EMBERS in forecasting significant societal events in the past 2 years. © Mary Ann Liebert, Inc. 2014."
"10.1089/big.2014.0044","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84991809186&doi=10.1089%2fbig.2014.0044&partnerID=40&md5=b2adfa5a734842d8de1c6f6b0e1c2717","Given a simple noun such as apple, and a question such as ""Is it edible?,"" what processes take place in the human brain? More specifically, given the stimulus, what are the interactions between (groups of) neurons (also known as functional connectivity) and how can we automatically infer those interactions, given measurements of the brain activity? Furthermore, how does this connectivity differ across different human subjects? In this work, we show that this problem, even though originating from the field of neuroscience, can benefit from big data techniques; we present a simple, novel good-enough brain model, or GeBM in short, and a novel algorithm Sparse-SysId, which are able to effectively model the dynamics of the neuron interactions and infer the functional connectivity. Moreover, GeBM is able to simulate basic psychological phenomena such as habituation and priming (whose definition we provide in the main text). We evaluate GeBM by using real brain data. GeBM produces brain activity patterns that are strikingly similar to the real ones, where the inferred functional connectivity is able to provide neuroscientific insights toward a better understanding of the way that neurons interact with each other, as well as detect regularities and outliers in multisubject brain activity measurements. © Mary Ann Liebert, Inc. 2014."
"10.1089/big.2014.0063","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84991806460&doi=10.1089%2fbig.2014.0063&partnerID=40&md5=0fe4c850e03e17466a989fa384a0bb38","Our society is increasingly relying on digitalized, aggregated opinions of individuals to make decisions (e.g., product recommendation based on collective ratings). One key requirement of harnessing this ""wisdom of crowd"" is the independency of individuals' opinions; yet, in real settings, collective opinions are rarely simple aggregations of independent minds. Recent experimental studies document that disclosing prior collective ratings distorts individuals' decision making as well as their perceptions of quality and value, highlighting a fundamental discrepancy between our perceived values from collective ratings and products' intrinsic values. Here we present a mechanistic framework to describe herding effects of prior collective ratings on subsequent individual decision making. Using large-scale longitudinal customer rating datasets, we find that our method successfully captures the dynamics of ratings growth, helping us separate social influence bias from inherent values. Leveraging the proposed framework, we quantitatively characterize the herding effects existing in product rating systems and promote strategies to untangle manipulations and social biases. © Mary Ann Liebert, Inc. 2014."
"10.1089/big.2014.1527","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84991776970&doi=10.1089%2fbig.2014.1527&partnerID=40&md5=23f267ecca0f4eb54fde9096998180f1",[No abstract available]
"10.1186/2196-1115-1-3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018392571&doi=10.1186%2f2196-1115-1-3&partnerID=40&md5=13ad45f35baff725918c76600a48b704","This is a case study on an airline’s miles program resource optimization. The airline had a large miles loyalty program but was not taking advantage of recent data mining techniques. As an example, to predict whether in the coming month(s), a new passenger would become a privileged frequent flyer or not, a linear extrapolation of the miles earned during the past months was used. This information was then used in CRM interactions between the airline and the passenger. The correlation of extrapolation with whether a new user would attain a privileged miles status was 39% when one month of data was used to make a prediction. In contrast, when GBM and other blending techniques were used, a correlation of 70% was achieved. This corresponded to a prediction accuracy of 87% with less than 3% false positives. The accuracy reached 97% if three months of data instead of one were used. An application that ranks users according to their probability to become part of privileged miles-tier was proposed. The application performs real time allocation of limited resources such as available upgrades on a given flight. Moreover, the airline can assign now those resources to the passengers with the highest revenue potential thus increasing the perceived value of the program at no extra cost. © 2014, Berengueres and Efimov; licensee Springer."
"10.1186/2196-1115-1-2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013428711&doi=10.1186%2f2196-1115-1-2&partnerID=40&md5=b57f853818da44f9d1c16a1ea7e3aac0","The amount of data produced within Health Informatics has grown to be quite vast, and analysis of this Big Data grants potentially limitless possibilities for knowledge to be gained. In addition, this information can improve the quality of healthcare offered to patients. However, there are a number of issues that arise when dealing with these vast quantities of data, especially how to analyze this data in a reliable manner. The basic goal of Health Informatics is to take in real world medical data from all levels of human existence to help advance our understanding of medicine and medical practice. This paper will present recent research using Big Data tools and approaches for the analysis of Health Informatics data gathered at multiple levels, including the molecular, tissue, patient, and population levels. In addition to gathering data at multiple levels, multiple levels of questions are addressed: human-scale biology, clinical-scale, and epidemic-scale. We will also analyze and examine possible future work for each of these areas, as well as how combining data from each level may provide the most promising approach to gain the most knowledge in Health Informatics. © 2014, Herland et al.; licensee Springer."
"10.1186/2196-1115-1-1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85009345922&doi=10.1186%2f2196-1115-1-1&partnerID=40&md5=081f84d61a3eb39c59b0579cf0e921ed","Technical Support call centres frequently receive several thousand customer queries on a daily basis. Traditionally, such organisations discard data related to customer enquiries within a relatively short period of time due to limited storage capacity. However, in recent years, the value of retaining and analysing this information has become clear, enabling call centres to identify customer patterns, improve first call resolution and maximise daily closure rates. This paper proposes a Proof of Concept (PoC) end to end solution that utilises the Hadoop programming model, extended ecosystem and the Mahout Big Data Analytics library for categorising similar support calls for large technical support data sets. The proposed solution is evaluated on a VMware technical support dataset. © 2014, Duque Barrachina and O' Driscoll; licensee Springer."
"10.1186/2196-1115-1-6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84976246180&doi=10.1186%2f2196-1115-1-6&partnerID=40&md5=9c6f50eb28a25ecb3fca5716a6a33a65","I describe a research agenda for data science based on a decade of research and operational work in data-intensive systems at NASA, the University of Southern California, and in the context of open source work at the Apache Software Foundation. My vision is predicated on understanding the architecture for grid computing; on flexible and automated approaches for selecting data movement technologies and on their use in data systems; on the recent emergence of cloud computing for processing and storage, and on the unobtrusive and automated integration of scientific algorithms into data systems. Advancements in each of these areas are a core need, and they will fundamentally improve our understanding of data science, and big data. This paper identifies and highlights my own personal experience and opinion growing into a data scientist. © 2014, Mattmann; licensee Springer."
"10.1186/2196-1115-1-5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84942770326&doi=10.1186%2f2196-1115-1-5&partnerID=40&md5=dc4e4c5929bdb9ee3bcc531b998bdd8b","With unlimited growth of real-world data size and increasing requirement of real-time processing, immediate processing of big stream data has become an urgent problem. In stream data, hidden patterns commonly evolve over time (i.e.,concept drift), where many dynamic learning strategies have been proposed, such as the incremental learning and ensemble learning. To the best of our knowledge, there is no work systematically compare these two methods. In this paper we conduct comparative study between theses two learning methods. We first introduce the concept of “concept drift”, and propose how to quantitatively measure it. Then, we recall the history of incremental learning and ensemble learning, introducing milestones of their developments. In experiments, we comprehensively compare and analyze their performances w.r.t. accuracy and time efficiency, under various concept drift scenarios. We conclude with several future possible research problems. © 2014, Zang et al.; licensee Springer."
"10.1186/2196-1115-1-4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84929172140&doi=10.1186%2f2196-1115-1-4&partnerID=40&md5=c3a71fa5cb57b91b0bc361617c6d16bb","GIS application hosts are becoming more and more complicated. Theses hosts’ management is becoming more time consuming and less reliabale decreases with the increase in complexity of GIS applications. The resource management of GIS applications is becoming increasingly important in order to deliver to the user the desired Quality of Service. Map systems often serve dynamic web workloads and involve multiple CPU- and I/O-intensive tiers, which makes it challenging to meet the response time targets of map requests while using the resources efficiently. This paper proposes a virtualized web map service system, v-TerraFly, and its autonomic resource management in order to address this challenge. Virtualization facilitates the deployment of web map services and improves their resource utilization through encapsulation and consolidation. Autonomic resource management allows resources to be automatically provisioned to a map service and its internal tiers on demand. Specifically, this paper proposes new techniques to predict the demand of map workloads online and optimize resource allocations considering both response time and data freshness as the QoS target. The proposed v-TerraFly system is prototyped on TerraFly, a production web map service, and evaluated using real TerraFly workloads. The results show that v-TerraFly can accurately predict the workload demands: 18.91% more accurate; and efficiently allocate resources to meet the QoS target: improves the QoS by 26.19% and saves resource usages by 20.83% compared to traditional peak-load-based resource allocation. © 2014, Lu et al.; licensee Springer."
"10.1057/jma.2014.20","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031498901&doi=10.1057%2fjma.2014.20&partnerID=40&md5=c86b3f6b1f126ff5823fd4df02b87b60","This article uses measures of centrality from social network theory to identify influential marketing scholars and their institutions, who can serve as potential change agents for the field. It also develops interactive visualizations of two networks, one of marketing scholars and another of their institutions. © 2014 Macmillan Publishers Ltd."
"10.1057/jma.2014.17","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031494796&doi=10.1057%2fjma.2014.17&partnerID=40&md5=fc8cda1014be9b9b7bd8428fa6c40cb6","How is culture manifested in the consumer behaviour of ethnic minorities? This article addresses this question using three ethnic groups: Indians living in Britain (people who are descended from or are migrants from India), British Whites and Asian Indians. The cross-cultural study assesses the extent to which Indians living in Britain draw upon Asian Indian and British White cultural values in the purchase of a brown good (any non-kitchenbased electrical product, such as a television). The sample used in the quantitative study includes respondents from each of these three groups. Adopting this design enables a clear view of how British White and Asian Indian culture values influence the consumer behaviour of Indians living in Britain. The article finds the consumer decision-making process of Indians living in Britain to have similarities with, and be influenced by, the cultural values of both Asian Indians and British Whites. The results suggest that cultural adaptation has occurred, the implications of which are explored. © 2014 Macmillan Publishers Ltd."
"10.1057/jma.2014.18","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028261598&doi=10.1057%2fjma.2014.18&partnerID=40&md5=b39102b326cb17b2e3672dd99dbf0f98","True-lift modeling, also known as uplift modeling, combines predictive modeling and experimental method to enable marketers to identify the characteristics of ‘true’ treatment responders separately from the characteristics of ‘baseline’ or control responders (that is, those who would have responded anyway). By concentrating truly ‘persuadable’ treatment targets in the top deciles, true-lift models achieve the same (or more) amount of response with fewer treatments (and lower treatment costs). The identified characteristics of the ‘persuadable’ population can then guide the hypotheses of future experiments and pinpoint the most responsive recipients for the treatment in future. This article explains the concept of true-lift modeling in detail, reviews existing methods, contrasts with the traditional approach, proposes new methods that can be implemented with most standard software, and recommendsmetrics for model assessment and comparison in true-lift modeling. Several new and existing methods are applied to three data sets from the financial services, online merchandise and retail industries. Built on the findings from our study and prior experience, we recommend some guidelines on usage of true-lift modeling methods. © 2014 Macmillan Publishers Ltd."
"10.1057/jma.2014.16","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85024475778&doi=10.1057%2fjma.2014.16&partnerID=40&md5=cf3bd48694a30771d1c1d420666cfa6c","This article builds on the original Blattberg–Deighton (BD) model of customer equity to focus on short-term market share growth and long-term customer equity. We account for what we term the inertial segment – those consumers who will continue to purchase in the absence of retention spending – to explore ways to more optimally balance budget allocation between customer acquisition and retention. We demonstrate that previous focus on budget allocation with the single marketing objective of maximizing customer equity neglects the relevance of investment in acquisition. Our proposed model boosts customer equity and is achieved with a lower total budget. © 2014 Macmillan Publishers Ltd."
"10.2481/dsj.14-035","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84919930032&doi=10.2481%2fdsj.14-035&partnerID=40&md5=b22ea01d22974ca8a1c9fa4104a24e63","The associative classification method integrates association rule mining and classification. Constructing an efficient classifier with a small set of high quality rules is a highly important but indeed a challenging task. The lazy learning associative classification method successfully removes the need for a classifier but suffers from high computation costs. This paper proposes a Compact Highest Subset Confidence-Based Associative Classification scheme that generates compact subsets based on information gain and classifies the new samples without constructing classifiers. Experimental results show that the proposed system out performs both the traditional and the existing lazy learning associative classification methods."
"10.2481/dsj.14-030","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84919915510&doi=10.2481%2fdsj.14-030&partnerID=40&md5=90b3bec5370f7d07e3f03b537b5d38e7","Meta-analyses are studies that bring together data or results from multiple independent studies to produce new and over-arching findings. Current data curation systems only partially support meta-analytic research. Some important meta-analytic tasks, such as the selection of relevant studies for review and the integration of research datasets or findings, are not well supported in current data curation systems. To design tools and services that more fully support meta-analyses, we need a better understanding of meta-analytic research. This includes an understanding of both the practices of researchers who perform the analyses and the characteristics of the individual studies that are brought together. In this study, we make an initial contribution to filling this gap by developing a conceptual framework linking meta-analyses with data paths represented in published articles selected for the analysis. The framework focuses on key variables that represent primary/secondary datasets or derived socio-ecological data, contexts of use, and the data transformations that are applied. We introduce the notion of using variables and their relevant information (e.g., metadata and variable relationships) as a type of currency to facilitate synthesis of findings across individual studies and leverage larger bodies of relevant source data produced in small science research. Handling variables in this manner provides an equalizing factor between data from otherwise disparate data-producing communities. We conclude with implications for exploring data integration and synthesis issues as well as system development."
,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84910147156&partnerID=40&md5=2bc8a92b7f4b2e06b1dfb703fdcffef0","A high-dimensional feature selection having a very large number of features with an optimal feature subset is an NP-complete problem. Because conventional optimization techniques are unable to tackle large-scale feature selection problems, meta-heuristic algorithms are widely used. In this paper, we propose a particle swarm optimization technique while utilizing regression techniques for feature selection. We then use the selected features to classify the data. Classification accuracy is used as a criterion to evaluate classifier performance, and classification is accomplished through the use of k-nearest neighbour (KNN) and Bayesian techniques. Various high dimensional data sets are used to evaluate the usefulness of the proposed approach. Results show that our approach gives better results when compared with other conventional feature selection algorithms. © 2014, Committee on Data for Science and Technology. All rights reserved."
"10.2481/dsj.14-033","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84909999764&doi=10.2481%2fdsj.14-033&partnerID=40&md5=8b4c242bd5f63595e7f599b01ad1934c","The Semantic Web (Web 3.0) has been proposed as an efficient way to access the increasingly large amounts of data on the internet. The Linked Open Data Cloud project at present is the major effort to implement the concepts of the Seamtic Web, addressing the problems of inhomogeneity and large data volumes. RKBExplorer is one of many repositories implementing Open Data and contains considerable bibliographic information. This paper discusses bibliographic data, an important part of cloud data. Effective searching of bibiographic datasets can be a challenge as many of the papers residing in these databases do not have sufficient or comprehensive keyword information. In these cases however, a search engine based on RKBExplorer is only able to use information to retrieve papers based on author names and title of papers without keywords. In this paper we attempt to address this problem by using the data mining algorithm Association Rule Mining (ARM) to develop keywords based on features retrieved from Resource Description Framework (RDF) data within a bibliographic citation. We have demonstrate the applicability of this method for predicting missing keywords for bibliographic entries in several typical databases. © 2014, Committee on Data for Science and Technology. All rights reserved."
"10.2481/dsj.IFPDA-01","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84910145246&doi=10.2481%2fdsj.IFPDA-01&partnerID=40&md5=7d76be634918381f8d67c954a69a190f","The Polar Data Catalogue (PDC) is a growing Canadian archive and public access portal for Arctic and Antarctic research and monitoring data. In partnership with a variety' of Canadian and international multi-sector research programs, the PDC encompasses the natural, social, and health sciences. From its inception, the PDC has adopted international standards and best practices to provide a robust infrastructure for reliable security, storage, discoverability, and access to Canada's polar data and metadata. Current efforts focus on developing new partnerships and incentives for data archiving and sharing and on expanding connections to other data centres through metadata interoperability protocols. © 2014, Committee on Data for Science and Technology. All rights reserved."
,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84910121253&partnerID=40&md5=7932004227695b63be05230e33a2aa7c","Scientific data management is performed to ensure that data are curated in a manner that supports their qualified reuse. Curation usually involves actions that must be performed by those who capture or generate data and by a facility with the capability to sustainably archive and publish data beyond an individual project's lifecycle. The Australian Antarctic Data Centre is such a facility. How this centre is approaching the administration of Antarctic science data is described in the following paper and serves to demonstrate key facets necessary for undertaking polar data management in an increasingly connected global data environment. © 2014, Committee on Data for Science and Technology. All rights reserved."
"10.2481/dsj.IFPDA-04","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84910022147&doi=10.2481%2fdsj.IFPDA-04&partnerID=40&md5=ceb4d0067c14a8c02e0f609437fa2c3f","Data generated by environmental research in Antarctica are essential in evaluating how its biodiversity and environment are affected by global-scale changes triggered by ever-increasing human activities. In this work, we describe BrAntIS, the Brazilian Information System on Antarctic Environmental Research, which enables the acquiring, storing, and querying of research data generated by the Brazilian National Institute for Science and Technology on Antarctic Environmental Research. BrAntIS' data model reflects data acquisition and analysis conducted by scientists and organized around field expeditions. We describe future functionalities, such as the use of linked data techniques and support for scientific workflows."
"10.2481/dsj.IFPDA-05","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84910009863&doi=10.2481%2fdsj.IFPDA-05&partnerID=40&md5=1c6d501f9a44780c5f4dfb906ecc500c","The Polar Data Centre of the National Institute of Polar Research has had the responsibility to manage the data for Japan as a National Antarctic Data Centre for the last two decades. During the International Polar Year (IPY) 2007-2008, a considerable number of multidisciplinary metadata that mainly came from IPY-endorsed projects involving Japanese activities were compiled by the data centre. Although long-term stewardship of those amalgamated metadata falls to the data centre, the efforts are in collaboration with the Global Change Master Directory, the Polar Information Commons, and the newly established World Data System of the International Council for Science."
"10.2481/dsj.IFPDA-03","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84910001315&doi=10.2481%2fdsj.IFPDA-03&partnerID=40&md5=fdf9d874a81587c2700f9bfbb6312e40","Korea implemented its Antarctic research program in 1987 and diversified to the Arctic in 2002. Since the development of the Joint Committee on Antarctic Data Management, Korea has acknowledged the importance of data management. The launch of the Korea Polar Research Institute in 2004 also saw establishment of the Korea Polar Data Center (KPDC), which outlines and executes a Polar Data Management Policy. KPDC has set up an Information Technology infrastructure and has developed a metadata management system. However, there is still a long way to go, especially in terms of raising researcher recognition for improving data registration and sharing. © 2014, Committee on Data for Science and Technology. All rights reserved."
"10.2481/dsj.IFPDA-07","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84909990519&doi=10.2481%2fdsj.IFPDA-07&partnerID=40&md5=72f124003f05d54c4e06b819f2218932","An overview of the Interuniversity Upper atmosphere Global Observation NETwork (IUGONET) project is presented. This Japanese program is building a meta-database for ground-based observations of the upper atmosphere, in which metadata connected with various atmospheric radars and photometers, including those located in both polar regions, are archived. By querying the metadata database, researchers are able to access data file/information held by data facilities. Moreover, by utilizing our analysis software, users can download, visualize, and analyze upper-atmospheric data archived in or linked with the system. As a future development, we are looking to make our database interoperable with others."
"10.2481/dsj.IFPDA-06","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84909988629&doi=10.2481%2fdsj.IFPDA-06&partnerID=40&md5=95ee42b22cd18e47fbb7433a47bdd104","Polar information falls into at least six categories: information about researchers, organizations, research facilities, research projects, research datasets, and publications. The management of polar research datasets has been the focus of significant attention in recent years, but it is only one piece of the polar information world. The other information types are needed to provide context to, and extract knowledge from, the raw data. Here, I discuss the possibilities for linking the various types of information categories in Canada to create a truly holistic view of Canadian Arctic research."
"10.2481/dsj.IFODA-13","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84909646865&doi=10.2481%2fdsj.IFODA-13&partnerID=40&md5=fb0a6e969445c49670c1c384651e275f","The legacy of the International Polar Year 2007-2008(IPY) includes advances in open data and meaningful progress towards interoperability of data, systems, and standards. Enabled by metadata brokering technologies and by the growing adoption of international metadata standards, federated data search welcomes diversity in Arctic data and recognizes the value of expertise incommunity data repositories. Federated search enables specialized data holdings to be discovered by broader audiences and complements the role of metadata registries such as the Global Change Master Directory,providing interoperability across the Arctic web-of-repositories."
"10.2481/dsj.IFPDA-10","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84909644710&doi=10.2481%2fdsj.IFPDA-10&partnerID=40&md5=47c37681bdae3510612e764643c04977","The regional HydroMeteorological DataBase (HMDB) was designed for easy access to climate data via the Internet. It contains data on various climatic parameters (temperature, precipitation, pressure, humidity, and wind strength and direction) from 190 meteorological stations in Russia and bordering countries for a period of instrumental observations of over 100 years. Open sources were used to ingest data into HMDB. An analytical block was also developed to perform the most common statistical analysis techniques."
"10.2481/dsj.IFPDA-16","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84909643211&doi=10.2481%2fdsj.IFPDA-16&partnerID=40&md5=d360e3b4ed0d3eff8d2aedb858e097c4","Data management is integral to sound polar science. Through analysis of documents reporting on meetings of the Arctic data management community, a set of priorities and strategies are identified. These include the need to improve data sharing, make use of existing resources, and better engage stakeholders. Network theory is applied to a preliminary inventory of polar and global data management actors to improve understanding of the emerging community of practice. Under the name the Arctic Data Coordination Network, we propose a model network that can support the community in achieving their goals through improving connectivity between existing actors."
"10.2481/dsj.IFPDA-14","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84909642821&doi=10.2481%2fdsj.IFPDA-14&partnerID=40&md5=a4ca3e1c4c259774477ab12a926f32a2","The Inter-university Consortium for Political and Social Research (ICPSR), a domain repository with a 50-year track record of archiving social and behavioural science data, applied for - and acquired - the Data Seal of Approval (DSA) in 2010. DSA is a non-intrusive, straight forward approach to assessing organizational, technical, and operational infrastructure, and signifies a basic level of accreditation. DSA assessment helped ICPSR become more transparent, monitor and improve archival processesand procedures, and raise awareness within the organization and beyond about best practicesfor repositories. We relate our experiences with the DSA process, and describe challenges and opportunities associated with DSA assessment."
"10.2481/dsj.IFPDA-08","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84909637480&doi=10.2481%2fdsj.IFPDA-08&partnerID=40&md5=f08ecbecc3a0fd535b78d2d34144c3be","Ionospheric Prediction Services (IPS) has an extensive collection of data fromAntarctic field instruments,the oldestbeing ionospheric recordings fromthe 1950s.Itssensor network (IPSNET) spans Australasia andAntarcticacollectinginformation on space weather. In Antarctica, sensors include ionosondes, magnetometers, riometers, and cosmic ray detectors. The(mostly) real-timedata from these sensors flow into the IPS World Data Centre at Sydney, where the majority are available online toclients worldwide. When combined with other IPSNET-station data,they provide the basis forAntarctic space weather reports.This paper summarizes the datasets collected from Antarctica and their data management within IPS."
"10.2481/dsj.IFPDA-12","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84909629860&doi=10.2481%2fdsj.IFPDA-12&partnerID=40&md5=19d706839686a8cc985b862e020b53df","The Arctic Ocean boundary monitoring array has been maintained over many years by six research institutes located worldwide. Our approach to Arctic Ocean boundary measurements is generating significant scientific outcomes. However, it is not always easy to access Arctic data. On the basis of our last five years' experience of assembling pan-Arctic boundary data, and considering the success of Argo, I propose that Arctic data policy should be driven by specific scientific-based requirements. Otherwise, it will be hard to implement the International Polar Year data policy. This approach would also help to establish a consensus of future Arctic science."
"10.2481/dsj.IFPDA-15","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84909595248&doi=10.2481%2fdsj.IFPDA-15&partnerID=40&md5=20185b00d4ed7f764d8d3c962fa9bd65","The research data landscape of the last International Polar Year was dramatically different from its predecessors. Data scientists documented lessons learned about management of large, diverse, and interdisciplinary datasets to inform future development and practices. Improved, iterative, and adaptive data curation and system development methods to address these challenges will be facilitated by building collaborations locally and globally across the 'data ecosystem', thus, shaping and sustaining an international data infrastructure to fulfil modern scientific needs and societal expectations. International coordination is necessary to achieve convergence between domain-specific data systems and hence enable multidisciplinary approaches needed to solve the Global Challenges."
"10.2481/dsj.IFPDA-09","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84909583721&doi=10.2481%2fdsj.IFPDA-09&partnerID=40&md5=6c2e745e527479b547c05719f0eb0da5","Asystemto optimize the management of globalspace-weather observation networks has been developed by the National Institute of Information and Communications Technology (NICT). Named the WONM (Wide-area Observation Network Monitoring) system, itenables data acquisition, transfer, and storage through connection to the NICT Science Cloud, and has been supplied to observatories for supporting space-weather forecast and research. This system provides us with easier management of data collection than our previouslyemployed systems by means of autonomous system recovery, periodical state monitoring, and dynamic warning procedures. Operation of the WONM system is introducedin this report."
"10.1080/23270012.2014.988762","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017294015&doi=10.1080%2f23270012.2014.988762&partnerID=40&md5=9a1945187c682d7baa68a2af83340c5c","Treatment plan selection is a complex process because it sometimes needs sufficient experience and clinical information. Nowadays it is even harder for doctors to select an appropriate treatment plan for certain patients since doctors might encounter difficulties in obtaining the right information and analyzing the diverse clinical data. In order to improve the effectiveness of clinical decision making in complicated information system environments, we first propose a linked data-based approach for treatment plan selection. The approach integrates the patients' clinical records in hospitals with open linked data sources out of hospitals. Then, based on the linked data net, treatment plan selection is carried on aided by similar historical therapy cases. Finally, we reorganize the electronic medical records of 97 colon cancer patients using the linked data model and count the similarity of these records to help treatment selecting. The experiment shows the usability of our method in supporting clinical decisions. © 2014, © 2014 Antai College of Economics and Management, Shanghai Jiao Tong University."
"10.1080/23270012.2014.983196","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85011051156&doi=10.1080%2f23270012.2014.983196&partnerID=40&md5=967ff655c9724ab5b71b30042c9c64e2","The development of information technology (IT) in the last decade has ushered us into the ‘big data’ era. Whereas sampling error has been of high interest in such studies over the last century, extremely large datasets will virtually rule out the possibility that statistics generated from samples are not also true of the population. The critical discussion must move on to how much the exogenous and endogenous variables explain ultimate outcomes. The stress will have to move from generalizability to explanatory power. By examining the three common statistical tests–the T-value, the F-value and the beta coefficient (or ‘b’ in the sample)–this paper aims to show how important effect size analysis is with the trend toward larger and larger samples and big data. In the end, the paper points out the gap of what the acceptable ranges of effects that appears in our sundry scientific literatures are. We call for studies in this regard so that social sciences can move forward and strengthen weak theory bases, and recognize strong theories when they do appear. © 2014, © 2014 Antai College of Economics and Management, Shanghai Jiao Tong University."
"10.1080/23270012.2014.990527","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84990198216&doi=10.1080%2f23270012.2014.990527&partnerID=40&md5=cb9c8b86b682a4112e59e5f1214272df","By virtue of the US markets closing later than Asia-Pacific (AP) markets, returns observed in the US include information not reflected in AP until the next day. Provided there is enough integration among markets, this asymmetry should then generate Granger causality from the US returns towards returns in the AP region. It is thus obvious that when testing for Granger causality among the Western and AP markets, interdependence due to non-synchronicity should be clearly identified and factored out. Our novel method is to measure contagion, usually defined as excessive market integration during crises, as excess interdependence beyond the non-synchronicity induced. We test for contagion of the 2008 Financial Crisis from the US to AP. The unmistakable emergence of a new cointegration relation that intertwines the US and AP markets only during the crisis, but not before, presents evidence of long-run contagion, that is a new channel that transmits crisis-related info. We find strong contagion from US to Korea, no contagion to Hong Kong, and Australia is a mixed case. Our explanation is that the Korean economy is the most vulnerable due to its export orientation, Hong Kong becomes disentangled due to its relation to China, which is consistent with the theory of ‘decoupling’ of China, and the Australian case is somewhere in the middle, probably due to its close ties to the US but also its strong reliance on natural resources and gold. © 2014, © 2014 Antai College of Economics and Management, Shanghai Jiao Tong University."
"10.1080/23270012.2014.991357","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84969925582&doi=10.1080%2f23270012.2014.991357&partnerID=40&md5=6caffcf9a294e432a86e458950c446e5","This research examines the dimensions of a supply chain collaboration (SCC) process in Jordanian partners and measures the effectiveness of its implementation using two structural models. The first model examines the relationship between the SCC process and relevant structural and social dimensions, including: governance (GO), administrative structure (AD), organization autonomy (OA), norms (NO), and mutuality (MU). The second model studies the effectiveness of SCC implementation via five dimensions, involving: information sharing (IS), resource sharing (RS), goal congruence (GC), decision synchronization (DS), and joint knowledge (JK) creation. A total of 95 organizations are surveyed and then the collected data are analyzed. Results reveal that GO, AD, OA, NO, and MU significantly affect SSC, while AD and MU have the largest effects. On the other hand, SCC implementation is significantly affected by DS and IS, while partially affected by RS. Finally, JK and GC show no significant effects on SCC implementation. In conclusion, although Jordanian collaborative partners successfully established SCC relations, still more efforts should directed to strengthen SCC implementation. © 2014, © 2014 Antai College of Economics and Management, Shanghai Jiao Tong University."
"10.1080/23270012.2014.995143","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84968358544&doi=10.1080%2f23270012.2014.995143&partnerID=40&md5=1a1eb1c25c02ab924ab5e69e1888dc94","In recent years we have witnessed that e-commerce has become an international phenomenon in which sellers often need to do business with buyers who use different languages. As customers, including international customers, often make purchase decisions based on online reviews, comments and recommendations made by other customers, understanding the sentiment of online comments becomes necessary for sellers who want to provide customized recommendations to users for a higher level of customer satisfaction. To this end, we have recently developed a bilingual model to handle both English and Chinese user comments posted on e-commerce websites. This paper addresses several key issues including Chinese segmentation, data mining models and systematic design. An experiment in mining user satisfaction sentiment with English and Chinese online user comments illustrates the value of our developed bilingual model. © 2014, © 2014 Antai College of Economics and Management, Shanghai Jiao Tong University."
"10.4018/ijban.2014100102","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046295694&doi=10.4018%2fijban.2014100102&partnerID=40&md5=f694156424411f920fe6e87df78f188b","Numerous innovative technologies are available to assist the struggling hardwood lumber industry adapt to changing market demands and environmental concerns. However, most mill owners do not utilize automated lumber systems because they do not realize how substantial volume and value gains can be. Thus, there is a need to quantify improved efficiencies while also providing reliable information about how these measures translate into profitability for the mill. This study highlights new hardwood sawmill technologies, specifically in the areas of information systems and visualization technologies, and assesses the environmental impacts alongside the practicality of widespread application. Results from on-site testing were combined with other research in the field, concluding that properly applying visualization, optimization, and information technologies across the manufacturing process can significantly improve overall yield values. Combining engineering technologies with IS and strategic supply chain management leads to reduced waste and increased profits, benefiting local economies and forest resources across the globe. Copyright © 2014 IGI Global."
"10.4018/ijban.2014100103","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046292550&doi=10.4018%2fijban.2014100103&partnerID=40&md5=6bd9f4cad3d35a98b68b1b01ace4aa84","The task of a recommender system evaluation has often been addressed in the literature, however there exists no consensus regarding the best metrics to assess its performance. This research deals with collaborative filtering recommendation systems, and proposes a new approach for evaluating the quality of neighbor selection. It theorizes that good recommendations emerge from good selection of neighbors. Hence, measuring the quality of the neighborhood may be used to predict the recommendation success. Since user neighborhoods in recommender systems are often sparse and differ in their rating range, this paper designs a novel measure to asses a neighborhood quality. First it builds the realization based entropy (RBE), which presents the classical entropy measure from a different angle. Next it modifies the RBE and propose the realization based distance entropy (RBDE), which considers also continuous data. Using the RBDE, it finally develops the consent entropy, which takes into account the absence of rating data. The paper compares the proposed approach with common approaches from the literature, using several recommendation evaluation metrics. It presents offline experiments using the Netflix database. The experimental results confirm that consent entropy performs better than commonly used metrics, particularly with high sparsity neighborhoods. This research is supported by The Israel Science Foundation, Grant #1362/10. This research is supported by NHECD EC, Grant #218639. Copyright © 2014 IGI Global."
"10.4018/ijban.2014100104","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962598754&doi=10.4018%2fijban.2014100104&partnerID=40&md5=98a2c5153cb0f91d1a2dc01ef06c0f1e","Modern technologies have allowed for the amassment ofdata at a rate never encounteredbefore. Organizations are now able to routinely collect and process massive volumes of data. A plethora of regularly collected information can be ordered using an appropriate time interval. The data would thus be developed into a time series. Time series data mining methodology identifies commonalities between sets of time-ordered data. Time series data mining detects similar time series using a technique known as dynamic time warping (DTW). This research provides a practical application of time series data mining. A real-world data set was provided to the authors by dunnhumby. A time series data mining analysis is performed using retail grocery store chain data and results are provided. Copyright © 2014 IGI Global."
"10.1089/big.2014.0026","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84991818059&doi=10.1089%2fbig.2014.0026&partnerID=40&md5=4796b35f13a9927b84658b85db6d8f1f","Global climate change and its impact on human life has become one of our era's greatest challenges. Despite the urgency, data science has had little impact on furthering our understanding of our planet in spite of the abundance of climate data. This is a stark contrast from other fields such as advertising or electronic commerce where big data has been a great success story. This discrepancy stems from the complex nature of climate data as well as the scientific questions climate science brings forth. This article introduces a data science audience to the challenges and opportunities to mine large climate datasets, with an emphasis on the nuanced difference between mining climate data and traditional big data approaches. We focus on data, methods, and application challenges that must be addressed in order for big data to fulfill their promise with regard to climate science applications. More importantly, we highlight research showing that solely relying on traditional big data techniques results in dubious findings, and we instead propose a theory-guided data science paradigm that uses scientific theory to constrain both the big data techniques as well as the results-interpretation process to extract accurate insight from large climate data. © Copyright 2014, Mary Ann Liebert, Inc."
"10.1089/big.2014.0035","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84991764924&doi=10.1089%2fbig.2014.0035&partnerID=40&md5=424308cc4b692c7a9d8a639b2694543a",[No abstract available]
"10.1089/big.2014.0023","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84991758284&doi=10.1089%2fbig.2014.0023&partnerID=40&md5=18b0eb0b5362519b34cebcffa80da972","Data mining and knowledge discovery techniques have greatly progressed in the last decade. They are now able to handle larger and larger datasets, process heterogeneous information, integrate complex metadata, and extract and visualize new knowledge. Often these advances were driven by new challenges arising from real-world domains, with biology and biotechnology a prime source of diverse and hard (e.g., high volume, high throughput, high variety, and high noise) data analytics problems. The aim of this article is to show the broad spectrum of data mining tasks and challenges present in biological data, and how these challenges have driven us over the years to design new data mining and knowledge discovery procedures for biodata. This is illustrated with the help of two kinds of case studies. The first kind is focused on the field of protein structure prediction, where we have contributed in several areas: by designing, through regression, functions that can distinguish between good and bad models of a protein's predicted structure; by creating new measures to characterize aspects of a protein's structure associated with individual positions in a protein's sequence, measures containing information that might be useful for protein structure prediction; and by creating accurate estimators of these structural aspects. The second kind of case study is focused on omics data analytics, a class of biological data characterized for having extremely high dimensionalities. Our methods were able not only to generate very accurate classification models, but also to discover new biological knowledge that was later ratified by experimentalists. Finally, we describe several strategies to tightly integrate knowledge extraction and data mining in order to create a new class of biodata mining algorithms that can natively embrace the complexity of biological data, efficiently generate accurate information in the form of classification/regression models, and extract valuable new knowledge. Thus, a complete data-to-information-to-knowledge pipeline is presented. © Copyright 2014, Mary Ann Liebert, Inc."
"10.1089/big.2014.0031","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84991745845&doi=10.1089%2fbig.2014.0031&partnerID=40&md5=7b55add15bcb7cbe8e6093fe8f526706","In August 2013, we held a panel discussion at the KDD 2013 conference in Chicago on the subject of data science, data scientists, and start-ups. KDD is the premier conference on data science research and practice. The panel discussed the pros and cons for top-notch data scientists of the hot data science start-up scene. In this article, we first present background on our panelists. Our four panelists have unquestionable pedigrees in data science and substantial experience with start-ups from multiple perspectives (founders, employees, chief scientists, venture capitalists). For the casual reader, we next present a brief summary of the experts' opinions on eight of the issues the panel discussed. The rest of the article presents a lightly edited transcription of the entire panel discussion. © Copyright 2014, Mary Ann Liebert, Inc."
"10.1057/jma.2014.15","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031497817&doi=10.1057%2fjma.2014.15&partnerID=40&md5=b2a11e84b716463b82d09d8b8a61ec02","Online repositories are providing business opportunities to gain feedback and opinions on products and services in the form of digital deposits. Such deposits are, in turn, capable of influencing the readers’ views and behaviours from the posting of misinformation intended to deceive or manipulate. Establishing the veracity of these digital deposits could thus bring key benefits to both online businesses and internet users. Although machine learning techniques are well established for classifying text in terms of their content, techniques to categorise them in terms of their veracity remain a challenge for the domain of feature set extraction and analysis. To date, text categorisation techniques for veracity have reported a wide and inconsistent range of accuracies between 57 and 90 per cent. This article evaluates the accuracy of detecting online deceptive text using a logistic regression classifier based on part of speech tags extracted from a corpus of known truthful and deceptive statements. An accuracy of 72 per cent is achieved by reducing 42 extracted part of speech tags to a feature vector of six using principle component analysis. The results compare favourably to other studies. Improvements are anticipated by training machine learning algorithms on more complex feature vectors by combining the key features identified in this study with others from disparate feature domains. © 2014 Macmillan Publishers Ltd."
"10.1057/jma.2014.13","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031495687&doi=10.1057%2fjma.2014.13&partnerID=40&md5=7c0c558387f5ffcdb7a4a15df6b4ea96","The practice of dispensing free samples has been widely adopted by US pharmaceutical companies. Despite physicians’ significant gatekeeper-and decision-maker role in dispensing samples, very few studies in the pharmaceutical marketing literature have empirically examined physicians’ free-sample dispensing decisions or their influence on future prescription decisions. The primary objective of this article is to fill the gap in the literature by examining the key determinants in physician sample-dispensing and quantity decisions at the individual-physician level while controlling for targeted marketing activities and unobserved physician heterogeneity. We conceptualize the dual roles of drug samples, experimentation and subsidy, in physicians’ prescription decision making, empirically test for the existence of these dual roles, and quantify their long-run sales impact using physician panel data from two therapeutic categories. Our analysis yields strong empirical support for the existence of the dual roles in physician sample-dispensing behavior. Finally, we discuss implications of our findings for pharmaceutical managers and policymakers. © 2014 Macmillan Publishers Ltd."
"10.1057/jma.2014.14","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031495332&doi=10.1057%2fjma.2014.14&partnerID=40&md5=c6c023316cb5fc1a7e1dc9cf4a91dafc","The article outlines a framework for online advertising budget allocation. First, it explores the empirical Bayes methodology for learning the effectiveness of different online ad placements – from historical data of varying quality. Second, it describes an analytical procedure for optimal budget allocation, which builds on risk management and reinforcement learning techniques. © 2014 Macmillan Publishers Ltd."
"10.1057/jma.2014.3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025462520&doi=10.1057%2fjma.2014.3&partnerID=40&md5=c3b2ab4b9ce1b93c984536a3100e2bb5","An algorithm to model both time and revenue response to spend for media mix modeling is proposed in this article. A Monte Carlo simulation study is conducted to investigate the possibility of extracting time and revenue response simultaneously from both revenue- and channel-spend data. The quality and reliability of the underlying model parameter reconstruction from various sizes of data are also inspected. The outcome of re-allocating channel spend optimally based on extracted revenue response is evaluated. Simulation results show that nearly a 60 per cent increase in revenue can be achieved by channel-spend optimization, relative to arbitrary channel-spend assignment. The algorithm presented here is very general and can be applied to any budget allocation optimization at various levels. © 2014 Macmillan Publishers Ltd."
"10.2481/dsj.14-016","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84907275168&doi=10.2481%2fdsj.14-016&partnerID=40&md5=85f29f57c47202e43ebc92752048b5ef","Modern science is increasingly data-intensive, multidisciplinary, and network-centric. There is an emerging consensus among the members of the academic research community that the practices of this new science paradigm should be congruent with ""open science"". This entails that the bonanza of research data, the wide availability of algorithms, data tools, and data services produced by the members of the research community must be discoverable, understandable, and usable by overcoming all kinds of heterogeneity and logical inconsistencies. The main concept for coping with the many dimensions of heterogeneity and logical inconsistency is mediation. Mediation is achieved by mediators or brokers. These are software modules that exploit encoded knowledge about certain datasets, data services, and user needs in order to implement an intermediary service. A mediating environment is an environment that provides a core set of intermediary services. Mediation should be a distinct functionality of future research data infrastructures. This paper surveys the different levels of interoperability, i.e., exchangeability, compatibility, and usability, their properties and relationships, mediation concepts, functions, and intermediary services. The current interoperability landscape is also illustrated. Finally, the paper advocates the need for mediating environments to be supported by future research data infrastructures and envisions that one of the most important features of future research data infrastructures will be mediation software."
"10.2481/dsj.14-019","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84907278361&doi=10.2481%2fdsj.14-019&partnerID=40&md5=4971f9def5f7fe6cd0f62a5834d8e73b","This article documents a systematic bias in surface wind directions between the TAO buoy measurements at 0°, 170°W and the ECMWF analysis and forecasts. This bias was of the order 10° and persisted from November 2008 to January 2010, which was consistent with a post-recovery calibration drift in the anemometer vane. Unfortunately, the calibration drift was too time-variant to be used to correct the data so the quality flag for this deployment was adjusted to reflect low data quality. The primary purpose of this paper is to inform users in the modelling and remote-sensing community about this systematic, persistent wind directional bias, which will allow users to make an educated decision on using the data and be aware of its potential impact to their downstream product quality. The uncovering of this bias and its source demonstrates the importance of continuous scientific oversight and effective user-data provider communication in stewarding scientific data. It also suggests the need for improvement in the ability of buoy data quality control procedures of the TAO and ECMWF systems to detect future wind directional systematic biases such as the one described here."
"10.1080/23270012.2014.971890","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057061268&doi=10.1080%2f23270012.2014.971890&partnerID=40&md5=b5d5c4e2aa9a24fa93d2f8556de1dfac","This paper uses quantitative methods to estimate the size of the time buffer in lean operations controlled by the Theory of Constraints (TOC). The main focus of the operations system of TOC is the identification and management of constraints. To protect the constraints, they should be buffered with time buffers, which protecting the system's output from disruptions and fluctuations. Despite the importance of time buffers, the approaches to identifying the size of the time buffer in literature are very empirical. In this paper, a queuing-theory based method is applied to calculate the time buffer sizes. It provides a mathematically exact expression for the coefficient of variation of waiting time for Markov queues. It then applies the concept of isomorphism to approximate the variance of customer waiting time in a general queue. Simulation experiments are conducted to verify the accuracy of approximation. © 2014, © 2014 Antai College of Economics and Management, Shanghai Jiao Tong University."
"10.1080/23270012.2014.973916","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056874694&doi=10.1080%2f23270012.2014.973916&partnerID=40&md5=5af2e771a8407c218da36604840a7557","For a classical order quantity/pricing problem, we present a geometric programming (GP) approach to find the optimal selling price, order quantity and quality level to maximize the profit for the retail firm. Traditional models such as EOQ are not able to handle the nonlinearity of costs and demand. We adopt the GP approach and make a proper transformation of the model so as to solve this classical problem and obtain the global optimal solution. In addition to the optimal solutions, we also perform a sensitivity analysis. The study shows once more that GP is an excellent approach when decision variables interact in a nonlinear, especially exponential manner. © 2014, © 2014 Antai College of Economics and Management, Shanghai Jiao Tong University."
"10.1080/23270012.2014.994232","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029759035&doi=10.1080%2f23270012.2014.994232&partnerID=40&md5=e821625a7fc60a525b1e26fd4f196871","Entrepreneurial intention is a key part of entrepreneurship. This paper aims to explore internal and external impact factors on entrepreneurial intention from the perspective of information transfer. It examines how cultural differences, environmental factors, and environmental education affect entrepreneurial intention. A questionnaire-based survey on Chinese and American college students is conducted to verify three hypotheses. The results show that there is no significant difference in entrepreneurial intention level between college students in China and America (American-born Chinese) college students, that individuals' perceived environmental support is positively related to their entrepreneurial intention, and that individuals' entrepreneurial education level is not positively related to their entrepreneurial intention. © 2014, © 2014 Antai College of Economics and Management, Shanghai Jiao Tong University."
"10.1080/23270012.2014.971889","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016598246&doi=10.1080%2f23270012.2014.971889&partnerID=40&md5=68c642453fe4e48bb0b21b999b86043b","This paper develops an economic production quantity (EPQ) model under the effect of inflation and time value of money. The rate of replenishment is considered to be a variable and the generalized unit production cost function is formulated by incorporating several factors, such as raw material, labour, replenishment rate, advertisements and other factors of the manufacturing system. The selling price of a unit is determined by a mark-up over the production cost. We have considered three types of continuous probabilistic deterioration function, and also considered that the holding cost of the item per unit time is assumed to be an increasing linear function of time spent in storage. In addition, shortages are allowed and partially backlogged. This model aids in minimizing the total inventory cost by finding the optimal cycle length and the optimal production quantity. The optimal solution of the model is illustrated with the help of numerical examples. © 2014, © 2014 Antai College of Economics and Management, Shanghai Jiao Tong University."
"10.1080/23270012.2014.968643","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84996889593&doi=10.1080%2f23270012.2014.968643&partnerID=40&md5=8e8f4e8709cacccfdde2a365805a5307","Big data analytics have been embraced as a disruptive technology that will reshape business intelligence, particularly marketing intelligence, which has have traditionally relied on market surveys to understand consumer behavior and product design. In this paper, we investigate how big data analytics will affect the landscape of business intelligence, leading to big data intelligence. Rooted in the recent literature, we delineate business opportunities and managerial challenges brought forward by the emergence of big data analytics and outline a number of research directions in big data intelligence for business. © 2014, © 2014 Antai College of Economics and Management, Shanghai Jiao Tong University."
"10.4018/ijban.2014070101","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046257686&doi=10.4018%2fijban.2014070101&partnerID=40&md5=b913e83a6b6b440d31cec977c4599e89","Estimating the benefits of network security systems is important for security decisions since considerable resources are spent on them and organizations need to know the returns on their investments. The objective of the model presented here is to improve management decisions. Better decisions imply greater security for the budget. This model has a number of novel features such as a probabilistic sub-model for the detection and response process, a new attack/damage matrix based on damage-type and cyber-attacks by category, and extensive sensitivity analyses. The results suggest a number of insights into the factors affecting the benefits from sensors such as the effects of non-linear relationships between the rate of attacks and the damages caused. The key role of the value of sensitive information is identified. The model helps identify the conditions under which a new security system provides enough benefits to justify its purchase. Copyright © 2014 IGI Global."
"10.4018/ijban.2014070104","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046250122&doi=10.4018%2fijban.2014070104&partnerID=40&md5=c5143d2989c9fb65658f749e691d395e","Prior research has devoted considerable attention to consumer hedonic experience and customer satisfaction in retailing and online shopping settings. However, very little effort has been made to investigate the relationship between hedonic experience and customer satisfaction in the context of information system use. To fill this gap, this study investigates the effect of hedonic experience on game player satisfaction using an adapted technology acceptance model. By doing so, this article addresses the core research question: What drives online game player satisfaction? In support of the model and most of the hypotheses, the empirical results not only identify key antecedents of enjoyment and perceived usefulness, but also confirm the significant role of the two constructs in predicting player satisfaction. This study thus helps both information systems and marketing researchers attain a better understanding of customer satisfaction, expand their baseline knowledge of hedonic experience constructs, and conduct more fruitful and illuminating future research on e-commerce service. Copyright © 2014 IGI Global."
"10.4018/ijban.2014070102","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041644265&doi=10.4018%2fijban.2014070102&partnerID=40&md5=348675131efb1a75db968f6b952ca944","The increasing complexity in Military Command and Control (C2) systems has led to greater vulnerability due to system availability and integrity caused by internal vulnerabilities and external threats. Several studies have proposed measures of availability and integrity for the assets in the C2 systems using precise and certain measures (i.e., the exact number of attacks on the availability and the integrity, the number of countermeasures for the availability and integrity attacks, the effectiveness of the availability and integrity countermeasure in eliminating the threats, and the financial impact of each attack on the availability and integrity of the assets). However, these measures are often uncertain in real-world problems. The source of uncertainty can be vagueness or ambiguity. Fuzzy logic and fuzzy sets can represent vagueness and ambiguity by formalizing inaccuracies inherent in human decision-making. In this paper, the authors extend the risk assessment literature by including fuzzy measures for the number of attacks on the availability and the integrity, the number of countermeasures for the availability and integrity attacks, and the effectiveness of the availability and integrity countermeasure in eliminating these threats. They analyze the financial impact of each attack on the availability and integrity of the assets and propose a comprehensive cyber-risk assessment system for the Military C2 in the fuzzy environment. Copyright © 2014 IGI Global."
"10.4018/ijban.2014070103","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85001731954&doi=10.4018%2fijban.2014070103&partnerID=40&md5=8a969d4ac41d0f585eeb97e8a3b314ad","Supply chain networks have expanded globally in today’s business environment due to cost efficiencies, advanced technology, and market growth. This expansion makes the supply chains more vulnerable to disruption risks in different countries. A disruption in one country can cause serious global impacts. In this article, we formulate a multi-criteria optimization model for supporting strategic supply chain network design decisions. The modelconsiders disruption riskofsupply chain components (i.e., facilities andtransportation links)as well as profit and customer responsiveness as conflicting criteria. This consideration is important since disruption at any supply chain component may lead to the disruption of the entire supply chain network. We apply goal programming (GP) techniques to handle multiple and conflicting network design objectives. We also present a numerical example to illustrate how to incorporate disruption risk when making strategic supply chain decisions. The results demonstrate how supply chain network designs that over emphasize profit may include inexpensive supply chain components with high disruption risk. Therefore, more attention must be paid to managing potential disruptions and designing supply chain networks that balance profit and risk. We discuss tradeoffs among multiple design solutions and identify opportunities for future research. Copyright © 2014 IGI Global."
"10.2481/dsj.14-009","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84903146010&doi=10.2481%2fdsj.14-009&partnerID=40&md5=519d7760139f30e7057a5b71352461d0","A peer review scheme comparable to that used in traditional scientific journals is a major element missing in bringing publications of raw data up to standards equivalent to those of traditional publications. This paper introduces a quality evaluation process designed to analyse the technical quality as well as the content of a dataset. This process is based on quality tests, the results of which are evaluated with the help of the knowledge of an expert. As a result, the quality is estimated by a single value only. Further, the paper includes an application and a critical discussionon the potential for success, the possible introduction of the process into data centres, and practical implications of the scheme."
"10.1089/big.2014.0012","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84991728113&doi=10.1089%2fbig.2014.0012&partnerID=40&md5=53270a65b1476b0a9f8c7003c222ef9b","TV audience measurement involves estimating the number of viewers tuned into a TV show at any given time as well as their demographics. First introduced shortly after commercial television broadcasting began in the late 1940s, audience measurement allowed the business of television to flourish by offering networks a way to quantify the monetary value of TV audiences for advertisers, who pay for the estimated number of eyeballs watching during commercials. The first measurement techniques suffered from multiple limitations because reliable, large-scale data were costly to acquire. Yet despite these limitations, measurement standards remained largely unchanged for decades until devices such as cable boxes, video-on-demand boxes, and cell phones, as well as web apps, Internet browser clicks, web queries, and social media activity, resulted in an explosion of digitally available data. TV viewers now leave digital traces that can be used to track almost every aspect of their daily lives, allowing the potential for large-scale aggregation across data sources for individual users and groups and enabling the tracking of more people on more dimensions for more shows. Data are now more comprehensive, available in real time, and cheaper to acquire, enabling accurate and fine-grained TV audience measurement. In this article, I discuss the evolution of audience measurement and what the recent data explosion means for the TV industry and academic research. © Mary Ann Liebert, Inc. 2014."
"10.1089/big.2014.1524","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84991677908&doi=10.1089%2fbig.2014.1524&partnerID=40&md5=efd37bcf070e13b31442e60b56190ecf",[No abstract available]
"10.1089/big.2014.0018","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84991585738&doi=10.1089%2fbig.2014.0018&partnerID=40&md5=8587fb5d0311c0ce9fd5601c4cd9e3b0","Our goal is to design a prediction and decision system for real-time use during a professional car race. In designing a knowledge discovery process for racing, we faced several challenges that were overcome only when domain knowledge of racing was carefully infused within statistical modeling techniques. In this article, we describe how we leveraged expert knowledge of the domain to produce a real-time decision system for tire changes within a race. Our forecasts have the potential to impact how racing teams can optimize strategy by making tire-change decisions to benefit their rank position. Our work significantly expands previous research on sports analytics, as it is the only work on analytical methods for within-race prediction and decision making for professional car racing. © Mary Ann Liebert, Inc. 2014."
"10.1089/big.2014.0010","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84983665827&doi=10.1089%2fbig.2014.0010&partnerID=40&md5=24026a51b20f6e770a741baf6c4b205a","Many firms depend on third-party vendors to supply data for commercial predictive modeling applications. An issue that has received very little attention in the prior research literature is the estimation of a fair price for purchased data. In this work we present a methodology for estimating the economic value of adding incremental data to predictive modeling applications and present two cases studies. The methodology starts with estimating the effect that incremental data has on model performance in terms of common classification evaluation metrics. This effect is then translated into economic units, which gives an expected economic value that the firm might realize with the acquisition of a particular data asset. With this estimate a firm can then set a data acquisition price that targets a particular return on investment. This article presents the methodology in full detail and illustrates it in the context of two marketing case studies. © Mary Ann Liebert, Inc. 2014."
"10.1089/big.2014.0011","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84961133098&doi=10.1089%2fbig.2014.0011&partnerID=40&md5=1e97d27ee55bf20253b1a5d2f164147c",[No abstract available]
"10.1089/big.2014.1520","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84946887014&doi=10.1089%2fbig.2014.1520&partnerID=40&md5=3ebac4a5ff79d1bf3b3bc4b18129fbf1",[No abstract available]
"10.1057/jma.2014.7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031502598&doi=10.1057%2fjma.2014.7&partnerID=40&md5=dd0fc9c5ce8685754c67d63c04947546","The aims of this study are to provide a segmentation of Italian car users for full electric vehicles (FEVs) based on the perceived attractiveness of the benefits of a full electric city car offering, and to determine whether identified segments are denoted by different demographic characteristics, knowledge and attitudes toward FEVs. The analysis was conducted on a sample of 8423 Italian car users. Factor analysis was used to identify latent dimensions of benefits of the examined car offering. The k-means clustering algorithm was applied to identify latent dimensions for classifying respondents. χ2 and non-parametric Kruskal–Wallis and Dunn’s multiple comparison tests were used for verifying differences among segments. Four segments were identified. Each segment exhibited significant differences in demographic characteristics, product knowledge and attitudes toward FEVs. Dissimilar knowledge and attitudes toward FEVs among identified profiles, with heterogeneous perceived benefits, suggest the advisability of implementing differentiated communication strategies to foster a still embryonic market. The article provides further insight into the limited managerial literature devoted to FEVs in Italy, and provides a new perspective on the segmentation of car users, including the investigation of attitudes, whereas the majority of contributions on this subject address only the investigation of product functional benefits. © 2014 Macmillan Publishers Ltd."
"10.1057/jma.2014.10","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029417333&doi=10.1057%2fjma.2014.10&partnerID=40&md5=2afca5c8ffbeb9ad19cab48d334a3d90","For more than a decade, professional sports teams have mainly used their own websites as online vehicles in support of marketing efforts in services, communications, research and sales. Meanwhile, social media and mobile computing have been added to the mix of online as a response to raised service expectations and changing functional requirements for digital online presence and interaction, which has important implications for the marketing success of any professional team. We studied the cyberspace presence and appearance of four leading soccer teams in Europe and particularly looked at the mix of official team websites, social media and mobile applications. Following the methodology of previous studies on the subject, we used the TEDS framework for information artifact evaluation from a ‘human agent-centric’ perspective and extended this analytical approach to also include social media and mobile channels. Our results suggest that the new channels of social and mobile media have seemingly gained in importance putting the traditional websites to the backseat in the mix. However, in all four cases the integration of cyberspace channels appears to be in its infancy leaving much room for improvement from both a marketing perspective and an information artifact standpoint. © 2014 Macmillan Publishers Ltd."
"10.1057/jma.2014.8","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85015432595&doi=10.1057%2fjma.2014.8&partnerID=40&md5=7a1d6fe2e0b2f8911727e510770c3ee0","This study uses an eye-tracking method to explore the framing effect on observed eye movements and purchase intention in online shopping. The results show that negative framing induces more active eye movements. Function attributes and nonfunctionality attributes attract more eye movements and with higher intensity. And the scanpath on the areas of interest reveals a certain pattern. These findings have practical implications for e-sellers to improve communication with customers. © 2014 Macmillan Publishers Ltd."
"10.1057/jma.2014.9","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84938422658&doi=10.1057%2fjma.2014.9&partnerID=40&md5=753b299e48cd7a6e8350e9980b604f8a","Today, organizations require additional efforts to develop new streams of revenue as competition is intense and new customers are hard to secure at a mature stage. The advent of social networking sites serves as an alternative tactic for organizations to form online brand communities, engage customers and hence foster brand loyalty. This article presents a research model of antecedents and consequences of customer engagement in online brand communities on social networking sites. Specifically, we examined how system support, community value, freedom of expression, and rewards and recognition encourage customer engagement, as well as how customer engagement influences repurchase intention and word-of-mouth intention. We tested the research model with a sample of 276 online brand community members. Empirical data supported our hypotheses, and revealed that customer engagement mediates relationships between community characteristics and brand loyalty. The current study validated the propositions from prior conceptual frameworks, and shed light for practitioners and scholars. © 2014 Macmillan Publishers Ltd."
"10.2481/dsj.14-001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84901704849&doi=10.2481%2fdsj.14-001&partnerID=40&md5=5e55b91f981637a89d46bd43e7438c6f","There is a clear need for a public domain data set of road networks with high special accuracy and global coverage for a range of applications. The Global Roads Open Access Data Set (gROADS), version 1, is a first step in that direction. gROADS relies on data from a wide range of sources and was developed using a range of methods. Traditionally, map development was highly centralized and controlled by government agencies due to the high cost or required expertise and technology. In the past decade, however, high resolution satellite imagery and global positioning system (GPS) technologies have come into wide use, and there has been significant innovation in web services, such that a number of new methods to develop geospatial information have emerged, including automated and semi-automated road extraction from satellite/aerial imagery and crowdsourcing. In this paper we review the data sources, methods, and pros and cons of a range of road data development methods: heads-up digitizing, automated/semi-automated extraction from remote sensing imagery, GPS technology, crowdsourcing, and compiling existing data sets. We also consider the implications for each method in the production of open data."
"10.2481/dsj.13-061","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84899795662&doi=10.2481%2fdsj.13-061&partnerID=40&md5=2a3a99e234b456fe21de2c0aa14876cb","A Materials Engineering Application (MEA) has been presented as a solution for the problems of materials design, solutions simulation, production and processing, and service evaluation. Large amounts of data are generated in the MEA distributed and heterogeneous environment. As the demand for intelligent engineering information applications increases, the challenge is to effectively organize these complex data and provide timely and accurate on-demand services. In this paper, based on the supporting environment of Open Cloud Services Architecture (OCSA) and Virtual DataSpace (VDS), a new semantic-driven knowledge representation model for MEA information is proposed. Faced with the MEA constantly changing user requirements, this model elaborates the semantic representation of data, services and their relationships to support the construction of domain knowledge ontology. Then, based on the ontology modeling in VDS, the semantic representations of association mapping, rule-based reasoning, and evolution tracking are analyzed to support MEA knowledge acquisition. Finally, an application example of knowledge representation in the field of materials engineering is given to illustrate the proposed model, and some experimental comparisons are discussed for evaluating and verifying the effectiveness of this method."
"10.2481/dsj.13-017","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84899876155&doi=10.2481%2fdsj.13-017&partnerID=40&md5=6c9bcd13c3abfc6ec5a778f33ebdd284","In recent times, the mining of association rules from XML databases has received attention because of its wide applicability and flexibility. Many mining methods have been proposed. Because of the inherent flexibility of the structures and the semantics of the documents, however, these methods are challenging to use. In order to accomplish the mining, an XML document must first be converted into a relational dataset, and an index table with node encoding is created to extract transactions and interesting items. In this paper, we propose a new method to mine association rules from XML documents using a new type of node encoding scheme that employs a Unique Identifier (UID) to extract the important items. The node scheme modified with UID encoding speeds up the mining process. A significance measure is used to identify the important rules found in the XML database. Finally, the mining procedure calculates the confidence that the identified rules are indeed meaningful. Experiments are conducted using XML databases available in the XML data repository. The results illustrate that the proposed method is efficient in terms of computation time and memory usage."
"10.2481/dsj.13-019","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84899893495&doi=10.2481%2fdsj.13-019&partnerID=40&md5=ca2384a2a988dc09b17eb102e58068dd","Customer segmentation is a process that divides a business's total customers into groups according to their diversity of purchasing behavior and characteristics. The data mining clustering technique can be used to accomplish this customer segmentation. This technique clusters the customers in such a way that the customers in one group behave similarly when compared to the customers in other groups. The customer related data are categorical in nature. However, the clustering algorithms for categorical data are few and are unable to handle uncertainty. Rough set theory (RST) is a mathematical approach that handles uncertainty and is capable of discovering knowledge from a database. This paper proposes a new clustering technique called MADO (Minimum Average Dissimilarity between Objects) for categorical data based on elements of RST. The proposed algorithm is compared with other RST based clustering algorithms, such as MMR (Min-Min Roughness), MMeR (Min Mean Roughness), SDR (Standard Deviation Roughness), SSDR (Standard deviation of Standard Deviation Roughness), and MADE (Maximal Attributes DEpendency). The results show that for the real customer data considered, the MADO algorithm achieves clusters with higher cohesion, lower coupling, and less computational complexity when compared to the above mentioned algorithms. The proposed algorithm has also been tested on a synthetic data set to prove that it is also suitable for high dimensional data."
"10.1080/23270012.2014.943137","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019590394&doi=10.1080%2f23270012.2014.943137&partnerID=40&md5=83c2756e841e435bb3af4fdba1023261","A liquid chemical spill or leakage at sea pollutes the marine environment, damages natural resources, and harms the health of local residents. To date, there has not been much research that has been devoted to this important maritime emergency rescue problem. In this study, we propose a mathematical programming model that considers a single accident site and multiple emergency rescue bases. Various resource constraints, such as volume and weight capacity at the emergency rescue bases, are considered. Rescue funding availability is also integrated into the model. The results from a numerical example show that the model is mathematically valid and practically feasible. The proposed model can be used to provide insightful decision support information to the liquid chemical leakage rescue effort. © 2014, © 2014 Antai College of Economics and Management, Shanghai Jiao Tong University."
"10.1080/23270012.2014.915130","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013071217&doi=10.1080%2f23270012.2014.915130&partnerID=40&md5=a03840680719b148e1c25635277f29f6","This paper focuses on the problem of detecting the geographical cluster with the most severe status in multiple groups of population given limited medical resources. Populations are grouped based on characteristics such as age, gender, and race. In the early stages of a disease, an outbreak may only present in specific population groups. Therefore, to efficiently detect the outbreak, we are particularly interested in monitoring and evaluating such groups. We define the objective of detection as the most severe cluster (MSC). Taking into account the interactions between population groups, a multivariate normal scan statistic is proposed to simultaneously determine the location and size of a significant MSC, as well as the specific population groups in which the MSC is located. The proposed method is applied to an example of lung cancer in New York State, where the MSC with the highest mortality rate at the aggregate level is detected. Further, the detection capacity of this method is evaluated using a simulation study based on the lung cancer example. © 2014, © 2014 Antai College of Economics and Management, Shanghai Jiao Tong University."
"10.1080/23270012.2014.969790","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84992700752&doi=10.1080%2f23270012.2014.969790&partnerID=40&md5=150d0e56476425e49a4de4247daafdd2","In this paper, a framework is developed that evaluates supermarket food safety and exposes its relative strengths and weaknesses. The proposed framework is divided into three stages. During the first stage, an index system is constructed that is associated with supermarket food safety based on the supply chain. In the second stage, the Analytic Hierarchy Process (AHP) is applied to evaluate supermarket food safety. The third stage is associated with the application of the Technique for Order Preference by Similarity to Ideal Solution (TOPSIS). The proposed framework can assist managers in comprehending the present strengths and weaknesses of their food safety. Managers can identify good practices and are able to benchmark them for improving their weaknesses. This framework also facilitates the decision makers to better understand the complex relationships of the relevant supermarket food safety factors in decision-making. © 2014, © 2014 Antai College of Economics and Management, Shanghai Jiao Tong University."
"10.1080/23270012.2014.915127","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84969344824&doi=10.1080%2f23270012.2014.915127&partnerID=40&md5=7ebe4ba6f24787d0b62e0fb1bf3ff00a","Appointment systems are used by health clinics to manage access to service providers. In such systems, a specified number of patients are scheduled in advance, but certain patients may not arrive or ‘show up’ to their appointments. The existence of no-show behaviour influences both the operational cost of the clinics and the waiting time of the patients. In this paper, we determine an optimal schedule that takes no-show behaviour into account to determine the time intervals between patients under the framework of the individual-block/variable-interval rule for minimising the overall cost of the patient waiting time, the practitioner idle time and overtime. Under the condition that the service time of each patient is exponentially distributed, we compare the results with a schedule designed for the same expected number of patients in the absence of no-shows and analyse the effect on the system performance from the perspectives of day-length, expected workload, no-show probability, ratio of overtime costs and no-golf policy. We extend our results to an equally-spaced appointment system, which is commonly used in practice. Our results show that not only do no-shows greatly affect the system performance compared with an appointment system with the same expected workload without no-shows, but they also affect the optimal scheduling behaviours in the dome-shaped distribution. In addition, overtime cannot be eliminated completely even if the day length is adequate for all patients because of the stochastic characteristic of service time. © 2014, © 2014 Antai College of Economics and Management, Shanghai Jiao Tong University."
"10.4018/ijban.2014040102","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046283785&doi=10.4018%2fijban.2014040102&partnerID=40&md5=58ac9d69ee12c332c9dd88e5f2949618","This study attempts to explore whether and how stock market responds to industrial accidents. We employ the event study method to look into the responses of stock markets to 83 accidents experienced by various listed companies in China, and explore how industrial accidents influence stock market in the different markets. Findings imply that the stock market shows negative reaction with respect to these accidents. However, as time goes by, the market reaction tapers off. In the bear market, the negative market reaction was highly significant. Small-sized companies, in comparison with other companies, have a most significant reaction to accidents and they also have the worst ability to recover from accidents. The findings of this study can help the investors to better understand how the stock market reacts to the industrial accidents in different market environments and under other conditions. © 2014,IGI Global."
"10.4018/ijban.2014040103","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046249493&doi=10.4018%2fijban.2014040103&partnerID=40&md5=906983bc0f5a3353fc9442c45847804c","Integrated quality and reliability models should be developed to improve system performance simultaneously, because quality and reliability are inherently related in a sense that quality inspection and monitoring decisions impact anticipated reliability and failure time distributions. Especially for degrading systems, important decisions including burn-in, quality inspection and preventive maintenance should be incorporated into an integratedmodelconsidering manufacturing variability andassociated failure mechanisms. For various linear and non-linear degradation models, this paper develops conditional reliability functions and truncated failure time distributions considering the impacts of burn-in and quality inspection at manufacturing phase. It shows that burn-in and quality inspection policies have significant impacts on reliability performance of products in field operation. Numerical examples are provided to demonstrate the results. The developed reliability models can be readily used for optimizing burn-in, quality inspection and maintenance decisions simultaneously. © 2014,IGI Global."
"10.4018/ijban.2014040104","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045863262&doi=10.4018%2fijban.2014040104&partnerID=40&md5=09ead5da0a01726b945002a80246b561","In conventional data envelopment analysis, it is assumed that the input versus output status of any particular performance measure is known. In some situations, finding the status of some variables from the input or output point of view is very difficult; these variables are treated as both inputs and outputs and are called flexible measures. In this paper, using the TOPSIS method, and also using a voting model, the status of such a variable will be determined, and the results obtained will be employed to evaluate the efficiency of homogeneous decision making units. Note that all the models used in this paper are linear programming models and there is no need to solve any integer programming model. The approach is illustrated by an example. © 2014,IGI Global."
"10.1089/big.2014.1517","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84991833346&doi=10.1089%2fbig.2014.1517&partnerID=40&md5=1987acfae77a5ca2c27af899831ebe43",[No abstract available]
"10.1089/big.2013.0036","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84991806369&doi=10.1089%2fbig.2013.0036&partnerID=40&md5=61811be685b791c843613a07646dcac7","This article presents a near real-time processing solution using MapReduce and Hadoop. The solution is aimed at some of the data management and processing challenges facing the life sciences community. Research into genes and their product proteins generates huge volumes of data that must be extensively preprocessed before any biological insight can be gained. In order to carry out this processing in a timely manner, we have investigated the use of techniques from the big data field. These are applied specifically to process data resulting from mass spectrometers in the course of proteomic experiments. Here we present methods of handling the raw data in Hadoop, and then we investigate a process for preprocessing the data using Java code and the MapReduce framework to identify 2D and 3D peaks. © Mary Ann Liebert, Inc. 2014."
"10.1089/big.2014.0004","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84991769717&doi=10.1089%2fbig.2014.0004&partnerID=40&md5=04e43b76f44ff0e1b3c3f6e75667c241","The importance of healthcare improvement is difficult to overstate. This article describes our collaborative work with experts at Seattle Children's to create a prioritized improvement system using performance benchmarking. We applied analytics and modeling approaches to compare and assess performance metrics derived from U.S. News and World Report benchmarking data. We then compared a wide range of departmental performance metrics, including patient outcomes, structural and process metrics, survival rates, clinical practices, and subspecialist quality. By applying empirically simulated transformations and imputation methods, we built a predictive model that achieves departments' average rank correlation of 0.98 and average score correlation of 0.99. The results are then translated into prioritized departmental and enterprise-wide improvements, following a data to knowledge to outcomes paradigm. These approaches, which translate data into sustainable outcomes, are essential to solving a wide array of healthcare issues, improving patient care, and reducing costs. © Mary Ann Liebert, Inc. 2014."
"10.1089/big.2013.0038","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84991759885&doi=10.1089%2fbig.2013.0038&partnerID=40&md5=649cff3e0a6b4dfc63521e10c45b0957","Along with the increasing availability of large databases under the purview of National Statistical Institutes, the application of data mining techniques to official statistics is now a hot topic that is far more important at present than it was ever before. Presented in this article is a thorough review of published work to date on the application of data mining in official statistics, and on identification of the techniques that have been explored. In addition, the importance of data mining to official statistics is flagged and a summary of the challenges that have hindered its development over the course of the last two decades is presented. © Mary Ann Liebert, Inc. 2014."
"10.1089/big.2013.0042","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84991746778&doi=10.1089%2fbig.2013.0042&partnerID=40&md5=82f5f34ea7431cc147d09db69e0aa811","There is much enthusiasm currently about the possibilities created by new and more extensive sources of data to better understand and manage cities. Here, I explore how big data can be useful in urban planning by formalizing the planning process as a general computational problem. I show that, under general conditions, new sources of data coordinated with urban policy can be applied following fundamental principles of engineering to achieve new solutions to important age-old urban problems. I also show that comprehensive urban planning is computationally intractable (i.e., practically impossible) in large cities, regardless of the amounts of data available. This dilemma between the need for planning and coordination and its impossibility in detail is resolved by the recognition that cities are first and foremost self-organizing social networks embedded in space and enabled by urban infrastructure and services. As such, the primary role of big data in cities is to facilitate information flows and mechanisms of learning and coordination by heterogeneous individuals. However, processes of self-organization in cities, as well as of service improvement and expansion, must rely on general principles that enforce necessary conditions for cities to operate and evolve. Such ideas are the core of a developing scientific theory of cities, which is itself enabled by the growing availability of quantitative data on thousands of cities worldwide, across different geographies and levels of development. These three uses of data and information technologies in cities constitute then the necessary pillars for more successful urban policy and management that encourages, and does not stifle, the fundamental role of cities as engines of development and innovation in human societies. © Mary Ann Liebert, Inc. 2014."
"10.1057/jma.2014.5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031508645&doi=10.1057%2fjma.2014.5&partnerID=40&md5=a6a838660a313340d3962df05cd7e45c","Representing unobserved heterogeneity or taste variations in Marketing Analytic behavioral-choice analysis is receiving increasing attention in the estimation of consumer-choice modeling. The mixed logit (MXL) model, which incorporates random coefficients into the multinomial logit model, has been widely adopted for this purpose. The most commonly adopted method in this context is to assume that the random coefficient follows a continuous, unimodal distribution, and the parameters of the distribution as well as the other parameters for the model can be obtained using maximum simulated likelihood estimation. In this article, we refer to this method as the continuous mixed logit (CMXL) model. This method requires the a priori assumption that the distribution of the random coefficient is continuous and, usually, unimodal. One way to relax this assumption is to estimate the distribution nonparametrically, by assuming a discrete distribution with finite support. We refer to this approach as the discrete mixed logit (DMXL) model. Based on the DMXL model, we propose the mass-point MXL model as one alternative to the continuousdistribution assumption and compare its performance with the latent class logit model (LCLM), also part of the DMXL family. Either model can be used to represent unobserved heterogeneity with a discrete distribution in the parameter space. In this article, we conduct empirical analyses and compare the continuous and discrete representations of unobserved heterogeneity in logit models using simulated data with known parameters and real data with discrete choices. Analysis with simulated data provides insights on the ability to distinguish between continuous and discrete parameter distributions and a better understanding of the goodness-of-fit measures used in evaluating model performance with real data. From the simulation study, we find that when the data is generated from a normal distribution, the CMXL model with the unimodal-distribution assumption is preferred to the DMXL mode. From the real data analysis, we find that the CMXL model fails to recover heterogeneity that is identified by the DMXL model. In conclusion, we suggest that when estimating a random-coefficient MXL model, one should start with a CMXL model, but should not accept a ‘no heterogeneity’ conclusion without estimating a series of DMXL models using either the mass-point MXL model or the LCLM with different starting values. © 2014 Macmillan Publishers Ltd."
"10.1057/jma.2014.4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031493368&doi=10.1057%2fjma.2014.4&partnerID=40&md5=baa4932c4a391abca5741b72168e989e","Brand management is typically defined as the way in which brands are positioned in the marketplace, both in terms of tangibles such as price, packaging and the marketing mix and intangibles such as consumer perceptions and brand equity. The conventional marketing mix model is often used to inform the tangible elements, but is lacking into two key aspects. Firstly, it ignores the role of intangibles. Secondly, the focus is solely on individual brands in isolation. This ignores the wider competitive context, where the decision to choose one brand is the simultaneous decision not to choose another. Successful brand management, however, requires a simultaneous holistic view of all players. To address both issues, this article argues for a dynamic time series version of the discrete choice attraction model. Firstly, the demand system structure treats the entire category as a single unit, capturing competitive steal, cannibalisation, halo and category expansion effects of brand-specific marketing. This provides accurate marketing return on investment and budget allocation, facilitating the manufacturer-retailer relationship. Secondly, the time series approach allows us to quantify the evolution and drivers of consumer brand tastes – critical to understanding brand intangibles. This enables managers to set marketing strategy for optimal long-term brand performance. © 2014 Macmillan Publishers Ltd."
"10.1057/jma.2014.6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031490655&doi=10.1057%2fjma.2014.6&partnerID=40&md5=8deb4341a5980d7d4c7db80308226594","This article examines how marketing executives employ target marketing strategy in business technology markets. This exploratory study evaluated market selection strategies (differentiated, single segment, segment-of-1 and undifferentiated) and target marketing success. Particular attention is paid toward choosing target markets by examining segment attractiveness criteria. The three most important criteria for target market selection were opportunities in the industry, sustainable differential advantage and profitability. The research also found that competitive analysis was a strong predictor of target marketing success. In addition, market-oriented firms (use customer needs or customer groups to define markets) are more successful in using technology than non-market-oriented firms when redefining markets. The work concludes with sections on strategic implications for business target marketing and a research agenda for segmentation scholars. © 2014 Macmillan Publishers Ltd."
"10.1057/jma.2014.1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85024491086&doi=10.1057%2fjma.2014.1&partnerID=40&md5=fb6116bf216725d86f444a33ace8603f","A core objective of customer–company relationship management is the allocation of marketing resources across customer acquisition and retention so as to maximize customer equity (CE). Although extant CE models cover much important ground, the literature continues to overlook two options commonly exercised by managers – the option to change the orientation or degree of focus of a channel on acquisition versus retention and the option to boost acquisition rates by using margin-reducing incentives such as discounts, coupons, gifts or bonuses. The present research addresses these issues by presenting a deterministic model that allows managers to determine whether and when sacrificing margins in acquisition is justified from the perspective of CE maximization. Sensitivity analyses and a real-world application with a large, private nonprofit organization in the United States illustrate the use of the model and its ability to identify an optimal CE greater than that in a competing model. © 2014 Macmillan Publishers Ltd."
"10.1057/jma.2014.2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85003930408&doi=10.1057%2fjma.2014.2&partnerID=40&md5=1fb943bc5f4a84a7398e0f74fa86c843","Loyalty programs have evolved in recent years to become a key component of customer relationship management. The creation of huge databases from these loyalty programs has created a need for methodologies capable of generating meaningful insights from analysis of the large quantities of longitudinal behavioral data flowing from them. Our research utilizes a group trajectory modeling approach to generate managerially important segments among members of a retail loyalty program based on the dynamics of their behaviors following the launch of the program. We profile the segments and discuss the managerial implications of such findings as segment sizes, spending trajectories, and possible drivers of differences among the groups, evaluating these issues relative to the effectiveness of the loyalty program design. © 2014 Macmillan Publishers Ltd."
"10.2481/dsj.12-048","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893259900&doi=10.2481%2fdsj.12-048&partnerID=40&md5=3b2f3e4cb630ac60013bbf85f4388850","Examining the scientific process in relation to endangered data, data reuse, and sharing is crucial in facilitating scientific workflow. Deterioration, format obsolescence, and insufficient metadata for discovery are significant problems leading to loss of scientific data. The research presented in this paper considers these potentially lost data. Four one-hour focus groups and a demographic survey were conducted with 14 scientists to learn about their attitudes toward endangered data, data sharing, data reuse, and their opinions of the DARI inventory. The results indicate that unavailability, lack of context, accessibility issues, and potential endangerment are key concerns to scientists."
"10.2481/dsj.12-058","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893279529&doi=10.2481%2fdsj.12-058&partnerID=40&md5=06bc12104f8a8a18f58212985c0c71d0","Persistent Identifiers (PIDs) have lately received a lot of attention from scientific infrastructure projects and communities that aim to employ them for management of massive amounts of research data and metadata objects. Such usage scenarios, however, require additional facilities to enable automated data management with PIDs. In this article, we present a conceptual framework that is based on the idea of using common abstract data types (ADTs) in combination with PIDs. This provides a well-defined interface layer that abstracts from both underlying PID systems and higher-level applications. Our practical implementation is based on the Handle System, yet the fundamental concept of PID-based ADTs is transferable to other infrastructures, and it is well suited to achieve interoperability between them."
"10.2481/dsj.12-047","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893218171&doi=10.2481%2fdsj.12-047&partnerID=40&md5=cd48ea7a027891c5462ec9ddef1451a5","Materials failure indicates the fault with materials or components during their performance. To avoid the reoccurrence of similar failures, materials failure analysis is executed to investigate the reasons for the failure and to propose improved strategies. The whole procedure needs sufficient domain knowledge and also produces valuable new knowledge. However, the information about the materials failure analysis is usually retained by the domain expert, and its sharing is technically difficult. This phenomenon may seriously reduce the efficiency and decrease the veracity of the failure analysis. To solve this problem, this paper adopts ontology, a novel technology from the Semantic Web, as a tool for knowledge representation and sharing and describes the construction of the ontology to obtain information concerning the failure analysis, application area, materials, and failure cases. The ontology represented information is machine-understandable and can be easily shared through the Internet. At the same time, failure case intelligent retrieval, advanced statistics, and even automatic reasoning can be accomplished based on ontology represented knowledge. Obviously this can promote the knowledge sharing of materials service safety and improve the efficiency of failure analysis. The case of a nuclear power plant area is presented to show the details and benefits of this method."
"10.1080/23270012.2014.889929","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030417450&doi=10.1080%2f23270012.2014.889929&partnerID=40&md5=54786c211ff6969ecc204b1ab8a51a97","This paper explores the time-varying relationship between the return and risk for the portfolios in the US real estate investment trusts (REITs) market. Three classes of REITs portfolios are sorted on size, momentum and book-to-market. The empirical results place a positive trade-off between the return and predictable covariation in all REITs portfolios. Furthermore, this paper examines the significance of intertemporal hedging demand in each REITs portfolio by extending the Intertemporal Capital Asset Pricing model (ICAPM) with a set of prevailing macroeconomic variables and financial market indicators, which indicate that the predictable movements in return could be attributed to the change in covariances with these innovations. The conclusive results show that innovations in inflation rate, de-trended short-term interest rate, Fama-French momentum factor, S&P/Case-Shiller home price index, and Barclay Capital long-term government/corporate bond index play a crucial role in hedging demand for REITs portfolios. © 2014, © 2014 Antai College of Economics and Management, Shanghai Jiao Tong University."
"10.1080/23270012.2014.889930","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017141022&doi=10.1080%2f23270012.2014.889930&partnerID=40&md5=750711477bb2ca68051f1c9811bfbff4","Brand extension is one of the central issues in marketing research. The current study employs event-related potentials (ERPs) to investigate the electrophysiological process when subjects make a decision to accept or reject a brand extension composed of two words (W1W2). W1 is always a beverage brand name, and W2 a product name among three product categories: household appliance product (extension type BH), snack product (BS), and beverage product (BB). The behavior and ERPs data showed that subjects found it easy to accept the brand extension in the BB condition, medium in the BS and hard in the BH situation. The negative slow waves in different cortexes suggested that there are two cognitive conflict processes in evaluating a brand extension in the W1W2 paradigm: the conflict between W1 and W2, and the conflict between the whole information of W1W2 and the information retrieved from memory about the products under the brand name in W1. © 2014, © 2014 Antai College of Economics and Management, Shanghai Jiao Tong University."
"10.1080/23270012.2014.889911","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85003649699&doi=10.1080%2f23270012.2014.889911&partnerID=40&md5=a059542073b8aa4345de8127944c8685","Outlier detection is one of the key issues in any data-driven analytics. In this paper, we propose Bi-super DEA, a super DEA-based method that constructs both efficient and inefficient frontiers for outlier detection. In evaluating its predictive performance, we develop a novel predictive DEA procedure, PDEA, which extends the conventional DEA approaches that have been primarily used for in-sample efficiency estimation, to predict outputs for the out-of-sample. This enables us to compare the predictive performance of our approach against several popular outlier detection methods including the parametric robust regression in statistics and non-parametric k-means in data mining. We conduct comprehensive simulation experiments to examine the relative performance of these outlier detection methods under the influence of five factors: sample size, linearity of production function, normality of noise distribution, homogeneity of data, and levels of random noise contaminating the data generating process (DGP). We find that, somewhat surprisingly, Bi-super CCR consistently outperforms Bi-super BCC in detecting outliers. Under the linearity, normality and homogeneity conditions, the parametric robust regression method works best. However, when the DGP violates these conditions, Bi-super DEA emerges as the better choice due to its distribution-free property. Our results shed light on the conditions that each method excels or fails and provide users with practical guidelines on how to choose appropriate methods to detect outliers. © 2014, © 2014 Antai College of Economics and Management, Shanghai Jiao Tong University."
"10.1080/23270012.2014.889912","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84992733730&doi=10.1080%2f23270012.2014.889912&partnerID=40&md5=d15fe8ee757a1de8a6ba544a619db925","The purpose of this paper is to report on a new system-theoretic based methodology and corresponding model for Enterprise Architecture development. This model captures the essence of the strategic, conceptual, doctrinal layer of the organization. Reusable Quality Technical Architectures (RQ-Tech) graphically reveals a comprehensive array of enterprise decision alternatives in easily understandable views; all while maintaining the hyperlinks to its provenance in strategic authoritative documentation. The RQ-Tech method has combined the practice of Enterprise Architectures with a modern perspective grounded in Systems Theory and the theory regarding the computer science-oriented Semantic Web. This recombination results in a distinctive methodology for developing models. This new methodology supports the conclusion that system-specific solutions produce islands of technology and can be prevented by employing better enterprise change planning. A review of the literature in three major areas illustrates the overlap common to all three domains. This review provides support for critical thinking concerning how to enrich the Enterprise Architecture practice. The research centered on finding the most significant factors to consider when translating the authoritative text and rich pictures that enterprise managers use to describe the strategic mission and vision of their complex, service-oriented enterprise into user-oriented semantic models. It provides the basis for RQ-Tech as a methodology that enables increased understanding of complex systems through use of Semantic Web standards. © 2014, © 2014 Antai College of Economics and Management, Shanghai Jiao Tong University."
"10.1080/23270012.2014.889915","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84979858157&doi=10.1080%2f23270012.2014.889915&partnerID=40&md5=ef957b655c9015b3f2cab1fe1ae74f10","Associative classification has attracted remarkable research attention for business analytics in recent years due to its merits in accuracy and understandability. It is deemed meaningful to construct an associative classifier with a compact set of rules (i.e., compactness), which is easy to understand and use in decision making. This paper presents a novel approach to fuzzy associative classification (namely Gain-based Fuzzy Rule-Covering classification, GFRC), which is a fuzzy extension of an effective classifier GARC. In GFRC, two desirable strategies are introduced to enhance the compactness with accuracy. One strategy is fuzzy partitioning for data discretization to cope with the ‘sharp boundary problem’, in that simulated annealing is incorporated based on the information entropy measure; the other strategy is a data-redundancy resolution coupled with the rule-covering treatment. Data experiments show that GFRC had good accuracy, and was significantly advantageous over other classifiers in compactness. Moreover, GFRC is applied to a real-world case for predicting the growth of sellers in an electronic marketplace, illustrating the classification effectiveness with linguistic rules in business decision support. © 2014, © 2014 Antai College of Economics and Management, Shanghai Jiao Tong University."
"10.1016/j.bdr.2014.07.004","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84918842618&doi=10.1016%2fj.bdr.2014.07.004&partnerID=40&md5=96ad9cf3dffbfe1090c1aea08d65ce79","In many real-world applications, data is collected in high dimensional spaces. However, not all dimensions are relevant for data analysis. Instead, interesting knowledge is hidden in correlated subsets of dimensions (i.e., subspaces of the original space). Detecting these correlated subspaces independently of the underlying mining task is an open research problem. It is challenging due to the exponential search space. Existing methods have tried to tackle this by utilizing Apriori search schemes. However, their worst case complexity is exponential in the number of dimensions; and even in practice they show poor scalability while missing high quality subspaces.This paper features a scalable subspace search scheme (4S), which overcomes the efficiency problem by departing from the traditional levelwise search. We propose a new generalized notion of correlated subspaces which gives way to transforming the search space to a correlation graph of dimensions. We perform a direct mining of correlated subspaces in this graph, and then, merge subspaces based on the MDL principle in order to obtain high dimensional subspaces with minimal redundancy. We theoretically show that our search scheme is more general than existing search schemes. Our empirical results reveal that 4. S in practice scales near-linearly with both database size and dimensionality, and produces higher quality subspaces than state-of-the-art methods. © 2014 Elsevier Inc."
"10.1016/j.bdr.2014.07.003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84918813362&doi=10.1016%2fj.bdr.2014.07.003&partnerID=40&md5=a966d5dfdd1a86e014848dea770091e3","The skyline query as an important aspect of big data management, has received considerable attention from the database community, due to its importance in many applications including multi-criteria decision making, preference answering, and so forth. Moreover, the uncertain data from many applications have become increasing distributed, which makes the central assembly of data at one location for storage and query infeasible and inefficient. The lack of global knowledge and the computational complexity derived from the introduction of the data uncertainty make the skyline query over distributed uncertain data extremely challenging. Although many efforts have addressed the skyline query problem over various distributed scenarios, existing studies still lack the approaches to efficiently process the query. In this paper, we extensively study the distributed probabilistic skyline query problem and propose an efficient approach GDPS to address the problem with an optimized iterative feedback mechanism based on the grid summary. Furthermore, many strategies for further optimizing the query are also proposed, including the optimization strategies for the local pruning, tuple selecting and the server pruning. Extensive experiments on real and synthetic data sets have been conducted to verify the effectiveness and efficiency of our approach by comparing with the state-of-the-art approaches. © 2014 Elsevier Inc."
"10.1016/j.bdr.2014.07.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84918806319&doi=10.1016%2fj.bdr.2014.07.001&partnerID=40&md5=96276281e3a9790cf4b284b19120934a","Increasingly larger scale applications are generating an unprecedented amount of data. However, the increasing gap between computation and I/O capacity on High End Computing machines makes a severe bottleneck for data analysis. Instead of moving data from its source to the output storage, in-situ analytics processes output data while simulations are running. However, in-situ data analysis incurs much more computing resource contentions with simulations. Such contentions severely damage the performance of simulation on HPE. Since different data processing strategies have different impact on performance and cost, there is a consequent need for flexibility in the location of data analytics. In this paper, we explore and analyze several potential data-analytics placement strategies along the I/O path. To find out the best strategy to reduce data movement in given situation, we propose a flexible data analytics (FlexAnalytics) framework in this paper. Based on this framework, a FlexAnalytics prototype system is developed for analytics placement. FlexAnalytics system enhances the scalability and flexibility of current I/O stack on HEC platforms and is useful for data pre-processing, runtime data analysis and visualization, as well as for large-scale data transfer. Two use cases - scientific data compression and remote visualization - have been applied in the study to verify the performance of FlexAnalytics. Experimental results demonstrate that FlexAnalytics framework increases data transition bandwidth and improves the application end-to-end transfer performance. © 2014 Elsevier Inc.."
"10.1016/j.bdr.2014.07.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84918796928&doi=10.1016%2fj.bdr.2014.07.002&partnerID=40&md5=d96f0a7c89ed7dc1acbd44b338ff02b9","Cloud computing is a type of parallel distributed computing system that has become a frequently used computer application. MapReduce is an effective programming model used in cloud computing and large-scale data-parallel applications. Hadoop is an open-source implementation of the MapReduce model, and is usually used for data-intensive applications such as data mining and web indexing. The current Hadoop implementation assumes that every node in a cluster has the same computing capacity and that the tasks are data-local, which may increase extra overhead and reduce MapReduce performance. This paper proposes a data placement algorithm to resolve the unbalanced node workload problem. The proposed method can dynamically adapt and balance data stored in each node based on the computing capacity of each node in a heterogeneous Hadoop cluster. The proposed method can reduce data transfer time to achieve improved Hadoop performance. The experimental results show that the dynamic data placement policy can decrease the time of execution and improve Hadoop performance in a heterogeneous cluster. © 2014 Elsevier Inc.."
"10.1016/j.bdr.2014.07.005","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84918793619&doi=10.1016%2fj.bdr.2014.07.005&partnerID=40&md5=ba6282bfdd4437c15e3842ce7fa655d1","As the number of sensors that pervade our lives increases (e.g., environmental sensors, phone sensors, etc.), the efficient management of massive amount of sensor data is becoming increasingly important. The infinite nature of sensor data poses a serious challenge for query processing even in a cloud infrastructure. Traditional raw sensor data management systems based on relational databases lack scalability to accommodate large-scale sensor data efficiently. Thus, distributed key-value stores in the cloud are becoming a prime tool to manage sensor data. Model-view sensor data management, which stores the sensor data in the form of modeled segments, brings the additional advantages of data compression and value interpolation. However, currently there are no techniques for indexing and/or query optimization of the model-view sensor data in the cloud; full table scan is needed for query processing in the worst case. In this paper, we propose an innovative index for modeled segments in key-value stores, namely KVI-index. KVI-index consists of two interval indices on the time and sensor value dimensions respectively, each of which has an in-memory search tree and a secondary list materialized in the key-value store. Then, we introduce a KVI-index-Scan-MapReduce hybrid approach to perform efficient query processing upon modeled data streams. As proved by a series of experiments at a private cloud infrastructure, our approach outperforms in query-response time and index-updating efficiency both Hadoop-based parallel processing of the raw sensor data and multiple alternative indexing approaches of model-view data. © 2014 Elsevier Inc."
"10.2481/dsj.14-042","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988311475&doi=10.2481%2fdsj.14-042&partnerID=40&md5=88b1cce1fb68da8860841d2a2c6feeda","The rapid growth in the volume of remote sensing data and its increasing computational requirements bring huge challenges for researchers as traditional systems cannot adequately satisfy the huge demand for service. Cloud computing has the advantage of high scalability and reliability, which can provide firm technical support. This paper proposes a highly scalable geospatial cloud platform named the Geospatial Data Cloud, which is constructed based on cloud computing. The architecture of the platform is first introduced, and then two subsystems, the cloud-based data management platform and the cloud-based data processing platform, are described. © 2014, Committee on Data for Science and Technology. All rights reserved."
"10.1140/epjds30","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84986331399&doi=10.1140%2fepjds30&partnerID=40&md5=d9e353459597305dacfa6893a658d66c","Evolution of online social networks is driven by the need of their members to share and consume content, resulting in a complex interplay between individual activity and attention received from others. In a context of increasing information overload and limited resources, discovering which are the most successful behavioral patterns to attract attention is very important. To shed light on the matter, we look into the patterns of activity and popularity of users in the Yahoo Meme microblogging service. We observe that a combination of different type of social and content-producing activity is necessary to attract attention and the efficiency of users, namely the average attention received per piece of content published, for many users has a defined trend in its temporal footprint. The analysis of the user time series of efficiency shows different classes of users whose different activity patterns give insights on the type of behavior that pays off best in terms of attention gathering. In particular, sharing content with high spreading potential and then supporting the attention raised by it with social activity emerges as a frequent pattern for users gaining efficiency over time. © 2014 Vaca Ruiz et al."
"10.1140/epjds21","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84933558146&doi=10.1140%2fepjds21&partnerID=40&md5=657a2aeccbb5291e8737ad6ffc66b9c1","The newly released Orange D4D mobile phone data base provides new insights into the use of mobile technology in a developing country. Here we perform a series of spatial data analyses that reveal important geographic aspects of mobile phone use in Cote d’Ivoire. We first map the locations of base stations with respect to the population distribution and the number and duration of calls at each base station. On this basis, we estimate the energy consumed by the mobile phone network. Finally, we perform an analysis of inter-city mobility, and identify high-traffic roads in the country. © 2014 Salnikov et al.; licensee Springer."
"10.1140/epjds/s13688-014-0031-z","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84933533294&doi=10.1140%2fepjds%2fs13688-014-0031-z&partnerID=40&md5=8fb731af313e2c8e16a244b99ebca05f","The Boston Marathon bombing presents a rare opportunity to study how a disruptive event can trigger inter-communal emotions and expressions - where members of one community express feelings about and support for members of a distant community. In this work, we use over 180 million geo coded tweets over an entire month to study how Twitter users from different cities expressed three different emotions: fear, sympathy and solidarity, in reaction to the bombings. We capture spikes in fear in different cities by using sentiment and time-series analyses, and track expressions of sympathy and solidarity based on the emergent use of two hash tags, # prayforboston and #bostonstrong , widely adopted after the bombings. We find first that the extent to which communities outside Boston express these emotions is correlated with their geographic proximity, social network connections, and residents’ direct, physical experiences with Boston (captured by the number of citizens who had visited Boston recently). This general effect shows interesting differences across the different kinds of emotions, however. Analyses show that the extent to which residents of a city visit Boston is the best predictor of fear and solidarity expression, as well as a strong predictor of the expression of sympathy. The expression of fear is also directly related to the expression of solidarity. Our study has theoretical implications regarding the diffusion of information and emotional contagion as well as practical implications for understanding how important information and social support can be effectively collected and distributed to populations in need."
"10.1140/epjds/s13688-014-0029-6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84933528672&doi=10.1140%2fepjds%2fs13688-014-0029-6&partnerID=40&md5=81fc5e5341d3532841a83a06e20b947c","We studied a dataset of care episodes in a regional Swedish hospital system. We followed how 2,314,477 patients moved between 8,507 units (hospital wards and outpatient clinics) over seven years. The data also included information on the date when patients tested positive with methicillin resistant Staphylococcus aureus. To simplify the complex flow of patients, we represented it as a network of units, where two units were connected if a patient moved from one unit to another, without visiting a third unit in between. From this network, we characterized the typical network position of units with a high prevalence of methicillin resistant Staphylococcus aureus, and how the patient’s location in the network changed upon testing positive. On average, units with medium values of the analyzed centrality measures had the highest average prevalence. We saw a weak effect of the hospital system’s response to the patient testing positive - after a positive test, the patient moved to units with a lower centrality measured as degree (i.e. number of links to other units) and in addition, the average duration of the care episodes became longer. The network of units was too random to be a strong predictor of the presence of methicillin resistant Staphylococcus aureus - would it be more regular, one could probably both identify and control outbreaks better. The migration of the positive patients with within the healthcare system, however, helps decreasing the outbreak sizes. © 2014 Ohst et al."
"10.1140/epjds/s13688-015-0040-6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84933527134&doi=10.1140%2fepjds%2fs13688-015-0040-6&partnerID=40&md5=3bb3901c05aa2b2709281677c0d90f2f","Spatial variations in the distribution and composition of populations inform urban development, health-risk analyses, disaster relief, and more. Despite the broad relevance and importance of such data, acquiring local census estimates in a timely and accurate manner is challenging because population counts can change rapidly, are often politically charged, and suffer from logistical and administrative challenges. These limitations necessitate the development of alternative or complementary approaches to population mapping. In this paper we develop an explicit connection between telecommunications data and the underlying population distribution of Milan, Italy. We go on to test the scale invariance of this connection and use telecommunications data in conjunction with high-resolution census data to create easily updated and potentially real time population estimates in time and space. © 2015 Douglass et al.; licensee Springer."
"10.1140/epjds/s13688-014-0034-9","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84933524858&doi=10.1140%2fepjds%2fs13688-014-0034-9&partnerID=40&md5=29acb95fcf0831ca0b42ee03039b2eb7","The application of functional analysis to infer networks in large datasets is potentially helpful to experimenters in various fields. In this paper, we develop a technique to construct networks of statistically significant transitions between variable pairs from a high-dimensional and multiscale dataset of teaching practices observed in Grade 5 and Grade 9 Mathematics classes obtained by the National Institute of Education in Singapore. From the Minimum Spanning Trees (MST) and Planar Maximally Filtered Graphs (PMFG) of the transition networks, we establish that teaching knowledge as truth and teacher-dominated talking serve as hubs for teaching practices in Singapore. These practices reflect a transmissionist model of teaching and learning. We also identify complex teacher-student-teacher-student interaction sequences of teaching practices that are over-represented in the data. © 2014 Goh et al."
"10.1140/epjds/s13688-014-0032-y","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84933523370&doi=10.1140%2fepjds%2fs13688-014-0032-y&partnerID=40&md5=0d3d2535d571220695cbf7605f5ece5c","Communication through social media mediates coordination and information diffusion across a range of social settings. However, online networks are large and complex, and their analysis requires new methods to summarize their structure and identify nodes holding relevant positions. We propose a method that generalizes the sociological theory of brokerage, originally devised on the basis of local transitivity and paths of length two, to make it applicable to larger, more complex structures. Our method makes use of the modular structure of networks to define brokerage at the local and global levels. We test the method with two different data sets. The findings show that our approach is better at capturing role differences than alternative approaches that only consider local or global network features. © 2014 González-Bailón et al."
"10.1140/epjds/s13688-014-0008-y","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84933520946&doi=10.1140%2fepjds%2fs13688-014-0008-y&partnerID=40&md5=6df04c134e91fe3758b3e48ea9edbc60","Social groups play a crucial role in online social media because they form the basis for user participation and engagement. Although widely studied in their static and evolutionary aspects, no much attention has been devoted to the exploration of the nature of groups. In fact, groups can originate from different aggregation processes that may be determined by several orthogonal factors. A key question in this scenario is whether it is possible to identify the different types of groups that emerge spontaneously in online social media and how they differ. We propose a general framework for the characterization of groups along the geographical, temporal, and socio-topical dimensions and we apply it on a very large dataset from Flickr. In particular, we define a new metric to account for geographic dispersion, we use a clustering approach on activity traces to extract classes of different temporal footprints, and we transpose the “common identity and common bond” theory into metrics to identify the skew of a group towards sociality or topicality. We directly validate the predictions of the sociological theory showing that the metrics are able to forecast with high accuracy the group type when compared to a human-generated ground truth. Last, we frame our contribution into a wider context by putting in relation different types of groups with communities detected algorithmically on the social graph and by showing the effect that the group type might have on processes of information diffusion. Results support the intuition that a more nuanced description of groups could improve not only the understanding of the activity of the user base but also the interpretation of other phenomena occurring on social graphs. ©2014Martin-Borregon et al."
"10.1140/epjds/s13688-014-0009-x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84933519374&doi=10.1140%2fepjds%2fs13688-014-0009-x&partnerID=40&md5=42a0463d87043aaffec04bcfa67242f4","We address the question to what extent the success of scientific articles is due to social influence. Analyzing a data set of over 100,000 publications from the field of Computer Science, we study how centrality in the coauthorship network differs between authors who have highly cited papers and those who do not. We further show that a Machine Learning classifier, based only on coauthorship network centrality metrics measured at the time of publication, is able to predict with high precision whether an article will be highly cited five years after publication. By this we provide quantitative insight into the social dimension of scientific publishing – challenging the perception of citations as an objective, socially unbiased measure of scientific success. © 2014 Sarigöl et al."
"10.1140/epjds/s13688-014-0007-z","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84933517793&doi=10.1140%2fepjds%2fs13688-014-0007-z&partnerID=40&md5=375e2d6f62b05f9ae1b71d867ecf35f6","Wikipedia is a prime example of today’s value production in a collaborative environment. Using this example, we model the emergence, persistence and resolution of severe conflicts during collaboration by coupling opinion formation with article editing in a bounded confidence dynamics. The complex social behavior involved in editing articles is implemented as a minimal model with two basic elements; (i) individuals interact directly to share information and convince each other, and (ii) they edit a common medium to establish their own opinions. Opinions of the editors and that represented by the article are characterised by a scalar variable. When the pool of editors is fixed, three regimes can be distinguished: (a) a stable mainstream article opinion is continuously contested by editors with extremist views and there is slow convergence towards consensus, (b) the article oscillates between editors with extremist views, reaching consensus relatively fast at one of the extremes, and (c) the extremist editors are converted very fast to the mainstream opinion and the article has an erratic evolution. When editors are renewed with a certain rate, a dynamical transition occurs between different kinds of edit wars, which qualitatively reflect the dynamics of conflicts as observed in real Wikipedia data. © 2014 Iñiguez et al"
"10.1140/epjds/s13688-014-0013-1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84933517429&doi=10.1140%2fepjds%2fs13688-014-0013-1&partnerID=40&md5=d15c992c2c96d935fdebcd288ae795a1","The Internet has unleashed the capacity for planetary-scale collective problem solving (also known as crowdsourcing), with ever increasing successful examples. A key hypothesis behind crowdsourcing is that, at a critical mass of participation, it has the capacity not only to agglomerate and coordinate individual contributions from thousands of individuals, but also to filter out erroneous contributions, and even malicious behavior. Mixed evidence on this front has arisen from limited observational data and controlled laboratory experiments with problems of moderate difficulty. We analyze behavioral data from our participation in the DARPA Shredder Challenge, an NP-hard combinatorial puzzle beyond computational reach, which involved 3,500 participants from five continents over three consecutive weeks. We study thousands of erroneous contributions and a number of large-scale attacks, and quantify the extent to which the crowd was able to detect, react, and recover from them. Whereas the crowd is able to self-organize to recover from errors, we observe that participants are (i) unable to contain malicious behavior (attacks) and (ii)the attacks displayed persistence over the subsequent participants, manifested in decreased participation and reduced problem solving efficiency. Our results raise caution in the application of crowd sourced problem solving for sensitive tasks involving Financial Markets and National Security"
"10.1140/epjds/s13688-014-0030-0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84933516842&doi=10.1140%2fepjds%2fs13688-014-0030-0&partnerID=40&md5=3a257a3f37e5868259792def65e7ad87","Traded corporations are required by law to have a majority of outside directors on their board. This requirement allows the existence of directors who sit on the board of two or more corporations at the same time, generating what is commonly known as interlocking directorates. While research has shown that networks of interlocking directorates facilitate the transmission of information between corporations, little is known about the extent to which such interlocking networks can explain the fluctuations of stock price returns. Yet, this is a special concern since the risk of amplifying stock fluctuations is latent. To answer this question, here we analyze the board composition, traders’ perception, and stock performance of more than 1,500 US traded corporations from 2007-2011. First, we find that the fewer degrees of separation between two corporations in the interlocking network, the stronger the temporal correlation between their stock price returns. Second, we find that the centrality of traded corporations in the interlocking network correlates with the frequency at which financial traders talk about such corporations, and this frequency is in turn proportional to the corresponding traded volume. Third, we show that the centrality of corporations was negatively associated with their stock performance in 2008, the year of the big financial crash. These results suggest that the strategic decisions made by interlocking directorates are strongly followed by stock analysts and have the potential to correlate and amplify the movement of stock prices during financial crashes. These results may have relevant implications for scholars, investors, and regulators. © 2014 Saavedra et al."
"10.1140/epjds/s13688-014-0027-8","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84933513751&doi=10.1140%2fepjds%2fs13688-014-0027-8&partnerID=40&md5=ccad88765bfcd199ef7b9dbf2664693e","Detecting and visualizing what are the most relevant changes in an evolving network is an open challenge in several domains. We present a fast algorithm that filters subsets of the strongest nodes and edges representing an evolving weighted graph and visualize it by either creating a movie, or by streaming it to an interactive network visualization tool. The algorithm is an approximation of exponential sliding time-window that scales linearly with the number of interactions. We compare the algorithm against rectangular and exponential sliding time-window methods. Our network filtering algorithm: (i) captures persistent trends in the structure of dynamic weighted networks, (ii) smoothens transitions between the snapshots of dynamic network, and (iii) uses limited memory and processor time. The algorithm is publicly available as open-source software. © 2014 Grabowicz et al."
"10.1140/epjds/s13688-014-0010-4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84933513624&doi=10.1140%2fepjds%2fs13688-014-0010-4&partnerID=40&md5=6027663b8d1e07eeb88a5cb442efde66","Privacy is ever-growing concern in our society and is becoming a fundamental aspect to take into account when one wants to use, publish and analyze data involving human personal sensitive information. Unfortunately, it is increasingly hard to transform the data in a way that it protects sensitive information: we live in the era of big data characterized by unprecedented opportunities to sense, store and analyze social data describing human activities in great detail and resolution. As a result, privacy preservation simply cannot be accomplished by de-identification alone. In this paper, we propose the privacy-by-design paradigm to develop technological frameworks for countering the threats of undesirable, unlawful effects of privacy violation, without obstructing the knowledge discovery opportunities of social mining and big data analytical technologies. Our main idea is to inscribe privacy protection into the knowledge discovery technology by design, so that the analysis incorporates the relevant privacy requirements from the start. © 2014 Monreale et al.; licensee Springer."
"10.1140/epjds/s13688-014-0033-x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84933508976&doi=10.1140%2fepjds%2fs13688-014-0033-x&partnerID=40&md5=197789f5eb23f9c5de1eae7b15e88f50","Aim of this paper is to introduce the complex system perspective into retail market analysis. Currently, to understand the retail market means to search for local patterns at the micro level, involving the segmentation, separation and profiling of diverse groups of consumers. In other contexts, however, markets are modelled as complex systems. Such strategy is able to uncover emerging regularities and patterns that make markets more predictable, e.g. enabling to predict how much a country’s GDP will grow. Rather than isolate actors in homogeneous groups, this strategy requires to consider the system as a whole, as the emerging pattern can be detected only as a result of the interaction between its self-organizing parts. This assumption holds also in the retail market: each customer can be seen as an independent unit maximizing its own utility function. As a consequence, the global behaviour of the retail market naturally emerges, enabling a novel description of its properties, complementary to the local pattern approach. Such task demands for a data-driven empirical framework. In this paper, we analyse a unique transaction database, recording the micro-purchases of a million customers observed for several years in the stores of a national supermarket chain. We show the emergence of the fundamental pattern of this complex system, connecting the products’ volumes of sales with the customers’ volumes of purchases. This pattern has a number of applications. We provide three of them. By enabling us to evaluate the sophistication of needs that a customer has and a product satisfies, this pattern has been applied to the task of uncovering the hierarchy of needs of the customers, providing a hint about what is the next product a customer could be interested in buying and predicting in which shop she is likely to go to buy it. © 2014 Pennacchioli et al.;"
"10.1140/epjds/s13688-014-0037-6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84933507145&doi=10.1140%2fepjds%2fs13688-014-0037-6&partnerID=40&md5=a84667fde7af4b5a50aef48f6577b326","The goal of this thematic series is to provide a discussion venue about recent advances in the study of networks and their applications to the study of collective behavior in socio-technical systems. The series includes contributions exploring the intersection between data-driven studies of complex networks and agent-based models of collective social behavior. Particular attention is devoted to topics aimed at understanding social behavior through the lens of data about technology-mediated communication. These include: modeling social dynamics of attention and collaboration, characterizing online group formation and evolution, and studying the emergence of roles and interaction patterns in social media environments. © 2014 Ciampaglia et al.; licensee Springer."
"10.1140/epjds/s13688-014-0025-x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84933506510&doi=10.1140%2fepjds%2fs13688-014-0025-x&partnerID=40&md5=daa826dd734c0f3c9107d0149d4a9ce0","The quantitative study of violent conflict and its mechanisms has in recent years greatly benefited from the availability of detailed event data. With a number of highly visible studies both in the natural sciences and in political science using such data to shed light on the complex mechanisms underlying violent conflict, researchers have recently raised issues of systematic (reporting) biases. While many sources of bias are qualitatively known, biases in event data are usually not studied with quantitative methods. In this study we focus on a unique case - the conflict in Iraq - that is covered by two independently collected datasets: Iraq Body Count (IBC) reports of civilian casualties and Significant Action (SIGACT) military data. We systematically identify a number of key quantitative differences between the event reporting in the two datasets and demonstrate that even for subsets where both datasets are most consistent at an aggregate level, the daily time series and timing signatures of events differ significantly. This suggests that at any level of analysis the choice of dataset may substantially affect any inferences drawn, with attendant consequences for a number of recent studies of the conflict in Iraq. We further outline how the insights gained from our analysis of conflict event data have broader implications for studies using similar data on other social processes. © 2014 Donnay and Filimonov"
"10.1140/epjds/s13688-014-0012-2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84933504911&doi=10.1140%2fepjds%2fs13688-014-0012-2&partnerID=40&md5=a93c1e57a0b40a3998b7c05578821912","One might think that, compared to traditional media, social media sites allow people to choose more freely what to read and what to share, especially for politically oriented news. However, reading and sharing habits originate from deeply ingrained behaviors that might be hard to change. To test the extent to which this is true, we propose a Political News Sharing (PoNS) model that holistically captures four key aspects of social psychology: gratification, selective exposure, socialization, and trust & intimacy. Using real instances of political news sharing in Twitter, we study the predictive power of these features. As one might expect, news sharing heavily depends on what one likes and agrees with (selective exposure). Interestingly, it also depends on the credibility of a news source, i.e., whether the source is a social media friend or a news outlet (trust & intimacy) as well as on the informativeness or the enjoyment of the news article (gratification). Finally, a Twitter user tends to share articles matching his own political leaning but, at times, the user also shares politically opposing articles, if those match the leaning of his followers (socialization). Based on our PoNS model, we build a prototype of a news sharing application that promotes serendipitous political readings along our four dimensions. © 2014 An et al.; licensee Springer."
"10.1140/epjds/s13688-014-0026-9","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84933501670&doi=10.1140%2fepjds%2fs13688-014-0026-9&partnerID=40&md5=8590f5417a226dbb1403323b39c60457","From many datasets gathered in online social networks, well defined community structures have been observed. A large number of users participate in these networks and the size of the resulting graphs poses computational challenges. There is a particular demand in identifying the nodes responsible for information flow between communities; for example, in temporal Twitter networks edges between communities play a key role in propagating spikes of activity when the connectivity between communities is sparse and few edges exist between different clusters of nodes. The new algorithm proposed here is aimed at revealing these key connections by measuring a node’s vicinity to nodes of another community. We look at the nodes which have edges in more than one community and the locality of nodes around them which influence the information received and broadcasted to them. The method relies on independent random walks of a chosen fixed number of steps, originating from nodes with edges in more than one community. For the large networks that we have in mind, existing measures such as betweenness centrality are difficult to compute, even with recent methods that approximate the large number of operations required. We therefore design an algorithm that scales up to the demand of current big data requirements and has the ability to harness parallel processing capabilities. The new algorithm is illustrated on synthetic data, where results can be judged carefully, and also on a real, large scale Twitter activity data, where new insights can be gained. © 2014 Mantzaris; licensee Springer."
"10.1140/epjds/s13688-014-0028-7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84933499388&doi=10.1140%2fepjds%2fs13688-014-0028-7&partnerID=40&md5=9e2ba075e5e2b327fd2d37055b1984d2","This paper addresses the need of characterizing system instability toward critical transitions in complex systems. We propose a novel information dynamic spectrum framework and a probabilistic light cone method to automate the analysis. Our framework uniquely investigates heterogeneously networked dynamical systems with transient directional influences, which subsumes unidirectional diffusion dynamics. When the observed instability of a system deviates from the prediction, the method automatically indicates the approach of an upcoming critical transition. We provide several demonstrations in engineering, economics, and social systems. The results suggest that early detecting critical transitions of synchronizations, sudden collapse, and exponential growth is possible. © 2014 Ni and Lu; licensee Springer."
"10.1140/epjds/s13688-014-0036-7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84933496696&doi=10.1140%2fepjds%2fs13688-014-0036-7&partnerID=40&md5=3173822da36c31760264dbf72e7df061","Food is a central element of humans’ life, and food preferences are amongst others manifestations of social, cultural and economic forces that influence the way we view, prepare and consume food. Historically, data for studies of food preferences stems from consumer panels which continuously capture food consumption and preference patterns from individuals and households. In this work we look at a new source of data, i.e., server log data from a large recipe platform on the World Wide Web, and explore its usefulness for understanding online food preferences. The main findings of this work are: (i) recipe preferences are partly driven by ingredients, (ii) recipe preference distributions exhibit more regional differences than ingredient preference distributions, and (iii) weekday preferences are clearly distinct from weekend preferences. © 2014 Wagner et al."
"10.1140/epjds29","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84930940517&doi=10.1140%2fepjds29&partnerID=40&md5=1f07fb52e12eacb7305eada322d10363","Despite growing interest in quantifying and modeling the scoring dynamics within professional sports games, relative little is known about what patterns or principles, if any, cut across different sports. Using a comprehensive data set of scoring events in nearly a dozen consecutive seasons of college and professional (American) football, professional hockey, and professional basketball, we identify several common patterns in scoring dynamics. Across these sports, scoring tempo - when scoring events occur - closely follows a common Poisson process, with a sport-specific rate. Similarly, scoring balance - how often a team wins an event - follows a common Bernoulli process, with a parameter that effectively varies with the size of the lead. Combining these processes within a generative model of game play, we find they both reproduce the observed dynamics in all four sports and accurately predict game out comes. These results demonstrate common dynamical patterns underlying within-game scoring dynamics across professional team sports, and suggest specific mechanisms for driving them. We close with a brief discussion of the implications of our results for several popular hypotheses about sports dynamics. © 2014 Salnikov et al.; licensee Springer."
"10.1140/epjds/s13688-014-0011-3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84930645322&doi=10.1140%2fepjds%2fs13688-014-0011-3&partnerID=40&md5=fc7b4ee618b7513870f6c6148d7c11b2","We present a novel algorithm and validation method for disambiguating author names in very large bibliographic data sets and apply it to the full Web of Science (WoS) citation index. Our algorithm relies only upon the author and citation graphs available for the whole period covered by the WoS. A pair-wise publication similarity metric, which is based on common co-authors, self-citations, shared references and citations, is established to perform a two-step agglomerative clustering that first connects individual papers and then merges similar clusters. This parameterized model is optimized using an h-index based recall measure, favoring the correct assignment of well-cited publications, and a name-initials-based precision using WoS metadata and cross-referenced Google Scholar profiles. Despite the use of limited metadata, we reach a recall of 87% and a precision of 88% with a preference for researchers with high h-index values. 47 million articles of WoS can be disambiguated on a single machine in less than a day. We develop an h-index distribution model, confirming that the prediction is in excellent agreement with the empirical data, and yielding insight into the utility of the h-index in real academic ranking scenarios. © 2014 Schulz et al."
"10.1140/epjds/s13688-014-0024-y","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84930627364&doi=10.1140%2fepjds%2fs13688-014-0024-y&partnerID=40&md5=6544b2d94685b313025df0632438240e","Analyzing a large data set of publications drawn from the most competitive journals in the natural and social sciences we show that research careers exhibit the broad distributions of individual achievement characteristic of systems in which cumulative advantage plays a key role. While most researchers are personally aware of the competition implicit in the publication process, little is known about the levels of inequality at the level of individual researchers. Here we analyzed both productivity and impact measures for a large set of researchers publishing in high-impact journals, accounting for censoring biases in the publication data by using distinct researcher cohorts defined over non-overlapping time periods. For each researcher cohort we calculated Gini inequality coefficients, with average Gini values around 0.48 for total publications and 0.73 for total citations. For perspective, these observed values are well in excess of the inequality levels observed for personal income in developing countries. Investigating possible sources of this inequality, we identify two potential mechanisms that act at the level of the individual that may play defining roles in the emergence of the broad productivity and impact distributions found in science. First, we show that the average time interval between a researcher’s successive publications in top journals decreases with each subsequent publication. Second, after controlling for the time dependent features of citation distributions, we compare the citation impact of subsequent publications within a researcher’s publication record. We find that as researchers continue to publish in top journals, there is more likely to be a decreasing trend in the relative citation impact with each subsequent publication. This pattern highlights the difficulty of repeatedly producing research findings in the highest citation-impact echelon, as well as the role played by finite career and knowledge life-cycles, and the intriguing possibility that confirmation bias plays a role in the evaluation of scientific careers. © 2014 Petersen and Penner; licensee Springer."
"10.1140/epjds20","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84924407716&doi=10.1140%2fepjds20&partnerID=40&md5=07a83bae0b58e7fb88c39223c963d351","Activity of modern scholarship creates online footprints galore. Along with traditional metrics of research quality, such as citation counts, online images of researchers and institutions increasingly matter in evaluating academic impact, decisions about grant allocation, and promotion. We examined 400 biographical Wikipedia articles on academics from four scientific fields to test if being featured in the world’s largest online encyclopedia is correlated with higher academic notability (assessed through citation counts). We found no statistically significant correlation between Wikipedia articles metrics (length, number of edits, number of incoming links from other articles, etc.) and academic notability of the mentioned researchers. We also did not find any evidence that the scientists with better WP representation are necessarily more prominent in their fields. In addition, we inspected the Wikipedia coverage of notable scientists sampled from Thomson Reuters list of ‘highly cited researchers’. In each of the examined fields, Wikipedia failed in covering notable scholars properly. Both findings imply that Wikipedia might be producing an inaccurate image of academics on the front end of science. By shedding light on how public perception of academic progress is formed, this study alerts that a subjective element might have been introduced into the hitherto structured system of academic evaluation. © 2014 Samoilenko and Yasseri; licensee Springer."
"10.1140/epjds19","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84911486213&doi=10.1140%2fepjds19&partnerID=40&md5=f23270f97417ccf402a1f368a1193ce7","Using open source data, we observe the fascinating dynamics of nighttime light. Following a global economic regime shift, the planetary center of light can be seen moving eastwards at a pace of about 60 km per year. Introducing spatial light Gini coefficients, we find a universal pattern of human settlements across different countries and see a global centralization of light. Observing 160 different countries we document the expansion of developing countries, the growth of new agglomerations, the regression in countries suffering from demographic decline and the success of light pollution abatement programs in western countries. © 2014 Cauwels et al."
"10.1140/epjds31","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84905501315&doi=10.1140%2fepjds31&partnerID=40&md5=4322fc9198318f7ae5c9bb23a4b046f3","This study leverages mobile phone data to analyze human mobility patterns in a developing nation, especially in comparison to those of a more industrialized nation. Developing regions, such as the Ivory Coast, are marked by a number of factors that may influence mobility, such as less infrastructural coverage and maturity, less economic resources and stability, and in some cases, more cultural and language-based diversity. By comparing mobile phone data collected from the Ivory Coast to similar data collected in Portugal, we are able to highlight both qualitative and quantitative differences in mobility patterns - such as differences in likelihood to travel, as well as in the time required to travel - that are relevant to consideration on policy, infrastructure, and economic development. Our study illustrates how cultural and linguistic diversity in developing regions (such as Ivory Coast) can present challenges to mobility models that perform well and were conceptualized in less culturally diverse regions. Finally, we address these challenges by proposing novel techniques to assess the strength of borders in a regional partitioning scheme and to quantify the impact of border strength on mobility model accuracy. © 2014 Amini et al."
"10.1504/IJBIDM.2014.068366","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84926675053&doi=10.1504%2fIJBIDM.2014.068366&partnerID=40&md5=4cc413eee3976a2299dbcb0e04aa5253","Spatial data warehouses (SDWs) and spatial OLAP (SOLAP) are well-known business intelligence (BI) technologies that aim to support multidimensional and online analysis of huge volumes of data with spatial reference. Spatial vagueness is one of the most neglected imperfections of spatial data. Although several works propose new ad-hoc models for handling spatial vagueness, their implementation in spatial database management systems (DBMS) and SDW is still in an embryonic state. In this paper, we present a new design method for SOLAP datacubes that allows handling vague spatial data analysis issues. This method relies on a risk management method applied to the potential risks of data misinterpretation and decision makers' tolerance levels to those risks. We also present a tool implementing our method and a validation of the method is done based on the designed datacubes schemas testing. Copyright © 2014 Inderscience Enterprises Ltd."
"10.1504/IJBIDM.2014.068458","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84926669855&doi=10.1504%2fIJBIDM.2014.068458&partnerID=40&md5=b8817c8e544d04303f484b9363c51807","The historic centres, by using a large sustainability concept, should be the places where the quality and the symbolic values may prevail on the quantitative and functional ones, but often, the poor resources and the low quality of the management do not allow to promote a sustainable retraining process for them. The plan decision makers should verify the impact of the technical choices and then the sustainability of the actions, but also the impact of the planning policies on the economic performance and then on the economic sustainability of these actions. Therefore they must be helped to identify the planning actions, their funds and the value system they want to promote. In this regard, this study proposes a model to support the management of the retraining plan for Pachino's historic centre. The proposed model is a DSS that is developed using the MAUT and the IMO-DRSA tool. Copyright © 2014 Inderscience Enterprises Ltd."
"10.1504/IJBIDM.2014.068367","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84926645972&doi=10.1504%2fIJBIDM.2014.068367&partnerID=40&md5=972c5c943938d61083b814e08fa1342d","Nowadays, an abundance of sensors are used to collect very large datasets containing spatial points which can be mined and analysed to extract meaningful patterns and information. This article examines patterns which describe the dispersion of 2D data around a central tendency. Several state of the art patterns for point cluster analysis are presented and critiqued before a new pattern, the oriented spatial box plot, is defined. The oriented spatial box plot extends the classical one-dimensional box plot for summarising and visualising 2D point clusters. The pattern is suitable for detecting outliers and understanding the spatial density of point clusters. Copyright © 2014 Inderscience Enterprises Ltd."
"10.1504/IJBIDM.2014.068457","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84926641316&doi=10.1504%2fIJBIDM.2014.068457&partnerID=40&md5=10a95e609520cebdb64710e5d51d577a","This paper investigates the use of artificial neural networks (ANNs) to combine time series forecast of high frequency data obtained from various temporal disaggregation methods without related series (e.g., Almon, 1988; Lisman and Sandee, 1964; Boot and Feibes, 1967; Boot et al., 1967; Stram and Wei, 1986; Zaier and Abed, 2014). We use the example of deriving quarterly US GDP from annual one to evaluate the performance of the proposed method. We demonstrate that combining with nonlinear ANNs generally produces better forecasts than forecasts obtained from individual temporal disaggregation methods and from also traditional linear combining procedures on the basis of performance measures. Copyright © 2014 Inderscience Enterprises Ltd."
"10.1504/IJBIDM.2014.068459","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84926640058&doi=10.1504%2fIJBIDM.2014.068459&partnerID=40&md5=36be17616e287e53ba925c6a3214b9ea","In this paper, we consider the extension of the cubic B-spline collocation method to price path-dependent and exotic options when the price dynamics of the underlying asset are governed by a Markovian process. In this setting, the classical Black-Scholes model is generalised to incorporate Markov-switching (regime-switching) properties which account for the influence of economic factors on asset price dynamics. Our numerical results presented using the Black-Scholes two regime-switching model demonstrate that the cubic B-spline collocation method not only yields second order convergent prices and hedging parameters, but it is also more accurate when the problem is convectively dominated. Copyright © 2014 Inderscience Enterprises Ltd."
"10.1504/IJBIDM.2014.068365","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84926620959&doi=10.1504%2fIJBIDM.2014.068365&partnerID=40&md5=d5e61845de3febe7445476419b631e4c","This paper provides the logical basis and the algorithms of an evaluation model of investment concerning the purchase of the bare dominium of a residential property. The model is based on the decision tree analysis in order to address the fundamental and specific aspect of this type of investment. The decision tree analysis is one of the first attempts to introduce a strategic approach to the evaluation of investments within the traditional methods of capital budgeting. The attempt remains incomplete when the distribution of the probability of events that characterise the different scenarios is not exactly known. However, what seems to be a limitation of this type of analysis, in the present case, is not so. The knowledge and the implementation of the probability of survival or death depending on the age of the usufructuary, allows to manage the risk of the investment. Copyright © 2014 Inderscience Enterprises Ltd."
"10.1504/IJBIDM.2014.068368","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84926617269&doi=10.1504%2fIJBIDM.2014.068368&partnerID=40&md5=5004e82535441c78ed3fb281efa791bc","In this paper, we address the problem of routing optimisation in the backbone of a surveillance network. A surveillance mission is characterised by the employment and collaboration of several agents processing diverse information sources' inputs in order to ensure a surveillance task. These communications rely in a predefined network infrastructure containing mobile nodes and static backbone nodes. As the backbone network is private and configurable, a global routing algorithm can be set up in order to optimise information exchange. We propose to model this problem as a single path multicommodity flow problem (SMCFP), where several commodities are to be shared. The considered objective function is to minimise the overall network congestion. As the complexity of the SPMFP is NP-Hard, a multi-start tabu search (MTS) is proposed as a solution approach. The empirical validation is done using a simulation environment called Inform Lab containing real instances of surveillance cases. A comparison to a state-of-the-art ant colony system (ACS) approach is performed based on two large testbeds. The same instances are optimally solved using CPLEX. The experimental results show that the MTS produces considerably better results than the ACS. Copyright © 2014 Inderscience Enterprises Ltd."
"10.1504/IJBIDM.2014.068349","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84926614701&doi=10.1504%2fIJBIDM.2014.068349&partnerID=40&md5=12d4d7c6a5183356e2a357da0df62c08","Support vector machines (SVMs) have shown superior performance compared to other machine learning techniques, especially in classification problems. Yet one limitation is the long computational training time which increases with the data size. This problem has been investigated thoroughly and different algorithms for classification have been used with various success rates. Among them, clustering techniques have shown a considerable success to reduce SVM's data training. However, once these solutions are used for large scale datasets it becomes clear that using only clustering approaches is insufficient. In this paper, we tackle the problem of how to combine clustering methods and feature reducing techniques to minimise efficiently SVM's complexity. Several experiments on different datasets show that the proposed solution can be a promised way for fast training SVMs on large scale datasets. Copyright © 2014 Inderscience Enterprises Ltd."
"10.1504/IJBIDM.2014.068456","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84926612095&doi=10.1504%2fIJBIDM.2014.068456&partnerID=40&md5=a39c503dc993ad3178a6e8a780767c4e","The growing scarcity of public financial in Italy, in opposition of the more significant problems of degradation of many urban areas, prompted the Legislature to standardise new processes of settlement transformation based on negotiation-type public-private partnerships (PPPN). However, these standards have not provided for benchmarks referring to the contents of partnerships or assessment procedures aimed at assessing the initiatives undertaken with respect to public utility objectives. This has often led to redevelopment initiatives geared more towards the satisfaction of private rather than public interests. The proposed methodology, structured on the integration of a benchmarking process with multi-criteria evaluation techniques known as benchmarking multi-criteria evaluation (BME) enables the definition of benchmarks through a participatory process of the different stakeholders involved in a PPPN to which the BME is applied. In order to verify the applicability of the proposed procedure, it has been applied to a type of PPPN: The integrated action programmes (PII) in the Lazio Region. The benchmarks can be used by Lazio's administrators both for renewing the planning of the PII concerned and for verifying the quality of the initiatives within the same PPPN process. Copyright © 2014 Inderscience Enterprises Ltd."
"10.1504/IJBIDM.2014.065100","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84907942343&doi=10.1504%2fIJBIDM.2014.065100&partnerID=40&md5=0d9ef4d1b808d93a74c4459d448c1444","Urban real estate property values are mainly conditioned by several aspects, which can be summarised in two main classes: intrinsic and extrinsic ones. Intrinsic characters are specific goods while extrinsic features are related to a diversity of goods. Therefore, there is an extremely close correlation between 'rigidity location' of property (fixed location) and its value. Possibilities offered by recent developments of statistical techniques, principally geographically weighted regression (GWR), in analysing housing market have given a new impetus in mass appraisal of urban property. More particularly, geographically weighted regression has been adopted in analysing housing market, in order to identify homogeneous areas and to define the marginal contribution that a single location (outlined by these areas) gives to the market value of the property. The model has been built on a sample of 280 data, related to the trades of residential real estate units occurred between 2008 and 2010 in the city of Potenza (Basilicata, southern Italy). The results of territory zoning into homogeneous market areas, in addition to the undoubted usefulness in the field of real estate valuations, has useful implications in terms of taxation, programming territorial transformations and checking ongoing or ex post planning decisions. Copyright © 2014 Inderscience Enterprises Ltd."
"10.1504/IJBIDM.2014.065074","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84907938239&doi=10.1504%2fIJBIDM.2014.065074&partnerID=40&md5=56ff30dd35d90656a6880015c2508e7a","In the real estate sector, regression analysis is the most used method for interpretative and predictive purposes. However, the presence of outliers in the estimative sample can lead to ordinary last squared regression models that do not represent the investigated market phenomenon, with the consequence of producing unreliable assessments. In the present research, the issues of the identification and the removal of outliers are discussed. The outliers identified by the least median of squares (LMS) regression and the minimum volume ellipsoid (MVE) estimator are compared in order to test the coincidence or the diversity. A complete diagnosis of the data of the initial estimative sample is carried out, combining the robust residuals obtained with LMS and the robust distances obtained with MVE. The data are classified into regular observations, vertical outliers, good leverage points and bad leverage points, and cases to delete and those to keep in the sample are identified. Copyright © 2014 Inderscience Enterprises Ltd."
"10.1504/IJBIDM.2014.065075","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84907932785&doi=10.1504%2fIJBIDM.2014.065075&partnerID=40&md5=9bec167e80e3e339efba3e6166d7122f","The efficiency of existing association rules mining algorithms afford large number of delivered rules that the user can not exploit them easily. Consequently, thinking about another mining of these generated rules becomes essential task. For this, the present paper explores metarules extraction in order to prune the irrelevant rules. It first focuses on clustering association rules for large datasets. This allows the user better organising and interpreting the rules. To more go down in our mining, different dependencies between rules of the same cluster are extracted using meta-rules algorithm. Then, pruning algorithm uses these dependencies to delete the deductive rules and keep just the representative rules for each cluster. The proposed approach is tested on different experiments including clustering, meta-rules and pruning steps. The result is very promising in terms of the number of returned rules and their quality. Copyright © 2014 Inderscience Enterprises Ltd."
"10.1504/IJBIDM.2014.065091","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84907932510&doi=10.1504%2fIJBIDM.2014.065091&partnerID=40&md5=de6a3643d2b88d3f0588efe1a004a62d","The prediction of a stock market direction may serve as an early recommendation system for short-term investors and as an early financial distress warning system for long-term shareholders. Many stock prediction studies focus on using macroeconomic indicators, such as CPI and GDP, to train the prediction model. However, daily data of the macroeconomic indicators are almost impossible to obtain. Thus, those methods are difficult to be employed in practice. In this paper, we propose a method that directly uses prices data to predict market index direction and stock price direction. An extensive empirical study of the proposed method is presented on the Korean Composite Stock Price Index (KOSPI) and Hang Seng Index (HSI), as well as the individual constituents included in the indices. The experimental results show notably high hit ratios in predicting the movements of the individual constituents in the KOSPI and HIS. Copyright © 2014 Inderscience Enterprises Ltd."
"10.1504/IJBIDM.2014.062882","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84903736178&doi=10.1504%2fIJBIDM.2014.062882&partnerID=40&md5=a935a15a14354812366f088822c39b44","The similarity search problem in streaming time series has become an interesting research topic because such data arise in so many applications of various areas. In this problem, the fact that data streams are updated continuously as new data arrive in real time is a challenge because of dimensionality reduction recalculation and index update costs. In this paper, using ideas of a delayed update policy on R*-tree proposed by Kontaki et al., we proposed an improved method in which indexable piecewise linear approximation (PLA) dimensionality reduction method with the support of Skyline index can be used to perform effectively the similarity search task in streaming time series. Experimental results show that the similarity search in streaming time series with the support of Skyline index is more efficient than the case of using R*-tree. Copyright © 2014 Inderscience Enterprises Ltd."
"10.1504/IJBIDM.2014.062879","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84903735132&doi=10.1504%2fIJBIDM.2014.062879&partnerID=40&md5=21ef65661bfff7353ca92e6024fdd1df","In the past, data management research has concentrated in separate data processing issues: heavy database like query processing, and throughput of stream data processing over high-rate data (CEP). However, in many practical contexts, high-rate stream and heavy data processing work together, for correlation, lookup, aggregation, merging or comparison with large amounts of previous data. We refer to these as stream-DB workloads. One way to provide scalability with any off-the-shelf engine is to have multiple machines and/or processor cores, and to parallelise the load (external scheduler), but nodes can still overload. We propose automated control for balancing and scalability over stream-DB workloads. The approach, called DynLW, offers scalability with an integrated mechanism that manages overload (re)scheduling, automated elasticity, shedding, admission control and overload alerts when resources are insufficient. As a result, the approach provides continuous and totally balanced operation, and avoids overload-related problems. Copyright © 2014 Inderscience Enterprises Ltd."
"10.1504/IJBIDM.2014.062883","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84903714179&doi=10.1504%2fIJBIDM.2014.062883&partnerID=40&md5=82984ad3deb9019f6d51d30ff618de50","This article presents a comprehensible and powerful method to automatically analyse large sets of full text articles on the level of single terms by creating a term connection graph, akin to a sophisticated tag cloud. As a result, it is possible to quickly receive a visual structuring of a research field, which is particularly helpful for those new to a field to get an overview of the field and for researchers as a whole to relate their own work to that of others. Following the design science research approach, we first explain the desired benefit of such a method that includes a flexible and fast way for researchers to compile complete journals and other scientific sources into quickly interpretable graphs, which have a quantitatively objective foundation. Then, we explain the method and its relation to other approaches such as co-citation analysis and text mining. Next, we present in detail the results of applying this method to 15 volumes/years of Journal of Artificial Societies and Social Simulation (JASSS). The article also links to our source code implementations and provides adjustable step-by-step guides so that others may better benefit from and extend this research. Copyright © 2014 Inderscience Enterprises Ltd."
"10.1504/IJBIDM.2014.062885","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84903712707&doi=10.1504%2fIJBIDM.2014.062885&partnerID=40&md5=482ecc2017db7183ef347e9f25733bab","The environmental damage due to industrial pollution activities to produce the adaptations in the behaviour of economic agents involved: if there is a decrease of environmental quality, it may lead to a negative change in real estate market values, identifying the quantities and the prices of traded goods. Starting from these considerations, a situation analysis was carried out about three municipalities of Apulia Region (Brindisi, Bari and Taranto), which are affected by major environmental industrial detractors. In particular the case of Taranto has been studied and the value of the property of the district 'Tamburi' and other neighbourhoods adjacent to the industrial area has been quantified. If they had not been affected by the same pole, thereby, arriving at an estimate of the decrease of the real estate value by the use of damage estimate of the externality on real estate market. Copyright © 2014 Inderscience Enterprises Ltd."
"10.1504/IJBIDM.2014.062884","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84903709361&doi=10.1504%2fIJBIDM.2014.062884&partnerID=40&md5=88f7284935e9c534a36501d27959033a","The dissimilarity among the combined units in common classification techniques leads to consider the opportunity of assigning each of them to more than a single group with different degrees of membership. In this paper, we propose a discriminant analysis structured by regressing such degrees on the classification variables. In particular, we show that even the sum of the estimated degrees of membership equals one for every unit. Polynomial regression models are actually more appropriate than linear ones, as the rate of increase or decrease of each dependent variable can vary depending on the values assumed by the independent variables; the order of the polynomial is to be chosen so as to ensure both the homogeneity within clusters and the parsimony of the entire regression model. The reliability of our proposal is showed in an applicative case, concerning the entrepreneurial propensity of the provinces in Central and Southern Italy. Copyright © 2014 Inderscience Enterprises Ltd."
"10.4018/ijban.2014010105","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046279333&doi=10.4018%2fijban.2014010105&partnerID=40&md5=b47203b5268c5c96afd78c675d5a9efb","Many different metrics have been developed to measure the capacity for resilience to a disaster event. In order to track the dynamic response of a community in the aftermath of a disaster, however, it is necessary to consider measures that vary over time and for which data points are actually available on a relatively frequent basis. Unemployment, construction GDP, leisure and hospitality GDP, manufacturing GDP and information and communication technology GDP are all examples of measures which provide the opportunity to quantitatively assess the relative rate and extent of community recovery at regular time intervals following a disaster. By quantitatively analyzing the relative amount of resilience exhibited by a community we may gain better insight into its ability to recover, and thus develop a better understanding of the factors that allow it to return to normal levels of activity. The authors apply their analytical approach to compare the communities of New Orleans, Louisiana and Gulfport, Mississippi in the context of Hurricane Katrina. Copyright © 2014 IGI Global."
"10.4018/ijban.2014010104","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041931339&doi=10.4018%2fijban.2014010104&partnerID=40&md5=2091edf443187d5a356294f7d4890155","Given the importance of operating and managing forward and reverse supply chains in an integrated manner, this article considers an integrated four-stage supply chain network with forward and reverse product flows. We consider a closed loop supply chain (CLSC) network with primarily commercial returns, which could be potentially recovered by light repair operations or by refurbishing. The annual estimate of commercial returns in the United States is in excess of $100 billion. This paper discusses the optimal design of a CLSC network.A mixed integer linear programming (MILP) model is developed to determine the optimal locations of the facilities and the distribution of flows between facilities in the CLSC to maximize the total profit. The model is illustrated using a realistic example applicable to the electronics industries. Even though recycling and refurbishing add cost, the overall supply chain profit increases due to a reduction in the raw material cost. Sensitivity analysis is carried out to determine the effect of return percentage and varying demands of customers who are willing to buy refurbished products. The analysis show that the total supply chain profit increases with the increase in refurbishing activity. Finally, changes in the network design with respect to the uncertainty in these return parameters are also studied. The results show that the changes in return parameters lead to changes in optimal network design implying the need to explicitly consider the uncertainty in these return parameters. Copyright © 2014 IGI Global."
"10.4018/ijban.2014010103","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041684477&doi=10.4018%2fijban.2014010103&partnerID=40&md5=601588b3df59041cad29a3687cad593e","The aim of this study was to examine two types of time lag related to IT innovations in Japan. The first is the time lag between newspaper articles and academic papers in the past and between newspaper articles and IT innovations in the present. The second is the ongoing time lag with regard to the big data trend between the United States and Japan. This article explores a new analytical process based on two-dimensional maps proposed by Krinder et al. (2005) for visualizing and measuring time lags. After overviewing the big data trend in Japan, the author analyzed 2,910 newspaper articles and 550 academic papers published in Japan related to past innovations, as well as 734 newspaper articles from the United States and 173 from Japan related to big data innovation. The results indicate that academic research for past trends lags business trends by 1-4 years. However, time lags in the big data trend could not be captured because of the difficulty predicting the point of inflection of S-shaped curves in an early stage of innovation. Accordingly, the author simulated future S-shaped curves to show the gap between the United States and Japan. Copyright © 2014 IGI Global."
"10.4018/ijban.2014010101","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025677987&doi=10.4018%2fijban.2014010101&partnerID=40&md5=60f8b72b0191c4ce5dc3168b64e871c5","Buildings consume about 40% of the total energy in most countries contributing to a significant amount of greenhouse gas (GHG) emissions and global warming. Therefore, reducing energy consumption in buildings, making buildings more energy efficient and operating buildings in more energy efficient manner are important tasks in today’s world. Analytics can play an important role in identifying energy saving opportunities in buildings by modeling and analyzing how energy is consumed in buildings. In this paper, a set of analytics which can assist building owners, facility managers, operators and tenants of buildings in assessing, benchmarking, diagnosing, tracking, forecasting, simulating and optimizing energy consumption is presented. Copyright © 2014 IGI Global."
"10.4018/ijban.2014010102","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84973562648&doi=10.4018%2fijban.2014010102&partnerID=40&md5=a644fdfd0ff02996388aae43b55dc7b9","This paper presents a Data Envelopment Analysis approach that evaluates the relative effectiveness of decision making units (DMUs) in the presence of predetermined targets which are to be achieved simultaneously. As such targets are a core element of common performance management systems, the approach is a desirable complement of these systems. The proposed method classifies the DMUs into subsets according to their ability to meet the targets and provides a complete ranking within each subset on the basis of a measure of relative effectiveness. For that purpose, we introduce a new concept called super-effectiveness. The approach is illustrated by means of a example referring to a European pharmacy chain. Copyright © 2014 IGI Global."
"10.1504/IJBIDM.2013.057744","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84889825306&doi=10.1504%2fIJBIDM.2013.057744&partnerID=40&md5=8b9b600245addd41d9fc40b1df9f6c79","Real-world applications frequently involve missing data, turning the data analysis into a non-trivial task. This paper presents an analysis of six representative regression algorithms, evaluating their predictive performance and sensitivity to missing data. For this purpose, we used 20 public datasets and manipulated them to hold controlled levels of missing data. Our empirical analysis shows that RepTree is the least influenced by missing data, being LinearRegression the next. IBK is the most influenced, presenting the highest error. However, M5P remains as the algorithm with best predictive performance, although being only the fourth less influenced by missing data. Copyright © 2013 Inderscience Enterprises Ltd."
"10.1504/IJBIDM.2013.057745","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84889816711&doi=10.1504%2fIJBIDM.2013.057745&partnerID=40&md5=06164f101fa60878f0bfe4b3feacc9f5","Query answering has been addressed as a key issue in distributed environments such as peer data management systems (PDMS). An important step in this process regards query routing, i.e., how to find peers (data sources) that are most likely to provide results according to the submitted query. In this process, queries are reformulated and propagated through network peers using the semantic mappings between neighbour peers' schemas. The successive processes of query reformulation may result in a semantic loss of the original query, i.e., concepts which belong to the original query may be lost when reformulated queries are produced. Thereby, this work proposes the use of semantic measures obtained from information quality (IQ) criteria aiming to avoid or minimise this semantic loss. Moreover, it combines semantic information and IQ, by presenting a model, which is instantiated to illustrate how this proposal produces the semantic measures and enhances query routing processes. Copyright © 2013 Inderscience Enterprises Ltd."
"10.1504/IJBIDM.2013.057751","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84889789664&doi=10.1504%2fIJBIDM.2013.057751&partnerID=40&md5=eee57630461f8548587d95da0dddd429","Structural relation patterns have been introduced to extend the search for complex patterns often hidden behind large sequences of data, with applications (e.g.) in the analysis of customer behaviour, bioinformatics and web mining. In the overall context of frequent itemset mining, the focus of attention in the structural relation patterns family has been on the mining of concurrent sequential patterns, where a companion approach to graph-based modelling can be illuminating. The crux of this paper sets out to establish the connection between concurrent sequential patterns and frequent partial orders, which are well known for discovering ordering information from sequence databases. It is shown that frequent partial orders can be derived from concurrent sequential patterns, under certain conditions, and worked examples highlight the relationship. Experiments with real and synthetic datasets contrast the results of the data mining and modelling involved. Copyright © 2013 Inderscience Enterprises Ltd."
"10.1504/IJBIDM.2013.057743","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84889785159&doi=10.1504%2fIJBIDM.2013.057743&partnerID=40&md5=bb65699ae9b699043817db3ebcb28181","Clustering is widely used in case of pattern recognition, decision-making, machine-learning, image processing and many more real world problems. Many algorithms have been developed for better clustering. This paper proposes an efficient way of clustering data using chemical reaction optimisation (CRO), a recently developed metaheuristics for solving optimisation problems. By taking into consideration some of the real world datasets, the performance of the proposed algorithm has been compared with K-means, genetic algorithm (GA), differential evolution (DE) and teaching-learning-based optimisation (TLBO). Experimental result shows that the performance of CRO-based clustering is better than K-means, GA, DE and TLBO, in terms of quantisation error, intra and inter cluster distance, etc. It is also observed that the proposed CRO-clustering algorithm converges remarkably faster in comparison to other algorithms. Copyright © 2013 Inderscience Enterprises Ltd."
"10.1504/IJBIDM.2013.057737","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84889777538&doi=10.1504%2fIJBIDM.2013.057737&partnerID=40&md5=b9af21e046b3adfb323ce51154a8910d","This paper presents a procedure that imputes missing values by using random forests on semi-supervised data. Applying our method to Hewlett-Packard Lab.'s spam data and Edgar Anderson's iris data, we found that the rate of correct classification is higher than that of other methods: a simple expansion of Liaw's 'rfImpute' for (un)supervised data and the k-nearest neighbour method (kNN). Our method allows missing predictor variables as well as missing response variable. An imputation that uses random forests for semi-supervised cases in the training dataset has never been implemented until now. Copyright © 2013 Inderscience Enterprises Ltd."
"10.2481/dsj.12-037","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84890404503&doi=10.2481%2fdsj.12-037&partnerID=40&md5=d7a139fdf946b4fbf1db696cbff5d6b4","Data sharing has gained importance in scientific communities because scientific associations and funding organizations require long term preservation and dissemination of data. To support psychology researchers in data archiving and data sharing, the Leibniz Institute for Psychology Information developed an archiving facility for psychological research data in Germany: PsychData. In this paper we report different types of data requests that were sent to researchers with the aim of building up a sustainable data archive. Resulting response rates were rather low, however, comparable to those published by other authors. Possible reasons for the reluctance of researchers to submit data are discussed."
"10.1089/big.2013.0031","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84991716528&doi=10.1089%2fbig.2013.0031&partnerID=40&md5=d2e47299762cd6f08c578927ffa79312","As analysts are expected to process a greater amount of information in a shorter amount of time, creators of big data software are challenged with the need for improved efficiency. Ray, our group's usable, scalable genome assembler, addresses big data problems by using optimal resources and producing one, correct and conservative, timely solution. Only by abstracting the size of the data from both the computers and the humans can the real scientific question, often complex in itself, eventually be solved. To draw a curtain over the specific computational machinery of big data, we developed RayPlatform, a programming framework that allows users to concentrate on their domain-specific problems. RayPlatform is a parallel message-passing software framework that runs on clouds, supercomputers, and desktops alike. Using established technologies such as C++ and MPI (message-passing interface), we handle the genomes of hundreds of species, from viruses to plants, using machines ranging from desktop computers to supercomputers. From this experience, we present insights on making computer time more useful - and user time much more valuable. © Mary Ann Liebert, Inc. 2013."
"10.1089/big.2013.0024","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84991677430&doi=10.1089%2fbig.2013.0024&partnerID=40&md5=3d6c4edd0b02bee92612ae25b92a5c32","Children with special healthcare needs (CSHCN) require health and related services that exceed those required by most hospitalized children. A small but growing and important subset of the CSHCN group includes medically complex children (MCCs). MCCs typically have comorbidities and disproportionately consume healthcare resources. To enable strategic planning for the needs of MCCs, simple screens to identify potential MCCs rapidly in a hospital setting are needed. We assessed whether the number of medications used and the class of those medications correlated with MCC status. Retrospective analysis of medication data from the inpatients at Seattle Children's Hospital found that the numbers of inpatient and outpatient medications significantly correlated with MCC status. Numerous variables based on counts of medications, use of individual medications, and use of combinations of medications were considered, resulting in a simple model based on three different counts of medications: outpatient and inpatient drug classes and individual inpatient drug names. The combined model was used to rank the patient population for medical complexity. As a result, simple, objective admission screens for predicting the complexity of patients based on the number and type of medications were implemented. © Mary Ann Liebert, Inc. 2013."
"10.1089/big.2013.0040","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84991672170&doi=10.1089%2fbig.2013.0040&partnerID=40&md5=60ddfa920ea388b4961d011ee207b463","The integrative personal omics profiling study introduced a novel, integrative approach based on personalized, longitudinal, multi-omics data. The study collected genomic, transcriptomic, proteomic, metabolomic, and autoantibody profiles from a single individual over a 14-month period. The results revealed various medical risks and extensive, dynamic changes in diverse molecular components and biological pathways across healthy and diseased conditions. The current article is a data publication that provides the checklists for the metadata of the proteomics (see Table 1) and metabolomics (see Table 2) datasets of the study. The proposed checklist was recently developed and endorsed by the Data-Enabled Life Sciences Alliance (DELSA Global). We call for the broader use of data publications using the metadata checklist to make omics data more discoverable, interpretable, and reusable, while enabling appropriate attribution to data generators and infrastructure science builders. © Mary Ann Liebert, Inc. 2013."
"10.1089/big.2013.0023","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84991660810&doi=10.1089%2fbig.2013.0023&partnerID=40&md5=a9f1e41b190c8b275f8e44a16b50610a","This article, written by researchers studying metadata and standards, represents a fresh perspective on the challenges of electronic health records (EHRs) and serves as a primer for big data researchers new to health-related issues. Primarily, we argue for the importance of the systematic adoption of standards in EHR data and metadata as a way of promoting big data research and benefiting patients. EHRs have the potential to include a vast amount of longitudinal health data, and metadata provides the formal structures to govern that data. In the United States, electronic medical records (EMRs) are part of the larger EHR. EHR data is submitted by a variety of clinical data providers and potentially by the patients themselves. Because data input practices are not necessarily standardized, and because of the multiplicity of current standards, basic interoperability in EHRs is hindered. Some of the issues with EHR interoperability stem from the complexities of the data they include, which can be both structured and unstructured. A number of controlled vocabularies are available to data providers. The continuity of care document standard will provide interoperability in the United States between the EMR and the larger EHR, potentially making data input by providers directly available to other providers. The data involved is nonetheless messy. In particular, the use of competing vocabularies such as the Systematized Nomenclature of Medicine-Clinical Terms, MEDCIN, and locally created vocabularies inhibits large-scale interoperability for structured portions of the records, and unstructured portions, although potentially not machine readable, remain essential. Once EMRs for patients are brought together as EHRs, the EHRs must be managed and stored. Adequate documentation should be created and maintained to assure the secure and accurate use of EHR data. There are currently a few notable international standards initiatives for EHRs. Organizations such as Health Level Seven International and Clinical Data Interchange Standards Consortium are developing and overseeing implementation of interoperability standards. Denmark and Singapore are two countries that have successfully implemented national EHR systems. Future work in electronic health information initiatives should underscore the importance of standards and reinforce interoperability of EHRs for big data research and for the sake of patients. © Mary Ann Liebert, Inc. 2013."
"10.1089/big.2013.0037","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84991624961&doi=10.1089%2fbig.2013.0037&partnerID=40&md5=3f4c6b7848f36724881b36cb5afe9bf2","With the increasingly widespread collection and processing of ""big data,"" there is natural interest in using these data assets to improve decision making. One of the best understood ways to use data to improve decision making is via predictive analytics. An important, open question is: to what extent do larger data actually lead to better predictive models? In this article we empirically demonstrate that when predictive models are built from sparse, fine-grained data - such as data on low-level human behavior - we continue to see marginal increases in predictive performance even to very large scale. The empirical results are based on data drawn from nine different predictive modeling applications, from book reviews to banking transactions. This study provides a clear illustration that larger data indeed can be more valuable assets for predictive analytics. This implies that institutions with larger data assets - plus the skill to take advantage of them - potentially can obtain substantial competitive advantage over institutions without such access or skill. Moreover, the results suggest that it is worthwhile for companies with access to such fine-grained data, in the context of a key predictive task, to gather both more data instances and more possible data features. As an additional contribution, we introduce an implementation of the multivariate Bernoulli Naïve Bayes algorithm that can scale to massive, sparse data. © Mary Ann Liebert, Inc. 2013."
"10.1504/IJBIDM.2013.059262","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84894259752&doi=10.1504%2fIJBIDM.2013.059262&partnerID=40&md5=abd1e973692b8d10918aebbc1cc376a0","The objective of the present study is to investigate the possibility of developing an integrated database with information pertaining to the income of Italian families arising from two major surveys conducted by ISTAT (EU-SILC) and the Bank of Italy (household income survey). Since neither of the surveys has the scope to allow for the construction of a database of information pertaining to income, an integration has been sought between the data from the two archives, assuming that the surveys are reliable in terms of the accuracy of the sample design and control of the representativeness of the sample. The development of our analysis is primarily aimed at carrying out an in-depth comparison between the two surveys in terms of structure, definition of variables and sample homogeneity and secondly, through the use of an integrated dataset, at a verification measurement of the validity of the information, in particular, of the income component. Copyright © 2013 Inderscience Enterprises Ltd."
"10.1504/IJBIDM.2013.059264","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84894259340&doi=10.1504%2fIJBIDM.2013.059264&partnerID=40&md5=85d6a61de58d8453fa2f50a3bcd6186a","Functional link neural network (FLNN) has emerged as an important tool used for function approximation and IT application on physical time series prediction. The standard learning scheme used for the training of FLNN is the Backpropagation (BP) learning algorithm. However, one of the crucial problems with BP learning algorithm is it tends to easily get trapped on local minima and thus affect the performance of FLNN. This paper proposed an alternative learning scheme for FLNN by using an artificial bee colony (ABC) optimisation algorithm as an attempt to overcome this problem. The performance of FLNN-ABC model is measured based on the prediction task on the physical time series data. The result of the prediction made by FLNN-ABC is compared with the original FLNN architecture and towards the end we found that FLNN-ABC gives better result in predicting the next-day ahead prediction. Copyright © 2013 Inderscience Enterprises Ltd."
"10.1504/IJBIDM.2013.059260","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84894255812&doi=10.1504%2fIJBIDM.2013.059260&partnerID=40&md5=2f62fd8aa0b33c4070b1a40d9a550df5","The aim of this paper is examining the relations between EU airports and cities from the passengers' air transport point of view, observing the polarisation of flows towards some nodes of the European air transport network. The analysis is carried out highlighting nodal regions, obtained observing the interactions among cities and airports pairs in terms of the dominant flows. We use a selection from Eurostat Database of 277 airports and 3,572 routes between airport pairs in the 27 EU countries. Nystuen and Dacey's theory of network analysis is used to explore the characteristics of the 'European sky' to understand how nodal regions appear in terms of dominance of interactions, measured by means of air traffic flows, and what European - regional hierarchies can be observed. The analysis allowed observing both nodal regions centred on airports dominant over international connections, as well as national and local situations, where a node is dominant over national ones. Copyright © 2013 Inderscience Enterprises Ltd."
"10.1504/IJBIDM.2013.059263","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84894253780&doi=10.1504%2fIJBIDM.2013.059263&partnerID=40&md5=9236704ca65f0a10c49dae338318a9bc","In the current downturn of European real estate market, the sale of the bare ownership is creating a growing interest of market operators. The attention to this formula is not only local, but also comes from foreign investors, attracted by the substantial saving on the purchase price, the revaluation of the property over time, as well as the breakdown of the tax burden and the costs of management and maintenance. The aim of this research is, first of all, to deepen the knowledge of the bare ownership market of residential properties. Secondly, it is to develop an effective tool for estimating the bare ownership. Therefore, on the same database and with reference to the same explanatory variables of the price of bare ownership, two estimation models are implemented, one based on hedonic prices theory and another using artificial neural networks (ANN), in order to compare the respective performance. Copyright © 2013 Inderscience Enterprises Ltd."
"10.1504/IJBIDM.2013.059261","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84894243802&doi=10.1504%2fIJBIDM.2013.059261&partnerID=40&md5=1c6abd336e2688c82b9ef68397210cd4","Jump-diffusion option pricing models have the ability to fit various implied volatility patterns observed in market option prices. In the partial differential equations framework, pricing an American put when the underlying follows a jump process requires the solution of a partial integro-differential equation. For this problem, second-order finite difference discretisations have been commonly employed. This work develops a new scheme which is based on a high-order compact discretisation of the spatial terms of the equation and a fourth-order time integration scheme. We demonstrate that the scheme is highly accurate for at-the-money American options and oscillation-free greeks are computed. Copyright © 2013 Inderscience Enterprises Ltd."
"10.0000000","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893506747&doi=10.0000000&partnerID=40&md5=288ddf80c3862e999bfd245b3cd64332","Classifier building form distributed data sources has been a fundamental computational problem to realise distributed knowledge discovery. The classification rule extraction from distributed databases suffers from the problems of high communication cost, lack of interpretability of rules and poor performance in handling high categorical data. The aim of this paper is to extend fuzzy generalised association rule extraction technique which is well proved in handling such issues to extract classification rules from distributed datasets. This paper presents a distributed data driven fuzzy generalised associative classifier (D3FGAC) framework for distributed knowledge discovery which extracts data driven fuzzy generalisation rules from horizontally fragmented datasets with minimum communication cost and builds global compact classifier using extracted rules. The experiments conducted on UCI datasets and their comparisons to other existing model shown in article to prove the efficiency of proposed framework.Copyright © 2013 Inderscience Enterprises Ltd."
"10.0000000","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893504579&doi=10.0000000&partnerID=40&md5=0b823f46ed2958726bd15ede226d8d37","Data mining is a task of extracting useful patterns/episodes from large databases. Sequence data can be modelled using episodes. An episode is serial if the underlying temporal order is total. An episode rule of associating two episodes suggests a temporal implication of the antecedent episode to the consequent episode. We present two sound and complete mining algorithms for finding frequent and confident serial-episode association rules with their ideal occurrence/window widths, if existing, in event sequences based on the notion of minimal occurrences constrained by constant and mean maximum gap, respectively. Two empirical studies are summarised: one focused on the absence of dependencies of local-maximum confidence on the ideal occurrence/window widths in synthetic random datasets, and the other on the applicability of the episode-rule mining algorithms with a set of earthquake data.Copyright © 2013 Inderscience Enterprises Ltd."
"10.0000000","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893485879&doi=10.0000000&partnerID=40&md5=de77d74e68aaa102eb86a2bca2681bf7","A distance metric learned from data reflects the actual similarity between objects better than the geometric distance. So, in this paper, we propose a new distance that is based on clustering. Because objects belonging to the same cluster usually share some common traits even though their geometric distance might be large. Thus, we perform several clustering runs to yield an ensemble of clustering results. The distance is defined by how many times the objects were not clustered together. To evaluate the ability of this new distance to reflect object similarity, we apply it to two types of data mining algorithms, classification (kNN) and selective sampling (LSS). We experimented on standard numerical datasets and on real colour images. Using our distance, the algorithms run on equivalence classes instead of single objects, yielding a considerable speedup. We compared the kNN-EC classifier and LSS-EC algorithm to the original kNN and LSS algorithms.Copyright © 2013 Inderscience Enterprises Ltd."
"10.0000000","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893483498&doi=10.0000000&partnerID=40&md5=66b3e2286d59aafbb0377995ede0d88e","Road networks are a national critical infrastructure. The road assets need to be monitored and maintained efficiently as their conditions deteriorate over time. The condition of one of such assets, road pavement, plays a major role in the road network maintenance programmes. Pavement conditions depend upon many factors such as pavement types, traffic and environmental conditions. This paper presents a data analytics case study for assessing the factors affecting the pavement deflection values measured by the traffic speed deflectometer (TSD) device. The analytics process includes acquisition and integration of data from multiple sources, data pre-processing, mining useful information from them and utilising data mining outputs for knowledge deployment. Data mining techniques are able to show how TSD outputs vary in different roads, traffic and environmental conditions. The generated data mining models map the TSD outputs to some classes and define correction factors for each class.Copyright © 2013 Inderscience Enterprises Ltd."
"10.0000000","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893439062&doi=10.0000000&partnerID=40&md5=e47642d6cad2181699930eb2c6bb6c45","Real-time warehousing has been researched and tried in practice by several companies, but there should be a definition of an architecture and an evaluation of its merits. Based on both industry and theoretical insights, we define a data warehouse architecture for constant integration of data without compromising query performance, and we evaluate its capacity to provide realtime. For this real-time data warehouse (RTDW) we define a dynamic warehouse component and a static warehouse component to represent the recently integrated data and the rest of the data, respectively, with relevant choices concerning how the components merge together. We propose design choices concerning query computation mechanisms and evaluate the alternatives to conclude which is the most efficient implementation of those mechanisms. The realtime data warehouse architecture is evaluated with a realtime benchmark setup that considers online loading with simultaneous query workload processing. Results prove the validity of the architecture, compare different mechanisms and quantify its efficiency in the constant integration context. The performance is carefully analysed taking into account several important factors.Copyright © 2013 Inderscience Enterprises Ltd."
"10.2481/dsj.12-039","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84890336217&doi=10.2481%2fdsj.12-039&partnerID=40&md5=30a429df3220690f0a68c4bec7e789a0","Among the key services that institutional data management infrastructures must provide are provenance and lineage tracking and the ability to associate data with contextual information needed for understanding and use. These functionalities are critical for addressing a number of key issues faced by data collectors and users, including trust in data, results traceability, data transparency, and data citation support. In this paper, we describe the support for these services within the Data Conservancy Service (DCS) software. The DCS provenance, context, and lineage services cross the four layers in the DCS data curation stack model: storage, archiving, preservation, and curation."
"10.1057/jma.2013.17","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84974846498&doi=10.1057%2fjma.2013.17&partnerID=40&md5=386c5517b018027b14319ecd0f240339","Improper selection of segmentation variables and tools may have an effect on segmentation results and can cause a negative financial impact. There have been numerous traditional models in the literature to segment customers; the most effective one is based on two-stage clustering methodology. However, none of the traditional approaches has the ability to establish non-strict customer segments that are significantly crucial for today’s competitive consumer markets. The aim of this study is to propose a two-stage clustering model for customer segmentation using Artificial Neural Networks and Fuzzy Logic. Segmenting customers was done according to the purchasing behaviours of customers via utilising Recency, Frequency and Monetary values. By using a secondary data set from a UK retail company, the model was also compared with traditional two-stage method based on two clustering validity indices. The findings indicated that the proposed model provided better insights and managerial implications with respect to the chosen validity indices. © 2013 Macmillan Publishers Ltd."
"10.1057/jma.2013.20","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84963608235&doi=10.1057%2fjma.2013.20&partnerID=40&md5=f9991d4c21480e7fb469dce31cb83b3b","There is much hype associated with the term ‘Big Data’ (BD), and much opportunity in the data that are associated with that term, along with the tools and techniques in existence and being developed to leverage it for decision making and improving the condition of living beings, firms and society. However, many are not clear on what are, or what is meant by the term, ‘Big Data’. The focus of this research was to explore the meaning of BD and to identify important paths of research on BD. As part of this process, we called upon a diverse set of marketing scholars who possess expertise and special insights. We discovered that different communities have different perspectives. It could be argued that they are all correct as they reflect the preferred perspectives of different communities. We find it helpful to think of BD as a term that represents a period of time or era, a process, and data that are from a variety of sources, of various structures or forms, and in a variety of locations. Important research questions and issues related to BD are discussed. © 2013 Macmillan Publishers Ltd."
"10.1057/jma.2013.18","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84954226378&doi=10.1057%2fjma.2013.18&partnerID=40&md5=e1db6413740b976344f712e32926b371","Corporate social responsibility (CSR) has become one of the firms’ priorities and has increasingly become a determinant for consumer shopping behavior. Since the 1950s, authors have been providing definitions for CSR and, interestingly, it has evolved into an integral mindset. Consumers have become more interested in firms’ actions and have, in turn, begun to strongly consider them in their purchasing decision making; yet, a lack of full awareness of what CSR entails, strong brand loyalty and the absence of information regarding firms’ CSR engagement are aspects that halt Mexican consumers from being highly influenced by CSR in such a degree as to alter shopping behavior. This study contributes to the understanding of consumer shopping behavior at retailing firms based on their involvement in CSR practices, by providing insights regarding consumer indifference. © 2013 Macmillan Publishers Ltd."
,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84888223876&partnerID=40&md5=93843ad94b123ec0c4399876decfb535","The aim of this paper is to discuss the organizational architecture and standard system for sharing research data at the national level The Data Sharing Network of Earth System Science (DSNESS) is one of the nine pilot projects of the ScientWc Data Sharing Project in China that has become a long-term operational research data-sharing platform in the National Science and Technology Infrastructure (NST1) of China. First, a data sharing union mechanism was designed with the core principle being, ""data come from research and will be reused in research '. Second, a data sharing organizational architecture was constructed that consists of three sections: data resource architecture, data management architecture, and data services architecture. A physical data sharing network was constructed that includes one general center and 15 distributed sub-centers based on the architecture. Third, a series of data sharing standards and spec 4fIcations were designed and implemented in the DSJ'IESS. The reference model of the DSJVESS standard system includes three levels of standards: directive standards, general standards, and application standards. In total, 21 high level standards and specUlcations were developed and implemented in the DSJ'JESS. Several core standards and specfications, such as the extensible metadata standard, data quality control specfications, and so on, were analyzed in detail Finally, the data service effect was summarized in three aspects: dataset services, standard and spec Wcat ion services, and international cooperation services. This research shows that the organizational architecture and standard system is a very important soft environment for research data sharing. The practices of DSNESS will provide useful experiences for multi-disciplinary data sharing in Earth science and will hejp to eliminate the data gap between the rich and poor at the national level."
"10.2481/dsj.12-043","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84887468180&doi=10.2481%2fdsj.12-043&partnerID=40&md5=52765b1075e01ac289ad3fba6eed580d","New high-throughput scientific instruments, telescopes, satellites, accelerators, supercomputers, sensor networks, and running simulations are generating massive amounts of data. In order to be able to exploit these huge volumes of data, a new type of e-infrastructure, the Global Research Data Infrastructure (GRDI), must be developed for harnessing the accumulating data and knowledge produced by the communities of research. This paper identifies the main challenges faced by the future GRDIs, defines a conceptual framework for GRDIs based on the ecosystem metaphor, describes a core set of functionality that these GRDIs must provide, and gives a set of recommendations for building the future GRDIs."
,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84887882214&partnerID=40&md5=d8f1f30e70c7139b85700479068ce229","The value of data in society is increasing rapidly. Organisations that work with data should have standard practices in place to ensure successJitl curation of data. The World Data System (WDS) consists of a number of data centres responsible for curating research data sets for the scientific community. The WDS has no formal data curation framework or model in place to act as a guideline for member data centres. The objective of this research was to develop a framework for the curation of data in the WDS. A multiple-case case study was conducted Interviews were used to gather qualitative data and analysis of the data, which led to the development of this framework. The proposed framework is largely based on the Qoen Archival Information System (DAIS) functional model and caters for the curation of both analogue and digital data."
"10.2481/dsj.13-005","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84887911355&doi=10.2481%2fdsj.13-005&partnerID=40&md5=327a41c6ce856f7cd4dc3d0b5051bdff","A 015 for ocean data applications named ""Ocean Data and Information Systems (ODIS)"" was designed and developed. The system is based on the University of]vPznnesota MapServer, an open source platfonn for publishing spatial data and interactive mapping applications to the web with MySQL as the bac/cend database server. This paper discusses some of the details of the storage and oiganization of oceanographic data, methods employed for visualization ofparameter plots, and mapping of the data. ODIS is conceived to be an end-to-end system comprising acquisition of data from a variety of heterogeneous ocean platforms, processing, integration, quality contrci and web-based dissemination to users for operational and research activities. ODIS provides efficient data management and potential mapping and visualization functions for oceanographic data."
"10.1089/big.2013.0018","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84991800688&doi=10.1089%2fbig.2013.0018&partnerID=40&md5=f37234fb5999b78f5b48771cfb2b3a2e","It is time for the healthcare industry to move from the era of ""analyzing our health history"" to the age of ""managing the future of our health."" In this article, we illustrate the importance of real-time analytics across the healthcare industry by providing a generic mechanism to reengineer traditional analytics expressed in the R programming language into Storm-based real-time analytics code. This is a powerful abstraction, since most data scientists use R to write the analytics and are not clear on how to make the data work in real-time and on high-velocity data. Our paper focuses on the applications necessary to a healthcare analytics scenario, specifically focusing on the importance of electrocardiogram (ECG) monitoring. A physician can use our framework to compare ECG reports by categorization and consequently detect Arrhythmia. The framework can read the ECG signals and uses a machine learning-based categorizer that runs within a Storm environment to compare different ECG signals. The paper also presents some performance studies of the framework to illustrate the throughput and accuracy trade-off in real-time analytics. © Copyright 2013, Mary Ann Liebert, Inc. 2013."
"10.1089/big.2013.0026","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84991794935&doi=10.1089%2fbig.2013.0026&partnerID=40&md5=27d4e29fb687669f153138b768a998f3",[No abstract available]
"10.1089/big.2013.0012","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84991786835&doi=10.1089%2fbig.2013.0012&partnerID=40&md5=0c9c24a44037e59efacd7ce1f7f28e54","The era of ""big data"" presents immense opportunities for scientific discovery and technological progress, with the potential to have enormous impact on research and development in the public sector. In order to capitalize on these benefits, there are significant challenges to overcome in data analytics. The National Institute of Allergy and Infectious Diseases held a symposium entitled ""Data Science: Unlocking the Power of Big Data"" to create a forum for big data experts to present and share some of the creative and innovative methods to gleaning valuable knowledge from an overwhelming flood of biological data. A significant investment in infrastructure and tool development, along with more and better-trained data scientists, may facilitate methods for assimilation of data and machine learning, to overcome obstacles such as data security, data cleaning, and data integration. © Copyright 2013, Mary Ann Liebert, Inc. 2013."
"10.1089/big.2013.0019","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84991769395&doi=10.1089%2fbig.2013.0019&partnerID=40&md5=270967b3e798912a8d4f42cd88e6c593","As healthcare providers transition to outcome-based reimbursements, it is imperative that they make the transition to population health management to stay viable. Providers already have big data assets in the form of electronic health records and financial billing system. Integrating these disparate sources together in patient-centered datasets provides the foundation for probabilistic modeling of their patient populations. These models are the core technology to compute and track the health and financial risk status of the patient population being served. We show how the probabilistic formulation allows for straightforward, early identification of a change in health and risk status. Knowing when a patient is likely to shift to a less healthy, higher risk category allows the provider to intervene to avert or delay the shift. These automated, proactive alerts are critical in maintaining and improving the health of a population of patients. We discuss results of leveraging these models with an urban healthcare provider to track and monitor type 2 diabetes patients. When intervention outcome data are available, data mining and predictive modeling technology are primed to recommend the best type of intervention (prescriptions, physical therapy, discharge protocols, etc.) with the best likely outcome. © Copyright 2013, Mary Ann Liebert, Inc. 2013."
"10.1089/big.2013.0020","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84991769393&doi=10.1089%2fbig.2013.0020&partnerID=40&md5=e7d84781cfaf22f509b33ea722b21be0","The Internet has forever changed the way people access information and make decisions about their healthcare needs. Patients now share information about their health at unprecedented rates on social networking sites such as Twitter and Facebook and on medical discussion boards. In addition to explicitly shared information about health conditions through posts, patients reveal data on their inner fears and desires about health when searching for health-related keywords on search engines. Data are also generated by the use of mobile phone applications that track users' health behaviors (e.g., eating and exercise habits) as well as give medical advice. The data generated through these applications are mined and repackaged by surveillance systems developed by academics, companies, and governments alike to provide insight to patients and healthcare providers for medical decisions. Until recently, most Internet research in public health has been surveillance focused or monitoring health behaviors. Only recently have researchers used and interacted with the crowd to ask questions and collect health-related data. In the future, we expect to move from this surveillance focus to the ""ideal"" of Internet-based patient-level interventions where healthcare providers help patients change their health behaviors. In this article, we highlight the results of our prior research on crowd surveillance and make suggestions for the future. © Copyright 2013, Mary Ann Liebert, Inc. 2013."
"10.1089/big.2013.0021","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84991764201&doi=10.1089%2fbig.2013.0021&partnerID=40&md5=c122d76a12a3bf3e8c32da231d27449f","Big data in healthcare can bring significant clinical and cost benefits. Of equal but often overlooked importance is the role of patient satisfaction data in improving the quality of healthcare service and treatment, where satisfaction is measured through feedback by patients on their meetings with medical specialists and experts. One of the major problems in analyzing patient feedback data is the nonstandard research designs often used for gathering such data: the designs can be uncrossed, unbalanced, and fully nested. Traditional measures of data reliability are more difficult to calculate for such data. Also, patient data can contain significant proportions of missing values that further complicate the calculation of reliability. This paper describes a reliability approach that is robust in the face of nonstandard research designs and missing values for use with large-scale patient survey data. The dataset contains nearly 85,000 patient responses to over 2,000 healthcare practitioners in five different subtypes over a 15-year period in the United Kingdom. Reliability measures are calculated to provide benchmarks involving minimum numbers of patients and practitioners for deeper drill-down analysis. The paper concludes with a demonstration of how regression models generated from big patient feedback data can be assessed in terms of reliability at the total data level as well as drill-down levels. © Copyright 2013, Mary Ann Liebert, Inc. 2013."
"10.1504/IJBIDM.2013.055783","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84882712969&doi=10.1504%2fIJBIDM.2013.055783&partnerID=40&md5=0371303c50a0e79f390503e678ce31a0","Data mining is a process to discover useful patterns in large volumes of data through the application of appropriated algorithms, tools, and techniques. However, building scalable, extensible, and easy-to-use data mining systems has proved to be a very difficult task. This paper presents the web platform called SMINER, which aims at interoperability and facility of integration in the development of data mining applications. The platform is based on the paradigm of service-oriented architecture (SOA) using the web services standard for extensibility and interoperability. This platform is composed of two main components: 1) web services, which implement data mining algorithms; 2) a user web interface, which can be used for modelling applications that use the services of the platform. Experimental results demonstrate that our SOA-based platform makes it easy to construct a flexible and scalable data mining system. © 2013 Inderscience Enterprises Ltd."
"10.1504/IJBIDM.2013.055789","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84882703209&doi=10.1504%2fIJBIDM.2013.055789&partnerID=40&md5=adde8af4c53fc6b0b493c12e77dd29e7","Although the extent of e-commerce and m-commerce is continually increasing, the vast majority of the payment transaction potential remains in real-world scenarios such as customary retail POS. However, without adding substantial value, current business models rely on simple transaction-based revenues. The aim of this paper is to evaluate and exploit the freedom degrees of the m-payment business model framework, with special regard to new developments connected to the smartphone economy age. Therefore, we examine methods of data collection and usage to analyse customer preferences and enable evidence-based marketing both theoretically and by case study research. To generate new revenues from third parties, we extend the mobile payment reference model (MPRM) with new business model elements and roles. For the shift from existing simple transaction-based systems to upcoming modern m-payment procedures built around the central value of customer data, the MPRM 2.0 provides a theoretical background and according reference value architecture as well as an analysis tool to be used in research and practice. © 2013 Inderscience Enterprises Ltd."
"10.1504/IJBIDM.2013.055788","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84882612921&doi=10.1504%2fIJBIDM.2013.055788&partnerID=40&md5=7f4386c995d898c20f8383c32e237ab0","In Wikipedia, users can create and edit information freely. Few editors take responsibility for editing the articles. Therefore, information of many Wikipedia articles is lacking. Furthermore, Wikipedia has different levels of value of its information depending on the language version of the site. In this paper, we propose the extraction of complementary information from different language Wikipedia and its automatic presentation. The important points of our method are: 1) extraction of comparison articles from different language Wikipedia; 2) extraction of complementary information; 3) presentation of complementary information. © 2013 Inderscience Enterprises Ltd."
"10.1504/IJBIDM.2013.055787","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84882578319&doi=10.1504%2fIJBIDM.2013.055787&partnerID=40&md5=0a174399b64f059adfa5c6614598f5f9","This paper describes a web mining method for clustering research documents automatically. Web hit counts of AND-search for two words are used to form a document feature vector. Target documents are clustered using the k-means clustering method twice, in which cosine similarity is used to calculate the distance measure. © 2013 Inderscience Enterprises Ltd."
"10.1504/IJBIDM.2013.055786","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84882565184&doi=10.1504%2fIJBIDM.2013.055786&partnerID=40&md5=7227b15c05bc297bf549abed9b179a6e","Payments involving small amounts of money are usual in the online purchases of low-value services, goods or pieces of information. In these kinds of payments, called micropayments, a trade-off between efficiency and security requirements has to be provided. In a previous work, we presented an efficient and secure micropayment scheme which fulfils both the security properties that guarantee no financial risks and the desired privacy for customers. In order to prove the viability of the proposal, we have proceeded in two directions. First we have formally verified the protocol using coloured Petri nets (CPN). Secondly, once we are able to assure, from the results of the formal verification, that the proposal satisfies the claimed properties, we have successfully implemented the scheme on the Android platform. Using the developed implementation, we have evaluated its performance to prove that the proposal is viable using current mobile devices. Several tests validate that our scheme can execute very fast payments with our simple but secure spending protocol. With the presented formal verification and performance analysis we can assure that the protocol is ready to be used. © 2013 Inderscience Enterprises Ltd."
"10.2481/dsj.13-022","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84887469623&doi=10.2481%2fdsj.13-022&partnerID=40&md5=9c6799da7bb0c1f3fdc605637f73566c","In this paper we consider the Bayesian estimators for the unknown parameters of Gumbel type-II distribution. The Bayesian estimators cannot be obtained in closed forms. Approximate Bayesian estimators are computed using the idea of Lindley's approximation under different loss functions. The approximate Bayes estimates obtained under the assumption of non-informative priors are compared with their maximum likelihood counterparts using Monte Carlo simulation. A real data set is analyzed for illustrative purpose."
"10.2481/dsj.GRDI-011","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883131010&doi=10.2481%2fdsj.GRDI-011&partnerID=40&md5=251ea34f22a28cbe6f13bcdffefc2bc0","Data quality is an issue that touches on every aspect of the research data landscape and is appropriate to examine in the context of planning for future research data infrastructures. Researchers want to believe that they produce high quality data as producers and they want to obtain data of the highest quality as consumers. Data centers have stringent controls to ensure that they only acquire and disseminate data of the highest quality. Data managers will that they improve the quality of the data they are responsible for. Much of the infrastructure that will emit, transform, integrate, visualize, manage, analyze, and disseminate data during its life will have explicit or implicit dependencies on the quality of the data it is dealing with. One useful perspective on quality parameters is offered by the work of Wang and Strong, which reduces several parameters from many domains to 15 essential domain-independent measures."
,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883106143&partnerID=40&md5=fd5d275fe334308e14a036c271cf6e7c","Diego Lopez states that there are a range of reasons for securing data and systems, ranging from existing ones, such as IPR, licenses, privacy, and confidentiality to others that may appear in the future, such as distributed trust, reputation building, and social collaboration. The systems need to manage the data according to these restrictions while maintaining other requirements for data infrastructures, while being secure against potential external attacks. Data security covers a lot of different aspects of data infrastructures that may be both technical and organizational, such as procedures, policies, and physical access. Data security must be implemented for all components of a data infrastructure as a single loose link would potentially break the secure chain."
,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883095732&partnerID=40&md5=581f55dd3eaec0da52cd9c4605ea2176","David Fergusson states that significant effort and concentration has been put into devising training regimes for a number of different technologies in distributed and high-performance computing. It is expected that e-infrastructure and data management training will be included in the academic curricula in Europe and other places by 2020 to address these issues. Elements of data management training are included in the secondary educational system and a growing number of skilled ICT pupils and students exist. These goals and challenges for e-Science education have been emphasized in curricula development discussions at the ICEAGE Curricula Development Workshop held in Brussels from February 14-15, 2008. e-Science and distributed computing educators from academic institutions in Europe and the US have attended the workshop, which answered a call to action voiced by OGF and e-IRG members."
"10.2481/dsj.GRDI-013","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883085439&doi=10.2481%2fdsj.GRDI-013&partnerID=40&md5=261d93ab234c4f508a974ca8e79657ab","Virtual Research Environments are innovative, web-based, community-oriented, comprehensive, flexible, and secure working environments conceived to serve the needs of modern science. We overview the existing initiatives developing these environments by highlighting the major distinguishing features. We envisage a future where regardless of geographical location, scientists will be able to use their Web browsers to seamlessly access data, software, and processing resources that are managed by diverse systems in separate administration domains via Virtual Research Environments. We identify and discuss the major challenges that should be resolved to fully achieve the proposed vision, i.e., large-scale integration and interoperability, sustainability, and adoption."
"10.2481/dsj.GRDI-008","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84882981273&doi=10.2481%2fdsj.GRDI-008&partnerID=40&md5=7d8c253aae6a1945f14c815754f8858e","Mark A Parsons suggests that the first purpose of data policy needs to be to serve the objectives of the organization or project sponsoring the collection of the data. Data policy should also serve the broader goals of advancing scientific and scholarly inquiry and society at large with research data. This is especially true with government-funded data, which comprise of the majority of research data. Data policy needs to address multiple issues, depending on the nature and objectives of the data. These issues include data access requirements, data preservation and stewardship requirements, standards and compliance mechanisms, data security issues, privacy and ethical concerns, and potentially specific collection protocols and defined data flows. The specifics of different policies can vary and need to be accessible and preserved for future access."
"10.2481/dsj.GRDI-009","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84882977842&doi=10.2481%2fdsj.GRDI-009&partnerID=40&md5=c864d929a249eb1cefd1dce224ac1fcf","Digital information is a vital resource in knowledge economy, valuable for research and education, science and the humanities, creative and cultural activities, and public policy. These data are used by decision makers for improving the quality of life of citizens. The project PARSE.Insight has conducted a survey that has identified the main threats to the permanent access to the data collected by different scientific disciplines and stakeholders. Digital preservation aims at countering these threats by maintaining digital information accessible, independently understandable and usable by a designated community, and with evidence supporting its authenticity, over the long term. Digital preservation is a relatively new discipline whose importance increases as the amount of knowledge encoded exclusively in digital form increases. The basic concepts underlying digital preservation, including its definition, are set by the Open Archive Information System (OAIS) Reference Model."
"10.2481/dsj.GRDI-010","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84882964442&doi=10.2481%2fdsj.GRDI-010&partnerID=40&md5=a3d1701c5b6bb3db296e39934c687131","Provenance is important in a number of contexts and for various reasons for trustworthy data. database record might represent the results of a physical examination of a patient in a hospital, and the transformation might be the application of a certain treatment. The record might be the values of input parameters in a scientific data management context, and the transformation might be an experiment executed for these parameter values. Provenance, digitally captured, can be used to answer questions about the before images and after effects of real-life situations. The most important questions focus on explanation, accountability, and repeatability. Explanation allows users to offer the reasons behind some digital artifact appearing. Accountability allows them to identify those responsible for the existence of a digital artifact."
"10.2481/dsj.GRDI-001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84882936514&doi=10.2481%2fdsj.GRDI-001&partnerID=40&md5=ba30a7298660ff5d24e2a60332d41511","A general-purpose Global Research Data Infrastructure (GRDI) for all sciences and research purposes is not conceivable for the next decade as there are too many discipline-specific modalities that currently prevail for such generalisation efforts to be effective. On the other hand, a more pragmatic approach is to start from what currently exists, identify best practices and key issues, and promote effective inter-domain collaboration among different components forming an ecosystem. This will promote interoperability, data exchange, data preservation, and distributed access (among others). This ecosystem of interoperable research data infrastructures will be composed of regional, disciplinary, and multidisciplinary components, such as libraries, archives, and data centres, offering data services for both primary datasets and publications. The ecosystem will support data-intensive science and research and stimulate the interaction among all its elements, thus promoting multidisciplinary and interdisciplinary science. This special issue includes a set of independent papers from renowned experts on organisational and technological issues related to GRDIs. These documents feed into and compliment the GRDI2020 roadmap, which supports a Global Research Data Infrastructure ecosystem."
"10.1057/jma.2013.15","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031496420&doi=10.1057%2fjma.2013.15&partnerID=40&md5=c208fbe5e8aea051f86f7defb3173878","The purpose of this study is to identify those consumers who have a high intention to purchase an automobile in a multiracial and multicultural society such as the city of Kuala Lumpur, Malaysia. The research examines the demographic factors that influence the consumer’s purchase intention moderated by their media usage. The research conducted by this study utilizes a hierarchical multilevel regression analysis. The research sample utilized the snowball sampling method via online survey. The population sample consisted of 523 respondents of Kuala Lumpur, Malaysia. The results indicate that the purchase intention of automobile consumers is affected by their media usage, income and race. The value of the study results is that they assist in identifying those consumers who have a high intention to purchase an automobile in a multiracial and multicultural society. © 2013 Macmillan Publishers Ltd."
"10.1057/jma.2013.16","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85009180203&doi=10.1057%2fjma.2013.16&partnerID=40&md5=aad4592a69dc04ad0ffe0595c3f7d97f","Effective marketing depends on knowledge of customer behavior and competition. This is especially true for location-based marketers who risk sending alerts at inopportune moments or to the wrong targets. This study analyzes check-ins around Times Square in New York City using Foursquare, a location-based mobile application. The findings reveal interesting patterns of certain consumer segments frequenting different businesses in the Retail and Foods sectors at specific times and days. Retailers can use the results to develop better marketing plans for specific segments. © 2013 Macmillan Publishers Ltd."
"10.1057/jma.2013.13","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84991093662&doi=10.1057%2fjma.2013.13&partnerID=40&md5=2ea76a7ef2086f9e575fbb5d08977798","The increasing prevalence of e-commerce in the consumer context raises the issue of the difference in consumer perception and behavior considering single-brand e-retailers and multi-brand e-retailers. In which situations and conditions, if any, do customers prefer one of these types of e-retailers? Little empirical research has been conducted in area. We investigate an online purchasing behavior model that includes the two types of e-retailers. The model confirms a difference in customers’ perceptions between single-brand e-retailers and multi-brand e-retailers. In fact, for single-brand e-retailers, value, and implicitly quality, are indicators of repurchase intention, while in both websites, when customers believe that the brand of the product gives them prestige, they tend to repurchase. However, the impact of price on prestige differs between these two types of websites. © 2013 Macmillan Publishers Ltd."
"10.1057/jma.2013.11","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84977507629&doi=10.1057%2fjma.2013.11&partnerID=40&md5=bf16401c73ed43d92f30f7965bd2d663","On the basis of an exclusive business-to-business database comprising 5000 customers, efficient and optimal customer portfolios are determined, and it is shown how marketing decision-makers can use this information in their marketing strategies to optimize the revenue growth of the company. The applicability of portfolio analysis is documented, and it is examined how such an optimization analysis can be used to explore the growth potential of a company. Finally, methods to measure customer performance are developed and it is shown how these performance measures complement the optimization analysis. © 2013 Macmillan Publishers Ltd."
"10.1057/jma.2013.12","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84941661380&doi=10.1057%2fjma.2013.12&partnerID=40&md5=a02ff392666d5d1bf54f504aee4ad099","The purpose of this study is to investigate what factors influence young consumers to become fans of brand pages on social-networking sites and their intentions to purchase products or services from them. The data for this study were collected from college-age social media users using snowball sampling techniques. A modified Technology Acceptance Model (TAM) was used to examine the relationships between the constructs. The conceptual model was tested by using structural equation modeling. Perceived ease of use of social-networking sites and perceived usefulness of advertising on social-networking sites are significant predictors of attitudes. Both attitudes toward brand pages and social influence have positive impact on young consumers’ intentions to join (IJ) brand pages. Becoming fans of brand pages also affect consumers’ purchase intentions. This study provides marketers a better understanding of the effects of advertising informational values and peer influence on young consumers’ IJ brand pages. Capitalizing on these effects, marketers can build more appealing and involving content to recruit and increase brand page fans. © 2013 Macmillan Publishers Ltd."
,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883090824&partnerID=40&md5=a56408053aa2ca0efde31d72be070829","In the vision of Global Research Data Infrastructures (GRDIs), data storage and management plays a crucial role. A successful GRDI will require a common globally interoperable distributed data system, formed out of data centres, that incorporates emerging technologies and new scientific data activities. The main challenge is to define common certification and auditing frameworks that will allow storage providers and data communities to build a viable partnership based on trust. To achieve this, it is necessary to find a long-term commitment model that will give financial, legal, and organisational guarantees of digital information preservation. In this article we discuss the state of the art in data storage and management for GRDIs and point out future research directions that need to be tackled to implement GRDIs."
"10.2481/dsj.GRDI-006","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883046096&doi=10.2481%2fdsj.GRDI-006&partnerID=40&md5=8df8e4b5db08c1d8b4bf8e663aa0243e","Discussions about funding, sustainability, and governance of any e-infrastructure service tend to create tensions between bottom-up and top-down, static and dynamic, and prescriptive and descriptive approaches. Data management brings in additional challenges due to the time aspect. Funding commitments need to be made for long periods of time, and decisions about sustainability and governance models could be seen as setting a precedent that ties down parties in the future. Funding of the data services is seen as important and there is a consensus about the need to secure funding for storing data. Sustainability of the data management is harder to evaluate, as it is mentioned as an inherent aspect of the funding commitments. Governance issues beyond discipline-based solutions are in an early stage of development. This area will be influenced by the surrounding regulatory framework along with the discussions about the value of the data for new user communities."
"10.2481/dsj.GRDI-005","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883025513&doi=10.2481%2fdsj.GRDI-005&partnerID=40&md5=e87d3af1036ff59d4c035cca26f40877","Discovery of documents, data sources, facts, and opinions is at the core of digital information and knowledge services. The ability to search, discover, compile, and analyze relevant information for a user's specific tasks is important in science, business, and the society. Information discovery is based on search engines, which in turn have mostly focused on finding documents of various kinds, such as publications, Web pages, and news articles. Search engine technology has been developed for both Internet/Web search and with different requirements, enterprise search within companies and intranets of organizations. A few commercial stakeholders such as Google and Microsoft have dominated Internet search. They provide excellent support for simple queries to satisfy popular information needs by as opposed to expert-level needs by advanced users. The spectrum of search scopes and items to be discovered has been expanded by multimedia data, such as photos, videos, and music, and by social media, such as blogs, tweets, and online forums."
"10.2481/dsj.GRDI-002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883004945&doi=10.2481%2fdsj.GRDI-002&partnerID=40&md5=cba85b9d296621a408cff6e5c85dd8a4","The High-Level Expert Group on Scientific Data (HLEG, 2010) was given the responsibility by the European Commission's Directorate-General for Information Society and Media to prepare a 'vision 2030' for the evolution of e-infrastructure for scientific data. The HLEG, 2010 was given the responsibility by the European Commission's Directorate-General for Information Society and Media to prepare the 'vision 2030' for the evolution of e-infrastructure for scientific data to address existing limitations and to allow scientists to discover all data that is relevant for their task. HLEG, 2010 recommended in its final report for the creation of a global framework in which the data itself became the infrastructure, emerging as a valuable asset, on which science, technology, the economy, and society could advance."
"10.2481/dsj.GRDI-003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84882989442&doi=10.2481%2fdsj.GRDI-003&partnerID=40&md5=2793adc31b1c1c0f6e482a3e34eedb60","Aristides Gionis presents a report to highlight opportunities for enhancing global research data infrastructures from the point of view of data analysis. The report discusses various directions and data-analysis functionalities for supporting such infrastructures. It groups the proposed data-analysis challenges around two main themes, such as text analytics, information retrieval, filtering, aggregation, and dissemination via social platforms and mining data consortiums. A broad distinction of the two themes is that the first theme refers to data that contain textual content and involve some amount of text processing while the second theme refers mostly to structured data. The discussion proceeds by providing a short description of each of the two themes and place them with respect to the state of the art. Advanced data-analysis techniques can then be applied to these datasets, either analyzing each dataset separately or performing joined analysis of appropriately selected subsets of datasets."
"10.2481/dsj.GRDI-004","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84882954494&doi=10.2481%2fdsj.GRDI-004&partnerID=40&md5=6bbaf183741438ef1f2f28bbee7c5552","Data interoperability continues to be a significant challenge for researchers to address several issues. The 'data' and 'interoperability' concepts are difficult to be fully perceived and actually lead to different perceptions in diverse communities. This problem is further amplified when considered in the context of research data infrastructures that are expected to serve a number of communities of practice potentially involved in diverse application scenarios, each characterized by a specific sharing problem. The term 'interoperability' does not have a clear definition shared by the overall community despite being used to describe a core class of problems in many systems and application scenarios. Data integration, and data exchange are confused, as they share some commonalities in terms of issues and goals. Implementing data interoperability requires realizing data integration and data exchange along with an enabling effective use of the data that become available."
"10.2481/dsj.G-034","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880078287&doi=10.2481%2fdsj.G-034&partnerID=40&md5=77fd4afdc2234f09edb15e95a0388584","Monitoring systems consisting of several magnetometers and cameras were installed to detect artificial magnetic disturbances at the Kakioka, Memambetsu, and Kanoya Magnetic Observatories in Japan. These systems calculate the magnetic dipole moment and source position of artificial magnetic disturbances as a routine matter. For sources of disturbance that are not magnetic dipole moments, each event must be tackled individually. For a non-dipole disturbance at Kanoya from 16 to 22 June 2011, the position and intensity of two lines of direct currents were calculated, and the baseline value affected by these currents was corrected appropriately."
"10.2481/dsj.G-039","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880077211&doi=10.2481%2fdsj.G-039&partnerID=40&md5=346fab812f7cbb6089e01c5543327578","There are six landmark architectures in the Kakioka Magnetic Observatory. The characteristicsof these buildings are asfollows:(1)The variation building is a semi-underground granite building that has two advantages: one is stability at room temperature in the basement, and the other is low humidity in the room above ground.(2) The thick wallsof the magnetic laboratory, the new variation building,and the new absolute building are made ofunusual white brick. (3) Theornamentationfound on the facade of the magnetic laboratory, the office building,and the electrometer hutshowinfluence from the building design trends of the era, and theoffice building is one of the first examples of Spanish detail implementation in modern Japan."
"10.2481/dsj.G-033","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880077174&doi=10.2481%2fdsj.G-033&partnerID=40&md5=55a8b3b8a80ea8f09f70ecfd561add2e","The Geospatial Information Authority of Japan (GSI) has been conducting geomagnetic surveys in Japan since 1949 to clarify the geographical distribution of direction, intensity of the geomagnetic field,and their secular variations. Recently, wehave carried out continuous observationsat geodetic observatories and continuous geomagnetic stations and repetitive observationsat several first-order geomagnetic stations. Theresultsof the surveys, ""geomagnetic charts"",havebeen published every 10 years. In 2011, GSI released the newest geomagnetic chartsfortheepoch 2010.0 created by adoptinganew spatial-temporal model. This model needs continuous dataof good quality. In addition to the GSI geodetic observatories, the Kakioka Magnetic Observatory, which has been conducting high-quality and stable observations for 100 years, makes a large contribution."
"10.2481/dsj.G-040","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84879966597&doi=10.2481%2fdsj.G-040&partnerID=40&md5=b454264b91b7403ef61f2caba2292714","The Japan Meteorological Agency (JMA) is operating four geomagnetic observatories in Japan. Kakioka Magnetic Observatory (KMO), commissioned in 1913, is the oldest. The hourly records at KMO The hourly records at KMO The hourly records at KMO The hourly records at KMO The hourly records at KMO The hourly records at KMO The hourly records at KMO The hourly records at KMO The hourly records at KMO The hourly records at KMO The hourly records at KMO The hourly records at KMO The hourly records at KMO The hourly records at KMO The hourly records at KMO The hourly records at KMO The hourly records at KMO The hourly records at KMO cover cover cover cover over over over over almost 10almost 10almost 10almost 10almost 10almost 100 years.0 years.0 years.0 years.0 years.0 years.0 years. KMO is JMA's headquarters for geomagnetic and geoelectric observations. Almost all data are available at the KMO website free of charge for researchers. KMO and two other observatories have been certified as INTERMAGNET observatories, and quasi-real-time geomagnetic data from them are available at the INTERMAGNET website."
"10.1089/big.2013.0013","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84991780941&doi=10.1089%2fbig.2013.0013&partnerID=40&md5=ed6bf21738d1324d5e9e6b95dd0f07e5",[No abstract available]
"10.1089/big.2013.0011","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84991773782&doi=10.1089%2fbig.2013.0011&partnerID=40&md5=5e0b3e446ab8b22b9d35977c472cd258","Apache Drill is a distributed system for interactive ad-hoc analysis of large-scale datasets. Designed to handle up to petabytes of data spread across thousands of servers, the goal of Drill is to respond to ad-hoc queries in a low-latency manner. In this article, we introduce Drill's architecture, discuss its extensibility points, and put it into the context of the emerging offerings in the interactive analytics realm. © Mary Ann Liebert, Inc. 2013."
"10.1089/big.2013.0010","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84991746870&doi=10.1089%2fbig.2013.0010&partnerID=40&md5=9f997afc0126746000f51a67e222d7e9",[No abstract available]
"10.2481/dsj.WDS-046","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878077426&doi=10.2481%2fdsj.WDS-046&partnerID=40&md5=488a5bcfc9e6fda1bd32c95b99ecc3eb","Many long-term geomagnetic observation results recorded on photographic bromide paper have not yet been fully digitized. To that end, we developed a method to automatically convert photographic records to one-minute digital data. We applied our method to the observation records of Kakioka Magnetic Observatory and confirmed that the resolution of time and amplitude could be greatly improved by numerical conversion compared with conventional data conversion by hand scaling. Our results suggest that highly precise digitization of analog magnetograms is possible."
"10.2481/dsj.WDS-043","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84877865800&doi=10.2481%2fdsj.WDS-043&partnerID=40&md5=a29482f5390a6840836ad8c7b1970092","Researchers across disciplines are increasingly utilizing electronic tools to collect, analyze, and organize data. However, when it comes to publishing their work, there are no common, well-established standards on how to make that data available to other researchers. Consequently, data are often not stored in a consistent manner, making it hard or impossible to find data sets associated with an article - even though such data might be essential to reproduce results or to perform further analysis. Data repositories can play an important role in improving this situation, offering increased visibility, domain-specific coordination, and expert knowledge on data management. As a leading STM publisher, Elsevier is actively pursuing opportunities to establish links between the online scholarly article and data repositories. This helps to increase usage and visibility for both articles and data sets and also adds valuable context to the data. These data-linking efforts tie in with other initiatives at Elsevier to enhance the online article in order to connect with current researchers' workflows and to provide an optimal platform for the communication of science in the digital era."
"10.2481/dsj.WDS-044","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84877861676&doi=10.2481%2fdsj.WDS-044&partnerID=40&md5=23979f6f4b916207a61b73f7e4e5bc4e","The Japanese Antarctic Research Expedition has conducted geomagnetic observations at Syowa Station, Antarctica, since 1966. Geomagnetic variation data measured with a fluxgate magnetometer are not absolute but are relative to a baseline and show drift. To enhance the importance of the geomagnetic data at Syowa Station, therefore, it is necessary to correct the continuous variation data by using absolute baseline values acquired by a magnetic theodolite and proton magnetometer. However, the database of baseline values contains outliers. We detected outliers in the database and then converted the geomagnetic variation data to absolute values by using the reliable baseline values."
"10.2481/dsj.WDS-045","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84877853840&doi=10.2481%2fdsj.WDS-045&partnerID=40&md5=cc0a5c78ad7535290f1388631efa4c61","Differences in modelling techniques and model performance assessments typically impinge on the quality of knowledge extraction from data. We propose an algorithm for determining optimal patterns in data by separately training and testing three decision tree models in the Pima Indians Diabetes and the Bupa Liver Disorders datasets. Model performance is assessed using ROC curves and the Youden Index. Moving differences between sequential fitted parameters are then extracted, and their respective probability density estimations are used to track their variability using an iterative graphical data visualisation technique developed for this purpose. Our results show that the proposed strategy separates the groups more robustly than the plain ROC/Youden approach, eliminates obscurity, and minimizes over-fitting. Further, the algorithm can easily be understood by non-specialists and demonstrates multi-disciplinary compliance."
"10.2481/dsj.WDS-041","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84877850343&doi=10.2481%2fdsj.WDS-041&partnerID=40&md5=c3c2154a8ccafa28df9a74adbacd193b","The World Ocean Database (WOD) is the most comprehensive global ocean profile-plankton database available internationally without restriction. All data are in one well-documented format and are available both on DVDs for a minimal charge and on-line without charge. The latest DVD version of the WOD is the World Ocean Database 2009 (WOD09). All data in the WOD are associated with as much metadata as possible, and every ocean data value has a quality control flag associated with it. The WOD is a product of the U.S. National Oceanographic Data Center and its co-located World Data Center for Oceanography. However, the WOD exists because of the international oceanographic data exchange that has occurred under the auspices of the Intergovernmental Oceanographic Commission (IOC) and the International Council of Science (ICSU) World Data Center (WDC) system. World Data Centers are part of the ICSU World Data System."
"10.2481/dsj.WDS-040","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878100662&doi=10.2481%2fdsj.WDS-040&partnerID=40&md5=64e7738fce57b5b8165fd6876b62f708","Cross-sectional studies have become important for an improved understanding of various Solar-Terrestrial Physics (STP) fields, given the great variety and different types of observations from the Sun to the Earth. In order to better combine, compare, and analyze different types of data together, a system named STARS (Solar-Terrestrial data Analysis and Reference System) has been developed. Cross-sectional study requires cooperative work. STARS has two functions for cooperative work, the ""Stars Project List (SPL)"" and the ""Event Listing"". The SPL is used for exchanges of plotting information by cooperating persons. The event list database provides all users of STARS hints for recognizing typical occurrences of STP phenomena."
"10.2481/dsj.WDS-039","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84877869854&doi=10.2481%2fdsj.WDS-039&partnerID=40&md5=80f45926ed53352f4202947f326d35c4","The Japan Agency for Marine-Earth Science and Technology (JAMSTEC) provides users of its data with comprehensive search services that enable users to find data in JAMSTEC's various data dissemination sites. These are the 'JAMSTEC Data Search Portal' that helps users to search for observational data on a map and the 'JAMSTEC Data Catalog' that enables users to find data sites by selecting science keywords. The 'Data Search Portal' and the 'Data Catalog' have been developed and operated as dedicated metadata publication and search services that collaborate with data sites in JAMSTEC."
"10.1057/jma.2013.6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031491731&doi=10.1057%2fjma.2013.6&partnerID=40&md5=8cf2f85e9e9e590ab4a66d0e0bd1211e","We use a flexible hierarchical Bayes approach to provide a method for developing a personalized consideration set recommender system. The proposed method determines which products to recommend and in what order to present these recommendations. We demonstrate our method in the context of internet retail for home appliances. The empirical results show that the proposed method offers significant advantages in terms of both hit measures and exploring preference distribution. The recommender system that we develop can be used to provide personalized consideration set suggestions based on consumer preferences at the abstract level and to generate a potential list of customers for new product messages. Implications and suggestions for future research are also provided. © 2013 Macmillan Publishers Ltd."
"10.1057/jma.2013.8","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031490822&doi=10.1057%2fjma.2013.8&partnerID=40&md5=bff013c2714d5337978104bdac4e75f7","Developing mathematical models does convey an impression or perception to the average business person that these tools are highly accurate due to the complexity in creating these tools. This article attempts to dispel this notion by conveying to the reader the accuracy limitations in developing predictive models. At the same time, the author discusses some of the rationale as to why these limitations exist. Yet, even with model accuracy being a limiting factor, these tools still yield tremendous business benefits that accrue right to the bottom line. © 2013 Macmillan Publishers Ltd."
"10.1057/jma.2013.4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85014016719&doi=10.1057%2fjma.2013.4&partnerID=40&md5=749f31c64ae9e8ec16634cca26c883dd","Advanced analytics methods, such as predictive modelling, are increasingly being deployed within business processes – helping companies to increase their revenues or reduce their costs. This article shows how advanced analytics may be embedded directly into business applications, through the use of software drawn from libraries of mathematical and statistical algorithms. We list examples of applications that may benefit from embedded algorithms and provide a case study example on the theme of propensity modelling for customer management. Finally, the article summarises the typical stages in building a business application containing embedded analytics, and the skill sets that are likely to be required. © 2013 Macmillan Publishers Ltd."
"10.1057/jma.2013.5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84926670248&doi=10.1057%2fjma.2013.5&partnerID=40&md5=8b88243274739e0fc2bb6202c16c5b26","The relationship between age and the importance of brand relation is investigated. In a first exploratory survey, this relation appeared to be U-shaped. Brand relations are relatively strong for both consumers under 30 and those over 60. In a second survey, a theory-based explanation is sought as to why the importance of brand relation increases with age over 50. The answer lies in a decreased tolerance of ambiguity, consistent with low cognitive-affective complexity. Given hedonic innovativeness and future time perspective, the relation between age and brand relation becomes stronger. The results imply that brands should communicate to older consumers: ‘we have a long future together, our new products are attractive and we don’t take you out of your comfort zone’. © 2013 Macmillan Publishers Ltd."
"10.1057/jma.2013.7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84919666054&doi=10.1057%2fjma.2013.7&partnerID=40&md5=a4198c3bd90ca5dff5919c12ce4b29bd","This article analyzes the use of single-item indicators in marketing research and their utilization in structural equation modeling (SEM). The study provides a literature review regarding the debate of the use of single-item measures in social sciences research and methodologically in SEM. The analysis of recent studies that use single-item indicators from topmarketing journals provides information regarding the types of constructs fit for single-item measurement and their use in SEM. The article presents clarifications to the debate regarding the use of single-item indicators in marketing research, gives examples of types of constructs measurable through single-item indicators and provides recommendations that add knowledge to the empirical analysis and methodology domains of marketing research. © 2013 Macmillan Publishers Ltd."
,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84877869393&partnerID=40&md5=c12d9a31336b4f9cd5cc773c33eb19c9","Digital data and service centers, such as those envisaged by the ICSU World Data System (WDS), are subject to a wide-ranging collection of requirements and constraints. Many of these requirements are traditionally difficult to assess and to measure objectively and consistently. As a solution to this problem, an approach based on a maturity model is proposed. This adds significant value not only in respect to objective assessment but also in assisting with evaluation of overlapping and competing criteria, planning of continuous improvement, and progress towards formal evaluation by accreditation authorities."
"10.2481/dsj.WDS-037","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84877863303&doi=10.2481%2fdsj.WDS-037&partnerID=40&md5=3d4de3554b4b503ede20cbcd694a2257","From 1926 to 1969, a long term solar full disk observation was done by Kyoto University. Daily Ca II K (393.4 nm) spectroheliographic images and white light images were recorded on photographic plates. In this report, we will give the current status of our project to digitize all these images and to construct a database of these images for public use through the IUGONET system. In addition, we will discuss our perspective on the scientific analysis of the database by taking the solar CaII K brightness as a proxy measure of the solar UV irradiance on the terrestrial upper atmosphere."
"10.2481/dsj.WDS-034","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84877862709&doi=10.2481%2fdsj.WDS-034&partnerID=40&md5=4064a4f024c3278fe1f2951568d0a672","An international data archive is critical for understanding the climate system dynamics of the cryosphere. Currently, no such system exists though data collection and integration efforts are ongoing. The Cryosphere Data Archive Partnership (CrDAP) is developing an open system for storing cryospheric observation data and metadata. First stage data handling in CrDAP focused on integrating point observational and photographic data. The metadata structure of CrDAP was extended based on ISO 19115, which is a geographic information metadata standard of the International Organization for Standardization (ISO)."
"10.2481/dsj.WDS-033","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84877862242&doi=10.2481%2fdsj.WDS-033&partnerID=40&md5=85d968a2f734b2d655b0cdb1abbd5f4b","The World Data Center for Geophysics in Boulder, Colorado is hosted by the National Geophysical Data Center (NGDC). NGDC's vision is to be the world's leading provider of geophysical and environmental data, information, and products. NGDC's mission is to provide long-term scientific data stewardship for geophysical data, ensuring quality, integrity, and accessibility. Faced with ever expanding data volumes and types of data, NGDC is developing more innovative techniques for science data stewardship based in part on data mining and fuzzy logic. Use of these techniques will allow NGDC to more effectively provide data stewardship for its own scientific data archives and perhaps the broader World Data System."
"10.2481/dsj.WDS-035","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84877856549&doi=10.2481%2fdsj.WDS-035&partnerID=40&md5=454ae04488a2132adcfa10a7cfd2ca0b","Since 1997, the Global Geodynamics Project (GGP) stations have used a text-based data format. The main drawback of this type of data coding is the lack of data integrity during the data flow processing. As a result, metadata and even data must be checked by human operators. In this paper, we propose a new format for representing the GGP data. This new format is based on the eXtensible Markup Language (XML)."
"10.2481/dsj.WDS-036","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84877850732&doi=10.2481%2fdsj.WDS-036&partnerID=40&md5=7a39e1c67da20f518dc6cba073e9e1df","To optimize space weather research and information services, it is important to establish a comprehensive system that enables us to analyze observation and simulation data in an integrated manner. For this, we recently constructed a new computing environment called the ""Space Weather Cloud Computing System"" of the National Institute of Information and Communications Technology (NICT). Currently, the Space Weather Cloud contains a high performance computer, a distributed mass storage system using the Grid Data Farm (Gfarm) technology, servers for analysis and visualization of data, a job service based on the RCM (R&D Chain Management) system, servers for Solar-Terrestrial data Analysis, and the Reference System (STARS)."
"10.2481/dsj.WDS-024","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84876195531&doi=10.2481%2fdsj.WDS-024&partnerID=40&md5=077df29177477e0e11e738aec20ec0fd","It is often discussed that the fourth methodology for science research is ""informatics"". The first methodology is a theoretic approach, the second one is observation and/or experiment, and the third one is computer simulation. Informatics is a new methodology for data intensive science, which is a new concept based on the fact that most scientific data are digitalized and the amount of data is huge. The facilities to support informatics are cloud systems. Herein we propose a cloud system especially designed for science. The basic concepts, design, resources, implementation, and applications of the NICT science cloud are discussed."
"10.2481/dsj.WDS-026","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84876175508&doi=10.2481%2fdsj.WDS-026&partnerID=40&md5=a76a61830c7b57306e080ac38f85148f","A method for prediction and simulation based on the Cell Based Geographic Information System(GIS) as Cellular Automata (CA) is proposed together with required data systems, in particular metasearch engine usage in an unified way. It is confirmed that the proposed cell based GIS as CA has flexible usage of the attribute information that is attached to the cell in concert with location information and does work for disaster spreading simulation and prediction."
"10.2481/dsj.WDS-030","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84876172757&doi=10.2481%2fdsj.WDS-030&partnerID=40&md5=b297c6a38d2866b6bc8722ae47dca341","An overview of the Inter-university Upper atmosphere Global Observation NETwork (IUGONET) project is presented with a brief description of the products to be developed. This is a Japanese inter-university research program to build the metadata database for ground-based observations of the upper atmosphere. The project also develops the software to analyze the observational data provided by various universities/institutes. These products will be of great help to researchers in efficiently finding, obtaining, and utilizing various data dispersed across the universities/institutes. This is expected to contribute significantly to the promotion of interdisciplinary research, leading to more a comprehensive understanding of the upper atmosphere."
"10.2481/dsj.WDS-025","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84876172080&doi=10.2481%2fdsj.WDS-025&partnerID=40&md5=97f7948dd284c801c320418dddae4270","The Space Physics Archive Search and Extract (SPASE) project is an international collaboration among Heliophysics (solar and space physics) groups concerned with data acquisition and archiving. The S PASE group has simplified the search for data through the development of the SPASE Data model as a common method to describe data sets in the archives. The data model is an XML-based schema and is now in operational use. The use is expanding, but there are still other groups who could benefit from adopting SPASE. We discuss the present state of SPASE usage and how we foresee development in the future."
"10.2481/dsj.WDS-028","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84876171523&doi=10.2481%2fdsj.WDS-028&partnerID=40&md5=231d7901e7aa7bc191fdeb00c100a9ce","Environmental monitoring in ecological and hydrological watershed-scale research is an important and promising area of application for wireless sensor networks. This paper presents thesystem design of the IPv6 wireless sensor network (IPv6WSN) in the Heihe River watershed in the Gansu province of China to assist ecological and hydrological scientists collecting field scientific data in an extremely harsh environment. To solve the challenging problemsthey face, this paper focuseson the key technologies adopted in our project, metadata modeling for the IPv6WSN. The system design introduced in this paper provides a solid foundation for effective use of a self-developed IPv6 wireless sensor network by ecological and hydrological scientists."
"10.2481/dsj.WDS-027","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84876165451&doi=10.2481%2fdsj.WDS-027&partnerID=40&md5=6fe9c5382a54607aad2523975cf0b512","This paper focuses on using data mining technology to efficiently and accurately discover habitats and stopovers of migratory birds. The three methods we used are as follows: 1. a density-based clustering method, detecting stopovers of birds during their migration through density-based clustering of location points; 2. A location histories parser method, detecting areas that have been overstayed by migratory birds during a set time period by setting time and distance thresholds; and 3. A time-parameterized line segment clustering method, clustering directed line segments to analyze shared segments of migratory pathways of different migratory birds and discover the habitats and stopovers of these birds. Finally, we analyzed the migration data of the bar-headed goose in the Qinghai Lake Area through the three above methods and verified the effectiveness of the three methods and, by comparison, identified the scope and context of the use of these three methods respectively."
"10.2481/dsj.WDS-023","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84876152257&doi=10.2481%2fdsj.WDS-023&partnerID=40&md5=873edcecd1b700d0034720dd5dac3ac1","We present a new concept of analysis using visualization of large quantities of simulation data. The time development of 3D objects with high temporal resolution provides the opportunity for scientific discovery. We visualize large quantities of simulation data using the visualization application 'Virtual Aurora' based on AVS (Advanced Visual Systems) and the parallel distributed processing at ""Space Weather Cloud"" in NICT based on Gfarm technology. We introduce two results of high temporal resolution visualization: themagnetic fluxrope generation process and dayside reconnection using a system of magnetic field line tracing."
"10.2481/dsj.WDS-029","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84876137253&doi=10.2481%2fdsj.WDS-029&partnerID=40&md5=ee09371f10d7e13da6969a5784588eea","An outline of a planned system for the global space-weather monitoring network of NICT (National Institute of Information and Communications Technology) is given. This system can manage data collection much more easily than our current system by installations of autonomous recovery, periodical state monitoring, and dynamic warning procedures. According to a provisional experiment using a network simulator, the new system will work under limited network conditions, e.g., a 160 msec delay, a 10 % packet loss rate, and a 500 Kbps bandwidth."
"10.2481/dsj.WDS-021","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84876182847&doi=10.2481%2fdsj.WDS-021&partnerID=40&md5=54d78bed2ef36db1958dcf12892f26e4","Diverse data accumulated by many science projects make up the most significant legacy of the International Polar Year (IPY 2007-2008). The Polar Data Center (PDC) of the National Institute of Polar Research (NIPR) has a responsibility to manage these data for Japan as a National Antarctic Data Center (NADC) and as the World Data Center (WDC) for Aurora. During the IPY, a significant number of multidisciplinary metadata records were compiled from IPY endorsed projects with Japanese activity. A tight collaboration was established between the Global Change Master Directory (GCMD), the Polar Information Commons (PIC), and the newly established World Data System (WDS)."
"10.2481/dsj.WDS-022","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84876181920&doi=10.2481%2fdsj.WDS-022&partnerID=40&md5=118fdd3455e478071fd34fef6fdbf242","In this paper we discuss environmental changes along the coastal line of Nigeria, especially in the region around Lagos, based on provisional multi-disciplinary analyses of meteorological and maritime observations. This study has revealed that recent environmental change in the Nigerian coastal region has been much more apparent than that of a few years back (1989-2007). Various kinds of ocean debris, transported mainly by coastal wind, are severely affecting the marine and coastal environment. Because the current ocean monitoring system has been found to be troubled by ocean debris, establishing a new system to obtain reliable observational data to monitor and preserve the environment of the coastal region is urgent."
"10.2481/dsj.12-036","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875391968&doi=10.2481%2fdsj.12-036&partnerID=40&md5=b382d226f8a53e47bf98754dc8151c94","Several scientific communities relying on e-science infrastructures are in need of persistent identifiers for data and contextual information. In this article, we present a framework for persistent identification that fundamentally supports context information. It is installed as a number of low-level requirements and abstract data type descriptions, flexible enough to envelope context information while remaining compatible with existing definitions and infrastructures. The abstract data type definitions we draw from the requirements and exemplary use cases can act as an evaluation tool for existing implementations or as a blueprint for future persistent identification infrastructures. A prototypic implementation based on the Handle System is briefly introduced. We also lay the groundwork for establishing a graph of persistent entities that can act as a base layer for more sophisticated information schemas to preserve context information."
,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875397170&partnerID=40&md5=1e3e079a1df39738f3198d27ea95b871","Environment Climate Data Sweden (ECDS) is a new Swedish research infrastructure, furthering the reuse of scientific data in the domains of environment and climate. ECDS consists of a technical infrastructure and a service organization, supporting the management, exchange, and re-use of scientific data. The technical components of ECDS include a portal and an underlying data catalogue with information on datasets. The datasets are described using a metadata profile compliant with international standards. The datasets accessible through ECDS can be hosted by universities, institutes, or research groups or at the new Swedish federated data storage facility Swestore of the Swedish National Infrastructure for Computing (SNIC)."
"10.2481/dsj.WDS-016","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84876192436&doi=10.2481%2fdsj.WDS-016&partnerID=40&md5=9db5d1a87fe68030a089b72f82f259c1","Ptplot is a set of two dimensional signal plotters components written in Java with multiple properties, such as being embeddable in applets or applications, utilizing automatic or manual tick marks, logarithmic axes, infinite zooming, and much more. The World Data Centre of IPS applies Ptplot as a multiple function online data plot tool by converting various text format data files into Ptplot recognizable XML files with the AWK language. At present, Ptplot has allowed eight archived solar-terrestrial science data sets to be easily plotted, viewed, and downloaded from the IPS web site."
"10.2481/dsj.WDS-020","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84876182938&doi=10.2481%2fdsj.WDS-020&partnerID=40&md5=af57aa9647c1b191aaa08dabee733cab","The World Data System (WDS) requires that WDS data centers have significant data holdings and sustainable data sources integration and sharing mechanism. Research data is one of the important science data resources, but it is difficult to be archived and shared. To develop a long term data integration and sharing mechanism, a new approach to data archiving of research data derived from science research projects has been developed in China. In 2008, the host agency of the World Data Center for Renewable Resources and Environment, authorized by the Ministry of Science and Technology of China, began to implement the first pilot experiment for research data archiving. The approach's data archiving process includes four phases: data plan development, data archiving preparation, data submission, and data sharing and management. In order to make data archiving operate more smoothly, a data archiving environment was established. This includes a uniform core metadata standard, data archiving specifications, a smart metadata register tool, and a web-based data management and sharing platform. During the last 3 years, research data from 49 projects has been collected by the sharing center. The datasets are about 2.26 TB in total size and have attracted over 100 users."
"10.2481/dsj.WDS-018","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84876177058&doi=10.2481%2fdsj.WDS-018&partnerID=40&md5=09731af603d2dcc45f28ebc717de90d4","We describe our knowledge-based service architecture for multi-risk environmental decision-support, capable of handling geo-distributed heterogeneous real-time data sources. Data sources include tide gauges, buoys, seismic sensors, satellites, earthquake alerts, Web 2.0 feeds to crowd source 'unconventional' measurements, and simulations of Tsunami wava propagation. Our system of systems multi-bus architecture provides a scalable and high performance messaging backbone. We are overcoming semantic interoperability between heterogeneous datasets by using a self-describing 'plug-in' data source approach. As crises develop we can agilely steer the processing server and adapt data fusion and mining algorithm configurations in real-time."
"10.2481/dsj.WDS-019","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84876160862&doi=10.2481%2fdsj.WDS-019&partnerID=40&md5=6bff8e6b1e7a5de55ca5022f3c291d97","In this paper, a new approach to the detection of anomalies in geophysical records is connected with a fuzzy mathematics application. The theory of discrete mathematical analysis and collection of algorithms for time series processing constructed on its basis represents the results of this research direction. These algorithms are the consequence of fuzzy modeling of the logic of an interpreter who visually recognizes anomalies in records. They allow analyzing large data sets that are not subjected to manual processing. The efficiency of these algorithms is demonstrated in several important geophysical applications. Plans for an extension of the Russian INTERMAGNET segment are presented."
"10.2481/dsj.WDS-017","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84876156346&doi=10.2481%2fdsj.WDS-017&partnerID=40&md5=d1873df31b6c4a936190a28edf15a526","This paper summarizes our effort towards managing the multi-disciplinary disaster-related data from the Great East Japan Earthquake, which happened on March 11, 2011 off the coast of Northeast Japan. This earthquake caused the largest tsunami in the recorded history of Japan, killed many people along the coast, and caused a nuclear disaster in Fukushima, which continues to affect a large area of Japan. Just after the earthquake, we started crisis response data management activities to provide useful information for supporting disaster response and recovery. This paper introduces the various types of datasets we made from the viewpoint of data management processing and draws lessons from our post-disaster activities."
,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875400474&partnerID=40&md5=2ad04eca6df7522bdf06f672de9c3926","The Russian World Data Center for Solar-Terrestrial Physics and the World Data Center for Solid Earth Physics have been collecting, analyzing, archiving, and disseminating data and information on a wide range of geophysical disciplines since the International Geophysical Year 1957-1958. The centers provide free and convenient access for users to their large and permanently increasing volumes of data. Russian WDCs participate in scientific national and international programs and projects, such as InterMAGNET, InterMARGINS, and the International Polar Year. Since 2008 there has been an association of five Russian WDCs and one Ukrainian WDC in a regional segment of the World Data Centers."
,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875394922&partnerID=40&md5=96f4d5e18888f693c88efb08581fa41e","In this paper, we introduce data and information activities of the International Center for Space Weather Science and Education (ICSWSE), Kyushu University, Japan. The principal data source is the MAGDAS (MAGnetic Data Acquisition System) project, which is a global network of geomagnetic observations operated by collaborations between ICSWSE and institutions in many countries. We operate 66 stations including more than 30 stations distributed along the 210° magnetic meridian and more than 10 stations along the magnetic equator. We have established a semi-automatic data acquisition system via the Internet. Provisional data plots and geomagnetic indices derived from the project are available to the scientific community."
,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875391690&partnerID=40&md5=cff3e1875d94cfcfc0b1d4ed0dd061f6","The Japan Oceanographic Data Center has been submitting oceanographic data to the World Data Center for Oceanography through the framework of the International Oceanographic Data and Information Exchange Committee sponsored by the UNESCO/IOC. In the World Ocean Database 2009, which is the compiled database of the WDC for Oceanography, the Japanese contribution has reached about 16% of the total. Japan is one of the main data suppliers for the WDC for Oceanography. JODC would like to continue to contribute to the World Data System with the WDC."
"10.1089/big.2012.0004","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84991824220&doi=10.1089%2fbig.2012.0004&partnerID=40&md5=b80b7f619092089722a7eb7953eee6db",[No abstract available]
"10.1089/big.2013.1508","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84991818987&doi=10.1089%2fbig.2013.1508&partnerID=40&md5=be014a6316c19f95446665086abf8f85","Companies have realized they need to hire data scientists, academic institutions are scrambling to put together data-science programs, and publications are touting data science as a hot - even ""sexy"" - career choice. However, there is confusion about what exactly data science is, and this confusion could lead to disillusionment as the concept diffuses into meaningless buzz. In this article, we argue that there are good reasons why it has been hard to pin down exactly what is data science. One reason is that data science is intricately intertwined with other important concepts also of growing importance, such as big data and data-driven decision making. Another reason is the natural tendency to associate what a practitioner does with the definition of the practitioner's field; this can result in overlooking the fundamentals of the field. We believe that trying to define the boundaries of data science precisely is not of the utmost importance. We can debate the boundaries of the field in an academic setting, but in order for data science to serve business effectively, it is important (i) to understand its relationships to other important related concepts, and (ii) to begin to identify the fundamental principles underlying data science. Once we embrace (ii), we can much better understand and explain exactly what data science has to offer. Furthermore, only once we embrace (ii) should we be comfortable calling it data science. In this article, we present a perspective that addresses all these concepts. We close by offering, as examples, a partial list of fundamental principles underlying data science. © Copyright 2013, Mary Ann Liebert, Inc. 2013."
"10.1089/big.2013.1506","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84991759486&doi=10.1089%2fbig.2013.1506&partnerID=40&md5=90b78fe724c4ea1bc6e42dbd42ee06cf",[No abstract available]
"10.1089/big.2012.1501","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84991756964&doi=10.1089%2fbig.2012.1501&partnerID=40&md5=767960fe324f1f7ce045c63efaab8063","Hadoop is currently the large-scale data analysis ""hammer"" of choice, but there exist classes of algorithms that aren't ""nails"" in the sense that they are not particularly amenable to the MapReduce programming model. To address this, researchers have proposed MapReduce extensions or alternative programming models in which these algorithms can be elegantly expressed. This article espouses a very different position: that MapReduce is ""good enough,"" and that instead of trying to invent screwdrivers, we should simply get rid of everything that's not a nail. To be more specific, much discussion in the literature surrounds the fact that iterative algorithms are a poor fit for MapReduce. The simple solution is to find alternative, noniterative algorithms that solve the same problem. This article captures my personal experiences as an academic researcher as well as a software engineer in a ""real-world"" production analytics environment. From this combined perspective, I reflect on the current state and future of ""big data"" research. © Copyright 2013, Mary Ann Liebert, Inc. 2013."
"10.1089/big.2013.1509","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84991741344&doi=10.1089%2fbig.2013.1509&partnerID=40&md5=21caee62c70a0b6a247179a33c5a2344","""Big data"" has become a major force of innovation across enterprises of all sizes. New platforms with increasingly more features for managing big datasets are being announced almost on a weekly basis. Yet, there is currently a lack of any means of comparability among such platforms. While the performance of traditional database systems is well understood and measured by long-established institutions such as the Transaction Processing Performance Council (TCP), there is neither a clear definition of the performance of big data systems nor a generally agreed upon metric for comparing these systems. In this article, we describe a community-based effort for defining a big data benchmark. Over the past year, a Big Data Benchmarking Community has become established in order to fill this void. The effort focuses on defining an end-to-end application-layer benchmark for measuring the performance of big data applications, with the ability to easily adapt the benchmark specification to evolving challenges in the big data space. This article describes the efforts that have been undertaken thus far toward the definition of a BigData Top100 List. While highlighting the major technical as well as organizational challenges, through this article, we also solicit community input into this process. © Copyright 2013, Mary Ann Liebert, Inc. 2013."
"10.1089/big.2012.1505","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84920279304&doi=10.1089%2fbig.2012.1505&partnerID=40&md5=ad280d70ea8592f1625f6409f09ff6da","The life sciences have entered into the realm of big data and data-enabled science, where data can either empower or overwhelm. These data bring the challenges of the 5 Vs of big data: volume, veracity, velocity, variety, and value. Both independently and through our involvement with DELSA Global (Data-Enabled Life Sciences Alliance, DELSAglobal.org), the Kolker Lab (kolkerlab.org) is creating partnerships that identify data challenges and solve community needs. We specialize in solutions to complex biological data challenges, as exemplified by the community resource of MOPED (Model Organism Protein Expression Database, MOPED.proteinspire.org) and the analysis pipeline of SPIRE (Systematic Protein Investigative Research Environment, PROTEINSPIRE.org). Our collaborative work extends into the computationally intensive tasks of analysis and visualization of millions of protein sequences through innovative implementations of sequence alignment algorithms and creation of the Protein Sequence Universe tool (PSU). Pushing into the future together with our collaborators, our lab is pursuing integration of multi-omics data and exploration of biological pathways, as well as assigning function to proteins and porting solutions to the cloud. Big data have come to the life sciences; discovering the knowledge in the data will bring breakthroughs and benefits. © Copyright 2013, Mary Ann Liebert, Inc. 2013."
"10.2481/dsj.WDS-011","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875452525&doi=10.2481%2fdsj.WDS-011&partnerID=40&md5=06012cce9a5a09096ce66c2797d12611","The International VLBI Service for Geodesy and Astrometry (IVS) is a globally operating service that coordinates and performs Very Long Baseline Interferometry (VLBI) activities through its constituent components. The VLBI activities are associated with the creation, provision, dissemination, and archiving of relevant VLBI data and products. The data and products are stored in dedicated IVS components called 'Data Centers.' The three Primary Data Centers provide identical data holdings. We give a brief overview of the organizational structure of the IVS and describe the general data flow among the various IVS components from preparing observational plans to creating the final products."
,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875405817&partnerID=40&md5=f891020a508d1fd40e3bd7f59449db4f","Increasing demand within the geomagnetism community for high quality neartime or real-time or near-real-time observatory data means there is a requirement for data infrastructure capable of delivering geomagnetic data products over the Internet in a variety of formats we describe a new software system, developed at BGS, which will allow access to our geomagnetic data products both within our organisation's intranet and over the ntranet and over the Internet We demonstrate how the system is designed to afford easy access to the data by a wide range of software clients and allow rapid development of software utilizing our observatory data."
"10.2481/dsj.WDS-008","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875392977&doi=10.2481%2fdsj.WDS-008&partnerID=40&md5=212b0a7dcadd4208a4fcf713bbce2b44","The integration of Taiwan's biodiversity databases started in 2001, the same year that Taiwan joined GBIF as an that Taiwan joined GBIF as an that Taiwan joined GBIF as an that Taiwan joined GBIF as an that Taiwan joined GBIF as an that Taiwan joined GBIF as an that Taiwan joined GBIF as an that Taiwan joined GBIF as an associate participant.Taiwan, hence, embarked on a decade of integrating biodiversity data. Under the support of NSC and COANSC and COA, the database and websites of TaiBIF, TaiBNET (TaiCOL), TaiBOL have been established separately and collaborate with the GBIF, CGBIF, COL, BOL,and EOL respectively. A cross-agency committee was thus established in Academia Sinica 2008 to formulate policiescommittee was thus established in Academic sinca 2008 to formulate policies on data collection and integration as well the mechanism to make data available public. Any commissioned project will hereafter be asked to include these policy requirements in its contract. So far, TaiBIF has gained recognition in Taiwan and abroad for its road effort over the past several years. It can provide its experienceits and insights for others to reference or replicate."
,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875388025&partnerID=40&md5=86424c2edc4d99b830a90aceeb78e07c","The World Data Centre for Geomagnetism, Mumbai has functioned as a division of the Indian Institute of Geomagnetism, Navi Mumbai since its full fledged activities commenced in 1991 in coordination with the International Council of Scientific Unions (ICSU) Panel on World Data Centres. Responsibility for the compilation of final hourly absolute values from nine of the Indian magnetic observatories and deposition of this data to the World Data Centres is undertaken at the centre. We have utilized the full advantage of technology advancement in upgrading our data preservation and conservation policy at various levels. In recent years, the centre has prioritized its activities related to digital preservation to ensure digital archiving of magnetic data from the traditional media and also digital conservation of very old hand written/printed data volumes and magnetograms. In view of the scientific importance of data from the Colaba-Alibag Magnetic Observatory, old magnetograms and data volumes are being converted to digital images for long term preservation. In the digital preservation process, the creation of metadata has become an important component in storing information related to old and current scientific records for future use. The centre also hosts a database driven website to make datasets available online to the global scientific community."
"10.2481/dsj.WDS-009","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875383683&doi=10.2481%2fdsj.WDS-009&partnerID=40&md5=536959e1fe6a0fe39cb6d9504ed10bf7","ISRIC - World Soil Information has a mandate to serve the international community as custodian of global soil information and to increase awareness and understanding of the role of soils in major global issues. To adapt to the current demand for soil information, ISRIC is updating its enterprise data management system, including procedures for registering acquired data, such as lineage, versioning, quality assessment, and control. Data can be submitted, queried, and analysed using a growing range of web-based services -ultimately aiming at full and open exchange of data, metadata, and products- through the ICSU-accredited World Data Centre for Soils."
"10.2481/dsj.WDS-005","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873607762&doi=10.2481%2fdsj.WDS-005&partnerID=40&md5=f7fcab30338f4dcc2dc45021a41c538f","The British Geological Survey has operated a World Data Centre for Geomagnetism since 1966. Geomagnetic time-series data from around 280 observatories worldwide at a number of time resolutions are held along with various magnetic survey, model, and activity index data. The operation of this data centre provides a valuable resource for the geomagnetic research community. The operation of the WDC and details of the range of data held are presented. The quality control procedures that are applied to incoming data are described as is the work to collaborate with other data centres to distribute and improve the overall consistency of data held worldwide. The development of standards for metadata associated with datasets is demonstrated, and current efforts to digitally preserve the BGS analogue holdings of magnetograms and observatory yearbooks are described."
"10.2481/dsj.WDS-006","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873599765&doi=10.2481%2fdsj.WDS-006&partnerID=40&md5=fe32a1b50add20ec19b365598bc79a66","In this report we introduce the development of the WDC for Geophysics, Beijing included our activities in the electronic Geophysical Year (eGY) and in the transition period from WDC to WDS. We also present our future plans. We have engaged in the development of geophysical informatics and related data science. We began the data visualization of geomagnetic fields in the GIS system. Our database has been expanded from geomagnetic data to the data of solid geophysics, including geothermal data, gravity data, and the records of aurora sightings in ancient China. We also joined the study of the history of the development of geophysics in China organized by the Chinese Geophysical Society (CGS)."
"10.2481/dsj.WDS-007","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873597174&doi=10.2481%2fdsj.WDS-007&partnerID=40&md5=5b957d74178a678a215ec3cef79097ce","The Centre de Données astronomiques de Strasbourg (CDS), created in 1972, has been a pioneer in the dissemination of digital scientific data. Ensuring sustainability for several decades has been a major issue because science and technology evolve continuously and the data flow increases endlessly. The paper briefly describes CDS activities, major services, and its R&D strategy to take advantage of new technologies. The next frontiers for CDS are the new Web 2.0/3.0 paradigm and, at a more general level, global interoperability of astronomical on-line resources in the Virtual Observatory framework."
"10.2481/dsj.WDS-004","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873587407&doi=10.2481%2fdsj.WDS-004&partnerID=40&md5=6a72f8d98ef3e348c8d05b6e86de15c8","Geomagnetic indices are basic data in Solar-Terrestrial physics and in operational Space Weather activities. The International Service of Geomagnetic Indices (ISGI) is in charge of the derivation and dissemination of the geomagnetic indices that are acknowledged by the International Association of Geomagnetism and Aeronomy (IAGA, an IUGG association). Institutes that are not part of ISGI started early in the Internet age to circulate on-line preliminary values of geomagnetic indices. In the absence of quality stamping, this resulted in a very confusing situation. The ISGI label was found to be the simplest and the safest way to insure quality stamping of circulated geomagnetic indices."
"10.2481/dsj.WDS-003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873582171&doi=10.2481%2fdsj.WDS-003&partnerID=40&md5=e786ad1dd46eeb7ceca36938a2d2b047","Establishment of the Russian-Ukrainian WDS Segment and its state of the art, main priorities and research activities are described. One of the high priority tasks for Segment members is development of a common information space - transition from Legacy Systems and individual services to a common, globally interoperable, distributed data system that incorporates emerging technologies and new scientific data activities. The new system will build on the potential and added value offered by advanced interconnections between data management and data processing components for disciplinary and multidisciplinary applications. Thus, the principles of the architectural organization of intelligent data processing systems are discussed in this paper."
"10.2481/dsj.WDS-042","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873581098&doi=10.2481%2fdsj.WDS-042&partnerID=40&md5=e9c8a6d887f0cf6c05445f2f2228891f","International attention to scientific data continues to grow. Opportunities emerge to re-visit long-standing approaches to managing data and to critically examine new capabilities. We describe the cognitive importance of metaphor. We describe several metaphors for managing, sharing, and stewarding data and examine their strengths and weaknesses. We particularly question the applicability of a ""publication"" approach to making data broadly available. Our preliminary conclusions are that no one metaphor satisfies enough key data system attributes and that multiple metaphors need to co-exist in support of a healthy data ecosystem. We close with proposed research questions and a call for continued discussion."
"10.2481/dsj.WDS-001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873580538&doi=10.2481%2fdsj.WDS-001&partnerID=40&md5=f9f71a5b656374d46c4fc018ed26a1cd","The International Council for Science (ICSU) vision explicitly recognises the value of data and information to science and particularly emphasises the urgent requirement for universal and equitable access to high quality scientific data and information. A universal public domain for scientific data and information will be transformative for both science and society. Over the last several years, two ad-hoc ICSU committees, the Strategic Committee on Information and Data (SCID) and the Strategic Coordinating Committee on Information and Data (SCCID), produced key reports that make 5 and 14 recommendations respectively aimed at improving universal and equitable access to data and information for science and providing direction for key international scientific bodies, such as the Committee on Data for Science and Technology (CODATA) as well as a newly ratified (by ICSU in 2008) formation of the World Data System. This contribution outlines the framing context for both committees based on the changed world scene for scientific data conduct in the 21st century. We include details on the relevant recommendations and important consequences for the worldwide community of data providers and consumers, ultimately leading to a conclusion, and avenues for advancement that must be carried to the many thousands of data scientists world-wide."
"10.2481/dsj.WDS-002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873577097&doi=10.2481%2fdsj.WDS-002&partnerID=40&md5=b72ef242e13da833547cb73c64bbac33","The Global Observing Systems Information Center (GOSIC), which was initiated in 1997 at the request of the Global Climate Observing System (GCOS) Steering Committee, responds to a need identified by the global climate observing community for easier and more effective access to observational climate data and information. GOSIC manages an online portal providing an entry point for users of climate-related global observing systems data and information systems and also helps serve the needs of the World Data Center for Meteorology, Asheville. The GOSIC continues to evolve and expand its responsibilities, and this paper is an update to a similar paper (Diamond & Lief, 2009) that was presented at the 1st ICSU World Data System Conference in Kyoto, Japan in September 2011. Since 2009, there have been considerable updates made to the GOSIC portal that will be discussed in the paper."
"10.1057/jma.2012.2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028251967&doi=10.1057%2fjma.2012.2&partnerID=40&md5=06751b66fee2eef26ec2032956e7227b","This article explores the economic shift in the United States toward sustainability. Also this article explores the concepts of ‘Green Marketing’, specifically within the context of marketing to green communities and identifies the need for marketers to shift strategies as green communities are being built across the United States. Lastly, this article discusses ‘green’ business methods and advertising techniques that have proven to be successful for multiple organizations in the United States. © 2013 Macmillan Publishers Ltd."
"10.1057/jma.2013.1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84966884962&doi=10.1057%2fjma.2013.1&partnerID=40&md5=8b823d635a26828b80cd6a7a2548a243","Marketers and advertisers seek to get close to customers through data analytics procedures that allow for the measurement of personalized messages delivered across multiple communication touchpoints. This article tests a hierarchical integrated marketing communications data integration framework that utilizes customer information (transactional, demographic and psychographic) to develop personalized communication and communication campaigns distributed across multiple interactive customer touchpoints. Our model posits that by using basic customer data we can increase the priority for collecting other types of data needed to get close to customers. Our findings show that customer data needs are hierarchically ordered and that the sequential interaction between these variables impacts customer relationship management system quality and measurement of performance. © 2013 Macmillan Publishers Ltd."
"10.1057/jma.2012.1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84965176864&doi=10.1057%2fjma.2012.1&partnerID=40&md5=96557a20982a69d4b2d7907e5c091ced","In this study, we explore how the entrance of new international brands affects market dynamics in amarket where national, international and private brands are present. The entire focus of the present research is to understand the phenomena of consumer choice and preference in the backdrop of such a market. We model consumer choice in the context of these different categories of brands by using a multinomial discrete choice model. The estimated model provides us with several diagnostic findings of managerial interest especially with respect to what kinds of product attributes differentially affect choice probabilities of different categories of brands – the international, the national and the private. © 2013 Macmillan Publishers Ltd."
"10.1057/jma.2013.2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84938809958&doi=10.1057%2fjma.2013.2&partnerID=40&md5=b744dd92b36ab640d84f6a941807e1ae","The results of past studies that compared the performance of alternative growth models are generally inconclusive. The objective of the current study is to provide further empirical evidence regarding the performance of three popular growth curves, namely, the Bass, Logistic and Gompertz models in the context of online shopping diffusion in Australia. The results of model fitting to an online shopping time series (1998–2009) show that all three models represent the diffusion curve quite well and adequately; however, the Bass model described the online shopping diffusion curve more accurately than the other two models. Forecasting with early diffusion data (1998–2002) suggests that the Bass, Logistic and Gompertz models are unable to adequately describe the diffusion curve from limited data. Nevertheless, the Bass model with the adjusted market potential coefficient (m) produced forecast accuracy that is comparable to the Bass model fitted to the full data (1998–2009). Overall, our results suggest that the Bass model outperforms the Logistic and Gompertz models. © 2013 Macmillan Publishers Ltd."
"10.1140/epjds17","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84933517822&doi=10.1140%2fepjds17&partnerID=40&md5=0c750f432438cc2f0ee9a2e5b4be08c5","City-scale mass gatherings attract hundreds of thousands of pedestrians. These pedestrians need to be monitored constantly to detect critical crowd situations at an early stage and to mitigate the risk that situations evolve towards dangerous incidents. Hereby, the crowd density is an important characteristic to assess the criticality of crowd situations. In this work, we consider location-aware smartphones for monitoring crowds during mass gatherings as an alternative to established video-based solutions. We follow a participatory sensing approach in which pedestrians share their locations on a voluntary basis. As participation is voluntarily, we can assume that only a fraction of all pedestrians shares location information. This raises a challenge when concluding about the crowd density. We present a methodology to infer the crowd density even if only a limited set of pedestrians share their locations. Our methodology is based on the assumption that the walking speed of pedestrians depends on the crowd density. By modeling this behavior, we can infer a crowd density estimation. We evaluate our methodology with a real-world data set collected during the Lord Mayor’s Show 2011 in London. This festival attracts around half a million spectators and we obtained the locations of 828 pedestrians. With this data set, we first verify that the walking speed of pedestrians depends on the crowd density. In particular, we identify a crowd density-dependent upper limit speed with which pedestrians move through urban spaces. We then evaluate the accuracy of our methodology by comparing our crowd density estimates to ground truth information obtained from video cameras used by the authorities. We achieve an average calibration error of 0.36 m–2 and confirm the appropriateness of our model. With a discussion of the limitations of our methodology, we identify the area of application and conclude that smartphones are a promising tool for crowd monitoring. © 2013 Wirz et al."
"10.1140/epjds14","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84933504945&doi=10.1140%2fepjds14&partnerID=40&md5=e4302d15190aebafb320b6978944ae41","The paper presents an analysis of the length of comments posted in Internetdiscussion fora, based on a collection of large datasets from several sources. We foundthat despite differences in the forum language, the discussed topics and useremotions, the comment length distributions are very regular and described by thelognormal form with a very high precision. We discuss possible origins of thisregularity and the existence of a universal mechanism deciding the length of the userposts. We suggest that the observed lognormal dependence may be due to anentropy maximizing combination of two psychological factors which are perceivedon a non-linear, logarithmic scale in accordance with the Weber-Fechner law, namelythe time spent on post related considerations and the comment length itself. Thishypothesis is supported by an experimental check of text length recognition capacity,confirming proportionality of the ‘just noticeable differences’ for text lengths - thebasis of the Weber-Fechner law"
"10.1140/epjds16","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84933503982&doi=10.1140%2fepjds16&partnerID=40&md5=5ab910884c1dde57c7a515b94b11181b","Modifiable health behaviors, a leading cause of illness and death in many countries, are often driven by individual beliefs and sentiments about health and disease. Individual behaviors affecting health outcomes are increasingly modulated by social networks, for example through the associations of like-minded individuals - homophily - or through peer influence effects. Using a statistical approach to measure the individual temporal effects of a large number of variables pertaining to social network statistics, we investigate the spread of a health sentiment towards a new vaccine on Twitter, a large online social network. We find that the effects of neighborhood size and exposure intensity are qualitatively very different depending on the type of sentiment. Generally, we find that larger numbers of opinionated neighbors inhibit the expression of sentiments. We also find that exposure to negative sentiment is contagious - by which we merely mean predictive of future negative sentiment expression - while exposure to positive sentiments is generally not. In fact, exposure to positive sentiments can even predict increased negative sentiment expression. Our results suggest that the effects of peer influence and social contagion on the dynamics of behavioral spread on social networks are strongly content-dependent. © 2013 Salathé et al"
"10.1140/epjds18","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84902242689&doi=10.1140%2fepjds18&partnerID=40&md5=bffe90eae3dc5dd0c2f01aa41d80cbb0","In a diversified context with multiple social networking sites, heterogeneous activity patterns and different user-user relations, the concept of ‘information cascade’ is all but univocal. Despite the fact that such information cascades can be defined in different ways, it is important to check whether some of the observed patterns are common to diverse contagion processes that take place on modern social media. Here, we explore one type of information cascades, namely, those that are time-constrained, related to two kinds of socially-rooted topics on Twitter. Specifically, we show that in both cases cascades sizes distribute following a fat-tailed distribution and that whether or not a cascade reaches system-wide proportions is mainly given by the presence of so-called hidden influentials. These latter nodes are not the hubs, which on the contrary, often act as firewalls for information spreading. Our results contribute to a better understanding of the dynamics of complex contagion and, from a practical side, for the identification of efficient spreaders in viral phenomena. © 2013 Baños et al."
"10.1140/epjds13","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893098499&doi=10.1140%2fepjds13&partnerID=40&md5=b566ec3b02fc0eae5504c21c0f0e3c39","The importance of data sharing has become a mantra within the science research community. However, sharing has not been as easy (or as readily adopted) as advocates have suggested. Questions of privacy, individual scientist’s rights to their research, and industry-academia divides have been significant hurdles. This article looks at the history of the debates and problems associated with data access that occurred during the ‘human genome wars’ and their aftermath as a way to explore some of the challenges facing diverse research communities. © 2013 Jasny; licensee Springer."
"10.1140/epjds15","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84892176848&doi=10.1140%2fepjds15&partnerID=40&md5=dba058ccee335be0851425a54bbf8ca5","Background: Language has functions that transcend the transmission of information and varies with social context. To find out how language and social network structure interlink, we studied communication on Twitter, a broadly-used online messaging service. Results: We show that the network emerging from user communication can be structured into a hierarchy of communities, and that the frequencies of words used within those communities closely replicate this pattern. Consequently, communities can be characterised by their most significantly used words. The words used by an individual user, in turn, can be used to predict the community of which that user is a member. Conclusions: This indicates a relationship between human language and social networks, and suggests that the study of online communication offers vast potential for understanding the fabric of human society. Our approach can be used for enriching community detection with word analysis, which provides the ability to automate the classification of communities in social networks and identify emerging social groups. © 2013 Bryden et al."
"10.2481/dsj.009-027","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871872752&doi=10.2481%2fdsj.009-027&partnerID=40&md5=fe80f1d34a16fdaf21df5b035514600c","Tackling the global challenges relating to health, poverty, business, and the environment is heavily dependent on the flow and utilisation of data. However, while enhancements in data generation, storage, modelling, dissemination, and the related integration of global economies and societies are fast transforming the way we live and interact, the resulting dynamic, globalised, information society remains digitally divided. On the African continent in particular, this division has resulted in a gap between the knowledge generation and its transformation into tangible products and services. This paper proposes some fundamental approaches for a sustainable transformation of data into knowledge for the purpose of improving the people's quality of life. Its main strategy is based on a generic data sharing model providing access to data utilising and generating entities in a multi-disciplinary environment. It highlights the great potentials in using unsupervised and supervised modelling in tackling the typically predictive-in-nature challenges we face. Using both simulated and real data, the paper demonstrates how some of the key parameters may be generated and embedded in models to enhance their predictive power and reliability. The paper's conclusions include a proposed implementation framework setting the scene for the creation of decision support systems capable of addressing the key issues in society. It is expected that a sustainable data flow will forge synergies among the private sector, academic, and research institutions within and among countries. It is also expected that the paper's findings will help in the design and development of knowledge extraction from data in the wake of cloud computing and, hence, contribute towards the improvement in the people's overall quality of life. To avoid running high implementation costs, selected open source tools are recommended for developing and sustaining the system."
"10.2481/dsj.010-025","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871962720&doi=10.2481%2fdsj.010-025&partnerID=40&md5=2303fa85df0848e9b4403a356d8c3dff","This article is devoted to general problems of development of reference data on properties of nanosized objects. It has been shown that the peculiar features of physical characteristics of nanostructures influence the behavior of an expert engaged in building the relevant computer database of property data. The building procedure includes comprehensive data systematization on the basis of classification of nanostructures and detailed identification of a nano-inherent object within the selected class. The key features of data on nanosized objects are discussed, including variation of property nomenclature, dimensional effects, and a high level of data uncertainty. The approaches to data systematization proposed in the article are considered in terms of ISO recommendations. Along with systematization, we propose a procedure for data certification taking into account a quantitative statement of uncertainty as well as quality indicators. The latter indications address the completeness of the description of both an object and a measurement method as well as the reproducibility of results. As an example, property data of carbon nanoforms (nanotubes, graphene, etc.) are analyzed."
"10.1504/IJBIDM.2012.051710","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873883545&doi=10.1504%2fIJBIDM.2012.051710&partnerID=40&md5=e1e1a26d19119d53c056fd791aea104c","Frequent itemset finding is the most time consuming step in analysing large transactional databases. The use of sequential algorithms cannot give analytical ability for such very large databases especially in terms of run-time performance. Therefore, we must rely on high performance parallel computing. In this paper, we present a new parallel algorithm for frequent itemset mining, called 'HorVertical' algorithm. This algorithm introduces a new database partitioning called 'HorVertical' partitioning. This technique in partitioning the database reduces the dependency in the parallel computation and gives new properties to reduce the computations. The algorithm passes the database only one time and starts a new stage with the finished itemsets while some other itemsets in the same stage have not been finished yet. We present the result on the performance of our algorithm on various databases, and compare it against well known algorithms. Copyright © 2012 Inderscience Enterprises Ltd."
"10.1504/IJBIDM.2012.051712","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873874048&doi=10.1504%2fIJBIDM.2012.051712&partnerID=40&md5=9ce0e9df1f5d13a423af93489a9e1fd2","The automatic classification systems, prediction and data mining are used in many applications (marketing, finance, customer relationship management...) using large databases. In this paper we describe a new data mining approach based on decision trees. In the proposed approach we built a multi-layer decision tree model, where each layer consists of several decision trees. The aim of the multi decision tree (MDT) is to improve decision tree classifier. The performances of MDT are compared with C4.5 decision tree algorithm and some ensemble of decision tree classifiers, namely bagging decision tree, boosting decision trees (BDT) and random forests decision tree. Results show substantial improvements when compared to these approaches. Copyright © 2012 Inderscience Enterprises Ltd."
"10.1504/IJBIDM.2012.051734","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873855638&doi=10.1504%2fIJBIDM.2012.051734&partnerID=40&md5=fdf217807b1de3e0e77b5abdc76eb741","Recent research works pay more attention to time series prediction, in which some time series data mining approaches have been exploited. In this paper, we propose a new method for time series prediction which is based on the concept of time series motifs. Time series motif is a previously unknown pattern appearing frequently in a time series. In the proposed approach, we first search for time series motif by using EP-C algorithm and then exploit motif information for forecasting in combination of a neural network model. Experimental results demonstrate our proposed method performs better than artificial neural network (ANN) in terms of prediction accuracy and time efficiency. Besides, our proposed method is more robust to noise than ANN. Copyright © 2012 Inderscience Enterprises Ltd."
"10.1504/IJBIDM.2012.051714","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873824358&doi=10.1504%2fIJBIDM.2012.051714&partnerID=40&md5=af809aee3d4d46805e6d0f3c5dc29943","Association rule mining (ARM) algorithms work only with binary attributes, and expect quantitative attributes to be converted to binary ones using sharp partitions, like 'age = [25, 60]'. A better alternative is to convert quantitative attributes to fuzzy attributes, like 'age = middle-aged', to eliminate loss of information due to sharp partitioning, and then run a fuzzy ARM algorithm. The most popular fuzzy ARM algorithms are fuzzy adaptations of apriori. Fuzzy apriori, like apriori, is a slow algorithm, especially for most medium-sized (500 K to 1 M) and large ( > 1 M) datasets. We propose a new fuzzy ARM algorithm called FAR-miner for fast and efficient performance. Through experiments we show that FAR-miner is 8-19 and 6-10 times faster on large and medium-sized datasets respectively as compared to fuzzy apriori. This efficiency is due to properties like two-phased multiple-partition tidlist-style processing and byte-vector representation and effective compression of tidlists. Copyright © 2012 Inderscience Enterprises Ltd."
"10.1504/IJBIDM.2012.051713","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873812021&doi=10.1504%2fIJBIDM.2012.051713&partnerID=40&md5=4cb5c2ad026251e0f78d6e611110010d","Relationship management is critical in business. Particularly, it is important to detect abnormal relationships, such as fraudulent relationships between service providers and consumers. Surprisingly, in the literature there is no systematic study on detecting relationship outliers. Particularly, no existing methods can detect and handle relationship outliers between groups and individuals in groups. In this paper, we tackle this important problem by developing a simple yet effective model. The major novelty is that we identify two types of outliers and devise efficient detection algorithms. Our experiments on both real data and synthetic data confirm the effectiveness, efficiency and scalability of our approach. The techniques reported in this paper have been in production in a large scale business application. Copyright © 2012 Inderscience Enterprises Ltd."
"10.2481/dsj.AMDS-003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870451743&doi=10.2481%2fdsj.AMDS-003&partnerID=40&md5=76f03f16ae9d6f507e302e020c1a88de","With ICT Standards playing a key role in support of research and development in many disciplines, the European Commission Institute for Energy and Transport is keen to promote the development and adoption of ICT Standards for engineering data. In this respect, its MatDB Online facility is a Standards-based system for preserving, managing, and exchanging engineering materials test data. While MatDB Online has evolved over more than 30 years to incorporate the latest innovations in data preservation and exchange, such as XML-based data transfer and data citation using digital object identifiers, it continues to rely on a robust data model developed more than 30 years ago through the joint efforts of the National Research Institute for Metals (the predecessor to NIMS, the National Institute for Materials Science), the European Commission Joint Research Centre, and the National Institute of Standards and Technology. While this data model has endured over many years, there is no corresponding Standard. Similarly, related efforts by the engineering materials community to deliver a Standard representation for engineering materials, such as MatML, have failed to be ratified. In consequence of the continued absence of a Standard representation for engineering materials data, there is no common mechanism for preserving and exchanging materials data and no formal means of maintaining a data model to support advances in materials technology, such as the emergence of nanomaterials. It is for these reasons that the European Commission Institute for Energy and Transport is supporting SERES, a CEN Workshop on Standards for Electronic Reporting in the Engineering Sector. As one of more than thirty organisations supporting the SERES Workshop, the Institute for Energy and Transport will make the MatDB XML schema available as one of several resources that will be taken into consideration when the prenormative Standard for representing engineering materials data is formulated. With the participation of the Institute for Energy and Transport in the SERES Workshop taking place in parallel with a related project with Oak Ridge National Laboratory, there is good reason to expect that a Standard representation for engineering materials, which has so far eluded the materials community, will be realised. This paper describes MatDB support for engineering materials Standards and related innovative features."
"10.2481/dsj.011-003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870436599&doi=10.2481%2fdsj.011-003&partnerID=40&md5=0c5fccc56542368856485bf5e6d81488","Patent network analysis, an advanced method of patent analysis, is a useful tool for technology management. This method visually displays all the relationships among the patents and enables the analysts to intuitively comprehend the overview of a set of patents in the field of the technology being studied. Although patent network analysis possesses relative advantages different from traditional methods of patent analysis, it is subject to several crucial limitations. To overcome the drawbacks of the current method, this study proposes a novel patent analysis method, called the intelligent patent network analysis method, to make a visual network with great precision. Based on artificial intelligence techniques, the proposed method provides an automated procedure for searching patent documents, extracting patent keywords, and determining the weight of each patent keyword in order to generate a sophisticated visualization of the patent network. This study proposes a detailed procedure for generating an intelligent patent network that is helpful for improving the efficiency and quality of patent analysis. Furthermore, patents in the field of Carbon Nanotube Backlight Unit (CNT-BLU) were analyzed to verify the utility of the proposed method."
"10.2481/dsj.012-004","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870418938&doi=10.2481%2fdsj.012-004&partnerID=40&md5=17474fa80f938cd7f78168260e5af832","In this paper, we consider the Bayesian estimation of parameters in the proportional hazards model of random censorship for the Weibull distribution under different asymmetric loss functions. It is well-known for the Weibull distribution that a joint conjugate prior on the parameters does not exist; we use both the informative and noninformative priors on the model parameters. Bayes estimates under LINEX and general entropy loss functions are obtained using the Gibbs sampling scheme. A simulation study is carried out to observe the behavior of the proposed estimators for different sample sizes and for different censoring parameters. It is observed that the Bayes estimators under LINEX and general entropy loss functions can be used effectively with the appropriate choice of respective loss function parameters. One real data set is analyzed for illustrative purposes."
,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870447479&partnerID=40&md5=2f8b22f8ef981c28f59f7beb6049de45","The Semantic Web is a W3C approach that integrates the different sources of semantics within documents and services using ontology-based techniques. The main objective of this approach in the geoscience domain is the improvement of understanding, integration, and usage of Earth and space science related web content in terms of data, information, and knowledge for machines and people. The modeling and representation of semantic attributes and relations within and among documents can be realized by human readable concept maps and machine readable OWL documents. The objectives for the usage of the Semantic Web approach in the GFZ data center ISDC project are the design of an extended classification of metadata documents for product types related to instruments, platforms, and projects as well as the integration of different types of metadata related to data product providers, users, and data centers. Sources of content and semantics for the description of Earth and space science product types and related classes are standardized metadata documents (e.g., DIF documents), publications, grey literature, and Web pages. Other sources are information provided by users, such as tagging data and social navigation information. The integration of controlled vocabularies as well as folksonomies plays an important role in the design of well formed ontologies."
"10.2481/dsj.AMDS2012_Rumble-001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870395753&doi=10.2481%2fdsj.AMDS2012_Rumble-001&partnerID=40&md5=7714fdeb26caf0e4d09a3e8862cd6bed","Fundamental in building any materials database is the capability to describe the materials whose data are contained therein accurately. While many systems exist for describing traditional materials, such as metals, polymers, ceramics, and others, the evolving field of nanotechnology presents new challenges. In this paper, we define the goals of a materials description system and the information categories used to describe traditional materials. We then discuss the challenges presented by materials on the nanoscale and suggest ways of overcoming those challenges."
"10.2481/dsj.AMDS2012_Freiman-002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870395024&doi=10.2481%2fdsj.AMDS2012_Freiman-002&partnerID=40&md5=6c97033b9a037b78e96410ffefb0a2d9","Several challenges are involved in developing and maintaining materials property databases, including improvements in measurement procedures, the changing nature of materials, access to proprietary data on new materials, and the need for quality evaluation. In this paper we discuss each of these issues and their impact on the availability of high quality material property data, using ceramics as an example material."
"10.1504/IJBIDM.2012.049556","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867323712&doi=10.1504%2fIJBIDM.2012.049556&partnerID=40&md5=0b0a175216effaa4e14370a23506fc0f","Recently, analysing research papers to understand research trends has attracted attention. The aim of our research is to extract and visualise a researcher's research topics automatically from metainformation of research papers published on the internet. Our method is based on Maximum Margin Clustering (MMC). We describe how to represent research papers in form of vectors using metainformation about them and how to initialise the hyperplane for MMC automatically. In the experiments, we show that the purity of our method is higher than that achieved in previous work based on k-Means (0.58 vs 0.35) and entropy of our method is lower than that of previous work (0.415 vs 0.47). Experiment results also illustrates that keyword information of research papers affects the most to clustering result. Copyright © 2012 Inderscience Enterprises Ltd."
"10.1504/IJBIDM.2012.049554","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867320217&doi=10.1504%2fIJBIDM.2012.049554&partnerID=40&md5=7546f2b5ef3639a88c1ef25f746fd595","Shipping plays a vital role as trade facilitator in providing cost-effi cient transportation. The International Maritime Organisation (IMO) reports that over 90% of the world trade volume is carried by merchant ships. The analysis of shipping networks therefore can create invaluable insight into global trade. In this paper we study the appropriateness of various graph centrality measures to rate, compare and rank ports from various perspectives of global shipping networks. In particular, we illustrate the potential of such analysis on the example of shipping networks constructed from the schedules, readily available on the World Wide Web, of six shipping companies that transport 35-40% of the total volume traded (in TEUs) worldwide. Copyright © 2012 Inderscience Enterprises Ltd."
"10.1504/IJBIDM.2012.049555","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867302391&doi=10.1504%2fIJBIDM.2012.049555&partnerID=40&md5=fab8b3bd7ce0f84812c8cb9b37fe2cb2","Nowadays with time series accounting for an increasingly large fraction of world's supply of data, there has been an explosion of interest in mining time series data. This paper proposes an approach of creating a new data structure automatically, for multivariate time series classifi cation. For more accurate and comprehensive classifi cation, induction of valuable rules named soft discretisation decision forest is illustrated comparing with other machine learning methods such as traditional neural network, SVM and nearest neighbour algorithms. Moreover, some real time series instances from the training dataset will be selected as class dedicated patterns. And a splitting stage using fuzzy theory is prepared for comparing attributes of time series. The ideas of authors are confi rmed by simulation results with a set of Japanese vowel time series capably. Copyright © 2012 Inderscience Enterprises Ltd."
"10.1504/IJBIDM.2012.049553","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867296084&doi=10.1504%2fIJBIDM.2012.049553&partnerID=40&md5=de570f1972833dfe7c9f39b172ed5428","Many search systems exist on the Internet. However, no system searches for information based on a query related to a user's sentiment. We propose a system that searches for information based on a user's sentiment. In this paper, we propose a words-of-wisdom search system as a fi rst step of that research. Specifi cally, we propose a six-dimensional sentiment vector for words-of-wisdom. Next, we propose a method for calculating the value of sentiment words that consist of words-of-wisdom based on our experiment. Subsequently, we calculate sentiment values of words-of-wisdom using the value of sentiment words. We developed a prototype system and conducted experiments. Copyright © 2012 Inderscience Enterprises Ltd."
"10.1504/IJBIDM.2012.049551","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867285971&doi=10.1504%2fIJBIDM.2012.049551&partnerID=40&md5=13aac2b408943a416d58d5ab47f92a07","A formal query approach called Hypergraph Query-by-Example (HQBE) is presented that can be applied in for large-scale data analysis, deliberating the user from the knowledge on specific query language details or even the database structure. The query construction of HQBE makes use of the hypergraph-based multidimensional artifacts and relations between them as the unifying data representation. Since the construction of a query is supported by a semantic discovery process and a traversal of relationship structures and data elements, the query hypergraph patterns could support the flexible on-demand analysis for obtaining the desired results. Copyright © 2012 Inderscience Enterprises Ltd."
"10.1504/IJBIDM.2012.049552","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867283787&doi=10.1504%2fIJBIDM.2012.049552&partnerID=40&md5=b732c814797c0187ae2df388d7b6e5ed","Since the Internet is sufficiently established, information on the Web is significantly enriched every day. It induces a fact that the information on Web pages has become increasingly useful in daily life. Therefore, it has become very common for us to refer to information on the Web, particularly when writing documents or programs. If we want to revisit the same Web pages to modify some part of a file later, it can be very hard to track down the Web pages originally referred to. In this paper, we propose methods for extracting relationships between files and Web pages based on the co-occurrence of data in Web-access logs and file-access logs. These relationships are very useful for revisiting Web pages related to target files. There are two approaches for merging the logs to analyse co-occurrence in these two types of access logs, involving a trade-off between accuracy and execution time. We call them the Pre-Merge and Post-Merge methods. We have evaluated these two methods using actual access logs. Copyright © 2012 Inderscience Enterprises Ltd."
"10.2481/dsj.012-014","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866040730&doi=10.2481%2fdsj.012-014&partnerID=40&md5=575bd2f09146877027dbf81b8eeadd5d","We document the history and progress of two international ocean data management projects. The ""Global Oceanographic Data Archaeology and Rescue"" project was initiated in 1993 under the auspices of the UNESCO Intergovernmental Oceanographic Commission (IOC). The project has the goal of locating (archaeology) and digitizing or copying to modern electronic media (rescuing) historical (pre-1992) oceanographic data that exist in manuscript or electronic media form that are at risk of loss due to media decay. The IOC ""World Ocean Database"" project initiated in 2001 focuses on encouraging international data exchange for the post-1991 period and the development of regional atlases."
"10.1504/IJBIDM.2012.048725","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865998103&doi=10.1504%2fIJBIDM.2012.048725&partnerID=40&md5=40011e491507dcb4b340cfdb28db0a78","The work described in this paper is motivated by the fact that the structure of a website may not satisfy a larger population of the visiting users who may jump between pages of the website before they land on the target page(s); this is at least partially true because access patterns were not known when the website was designed. We developed a robust framework that tackles this problem by considering both web log data and web structure data to suggest a more compact structure that could satisfy a larger user group. The study assumes the trend recorded so far in the web log reflects well the anticipated behaviour of the users in the future. We separately analyse web log and web structure data using three techniques, namely clustering, frequent pattern mining and network analysis. The final outcome from the two stages is reflected on to one of the six models, namely the network of pages to report linking pages by the most appropriate connections. Copyright © 2012 Inderscience Enterprises Ltd."
"10.1504/IJBIDM.2012.048728","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865983327&doi=10.1504%2fIJBIDM.2012.048728&partnerID=40&md5=357b56304f1aa57637d878f27be86645","Co-clustering refers to the problem of deriving sub-matrices of the data matrix by simultaneously clustering the rows (data instances) and columns (features) of the matrix. While very effective in discovering useful knowledge, many of the co-clustering algorithms adopt a completely unsupervised approach. Integration of domain knowledge can guide the co-clustering process and greatly enhance the overall performance. We propose a semi-supervised Nonnegative Matrix-factorisation (SS-NMF) based framework to integrate domain knowledge in the form of must-link and cannot-link constraints. Specifically, we augment the data matrix by integrating the constraints using metric learning and then perform NMF to obtain co-clustering. Under the proposed framework, we present two approaches to integrate domain knowledge, viz. a distance metric learning approach and an information theoretic metric learning approach. Through experiments performed on real-world web service data and publicly available text datasets, we demonstrate the performance of the proposed SS-NMF based approach for data co-clustering. Copyright © 2012 Inderscience Enterprises Ltd."
"10.1504/IJBIDM.2012.048729","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865975796&doi=10.1504%2fIJBIDM.2012.048729&partnerID=40&md5=faeee70f3c5f56ea0553e3e418140f90","One very common criterion used to evaluate feature selection methods is the performance of a chosen classifier trained with the selected features. Another important evaluation criterion that has, until recently, been neglected is the stability of these feature selection methods. While other studies have shown interest in measuring the degree of agreement between the outputs of a technique trained on randomly selected subsets from the same input data, this study presents the importance of evaluating stability in the presence of noise. Experiments are conducted with 17 filters (six standard filter-based ranking techniques and 11 threshold-based feature selection techniques) on nine different real-world datasets. This paper identifies the techniques that are inherently more sensitive to class noise and demonstrates how certain characteristics (sample size and class imbalance) of the data can affect the stability performance of some feature selection methods. Copyright © 2012 Inderscience Enterprises Ltd."
"10.1504/IJBIDM.2012.048726","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865970397&doi=10.1504%2fIJBIDM.2012.048726&partnerID=40&md5=ffb4336b1b16bd036ec8c2c007ad60c7","Cybercrime detection solutions have recently received increased attention. Predicting the cybercrime potentiality of a request received by a server can reduce the risk of cybercrime. In this paper, we present an alternative solution to the current intrusion detection systems in that the socioeconomic characteristics of IP geo-locations of a request are used to predict its crime potentiality. The IP address of a request is used to exploit its socioeconomic characteristics. Using the IP address of a request, the physical location, from where the request has been sent, is identified. Socio-economic attributes of people living in that area are collected. These characteristics can specify the seriousness of a cybercrime associated with a request. Classification algorithms can be used to build a prediction model. We have conducted a case study in which we built a prediction model using a set of socio-economic attributes. Our results show the applicability of the proposed model. Copyright © 2012 Inderscience Enterprises Ltd."
"10.1504/IJBIDM.2012.048730","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865970358&doi=10.1504%2fIJBIDM.2012.048730&partnerID=40&md5=edd1cabdc9f35c2c6c154b49c40540c3","Two problems often encountered in machine learning are class imbalance and high dimensionality. In this paper we compare three different approaches for addressing both problems simultaneously, by applying both data sampling and feature selection. With the first two approaches, sampling is followed by feature selection. In the first approach, the features are selected based on the sampled data, and then the unsampled data is used with just the selected features. The second approach is similar, but the sampled data is used. Finally, with the third approach, feature selection is performed prior to sampling. To compare the approaches, we use seven datasets from different domains, employ nine feature rankers from three different families, apply three sampling techniques, and inject class noise to better simulate real-world datasets. The results show that the second and third approaches are both very good, with the third approach showing a slight (but not statistically significant) lead. Copyright © 2012 Inderscience Enterprises Ltd."
"10.1504/IJBIDM.2012.048727","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865958789&doi=10.1504%2fIJBIDM.2012.048727&partnerID=40&md5=cb568ce998560bf8b65b9e84cc2b1b39","This paper proposes a novel supervised discretisation algorithm based on Correlation Maximisation (CM) using Multiple Correspondence Analysis (MCA). MCA is an effective technique to capture the correlation between multiple variables. For each numeric feature, the proposed discretisation algorithm utilises MCA to measure the correlations between feature intervals/items and classes, and the set of cut-points yielding the maximum correlation is chosen as the discretisation scheme for that feature. Therefore, the discretised feature can not only produce a concise summarisation of the original numeric feature but also provide the maximum correlation information to predict class labels. Experiments are conducted by comparing to seven state-of-the-art supervised discretisation algorithms using six well-known classifiers on 19 UCI data sets. Experimental results demonstrate that the proposed discretisation algorithm can automatically generate a set of features (feature intervals) that produce the best classification results on average. Copyright © 2012 Inderscience Enterprises Ltd."
"10.2481/dsj.11-DS4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859707452&doi=10.2481%2fdsj.11-DS4&partnerID=40&md5=f90739bd7d9b7542ada233f407df40be","In most scientific fields, significant improvements have been made in terms of data sharing among scientists and researchers. Although there are clear benefits to data sharing, there is at least one field where this norm has yet to be developed: the behavioural sciences. In this paper, we propose an innovative methodology as a means to change existing norms within the behavioural sciences and move towards increased data sharing. Based on recent advances in social psychology, we theorize that a Survey Research Instrument that takes into account basic psychological processes can be effective in promoting data sharing norms."
"10.2481/dsj.010-037","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859702314&doi=10.2481%2fdsj.010-037&partnerID=40&md5=d6d6b6b05b88c146fc313a3d3d11b106","This paper discusses the usability of convertibility, a principle for data quality used by the Xplain-DBMS. Convertibility (uniqueness) of type definitions is a helpful criterion for database design, whereas convertibility of instances is a criterion for the uniqueness of instances (records). However, in many situations with or without generalization/specialization, convertibility appears to be an insufficient criterion for correctness of instances, which is illustrated by many examples. In order to be able to specify more rigorous rules for correctness of instances we propose to use new concepts such as 'identifying property'. These new concepts also facilitate the transformation of relational databases into Xplain databases."
"10.2481/dsj.10-007","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857000874&doi=10.2481%2fdsj.10-007&partnerID=40&md5=2ba700fb21d583aa2115af7a5c87029b","Precision-Recall is one of the main metrics for evaluating content-based image retrieval techniques. However, it does not provide an ample perception of the properties of an image dataset immersed in a metric space. In this work, we describe an alternative metric named H-Metric, which is determined along a sequence of controlled modifications in the image dataset. The process is named homogenization and works by altering the homogeneity characteristics of the classes of the images. The result is a process that measures how hard it is to deal with a set of images in respect to content-based retrieval, offering support in the task of analyzing configurations of distance functions and of features extractors."
"10.2481/dsj.10-011","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857007488&doi=10.2481%2fdsj.10-011&partnerID=40&md5=d879e8007f7be8ca54b85deb47035419","Speech recognition and language analysis of spontaneous speech arising in naturally spoken conversations are becoming the subject of much research. However, there is a shortage of spontaneous speech corpora that are freely available for academics. We therefore undertook the building of a natural conversation speech database, recording over 200 hours of conversations in English by over 600 local university students. With few exceptions, the students used their own cell phones from their own rooms or homes to speak to one another, and they were permitted to speak on any topic they chose. Although they knew that they were being recorded and that they would receive a small payment, their conversations in the corpus are probably very close to being natural and spontaneous. This paper describes a detailed case study of the problems we faced and the methods we used to make the recordings and control the collection of these social science data on a limited budget."
"10.1140/epjds4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84933515683&doi=10.1140%2fepjds4&partnerID=40&md5=b1561896414ee3b29996a8aa33b4de9a","Complex networks are often constructed by aggregating empirical data over time, such that a link represents the existence of interactions between the endpoint nodes and the link weight represents the intensity of such interactions within the aggregation time window. The resulting networks are then often considered static. More often than not, the aggregation time window is dictated by the availability of data, and the effects of its length on the resulting networks are rarely considered. Here, we address this question by studying the structural features of networks emerging from aggregating empirical data over different time intervals, focussing on networks derived from time-stamped, anonymized mobile telephone call records. Our results show that short aggregation intervals yield networks where strong links associated with dense clusters dominate; the seeds of such clusters or communities become already visible for intervals of around one week. The degree and weight distributions are seen to become stationary around a few days and a few weeks, respectively. An aggregation interval of around 30 days results in the stablest similar networks when consecutive windows are compared. For longer intervals, the effects of weak or random links become increasingly stronger, and the average degree of the network keeps growing even for intervals up to 180 days. The placement of the time window is also seen to affect the outcome: for short windows, different behavioural patterns play a role during weekends and weekdays, and for longer windows it is seen that networks aggregated during holiday periods are significantly different. © 2012 Krings et al.; licensee Springer."
"10.1140/epjds3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84933504997&doi=10.1140%2fepjds3&partnerID=40&md5=fe65c9e083665e3a50c9f57aa70963e5","We show that the frequency of word use is not only determined by the word length [1] and the average information content [2], but also by its emotional content. We have analyzed three established lexica of affective word usage in English, German, and Spanish, to verify that these lexica have a neutral, unbiased, emotional content. Taking into account the frequency of word usage, we find that words with a positive emotional content are more frequently used. This lends support to Pollyanna hypothesis [3] that there should be a positive bias in human expression. We also find that negative words contain more information than positive words, as the informativeness of a word increases uniformly with its valence decrease. Our findings support earlier conjectures about (i) the relation between word frequency and information content, and (ii) the impact of positive emotions on communication and social links. © 2012 Garcia et al.; licensee Springer."
"10.1140/epjds7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84933502418&doi=10.1140%2fepjds7&partnerID=40&md5=94cc665e9b3a25522bda1a9bd7c94547","Each year, crowd disasters happen in different areas of the world. How and why do such disasters happen? Are the fatalities caused by relentless behavior of people or a psychological state of panic that makes the crowd ‘go mad’? Or are they a tragic consequence of a breakdown of coordination? These and other questions are addressed, based on a qualitative analysis of publicly available videos and materials, which document the planning and organization of the Love Parade in Duisburg, Germany, and the crowd disaster on July 24, 2010. Our analysis reveals a number of misunderstandings that have widely spread. We also provide a new perspective on concepts such as ‘intentional pushing’, ‘mass panic’, ‘stampede’, and ‘crowd crushes’. The focus of our analysis is on the contributing causal factors and their mutual interdependencies, not on legal issues or the judgment of personal or institutional responsibilities. Video recordings show that people stumbled and piled up due to a ‘domino effect’, resulting from a phenomenon called ‘crowd turbulence’ or ‘crowd quake’. Crowd quakes are a typical reason for crowd disasters, to be distinguished from crowd disasters resulting from ‘mass panic’ or ‘crowd crushes’. In Duisburg, crowd turbulence was the consequence of amplifying feedback and cascading effects, which are typical for systemic instabilities. Accordingly, things can go terribly wrong in spite of no bad intentions from anyone. Comparing the incident in Duisburg with others, we give recommendations to help prevent future crowd disasters. In particular, we introduce a new scale to assess the criticality of conditions in the crowd. This may allow preventative measures to be taken earlier on. Furthermore, we discuss the merits and limitations of citizen science for public investigation, considering that today, almost every event is recorded and reflected in the World Wide Web. © 2012 Helbing and Mukerji; licensee Springer."
"10.1140/epjds12","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84930841377&doi=10.1140%2fepjds12&partnerID=40&md5=fa43811d7876a903a0ea638f7f93c0f9","Complex dynamics of social media emerge from the interaction between the patterns of social connectivity of users and the information exchanged along such social ties. Unveiling the underlying mechanisms that drive the evolution of online social systems requires a deep understanding of the interplay between these two aspects. Based on the case of the aNobii social network, an online service for book readers, we investigate the dynamics of link creation and the social influence phenomenon that may trigger information diffusion in the social graph. By confirming that social partner selection is strongly driven by structural, geographical, and topical proximity, we develop a machine-learning social link recommender for individual users trained on a set of features selected as best predictive out of several and we test it on the still widely unexplored domain of a network of interest. We also analyze the influence process from the two distinct perspectives of users and items. We show that link creation plays an immediate effect on the alignment of user profiles and that the established social ties are a good substrate for social influence. We quantitatively measure influence by tracking the patterns of diffusion of specific pieces of information and comparing them with appropriate null models. We discover an appreciable signal of social influence even though item consumption is a very slow process in this context. All the detected patterns of social attachment and influence are observed to be stronger when considering the social subgraph on which communication effectively occurs. Based on our study of the dynamics of the aNobii social network, we investigate the possibility to predict the evolution of such a complex social system. © 2012 Aiello et al.; licensee Springer."
"10.1140/epjds2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84923357451&doi=10.1140%2fepjds2&partnerID=40&md5=889b0cd25c330136ec9ad10f4db8e068","A main characteristic of social media is that its diverse content, copiously generated by both standard outlets and general users, constantly competes for the scarce attention of large audiences. Out of this flood of information some topics manage to get enough attention to become the most popular ones and thus to be prominently displayed as trends. Equally important, some of these trends persist long enough so as to shape part of the social agenda. How this happens is the focus of this paper. By introducing a stochastic dynamical model that takes into account the user’s repeated involvement with given topics, we can predict the distribution of trend durations as well as the thresholds in popularity that lead to their emergence within social media. Detailed measurements of datasets from Twitter confirm the validity of the model and its predictions. © 2012 Wang and Huberman; licensee Springer."
"10.1140/epjds11","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84905462672&doi=10.1140%2fepjds11&partnerID=40&md5=b5fa0207a9cf781baf252b433d40efde","We analyse a large data set of genetic markers obtained from populations of Cymodocea nodosa, a marine plant occurring from the East Mediterranean to the Iberian-African coasts in the Atlantic Ocean. We fully develop and test a recently introduced methodology to infer the directionality of gene flow based on the concept of geographical segregation. Using the Jensen-Shannon divergence, we are able to extract a directed network of gene flow describing the evolutionary patterns of Cymodocea nodosa. In particular we recover the genetic segregation that the marine plant underwent during its evolution. The results are confirmed by natural evidence and are consistent with an independent cross analysis. © 2012 Masucci et al.; licensee Springer."
"10.1140/epjds10","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84904877126&doi=10.1140%2fepjds10&partnerID=40&md5=169a78e30bd5e559f33d7a264987f005","We study spatiotemporal correlations and temporal diversities of handset-based service usages by analyzing a dataset that includes detailed information about locations and service usages of 124 users over 16 months. By constructing the spatiotemporal trajectories of the users we detect several meaningful places or contexts for each one of them and show how the context affects the service usage patterns. We find that temporal patterns of service usages are bound to the typical weekly cycles of humans, yet they show maximal activities at different times. We first discuss their temporal correlations and then investigate the time-ordering behavior of communication services like calls being followed by the non-communication services like applications. We also find that the behavioral overlap network based on the clustering of temporal patterns is comparable to the communication network of users. Our approach provides a useful framework for handset-based data analysis and helps us to understand the complexities of information and communications technology enabled human behavior. © 2012 Jo et al.; licensee Springer."
"10.1140/epjds5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84888257493&doi=10.1140%2fepjds5&partnerID=40&md5=3db1f4eec43ec4fa9189f4f607218be9","Online social media provide multiple ways to find interesting content. One important method is highlighting content recommended by user’s friends. We examine this process on one such site, the news aggregator Digg. With a stochastic model of user behavior, we distinguish the effects of the content visibility and interestingness to users. We find a wide range of interest and distinguish stories primarily of interest to a users’ friends from those of interest to the entire user community. We show how this model predicts a story’s eventual popularity from users’ early reactions to it, and estimate the prediction reliability. This modeling framework can help evaluate alternative design choices for displaying content on the site. © 2012 Hogg and Lerman; licensee Springer."
"10.1140/epjds9","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875863990&doi=10.1140%2fepjds9&partnerID=40&md5=8d9e88ce2e7a2f0cdccbb6f61e0cad45","Understanding social dynamics that govern human phenomena, such as communications and social relationships is a major problem in current computational social sciences. In particular, given the unprecedented success of online social networks (OSNs), in this paper we are concerned with the analysis of aggregation patterns and social dynamics occurring among users of the largest OSN as the date: Facebook. In detail, we discuss the mesoscopic features of the community structure of this network, considering the perspective of the communities, which has not yet been studied on such a large scale. To this purpose, we acquired a sample of this network containing millions of users and their social relationships; then, we unveiled the communities representing the aggregation units among which users gather and interact; finally, we analyzed the statistical features of such a network of communities, discovering and characterizing some specific organization patterns followed by individuals interacting in online social networks, that emerge considering different sampling techniques and clustering methodologies. This study provides some clues of the tendency of individuals to establish social interactions in online social networks that eventually contribute to building a well-connected social structure, and opens space for further social studies. © 2012 Ferrara; licensee Springer."
"10.1140/epjds6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870398813&doi=10.1140%2fepjds6&partnerID=40&md5=02c34df01aed1a8d77519fb6b83e445a","We examine partisan differences in the behavior, communication patterns and social interactions of more than 18,000 politically-active Twitter users to produce evidence that points to changing levels of partisan engagement with the American online political landscape. Analysis of a network defined by the communication activity of these users in proximity to the 2010 midterm congressional elections reveals a highly segregated, well clustered, partisan community structure. Using cluster membership as a high-fidelity (87% accuracy) proxy for political affiliation, we characterize a wide range of differences in the behavior, communication and social connectivity of leftand right-leaning Twitter users. We find that in contrast to the online political dynamics of the 2008 campaign, right-leaning Twitter users exhibit greater levels of political activity, a more tightly interconnected social structure, and a communication network topology that facilitates the rapid and broad dissemination of political information. © 2012 Conover et al.; licensee Springer."
"10.1140/epjds8","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867785739&doi=10.1140%2fepjds8&partnerID=40&md5=92de92be5381c42e1f0a731816feb2be","We present a contribution to the debate on the predictability of social events using big data analytics. We focus on the elimination of contestants in the American Idol TV shows as an example of a well defined electoral phenomenon that each week draws millions of votes in the USA. This event can be considered as basic test in a simplified environment to assess the predictive power of Twitter signals. We provide evidence that Twitter activity during the time span defined by the TV show airing and the voting period following it correlates with the contestants ranking and allows the anticipation of the voting outcome. Twitter data from the show and the voting period of the season finale have been analyzed to attempt the winner prediction ahead of the airing of the official result. We also show that the fraction of tweets that contain geolocation information allows us to map the fanbase of each contestant, both within the US and abroad, showing that strong regional polarizations occur. The geolocalized data are crucial for the correct prediction of the final outcome of the show, pointing out the importance of considering information beyond the aggregated Twitter signal. Although American Idol voting is just a minimal and simplified version of complex societal phenomena such as political elections, this work shows that the volume of information available in online systems permits the real time gathering of quantitative indicators that may be able to anticipate the future unfolding of opinion formation events. © 2012 Ciulla et al.; licensee Springer."
"10.2481/dsj.10-018","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84855386920&doi=10.2481%2fdsj.10-018&partnerID=40&md5=ca165db536be6f13a76103daf4d6ef4b","Africa is a rising star - one of the most desirable investment destinations in the world. Nonetheless, economic growth is uneven among African countries, and many obstacles must be overcome in order to realize the full potential of opportunity. To achieve long-term sustainable investment results, and ultimately progress towards Sustainable Development goals, many risks must be isolated, analyzed, and mitigated. This paper introduces the concept of Sustainability Risk, identifying a set of major risk components for Sub-Saharan Africa and building an integral measure to quantify the degree of remoteness of the forty-six Sub-Saharan Africa countries from the total set of threats considered. The countries are separated into distinct groups with similar characteristics in terms of Sustainability Risk, and an analysis for potential decision-making, based on the visualization of the countries' position in relation to the major sustainability threats, is performed for each group. The research identifies risks with maximum impacts."
,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-80054014833&partnerID=40&md5=00623fd1261c19b6b51a60a40c366474","The Absolute Measurement Session during the XIVth IAGA Workshop on Geomagnetic Observatory Instruments, Data Acquisition and Processing was held at the Changchun Magnetic Observatory (CNH) from September 14-17, 2010. Absolute measurements of declination and inclination were made at pillars 1, 3, 4, 5, and 6 fixed in the absolute house of the observatory. The pillars are 3 meters away from each other. The adopted pillars were determined by extensive measurements. The observatory baselines were determined on pillar 4 in the absolute house. Reduction of the measurements was made using the suspended FGE fluxgate magnetometer and GSM-90 magnetometer at the observatory. The data collected by test systems were directly compared to the corresponding samples collected by the Changchun Observatory system."
,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-80053555193&partnerID=40&md5=43f88ceae2255c0fce790f7b4329cacb","An experimental validation of the in-situ calibration procedure, which allows estimating parameters of observatory magnetometers (scale factors, sensor misalignment) without its operation interruption, is presented. In order to control the validity of the procedure, the records provided by two magnetometers calibrated independantly in a coil system, have been processed. The in-situ estimations of the parameters are in very good agreement with the values provided by the coil system calibration."
,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-80053548524&partnerID=40&md5=236a51fc15ec533a5b3edf217e4291af","NGII(National Geography Information Institute) of Korea consigned KIGAM(Korea Institute of Geoscience & Mineral Resources) to do absolute geomagnetic measurements on 32 geomagnetic repeat stations evenly distributed on the southern part of Korean Peninsula in the year 2010 and to produce geomagnetic field components' distribution maps for the year 2010.0. The result of the processing of the measured data, i. e., the geomagnetic field components' distribution, shows a near similarity with that calculated from IGRF-11 although the latter was processed without any real geomagnetic data measured on the Korean Peninsula as an input. This implies that we installed the repeat stations on sites with good geomagnetic conditions and that our result in accordance with the IGRF represents well the regional distribution trend, i. e., it is dominated by relatively long wavelength components."
,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-80053545651&partnerID=40&md5=1d87f8f4801bcdc5a4511f46595c344c","This paper analyzes the baselines of 8 geomagnetic observatories in the China Magnetic Observatory Network. The baselines of similar variometers were measured by two different fluxgate theodolites during the same time period. The results demonstrate that two baseline values measured by two independent absolute instruments did not completely coincide for the same components even though the differences between pillars and instruments had been corrected. The baseline values were still not smooth, and there existed obvious wave variations for the D, H, and Z components. The causes of this inconsistency might be the differences between the two pillars installed with two independent absolute instruments and instrument problems in some of the observatories. In other words, the difference in the geomagnetic field between two points in the same observational area is not a constant."
,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-80053537580&partnerID=40&md5=7aeb4c7c0d34733d7d8c552a29b0173f","The objective of this project is to develop a flexible observation mode for a geomagnetic abnormal phenomena tracking system. The instrument, based on ring core fluxgate magnetometer technology, improves the field environment performance. Using wireless technology provides on-the-spot mobile networking for the observational data, with efficient access to the earthquake precursor observation network. It provides a powerful detection method for earthquake short-term prediction through installation of a low-noise fluxgate magnetometer array, intensely observing the phenomenon of geomagnetic disturbances and abnormal low-frequency electromagnetic signals in different latitudes, then carrying out observational data processing and exploring the relationship between earthquake activity and geomagnetic field changes."
,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-80053532217&partnerID=40&md5=7b324488b93e3ed9106de96b065e5e39","In order to investigate quantitatively the accuracy of geomagnetic daily variation recorded by the FGE magnetometer, we analyzed the stability and precision of some groups' baseline values continuously recorded one day at seven observatories in 2009. The results show that the standard deviation and variable amplitude of the baseline values are small in D, H, and Z components, the standard deviation values are δD≤ 0.03', δH≤ 03nT, δZ≤ 03nT respectively, and the variable amplitude values are ΔD≤|0.05'|, ΔH≤|0.5'|, ΔZ≤| 0.5 | respectively. Then we selected the baseline values continuously recorded one day at CDP and KSH observatories in 2009 and 2010 and analyzed the influence of absolute measurement intervals on the stability of the baselines."
,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-80053530146&partnerID=40&md5=0e11fbb72d02f8263238190d60a81498","The Japan Meteorological Agency (JMA) has developed an advanced system to monitor geomagnetic environments consisting of magnetometers and monitoring cameras. The new system calculates the magnetic moments and positions of sources of artificial disturbances and then visually identifies the sources. The intensity and location of a source of artificial disturbance are calculated assuming the source is a magnetic dipole. This new system was installed at two branch observatories operated by the JMA, which will enable the remote monitoring of sites for geomagnetic observations from the headquarters at Kakioka Magnetic Observatory."
,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-80053528122&partnerID=40&md5=b66d6490e080355cd4d30688870ade41","The knowledge transfer project called ""Magnetic Valley"" that was launched in 2009 is presented below. This project is funded by the Belgian government to investigate and develop products and services that will improve the socio-economic development in the area around the ""Centre de Physique du Globe de l'IRM""."
,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-80053526178&partnerID=40&md5=c3e913f4b950425679b97e97fe5c4001","Long-term precise magnetic observations are being carried out at the Eilat test site as a part of active tectonic fault multi-sensor geophysical monitoring. The gradiometer system comprises three highly sensitive potassium total field sensors with short bases - up to 50 m. The gradiometer time series contain residuals of external magnetic field variations, which are essentially homogeneous over such short distances. Mutual regression analysis of the gradiometer and the vector magnetometer time series was proved to be an effective tool in reduction of the influence of external homogeneous variation on gradiometer readings. Monitoring results together with time dependence of regression coefficients are analyzed."
,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-80053519923&partnerID=40&md5=24551508aa91229b34712013ace6ccc4","The K index was devised by Bartels et al. (1939) to provide an objective monitoring of irregular geomagnetic activity. The K index was then routinely used to monitor the magnetic activity at permanent magnetic observatories as well as at temporary stations. The increasing number of digital and sometimes unmanned observatories and the creation of INTERMAGNET put the question of computer production of K at the centre of the debate. Four algorithms were selected during the Vienna meeting (1991) and endorsed by IAGA for the computer production of K indices. We used one of them (FMI algorithm) to investigate the impact of the geomagnetic data sampling interval on computer produced K values through the comparison of the computer derived K values for the period 2009, January 1st to 2010, May 31st at the Port-aux-Français magnetic observatory using magnetic data series with different sampling rates (the smaller: 1 second; the larger: 1 minute). The impact is investigated on both 3-hour range values and K indices data series, as a function of the activity level for low and moderate geomagnetic activity."
,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-80053518296&partnerID=40&md5=04196021e3924d12923ebf6791777281","The standard observatory procedure for determining a geomagnetic field's declination and inclination absolutely is the DI-flux measurement. The instrument consists of a non-magnetic theodolite equipped with a single-axis fluxgate magnetometer. Additionally, a scalar magnetometer is needed to provide all three components of the field. Using only 12 measurement steps, all systematic errors can be accounted for, but if only one of the readings is wrong, the whole measurement has to be rejected. We use a three-component sensor on top of the theodolites telescope. By performing more measurement steps, we gain much better control of the whole procedure: As the magnetometer can be fully calibrated by rotating about two independent directions, every combined reading of magnetometer output and theodolite angles provides the absolute field vector. We predefined a set of angle positions that the observer has to try to achieve. To further simplify the measurement procedure, the observer is guided by a pocket pc, in which he has only to confirm the theodolite position. The magnetic field is then stored automatically, together with the horizontal and vertical angles. The DI3 measurement is periodically performed at the Niemegk Observatory, allowing for a direct comparison with the traditional measurements."
,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-80053513237&partnerID=40&md5=6c08d8950b8b812317ba7d9d8f5f7519","The COMPASS (COmprehensive Magnetic Processes under the African Southern Sub-continent) program forms part of a collaboration project between Germany and South Africa, called Inkaba ye Africa, and aims to investigate the regional geomagnetic field in this area, particularly its evolutionary behaviour. Results obtained from field surveys conducted by the Hermanus Magnetic Observatory (HMO) and the Helmholtz GeoForschungsZentrum (GFZ) in South Africa, Namibia, and Botswana, in addition to geomagnetic field data from the 4 continuous recording magnetic observatories in southern Africa at Hermanus, Hartebeesthoek, Keetmanshoop and Tsumeb, were used to model the geomagnetic field time variation by means of a polynomial approach."
,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-80053513236&partnerID=40&md5=be9512945da8191ae7275a1952088ca8","Despite the advance of technology, the fully automatic recording of absolute magnetic field vector variation at observatories remains an elusive goal. Primary difficulties are the long term stability of sensor orientation and the stable operation of the sensor system. In standard practice, definitive data are produced through the combination of continuous operation of a variometer and the occasional absolute measurements that are used for calibration of the variometer data. A single, automatic instrument that can continuously acquire absolute vector measurements with 1-second resolution is desired. We introduce a device that will fulfil these requirements. Data are acquired using Serson's method: the ambient magnetic field is modulated by superposed fields. This method has been applied, mostly in connection with Proton magnetometers, for many years. In general it requires that the applied fields have a strength on the order of the Earth's magnetic field. But the sampling rate is limited for most existing systems. In contrast, our system only requires applied fields of about 5000nT, and the switching rate of polarities is 5Hz. This is possible because we use a fast self oscillating Csmagnetometer. The self oscillating Cs-magnetometer is calibrated by a Cs-He cell during times without additional fields (tandem-magnetometer)."
,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-80053513223&partnerID=40&md5=9f88d5a8174cd85651ca25f77691218c","The temporal and spatial distribution of SqH in China was analyzed based on geomagnetic digitized data from the geomagnetic observation network. It was found that: 1) there was obvious prenoon-postnoon asymmetry of SqH in summer and equinox;2) in summer, equinox, and winter, the averaged foci of Sq were located at 27 ° N, 29 ° N, and 35 ° N, respectively. However, in a single quiet day, the Sq foci might reach a latitude north of 50 ° N and south of 19 ° N; 3). There were some inconsistencies in the behavior of the SqH distributions in two longitudinal chains, such as the reverse of SqH variations in the same latitudinal chain and the increase (decrease) of the daily ranges from east to west. All these demonstrated that there might be latitudinal migration of the Sq focus or changes in current intensity during the longitudinal migration of the Sq focus from east to west."
,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-80053510085&partnerID=40&md5=ce0810247ada0baa104148cdf2f61101","The island Tristan da Cunha is located in the South Atlantic Anomaly, and until recently the area has been one of the largest gaps in the global geomagnetic observatory network. As part of the Danish project SAADAN we set up a geomagnetic observatory on the island. Here we report on how we established the observatory in 2009 and on its operation in 2010."
,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-80053505955&partnerID=40&md5=f8ed343c7687708b6c6015dd7fb752c8","There is a need to develop secular variation (SV) models using satellite data as the use of ground data is not always possible. Ground data has many limitations including limited data points and lack of data over ocean areas that are not easily accessible. Two regional geomagnetic field modelling techniques, namely polynomial surface modelling (PolyM) and Spherical Cap Harmonic Analysis (SCHA), were applied to CHAMP satellite data recorded between 2001 and 2005 to investigate the use of satellite data to develop a geomagnetic SV model over southern Africa. The resulting regional models of this investigation were validated against the two widely used global field models IGRF 10 and CHAOS-2 using the available ground survey data obtained during the same period over southern Africa. The results suggest that the regional field models can be derived based entirely on satellite data. However, the regional SV models can be improved using both high quality satellite and ground survey data, since they lack the high quality of a global field model like CHAOS-2."
,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-80053505477&partnerID=40&md5=bbc2b1b1c5e2dbb72f306ea035071e91","We report here on the new realization of an automatic fluxgate theodolite able to perform unattended absolute geomagnetic declination and inclination measurements: the AUTODIF MKII. The main changes of this version compared with the former one are presented as well as the better specifications we expect now. We also explain the absolute orientation procedure by means of a laser beam and a corner cube and the method for leveling the fluxgate sensor, which is different from a conventional DIflux theodolite."
,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-80053488284&partnerID=40&md5=8763dcff4cd5587ddead41aee3124576","Magnetic disturbances due to the traffic are tentatively modelled assuming that the sources are moving dipoles. The influencing section of the road (""useful"" portion) should be modelled in 3D. The parameters of the model (time of closest position to the magnetometer, velocity, including its sign, dipole moment) are fairly accurately estimated. The fit is improved with the incorporation of a small induction effect."
,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-80053486812&partnerID=40&md5=8e4a09a1ee7ea1edab7b14b42e5288f3","To enhance the quality and near real time production of K-based planetary magnetic activity indices, such as am, and longitude-sector indices, aλ, there is a requirement for local three-hourly K index values from the South Atlantic observatory at Port Stanley. We describe the computer algorithm used to estimate the solar regular variation, SR, and techniques used to establish the parameters required to derive the K indices. We analyse the results by comparing the distributions of K values with observatories at similar geomagnetic latitude and thus with likely similar levels of geomagnetic activity. We also look for biases by comparing the indices directly to those from nearby observatories. Both sets of results show a good overall agreement. However adjustment of input parameters will be necessary if improved agreement with the indices from the other observatories is required."
,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-80053482688&partnerID=40&md5=b797c8f6e99f62a8b09108052f56ba7b","The time accuracy of geomagnetic data is an important specification for one-second data distributions. We tested a procedure to verify the time accuracy of a fluxgate magnetometer by using a GPS pulse generator. The magnetometer was equipped with a high time resolution (100 Hz) output, so the data delay could be checked directly. The delay detected from one-second data by a statistical method was larger than those from 0.1-s-and 0.01-s-resolution data. The test of the time accuracy revealed the larger delay and was useful for verifying the quality of the data."
,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-80053476364&partnerID=40&md5=2140d033685f5a68d13d4132fcd62584","In this study, using one-minute definitive data published by a number of INTERMAGNET observatories, we apply a number of time-and frequency-domain techniques to characterise the global, natural geomagnetic signal and isolate the artificial noise at an individual observatory. With the aim of developing an analytical tool that can be used to identify observatory noise against the natural signal, we report on the suitability of these techniques to detect common observatory noise types."
,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-80053463884&partnerID=40&md5=9333e245cb04d946cd832b92aa1cb817","Principal component analysis is applied to analyze the horizontal component of geomagnetic data for the Panzhihua Ms 6.1 earthquake. We investigate temporal variations in eigenvalues and find that only the first principal component has good correlation with the Ap index for which the cross-correlation correlation R is larger than 0.6, which may imply solar-terrestrial activity. Both the second and third principal components show clear daily variation, being high during work hours and low at night and on weekends. The mean eigenvalue of the third component at night (00:00-04:00 LT) increased about 40 days before the earthquake and returned to normal 10 days before the earthquake. These features are likely to be correlated with the earthquake."
,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-80053462377&partnerID=40&md5=71b65f0a8fc20656a00f22826ca0f284","Good magnetic observatories are needed more than ever for global modeling and navigation. Magnetic satellite missions, once said to be the death of ground based observations, are now demanding quality data from fixed observations points on the Earth."
"10.1504/IJBIDM.2011.041956","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84860419450&doi=10.1504%2fIJBIDM.2011.041956&partnerID=40&md5=7156dff36993b49ecfc5d6e43f6c09aa","Most of the routing solutions in ad-hoc networks deal with the best-effort data traffic. Connections with Quality of Service (QoS) requirements are not supported. In this paper, we survey the research work on applicationof Genetic Algorithms (GAs) for QoS routing in ad-hoc networks. We present different approaches and give a comparison study between GA-based routing algorithms: GAMAN and GLBR. The performance evaluation via simulations showsthat the GAMAN algorithm has better behaviour than GAMAN-1 and GLBR algorithms and is a promising algorithm for QoS routing in Ad-Hoc networks. Finally, we discuss some improvements of GAMAN and investigated some future work. © 2011 Inderscience Enterprises Ltd."
"10.1504/IJBIDM.2011.041959","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84860416250&doi=10.1504%2fIJBIDM.2011.041959&partnerID=40&md5=1e18262264617849bdea2be24869d0bd","Privacy is one of the most important issues when dealing with the individual data. Typically, given a data set and a data-processing target, the privacy can be guaranteed based on the pre-specified standard by applying privacy data-transformation algorithms. Also, the utility of the data set must be considered while the transformation takes place. However, the data-transformation problem such that a privacy standard must be satisfied and the impact on the data utility must be minimised is an NP-hard problem. In this paper, we propose an approximation algorithm for the data transformation problem. The focused data processing addressed in this paper is classification using association rule, or associative classification. The proposed algorithm can transform the given data sets with O(κ log κ)-approximation factor with regard to the data utility comparing with the optimal solutions. The experiment results show that the algorithm is both effective and efficient comparing with the optimal algorithm and the other two heuristic algorithms. © 2011 Inderscience Enterprises Ltd."
"10.1504/IJBIDM.2011.041957","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84860413361&doi=10.1504%2fIJBIDM.2011.041957&partnerID=40&md5=cfcafa0053d71ad83efdf85bec79bf81","In many jurisdictions, automobile insurers have access to risk-sharing pools to which they can transfer some risks. We consider different feature selection and modelling approaches to maximise profitability of these transfers through better risk selection. For that purpose, we introduce a flexible scoring model and devise a robust feature synthesis and selection method. We show what should be the most suitable sorting criterion depending on pool regulations. We use a technique, similar to cross validation, but that is coherent with the sequential structure of insurance data. We explain how software maturity level impacts profitability. Copyright © 2011 Inderscience Enterprises Ltd."
"10.1504/IJBIDM.2011.041960","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84860399527&doi=10.1504%2fIJBIDM.2011.041960&partnerID=40&md5=054e7744c7573f648c3583e71f54f1ed","We propose a distributed construction scheme of MOLAP data cubes in the related sites on a network. Using the implementation scheme of multidimensional datasets based on the history-offset tuple encoding method, the tuple stream can be processed efficiently to construct cuboids in real-time on each site, while MOLAP operations can be processed against one of the data cube versions in background. In this paper, we describe our tuple stream processing scheme and distributed data cube construction, then evaluate the required communication cost. © 2011 Inderscience Enterprises Ltd."
"10.1504/IJBIDM.2011.041958","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84860396130&doi=10.1504%2fIJBIDM.2011.041958&partnerID=40&md5=7ec76a39e1e7877f037a5de38c54fb3b","This paper introduces a new approach to deal with the fault recognition problem in modern telecommunication networks, called Behavioural Proximity Discovery (BPD). BPD is a framework for root cause analysis that consists of three complementary clustering algorithms based on alarm behaviours. It provides a complete and adaptive solution to the network manager who can then decide of the repair actions to restore the network back to a normal state. An evaluation of this approach on both simulated and real data from 3G mobile-networks is presented and discussed. © 2011 Inderscience Enterprises Ltd."
"10.2481/dsj.009-004","https://www.scopus.com/inward/record.uri?eid=2-s2.0-80053474329&doi=10.2481%2fdsj.009-004&partnerID=40&md5=f651346eb96314f7fb95c6cf2610bbdc","Data mining is a valuable tool in meteorological applications. Properly selected data mining techniques enable researchers to process and analyze massive amounts of data collected by satellites and other instruments. Large spatial-temporal datasets can be analyzed using different linear and nonlinear methods. The Self-Organizing Map (SOM) is a promising tool for clustering and visualizing high dimensional data and mapping spatial-temporal datasets describing nonlinear phenomena. We present results of the application of the SOM technique in regions of interest within the European re-analysis data set. The possibility of detecting climate change signals through the visualization capability of SOM tools is examined."
"10.1504/IJBIDM.2011.039410","https://www.scopus.com/inward/record.uri?eid=2-s2.0-79953730307&doi=10.1504%2fIJBIDM.2011.039410&partnerID=40&md5=4780ae8b98f11ff2f6703e95fd080e2d","Two of the most data model in Data Warehouse (DW) and advanced database includes star and snowflake schema, which play pivotal roles in the underlying performance. Today, DW queries comprise a group of aggregations and joining operations. As a result, snowflake schema does not seem to be an adequate option since several relations must combine to provide answers for queries that involve aggregation. In spite of its widespread application and undeniable advantages, snowflaking technique has certain theoretical and practical demerits. This paper proposes Hierarchical Degenerate Snowflake (HDS) as an alternative logical data model to achieve DW structure to improve the query performance. Copyright © 2011 Inderscience Enterprises Ltd."
"10.1504/IJBIDM.2011.039409","https://www.scopus.com/inward/record.uri?eid=2-s2.0-79953726318&doi=10.1504%2fIJBIDM.2011.039409&partnerID=40&md5=3aa5777b821bbfc14a58d3bb84797fd1","Resources businesses often undergo turbulent and volatile periods, due to rapid increase of resource demand and poorly organised resources data volumes. This volatile industry operates multifaceted business units that manage heterogeneous data sources. Data integration and interactive business processes, distributed across complex business environments, need attention. Historical resources data, geographically (spatial dimension) archived for decades (periodic dimension), are source of analysing past business data dimensions and predicting their future turbulences. Periodic data, modelled in an integrated and robust warehouse environment, are explored using data mining methodologies. The data models presented, will optimise future inputs in the turbulent resources business environments. Copyright © 2011 Inderscience Enterprises Ltd."
"10.1504/IJBIDM.2011.039411","https://www.scopus.com/inward/record.uri?eid=2-s2.0-79953688580&doi=10.1504%2fIJBIDM.2011.039411&partnerID=40&md5=ddf9b87cdce29a8bf0a5d155766dd965","In this paper we describe our approach to discover trends for the biotechnology and pharmaceutical industry based on temporal text mining. Temporal text mining combines information extraction and data mining techniques upon textual repositories and our main objective is to identify changes of associations among entities of interest over time. It consists of three main phases; the Information Extraction, the ontology driven generalisation of templates and the discovery of associations over time. Treatment of the temporal dimension is essential to our approach since it influences both the annotation part (IE) of the system as well as the mining part. Copyright © 2011 Inderscience Enterprises Ltd."
"10.1504/IJBIDM.2011.039412","https://www.scopus.com/inward/record.uri?eid=2-s2.0-79953684015&doi=10.1504%2fIJBIDM.2011.039412&partnerID=40&md5=a4cc17335eab34cdc1301b31325acb1c","Existing Association Rules Mining (ARM) algorithms basically use multiple scans to extract a rule from a transaction database. Sometime ARM algorithms exit without a rule in the desktop environment due to the high volume of transactions. Matrix Algorithm (MA) is proposed to minimise this issue. However, it is a computational expensive solution. In this paper, we propose Advanced Matrix Algorithm (AMA), to generate an efficient rule by a single scan using the Boolean matrix concept. AMA is comparatively effective and efficient than traditional approaches in terms of computational cost for database scan and frequently candidate sets generation. Copyright © 2011 Inderscience Enterprises Ltd."
"10.2481/dsj.Kraines","https://www.scopus.com/inward/record.uri?eid=2-s2.0-79551599580&doi=10.2481%2fdsj.Kraines&partnerID=40&md5=a27ded5ccc08615040359ca074ae6060","Work towards creation of a knowledge sharing system for sustainability science through the application of semantic data modeling is described. An ontology grounded in description logics was developed based on the ISO 15926 data model to describe three types of sustainability science conceptualizations: situational knowledge, analytic methods, and scenario frameworks. Semantic statements were then created using this ontology to describe expert knowledge expressed in research proposals and papers related to sustainability science and in scenarios for achieving sustainable societies. Semantic matching based on logic and rule-based inference was used to quantify the conceptual overlap of semantic statements. This shows the semantic similarity of topics studied by different researchers in sustainability science, similarities that might be unknown to the researchers themselves."
"10.1504/IJBIDM.2011.044997","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857292133&doi=10.1504%2fIJBIDM.2011.044997&partnerID=40&md5=7370bbd2f20681f474e5e3ee9030ed3b","Nowadays, enterprises and their business processes are becoming more dynamic. Business Processes (BPs) need to be able to adapt to changes in open business systems environments. This paper presents a new approach for BP flexibility based on context environments and artefacts. This approach enables a better reusability and flexibility of independent BP modules in a context-aware run-time environment. We present a context-aware process mining framework where contexts will be automatically captured from execution environments to maximise the flexibility of BPs. Process mining techniques are used to extract information from BP run-time. The reasoning about those information using artefact is shown. Copyright © 2011 Inderscience Enterprises Ltd."
"10.1504/IJBIDM.2011.044996","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857291456&doi=10.1504%2fIJBIDM.2011.044996&partnerID=40&md5=758c79b02b07b6895ac2ec7e62abedeb","This paper analyses the impact of biological intelligence on the problem of selecting the optimal solution in Web service composition. Thus, we propose two selection methods, one inspired by the behaviour of bees searching for food and another one inspired by the behaviour of cuckoos searching for the nests where to lay eggs. The methods use a composition graph to search for the optimal solution. The quality of a composition is evaluated based on QoS and semantic quality. To comparatively analyse the proposed methods we implemented an experimental prototype and performed tests on a set of scenarios from trip planning. Copyright © 2011 Inderscience Enterprises Ltd."
"10.1504/IJBIDM.2011.044977","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857287571&doi=10.1504%2fIJBIDM.2011.044977&partnerID=40&md5=d4d8ce49a81d54fb37bd4c34df0926e8","A scheme is developed that clusters services based on outcome attributes deemed important by customers. The algorithm that determined clusters used empirical field research data from 164 different services. The services are modelled as binary vectors. They are analysed using a clustering method based on the Ward algorithm. The analysis reveals six distinct clusters by customer desiderata. Additionally, the medoid is a natural representative of each cluster. The clusters are quite distinct from previously considered groupings based on process characteristics, and offer new insights into their common features. Implications for service innovation, strategic planning, and staffing are discussed. Copyright © 2011 Inderscience Enterprises Ltd."
"10.1504/IJBIDM.2011.044978","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857286673&doi=10.1504%2fIJBIDM.2011.044978&partnerID=40&md5=503ef334c34cff9c5109a7988f6759c9","Subgroup Analysis (SA) is a helpful technique in the context of randomised experiments and in observational studies. With reference to program evaluation, it helps in determining whether and how treatment effects vary across subgroups induced by baseline covariates. However, the choice of the optimal number of subgroups is often ambiguous and causes concern. Here, SA is conducted using the cluster-based approach introduced in D'Attoma and Camillo (2011) and the usage of the Information Complexity Criterion to select the optimal number of groups is proposed. A simulation study and a real case have been illustrated to show such promising approach. Copyright © 2011 Inderscience Enterprises Ltd."
"10.1504/IJBIDM.2011.044976","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857280079&doi=10.1504%2fIJBIDM.2011.044976&partnerID=40&md5=6f63e9981b8016e9c702abd637e60a49","Ant Colony Optimisation (ACO) algorithms use simple mutually cooperating agents (ants) to produce a robust and adaptive search system, which can be used for knowledge discovery. In this paper, a Support Vector Machine (SVM)-cAnt-Miner-based system for predicting the next-day's trend in stock markets is proposed. The trend predicted by the proposed system is then used to identify the appropriate time to buy and sell securities. Performance of the proposed system is evaluated against SVM-Ant-Miner, SVM-Ant-Miner2, Naïve-Bayes and an Artificial Neural Network (ANN)-based trend prediction system. The results indicate that the proposed system outperforms all the other techniques considered. Copyright © 2011 Inderscience Enterprises Ltd."
"10.1504/IJBIDM.2011.038274","https://www.scopus.com/inward/record.uri?eid=2-s2.0-79251580978&doi=10.1504%2fIJBIDM.2011.038274&partnerID=40&md5=e3bb63a34b17c4393a04f374bd5905d4","Association rule mining is one of the most popular data-mining techniques used to find associations existing between a set of objects or data. A time series is a sequence of observations stamped over the time; Time-series analysis has been used in a variety of applications like: business and health. The application of association mining to time series is very promised. The purpose of this article is to propose a new fast algorithm to discover the association that can exist between two time series. We use discretisation to segment time series to a number of shapes, and we classify these shapes to pre-defined shape classes to generate association rules using Genetic Algorithm (GA). Copyright © 2011 Inderscience Enterprises Ltd."
"10.1504/IJBIDM.2011.038271","https://www.scopus.com/inward/record.uri?eid=2-s2.0-79251571134&doi=10.1504%2fIJBIDM.2011.038271&partnerID=40&md5=6124e9a18f27dfab72e44441b46783c5","This paper presents an evaluation model for marketable quality and profitability of corporations considering sellers and buyers market. By defining a supply level of marketable quality considering the fair relationship of sellers and buyers with a condition that ""demand level is equal with supply level"", we carry out the analysis of marketable quality and profitability using the real data of some leading Japanese manufacturing corporations. From our analysis we found that in the variable of profit ratio, the sum of sellers profitability and buyers profitability increases gradually by marketable quality enhancement. Copyright © 2011 Inderscience Enterprises Ltd."
"10.1504/IJBIDM.2011.038272","https://www.scopus.com/inward/record.uri?eid=2-s2.0-79251557800&doi=10.1504%2fIJBIDM.2011.038272&partnerID=40&md5=da3ed8ca061170bda339269d06a4ca20","This paper proposes a method that divides bloggers to clusters according to their interests. The method calculates similarities between the bloggers based on three steps. That is, the method calculates similarities between target objects discussed in blog articles based on social annotations. It calculates similarities between impressions related to the target objects based on impression words included in blog articles. Here, products, works and services are examples of the target objects. Lastly, the method calculates similarities between the bloggers by combining the results of the method's first and second calculation steps, and divides the bloggers to clusters based on the similarities. The paper applies the method to the Commutents data and the Yahoo! Japan Movie data, and verifies the effectiveness of the method. Copyright © 2011 Inderscience Enterprises Ltd."
"10.1504/IJBIDM.2011.038273","https://www.scopus.com/inward/record.uri?eid=2-s2.0-79251549471&doi=10.1504%2fIJBIDM.2011.038273&partnerID=40&md5=22fca369edf6a01169cf56b2207049ae","In fully distributed Peer-to-Peer (P2P) overlay networks, each peer has to obtain information on objects distributed through communicating with its acquaintance peers. An acquaintance might hold obsolete information owing to the propagation delay and faults of peers. Hence, a peer has to collect correct object information only from trustworthy acquaintances. The subjective trustworthiness of an acquaintance is obtained by directly interacting with the acquaintance. The objective trustworthiness of an acquaintance is obtained by collecting the subjective trustworthiness from other peers. If a peer is confident of its own subjective trustworthiness, the peer takes the subjective trustworthiness. Otherwise, the peer takes the objective one. We evaluate how the subjective trustworthiness changes in change of satisfiablity of an acquaintance. Copyright © 2011 Inderscience Enterprises Ltd."
"10.1504/IJBIDM.2011.038275","https://www.scopus.com/inward/record.uri?eid=2-s2.0-79251548897&doi=10.1504%2fIJBIDM.2011.038275&partnerID=40&md5=d1a93fefb6d906d96e269f55b7c7e72a","At present, there have been many Vietnamese researches focusing on developing Knowledge Base (KB) for Vietnamese and exploiting its data tosupport others applications in wide areas. The paper introduces an approach, which applies Support Vector Machine (SVM) for Vietnamese KB (VKB) development, and new solution for user question's answering based on Dependent Grammar also VKB data exploitation. For further research, our primary targets in other are to improve VKB quality and answer-finding algorithms on faster text-searching solutions also lower run-time progress, and mechanism to generate answers in natural language format for found result of these algorithms. Copyright © 2011 Inderscience Enterprises Ltd."
"10.1504/IJBIDM.2011.038276","https://www.scopus.com/inward/record.uri?eid=2-s2.0-79251547981&doi=10.1504%2fIJBIDM.2011.038276&partnerID=40&md5=935a8d060ebcb6ab2a7bd90f273b982d","Web usage mining has been much concentrated on the discovery of relevant user behaviours from web access record data. In this paper, we present WebUser, an approach to discover unexpected usage in web access log. We present a belief-driven method for extracting unexpected web usage sequences, where the belief system consists of a temporal relation and semantics constrained sequence rules acquired with respect to prior knowledge. Our experiments show the effectiveness and usefulness of the proposed approach. Furthermore, discovered rules of unexpected web usage can be used for web content personalisation and recommendation, site structure optimisation and critical event prediction. Copyright © 2011 Inderscience Enterprises Ltd."
"10.1504/IJBIDM.2010.036123","https://www.scopus.com/inward/record.uri?eid=2-s2.0-77958578645&doi=10.1504%2fIJBIDM.2010.036123&partnerID=40&md5=60254d647fd82c85f240ee860df27877","We investigate in this paper the use of XML structure in multimedia retrieval, particularly in context-based image retrieval. We propose two methods to represent multimedia objects: the first one is based on an implicit use of textual and structural context of multimedia objects, whereas the second one is based on an explicit use of both sources. Experimental evaluation is carried out using the INEX MultimediaFragments Task 2006 and 2007. We show that there is a strong vocabulary relation between the query and the multimedia object representation, and that using XML structure improves significantly the effectiveness of multimedia retrieval. © 2010 Inderscience Enterprises Ltd."
"10.1504/IJBIDM.2010.036125","https://www.scopus.com/inward/record.uri?eid=2-s2.0-77958566271&doi=10.1504%2fIJBIDM.2010.036125&partnerID=40&md5=102fd5826be6fb49a77b3081bfd9aaa6","This paper presents the MAMView framework to help users and developers in understanding the data organisation in Metric Access Methods (MAM). Users and developers can explore and share dynamic and interactively 2- or 3-dimensional representations of a MAM. Such representations can be the steps of a similarity query or the insertion of an object, or the data organisation in a MAM. MAMView was developed as a practical tool that has been successfully applied in studying existing MAM, helping novice users to better understand the behaviour and properties of such structures, as well developers to verify and drill-down their new proposed structures. © 2010 Inderscience Enterprises Ltd."
"10.1504/IJBIDM.2010.036126","https://www.scopus.com/inward/record.uri?eid=2-s2.0-77958554984&doi=10.1504%2fIJBIDM.2010.036126&partnerID=40&md5=2e9a4d882d864e31355bf801a79d4299","In the Business-to-Business integration (B2Bi) domain, ebXML BPSS (ebBP) as dedicated B2Bi choreography standard is well-suited as means for agreement upon the overall message exchanges among integration partners while executable WS-BPEL orchestrations derived from ebBP choreographies are well-suited for governing the local message flow of each individual participant. In order to enable complex integration scenarios, this paper introduces the concept of partner-shared states into ebBP. We describe the modelling of shared states in ebBP and specify the formalisation and operational semantics of shared state-based ebBP models. A prototypic translation tool provides the realisation of the operational semantics as WS-BPEL orchestrations. © 2010 Inderscience Enterprises Ltd."
"10.1504/IJBIDM.2010.036124","https://www.scopus.com/inward/record.uri?eid=2-s2.0-77958534148&doi=10.1504%2fIJBIDM.2010.036124&partnerID=40&md5=f970cca08ac4fbbcc8bc767f3889d48b","Accurate and fast retrieval of relevant images is a challenging task mainly due to the limitation in understanding hidden knowledge in images, known as semantic gap. In this work, we propose a novel approach which incorporates local feature representation for retrieval of grey and colour images from an archive with user intervention. We used histogram features, which are computationally efficient, hence resulting in quick image retrieval. The computed image feature vectors are used for similarity matching with weighted feed-backed image retrieval. We experimented both on publicly available and annotated image data sets to illustrate the effectiveness of our approach. © 2010 Inderscience Enterprises Ltd."
"10.2481/dsj.009-007","https://www.scopus.com/inward/record.uri?eid=2-s2.0-78649621051&doi=10.2481%2fdsj.009-007&partnerID=40&md5=c79fe61dd40fa92590e4ad39f09a9e07","The use of scientific data is becoming increasingly dependent on the software that fosters such use. As theability to reuse software contributes to capabilities for reusing software-dependent data, instruments for measuring software reusability contribute to the reuse of software and related data. The development and current state of a proposed set of Reuse Readiness Levels (RRLs) are summarized, and potential uses of the software reusability measures are described, along with proposed use cases to support sponsorship of software projects, software production, software adoption, and data stewardship during the systems development lifecycle and the data lifecycle."
"10.2481/dsj.008-034","https://www.scopus.com/inward/record.uri?eid=2-s2.0-78649614668&doi=10.2481%2fdsj.008-034&partnerID=40&md5=bc18d8a76583d0db76f63a4c6084e462","An alternative ratio-cum-product estimator of population mean using the coefficient of kurtosis for two auxiliary variates has been proposed. The proposed estimator has been compared with a simple mean estimator, the usual ratio estimator, a product estimator, and estimators proposed by Singh (1967) and Singh et al. (2004). An empirical study is also carried out in support of the theoretical findings."
"10.2481/dsj.008-040","https://www.scopus.com/inward/record.uri?eid=2-s2.0-78649586806&doi=10.2481%2fdsj.008-040&partnerID=40&md5=d4344bea7cde8fc6bae8f3e6b3c73ad6","Micro data is a valuable source of information for research. However, publishing data about individuals for research purposes, without revealing sensitive information, is an important problem. The main objective of privacy preserving data mining algorithms is to obtain accurate results/rules by analyzing the maximum possible amount of data without unintended information disclosure. Data sets for analysis may be in a centralized server or in a distributed environment. In a distributed environment, the data may be horizontally or vertically partitioned. We have developed a simple technique by which horizontally partitioned data can be used for any type of mining task without information loss. The partitioned sensitive data at 'm' different sites are transformed using a mapping table or graded grouping technique, depending on the data type. This transformed data set is given to a third party for analysis. This may not be a trusted party, but it is still allowed to perform mining operations on the data set and to release the results to all the 'm' parties. The results are interpreted among the 'm' parties involved in the data sharing. The experiments conducted on real data sets prove that our proposed simple transformation procedure preserves one hundred percent of the performance of any data mining algorithm as compared to the original data set while preserving privacy."
"10.2481/dsj.009-012","https://www.scopus.com/inward/record.uri?eid=2-s2.0-78649604283&doi=10.2481%2fdsj.009-012&partnerID=40&md5=07614794e84ed126a302e59d1d5d8fba","In some areas of science, sophisticated web services and semantics underlie ""cyberinfrastructure"". However, in ""small science"" domains, especially in field sciences such as archaeology, conservation, and public health, datasets often resist standardization. Publishing data in the small sciences should embrace this diversity rather than attempt to corral research into ""universal"" (domain) standards. A growing ecosystem of increasingly powerful Web syndication based approaches for sharing data on the public Web can offer a viable approach. Atom Feed based services can be used with scientific collections to identify and create linkages across different datasets, even across disciplinary boundaries without shared domain standards."
"10.2481/dsj.008-041","https://www.scopus.com/inward/record.uri?eid=2-s2.0-78649571452&doi=10.2481%2fdsj.008-041&partnerID=40&md5=f0ef99ac76df7b0309d17ffcb1dc8b3d","We have rich information resources for materials science and engineering - raw measurement data, computational simulation methods, digitized handbooks, and digital libraries. However, these resources have a wide variety of formats, terminologies, and concepts, which makes it difficult to find appropriate information for materials design, development, and evaluation. One solution to this problem is to integrate these resources into a computer readable concept map, called a domain ontology, which describes concepts and relationships among the concepts in materials science and engineering. This paper describes a trial that constructs a standard of metadata description using ontology language and demonstrates the validity of this construction through data exchange among heterogeneous material databases. ""Materials Ontology,"" which consists of several sub ontologies corresponding to substance, process, environment, and property, is developed using the ontology language of the Semantic Web, OWL, which enables the definition of a flexible and detailed structure of materials information. A versatile ""materials data format"" is built on the Materials Ontology as a component of the materials information platform and is applied to exchange data among three different thermal property databases, maintained by two major materials science research institutes in Japan."
"10.1504/IJBIDM.2010.033359","https://www.scopus.com/inward/record.uri?eid=2-s2.0-77953172680&doi=10.1504%2fIJBIDM.2010.033359&partnerID=40&md5=3565a80964be8ff283fcf360e915e94e","This paper proposes a method dealing with missing values in the discovery of frequent patterns. Also, it proposes a method that effectively discovers the patterns from examples composed of attributes and their values. The method generates candidate patterns based on the combination of attributes and the combination of attribute values. It evaluates the patterns based on two supports. These supports are calculated based on the number of examples that do not include missing values in attributes composing target items and in all attributes. The proposed method is verified by comparing it with the existing methods dealing with missing values. Copyright © 2010 Inderscience Enterprises Ltd."
"10.1504/IJBIDM.2010.033362","https://www.scopus.com/inward/record.uri?eid=2-s2.0-77953171559&doi=10.1504%2fIJBIDM.2010.033362&partnerID=40&md5=d99274ed8675c334cde403faddfbcf3b","The mobile cellular systems are expected to support multiple services with guaranteed Quality of Service (QoS). But, the ability of wireless systems to accommodate expected growth of traffic load and broadband services is limited by available radio frequency spectrum. Call Admission Control (CAC) is one of the resource management functions, which regulates network access to ensure QoS provisioning. However, the decision for CAC is very challenging issue due to user mobility,limited radio spectrum, and multimedia traffic characteristics. In our previous work, we proposed a fuzzy-based CAC system and compared the performance of the proposed system with Shadow Cluster Concept (SCC). In this work, we extend our work by considering the priority of the on-going connections. We evaluate by simulations the performance of the proposed system and compare its performance with our previous work. The performance evaluation shows that the proposed system has a good behaviour in keeping the QoS of on-going connections. Copyright © 2010 Inderscience Enterprises Ltd."
"10.1504/IJBIDM.2010.033364","https://www.scopus.com/inward/record.uri?eid=2-s2.0-77953164942&doi=10.1504%2fIJBIDM.2010.033364&partnerID=40&md5=6ca264a67dc57c8d7c508a7b813ea745","This study empirically explores the use of a group, or ensemble, of classifiers to support managerial decision making in domains characterised by asymmetric misclassification costs. The approach developed in this study is intended to assist a decision maker in determining whether a current situation warrants the choice of an ensemble over an individual classifier. The decision is based primarily on misclassification costs in the decision context and the associated basis on which performance is assessed. We show that the criteria for evaluating classifier performance are fundamentally dependent on the symmetry or asymmetry of misclassification costs. The result of this study is a set of heuristics for identifying highly- and poorly-performing ensembles. Copyright © 2010 Inderscience Enterprises Ltd."
"10.1504/IJBIDM.2010.033363","https://www.scopus.com/inward/record.uri?eid=2-s2.0-77953159390&doi=10.1504%2fIJBIDM.2010.033363&partnerID=40&md5=7a0fdfab51878883201240fcb495b497","The clustering capabilities of the Non Negative Matrix Factorisation (NMF) algorithm is studied. The basis images are considered like the membership degree of the data to a particular class. A hard clustering algorithm is easily derived based on these images. This algorithm is applied on a multivariate image to perform image segmentation. The results are compared with those obtained by Fuzzy K-means algorithm and better clustering performances are found for NMF based clustering. We also show that NMF performs well when we deal with uncorrelated clusters but it cannot distinguish correlated clusters. This is an important drawback when we try to use NMF to perform data clustering. Copyright © 2010 Inderscience Enterprises Ltd."
"10.1504/IJBIDM.2010.033361","https://www.scopus.com/inward/record.uri?eid=2-s2.0-77953149919&doi=10.1504%2fIJBIDM.2010.033361&partnerID=40&md5=1ca2bd8cd267bb7d40a8d91714898357","This paper describes the relationship between trading network and WWW network from preferential attachment mechanism perspective. This mechanism is known to be the underlying principle in the network evolution and has been incorporated to formulate two famous web pages ranking algorithms, PageRank and Hypertext Induced Topic Search (HITS). We point out the differences between trading network and WWW network from preferential attachment perspective, derive the formulation of HITS-based ranking algorithm for trading network as a direct consequence of the differences, and apply the same framework when deriving the formulation back to the HITS formulation that turns to become a technique to accelerate its convergences. Copyright © 2010 Inderscience Enterprises Ltd."
"10.1504/IJBIDM.2010.033360","https://www.scopus.com/inward/record.uri?eid=2-s2.0-77953147031&doi=10.1504%2fIJBIDM.2010.033360&partnerID=40&md5=6d2e1e4ecc693d9244ab736c2c79e8ea","In this paper, the problem of constraint-based pattern discovery is investigated. By allowing more user-specified constraints other than traditional rule measurements, e.g., minimum support and minimum confidence, research work on this topic endeavoured to reflect real interest of analysts and relieve them from the overabundance of rules. Surprisingly, very little research has been conducted to deal with multiple types of constraints. In our previous work, we have studied this problem, specifically focusing on three different types of constraints, and an efficient Apriori-like algorithm, called MCFP, is proposed. In this paper, we propose a new algorithm called MCFPTree, which is based on a tree structure for keeping frequent patterns without suffering from the problem of candidate itemsets generation. Experimental results show that our MCFPTree algorithm is significantly faster than MCFP and an intuitive method FP-Growth+, i.e., post-processing the frequent patterns generated by FP-Growth, against user-specified constraints. Copyright © 2010 Inderscience Enterprises Ltd."
"10.2481/dsj.009-006","https://www.scopus.com/inward/record.uri?eid=2-s2.0-77953402808&doi=10.2481%2fdsj.009-006&partnerID=40&md5=fb5bbf7c2dc10197eb8f44e445cc36de","It is important to improve data reliability and data access efficiency for data-intensive applications in a data grid environment. In this paper, we propose an Information Dispersal Algorithm (IDA)-based parallel storage scheme for massive data distribution and parallel access in the Scientific Data Grid. The scheme partitions a data file into unrecognizable blocks and distributes them across many target storage nodes according to user profile and system conditions. A subset of blocks, which can be downloaded in parallel to remote clients, is required to reconstruct the data file. This scheme can be deployed on the top of current grid middleware. A demonstration and experimental analysis show that the IDA-based parallel storage scheme has better data reliability and data access performance than the existing data replication methods. Furthermore, this scheme has the potential to reduce considerably storage requirements for large-scale databases on a data grid."
"10.2481/dsj.008-003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-77953446976&doi=10.2481%2fdsj.008-003&partnerID=40&md5=8c0b3106f065164ae7bda39a4eb9f968","By comparing a hard real-time system and a soft real-time system, this article elicits the risk of over-design in soft real-time system designing. To deal with this risk, a novel concept of statistical design is proposed. The statistical design is the process accurately accounting for and mitigating the effects of variation in part geometry and other environmental conditions, while at the same time optimizing a target performance factor. However, statistical design can be a very difficult and complex task when using classical mathematical methods. Thus, a simulation methodology to optimize the design is proposed in order to bridge the gap between real-time analysis and optimization for robust and reliable system design."
"10.2481/dsj.008-030","https://www.scopus.com/inward/record.uri?eid=2-s2.0-77649203924&doi=10.2481%2fdsj.008-030&partnerID=40&md5=0c84a08518cd7dc083a1057c53e14dae","This article proposes an innovativ utility sentient approach for the mining of interesting association patterns from transaction databases. First, frequent patterns are discovered from the transaction database using the FPGrowth algorithm. From the frequent patterns mined, this approach extracts novel interesting association patterns with emphasis on significance, utility, and the subjective interests of the users. The experimental results portray the efficiency of this approach in mining utility-oriented and interesting association rules. A comparative analysis is also presented to illustrate our approach's effectiveness."
"10.1504/IJBIDM.2010.030300","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84889933868&doi=10.1504%2fIJBIDM.2010.030300&partnerID=40&md5=602ece6ce82a1d2272747270cd08ac97","The treatment of mental disorders has recently become an important research issue. The treatment of mental illness through biofeedback therapy can be defined as a combination method integrating biofeedback devices and self-help programs. This research aims at development of an online treatment system for panic patients by combining biofeedback therapy and web technologies. The system provides a more convenient communication between patients and medical professionals. Moreover, managing functions are provided for the medical professionals. The results of this research are expected to have a pivotal impact on the healthcare industry with increased and enhanced levels of technology and services. © 2010 Inderscience Enterprises Ltd."
"10.1504/IJBIDM.2010.031283","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84887568268&doi=10.1504%2fIJBIDM.2010.031283&partnerID=40&md5=523fe22be183f8cdd1b0d6ff597022bb","Data Warehouses (DWs) store historical information which support the decision-making process. Since this information is crucial, it has to be protected from unauthorised accesses by defining security constraints in all stages of the DW development process. In previous works, we applied the Model-Driven (MDA) philosophy to define secure DWs using several security models and transformations. Nevertheless, our conceptual model makes it possible to define complex security rules with OCL expressions which are not transformed automatically. This paper deals with this problem and completes our approach improving our conceptual metamodel and defining new transformation rules. Finally, an application example is shown. © 2010 Inderscience Enterprises Ltd."
"10.1504/IJBIDM.2010.031284","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868535726&doi=10.1504%2fIJBIDM.2010.031284&partnerID=40&md5=6de98cdfab6c15d47a22cf4a9b2aec6c","In this paper, we present algorithms for subgroup detection and demonstrated them with a real-time case study of USS Cole bombing terrorist network. The algorithms are demonstrated in an application by a prototype system. The system finds associations between terrorist and terrorist organisations and is capable of determining links between terrorism plots occurred in the past, their affiliation with terrorist camps, travel record, funds transfer, etc. The findings are represented by a network in the form of an Attributed Relational Graph (ARG). Paths from a node to any other node in the network indicate the relationships between individuals and organisations. The system also provides assistance to law enforcement agencies, indicating when the capture of a specific terrorist will more likely destabilise the terrorist network. In this paper, we discuss the important application area related to subgroups in a terrorist cell using filtering of graph algorithms. The novelty of the algorithms can be easily found from the results they produce. © 2010 Inderscience Enterprises Ltd."
"10.1504/IJBIDM.2010.030298","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865993581&doi=10.1504%2fIJBIDM.2010.030298&partnerID=40&md5=7aae8dd8ed463a0faeafc8db30a1d7de","Category-constrained search in which a category is selected to restrict the query to match instances in that category is a very popular mechanism provided by information sources. It reduces the search space, and improves both the response time and the quality of the query results. As more and more online sources are available, it is challenging to build an integrated search system which provides a unified interface and a metasearch capability to search and access all sources from different websites in one query submission. One of the fundamental problems with building such an integrated system is category mapping which maps the selected category in the unified interface to categories provided by the information sources. In this paper, we present an efficient algorithm for automatic category mapping. Our experiment shows that our approach is very convincing, and can be used to implement automatic category mapping for the integration of category-constrained search. © 2010 Inderscience Enterprises Ltd."
"10.1504/IJBIDM.2010.031287","https://www.scopus.com/inward/record.uri?eid=2-s2.0-80053180125&doi=10.1504%2fIJBIDM.2010.031287&partnerID=40&md5=975f6697c8e8b3173b22d31fa9236dd2","A novel Circular Augmented Rotational Trajectory (CART) algorithm to compute an R-Space based shape descriptors, allowing efficient shape matching, generalisation and classification, is given. The shape descriptor is rotation and scale invariant, capable of detecting invariant geometric properties despite the presence of considerable noise and quantisation errors. The method is capable of detecting distinctive features including general invariant curvatures and sharp features while properly addressing the ambiguity in shape approximation. Experimental analysis on complex and ambiguous shapes shows that the CART method can correctly detect and represent the inherent shape. Universality, robustness and consistent performance have been noted while applying to many difficult and ambiguous object boundaries. © 2010 Inderscience Enterprises Ltd."
"10.1504/IJBIDM.2010.030301","https://www.scopus.com/inward/record.uri?eid=2-s2.0-80052273840&doi=10.1504%2fIJBIDM.2010.030301&partnerID=40&md5=18c70e2ac6602e35970e9e5ba0ce6dcb","Cluster analysis in data mining and knowledge discovery is an essential business application. This investigation describes a new clustering approach named EIDBSCAN that extends expansion seed selection into a sampling-based DBSCAN clustering algorithm. Additionally, the proposed algorithm may reduce eight Marked Boundary Objects to add expansion seeds according to far centrifugal force, which increases coverage. Our experimental results reveal that the proposed EIDBSCAN yields more accurate clustering results. In addition, in all the cases we studied, the proposed approach has a lower execution time cost than several existing well-known approaches, such as DBSCAN, IDBSCAN and KIDBSCAN clustering algorithms. © 2010 Inderscience Enterprises Ltd."
"10.1504/IJBIDM.2010.030297","https://www.scopus.com/inward/record.uri?eid=2-s2.0-79960372180&doi=10.1504%2fIJBIDM.2010.030297&partnerID=40&md5=83fa910c95d052edabffb065ef07504e","This work proposes an enhanced Multi-Objective Genetic Algorithm (enhanced MOGA), which includes non-dominated sorting, crowded distance sorting, binary tournament selection, extended intermediate crossover and non-uniform mutation operators, for optimising mobile base station placement. The performance of the enhanced MOGA and Deb et al.’s NSGA-II are compared by applying these two codes to benchmark problem computations. Moreover, three cases of mobile base station placement, which include homogeneous and heterogeneous transmitters located in the placement regions, are studied by the present enhanced MOGA. The non-dominated solutions are presented in terms of realistic cellular placement with handover constraint in mobile network communications. © 2010 Inderscience Enterprises Ltd."
"10.1504/IJBIDM.2010.030299","https://www.scopus.com/inward/record.uri?eid=2-s2.0-79251591806&doi=10.1504%2fIJBIDM.2010.030299&partnerID=40&md5=f167f556214b6a065fcf50f4320f06b3","Cluster analysis is frequently used to study the trend of gene expression behaviours from microarray time series data. We adopt a partitioning-based clustering algorithm for such a task. After time series are discritised into sequences, a sequential pattern mining technique is applied to find patterns as the initial clusters. Longest Common Subseries Similarity is used to measure the similarity between time series which overcomes the ‘shift-effect’ influence. An object is re-assigned to the cluster which has most objects within the k nearest neighbours of the object. Similarity measurements, like Pearson correlation coefficient, are used to determine the neighbours. © 2010 Inderscience Enterprises Ltd."
"10.1504/IJBIDM.2010.030296","https://www.scopus.com/inward/record.uri?eid=2-s2.0-78649511733&doi=10.1504%2fIJBIDM.2010.030296&partnerID=40&md5=ad2b5370effa6d494fcf65e70154d8fc","In an urban area, the demand for taxis is not always matched up with the supply. This paper proposes mining historical data to predict demand distributions with respect to contexts of time, weather, and taxi location. The four-step process consists of data filtering, clustering, semantic annotation, and hotness calculation. The results of three clustering algorithms are compared and demonstrated in a web mash-up application to show that context-aware demand prediction can help improve the management of taxi fleets. © 2010 Inderscience Enterprises Ltd."
"10.1504/IJBIDM.2010.031285","https://www.scopus.com/inward/record.uri?eid=2-s2.0-77958604096&doi=10.1504%2fIJBIDM.2010.031285&partnerID=40&md5=b10963163f62b29b6b663c8a41f48572","Failure phenomena of web server systems are considered to depend on their workload characteristics. In this paper we focus on the Apache server system and analyse the real access/error logs. Based on parametric and non-parametric statistics, we characterise the web server failure from both theoretical and empirical points of view. As the result, it can be shown that the number of sessions strongly affects to the failure rate property of the Apache server. © 2010 Inderscience Enterprises Ltd."
"10.1504/IJBIDM.2010.031286","https://www.scopus.com/inward/record.uri?eid=2-s2.0-77958529188&doi=10.1504%2fIJBIDM.2010.031286&partnerID=40&md5=8784622fc0d01fa7b13da1a67ce65e82","Network-based Intrusion Detection Systems (NIDSs) analyse network traffic to detect instances of malicious activity. Typically, this is only possible when the network traffic is accessible for analysis. With the growing use of Virtual Private Networks (VPNs) that encrypt network traffic, the NIDS can no longer access this crucial audit data. In this paper, we present an implementation and evaluation of our approach proposed in Goh et al. (2009). It is based on Shamir’s secret-sharing scheme and allows a NIDS to function normally in a VPN without any modifications and without compromising the confidentiality afforded by the VPN. © 2010 Inderscience Enterprises Ltd."
"10.1504/IJBIDM.2009.029083","https://www.scopus.com/inward/record.uri?eid=2-s2.0-71049174872&doi=10.1504%2fIJBIDM.2009.029083&partnerID=40&md5=c1896eb29624e34fd6af27d7bd922470","In large companies, Online Analytical Processing (OLAP) technologies are widely used by business analysts as a decision-support tool. The exploration of the data is performed using operators such as drill-down, roll-up or slice. While exploring the cube, end-users are rapidly confronted with analysing a huge number of drill-paths according to the different dimensions. Generally, analysts are only interested in a small part of them, which corresponds to either high statistical associations between dimensions or atypical cell values. Moreover, identifying the most interesting cells is a matter for business analysts. Coupling OLAP technologies and mining methods may help them by the automation of this tedious task. This paper, in the scope of discovery-driven exploration, presents a method to facilitate the whole process of exploration of the data cube by identifying the most relevant dimensions to expand. A built-in rank on dimensions is displayed, at each step of the process, to the users, who are still free to choose the right dimension to expand for their analysis. Built-in rank on dimensions is performed through indicators computed on the fly according to the user-defined data selection. We present how this methodology offers a support to the decision-making, directly integrated to a commercial OLAP management system. A proof of concept implementation on the ORACLE 10g system is described at the end of the paper. Copyright © 2009, Inderscience Publishers."
"10.1504/IJBIDM.2009.029085","https://www.scopus.com/inward/record.uri?eid=2-s2.0-71049172047&doi=10.1504%2fIJBIDM.2009.029085&partnerID=40&md5=e53540c387ed0ca58f906588135dc7eb","Relief algorithms are successful attribute estimators. They are able to detect conditional dependencies between attributes and provide a unified view on the attribute estimation. In this paper, we propose a variant of ReliefF algorithm: ReliefMSS. We analyse the ReliefMSS parameters and compare ReliefF and ReliefMSS performances as regards the number of iterations, the number of random attributes, the noise effect, the number of nearest neighbours and the number of examples presented. We find that for the most of these parameters, ReliefMSS is better than ReliefF. Copyright © 2009, Inderscience Publishers."
"10.1504/IJBIDM.2009.029076","https://www.scopus.com/inward/record.uri?eid=2-s2.0-71049165094&doi=10.1504%2fIJBIDM.2009.029076&partnerID=40&md5=dd14a7c7d6f7393c698d0a413e38c8f7","XML data sources are gaining popularity in the context of Business Intelligence and On-Line Analytical Processing (OLAP) applications, due to the amenities of XML in representing and managing complex and heterogeneous data. However, XML-native database systems currently suffer from limited performance, both in terms of volumes of manageable data and query response time. Therefore, recent research efforts are focusing on horizontal fragmentation techniques, which are able to overcome the above limitations. However, classical fragmentation algorithms are not suitable to control the number of originated fragments, which instead plays a critical role in data warehouses. In this paper, we propose the use of the K-means clustering algorithm for effectively and efficiently supporting the fragmentation of very large XML data warehouses. We complement our analytical contribution with a comprehensive experimental assessment where we compare the efficiency of our proposal against existing fragmentation algorithms. Copyright © 2009, Inderscience Publishers."
"10.1504/IJBIDM.2009.029074","https://www.scopus.com/inward/record.uri?eid=2-s2.0-71049152823&doi=10.1504%2fIJBIDM.2009.029074&partnerID=40&md5=b207b0ce40ebbcd09f4a1f3a4852c6e3","Online Analytical Processing (OLAP) mining provides useful knowledge to decision makers from multidimensional data stored in data warehouses. However, it is still difficult to find data mining tools taking all the data specificities (e.g., multidimensionality, hierarchies, time) into account. In this paper, we propose an original method to discover multidimensional sequential patterns among several levels of hierarchies. We define two types of multidimensional sequences: convergent sequences, where the elements become more and more precise, and divergent sequences where the elements become more and more general. A pattern-growth based algorithm is proposed and is shown to be efficient in our experiments. Copyright © 2009, Inderscience Publishers."
"10.1504/IJBIDM.2009.029073","https://www.scopus.com/inward/record.uri?eid=2-s2.0-71049143946&doi=10.1504%2fIJBIDM.2009.029073&partnerID=40&md5=b96cbfaa5a2db4ce2fd7cd66bb7bf709","In this paper, we present a new OLAP Mining method for exploring interesting trend patterns. Our main goal is to mine the most (TOP-K) significant changes in Multidimensional Spaces (MDS) applying a gradient-based cubing strategy. The challenge is then finding maximum gradient regions, which maximises the task of detecting TOP-K gradient cells. Several heuristics are also introduced to prune MDS efficiently. In this paper, we motivate the importance of the proposed model, and present an efficient and effective method to compute it by: • evaluating significant changes by means of pushing gradient search into the partitioning process • measuring Gradient Regions (GR) spreadness for data cubing • measuring Periodicity Awareness (PA) of a change, assuring that it is a change pattern and not only an isolated event • devising a Rank Gradient-based Cubing to mine significant change patterns in MDS. Copyright © 2009, Inderscience Publishers."
"10.1504/IJBIDM.2009.029084","https://www.scopus.com/inward/record.uri?eid=2-s2.0-71049143945&doi=10.1504%2fIJBIDM.2009.029084&partnerID=40&md5=9ce4fee2ed6a46e87bfad87490b2391f","Online Analytical Processing (OLAP) systems allow one to quickly provide answers to analytical queries on multidimensional data. These systems are typically based on visualisations characterised by limited interaction and exploration capabilities. In this paper, we propose innovative visual and interaction techniques for the analysis of multidimensional data. The proposed solution is based on three-dimensional hypercube representations that can be explored using dynamic queries and that combines colour-coding, detail-on-demand, cutting planes and viewpoint control techniques. We demonstrate the effectiveness of our visualisation tool by providing some practical examples showing how it has been used for extracting information from real multidimensional data. Copyright © 2009, Inderscience Publishers."
"10.1504/IJBIDM.2009.029075","https://www.scopus.com/inward/record.uri?eid=2-s2.0-71049143513&doi=10.1504%2fIJBIDM.2009.029075&partnerID=40&md5=7a103d205cced0b79d0f9957b6d201be","In this paper, we investigate reduced representations for the Emerging Cube. We use the borders, classical in data mining, for the Emerging Cube. These borders can support classification tasks to know whether a trend is emerging or not. However, the borders do not make possible to retrieve the measure values. This is why we introduce two new and reduced representations without measure loss: the L-Emerging Closed Cube and Emerging Quotient Cube. We state the relationship between the introduced representations. Experiments performed on various data sets are intended to measure the size of the three reduced representations. Copyright © 2009, Inderscience Publishers."
"10.1504/IJBIDM.2009.029086","https://www.scopus.com/inward/record.uri?eid=2-s2.0-71049138080&doi=10.1504%2fIJBIDM.2009.029086&partnerID=40&md5=de430f8f4a9dee7bcebb31f907caf98f","We study the following warehouse view-selection problem: Given a data-warehouse schema, a set of parameterised aggregate queries and their frequency distribution, and a storage-limit constraint, find a set of aggregate views that, when materialised in the warehouse, would minimise the evaluation costs of the frequent queries, subject to a storage-limit constraint. Optimising the layout of stored data using view selection has a direct impact on the performance of data warehouses. We introduce an integer-programming model to obtain optimal solutions for the warehouse view-selection problem, and propose a heuristic to obtain competitive inexact solutions where our exact method is not applicable. Copyright © 2009, Inderscience Publishers."
"10.2481/dsj.008-004","https://www.scopus.com/inward/record.uri?eid=2-s2.0-76449122135&doi=10.2481%2fdsj.008-004&partnerID=40&md5=a8815bb866cebb74502766e99d534e8e","Based on an explorative analysis of the definitions of a data scientist and a community of data scientists as well, this paper points out that with the development of data science and data scientists, the promotion by CODATA and interested partners, the support and efforts of more and more countries and regions, and the drive of social need, building a community of data scientists is full of potentialities and hopes though faced with obstacles and challenges."
"10.2481/dsj.008-010","https://www.scopus.com/inward/record.uri?eid=2-s2.0-76449109993&doi=10.2481%2fdsj.008-010&partnerID=40&md5=fb2ac31c6b30cb96af3d6c10713e18a5","Bayes estimates of the unknown parameter and the reliability function for the generalized exponential model are derived. Bayes estimates are obtained under various losses such as the squared error, the absolute error, the squared log error, and the entropy loss functions. Monte Carlo simulations are presented to compare the Bayes estimates and the maximum likelihood estimates of the unknown parameter and the reliability function."
"10.2481/dsj.008-011","https://www.scopus.com/inward/record.uri?eid=2-s2.0-76449106763&doi=10.2481%2fdsj.008-011&partnerID=40&md5=3ba806b89e22cfebea371aa76f0372db","This paper presents a query translation mechanism between heterogeneous peers in Peer to Peer Database Sharing Systems (PDSSs). A PDSS combines a database management system with P2P functionalities. The local databases on peers are called peer databases. In a PDSS, each peer chooses its own data model and schema and maintains data independently without any global coordinator. One of the problems in such a system is translating queries between peers, taking into account both the schema and data heterogeneity. Query translation is the problem of rewriting a query posed in terms of one peer schema to a query in terms of another peer schema. This paper proposes a query translation mechanism between peers where peers are acquainted in data sharing systems through data-level mappings for sharing data."
"10.2481/dsj.007-038","https://www.scopus.com/inward/record.uri?eid=2-s2.0-76449105831&doi=10.2481%2fdsj.007-038&partnerID=40&md5=2fa64d32b4375f4f4386d1828951efec","In this paper, we present an information retrieval system based on the concept of fuzzy logic to relate vague and uncertain objects with un-sharp boundaries. The simple but comprehensive user interface of the system permits the entering of uncertain specifications in query forms. The system was modelled and simulated in a Matlab environment; its implementation was carried out using Borland C++ Builder. The result of the performance measure of the system using precision and recall rates is encouraging. Similarly, the smaller amount of more precise information retrieved by the system will positively impact the response time perceived by the users."
"10.2481/dsj.008-008","https://www.scopus.com/inward/record.uri?eid=2-s2.0-76449104607&doi=10.2481%2fdsj.008-008&partnerID=40&md5=96c13642729afb8dcdde74490cc89e96","The past two decades have seen the development of effective ways to acquire, store, and exchange data. This technology requires adoption of community-developed standards for data storage and description to form a basis for sharing data, information, and services. For the 50th anniversary of IGY, scientific societies have promoted the establishment of a system of Virtual Observatories. The Electronic Geophysical Year (eGY) concept embraced all available and upcoming geophysical data and helped organize them into a series of virtual geophysical observatories ""deployed"" in cyberspace. We describe the essential features of eGY and how it fits into a 21st century vision of informatics."
"10.2481/dsj.SS_IGY-005","https://www.scopus.com/inward/record.uri?eid=2-s2.0-76549119729&doi=10.2481%2fdsj.SS_IGY-005&partnerID=40&md5=66981c8c1c6e7d718915e03498ff7b51","The Yohkoh Legacy Data Archive (YLA) is one of the first group of Resident Archives (RAs) selected for funding for NASA's Virtual Observatories for the Heliophysics Data program. YLA provides the best corrected data set of solar X-ray images and spectra from the Yohkoh satellite with a user-friendly web interface. As a RA, we take responsibility to keep our products well maintained and easily accessible. In addition, we have launched the 'E-consultant service', an e-mail based support to individual users regarding data handling to bolster access and use from a wide range of communities."
"10.2481/dsj.008-007","https://www.scopus.com/inward/record.uri?eid=2-s2.0-76449121383&doi=10.2481%2fdsj.008-007&partnerID=40&md5=28f34cd38d47f625925f958fe8d2bc40","The representation of information and its exchange in a communication requires the use of a common information model to define the semantics and syntax of the representation and a common dictionary to define the meaning of the data items. These fundamental concepts are the basis of the new standard ISO 10303-235: 'Engineering properties for product design and verification' for the computer representation and exchange of material and any other engineering properties of a product and to provide an audit trail for the derivation of the property value. A related dictionary conforming to ISO 13584 can define testing methods and their properties and enable the information model to be used for any property of any product."
"10.2481/dsj.BR-04","https://www.scopus.com/inward/record.uri?eid=2-s2.0-76449119992&doi=10.2481%2fdsj.BR-04&partnerID=40&md5=36486901999faebe5cdcbbb5fb8e9b76","The aim of this paper is to analyze the current evidence on radiocerebral effects following exposure to <5 Sv. Dose-related cognitive and neurophysiological abnormalities among prenatally exposed children after the Chernobyl accident at gestation ages of +8 weeks were revealed at >20 mSv on the fetus and >300 mSv on the thyroid in utero; at 16-25 weeks, abnormalities were >10 mSv and >200 mSv, respectively. In adults, radiationassociated cerebrovascular effects were obtained at >0.15-0.25 Sv. Dose-related neuropsychiatric, neurophysiological, neuropsychological, and neuroimaging abnormalities following exposure to >0.3 Sv and neurophysiological and neuroimaging radiation markers at doses >1 Sv were revealed. Studies on radiation neuropsychiatric effects should be undertaken."
"10.2481/dsj.IGY-016","https://www.scopus.com/inward/record.uri?eid=2-s2.0-76449117319&doi=10.2481%2fdsj.IGY-016&partnerID=40&md5=2d8eba1951ffaaee4e98a77618386197","A novel hypothesis is proposed that assumes piezoelectricity of the inner core as the origin of geomagnetism. By high pressure, an electric charge is created on the surface and at the center of the earth. Inner core rotation yields a magnetic field. From the intensity and direction of geomagnetism at the present time, the surface charge density of the inner core is assumed to be -2x105C/m2. The rotation axis of the inner core is inclined by 10.4 degrees from that of the mantle. The inner core rotates with the mantle rotation. The reason for this is thought to be the eddy currents induced in the outer core of electrically conductive fluid that rotates with the mantle."
"10.2481/dsj.BR-05","https://www.scopus.com/inward/record.uri?eid=2-s2.0-76449116892&doi=10.2481%2fdsj.BR-05&partnerID=40&md5=013bb8599e20eada786d77fc4a9309b3","To understand the effects of low dose-rate radiation on genome structure in vivo, we examined gene mutation and chromosomal abnormality in mouse tissues. The mutation was studied on transgenes in the spleen and liver and also on the Dlb1 gene in intestinal stem cells. The mice were irradiated for 483 consecutive days at three dose-rates of 0.0323, 0.65, and 12.5 μGy/min, which resulted in total doses of 21, 414, and 8,000 mGy, respectively. Statistically significant increases were observed for all indices examined when the total dose was 8,000 mGy, whereas no significant difference was observed with 21 mGy or 414 mGy."
"10.2481/dsj.007-072","https://www.scopus.com/inward/record.uri?eid=2-s2.0-76449116241&doi=10.2481%2fdsj.007-072&partnerID=40&md5=9bec07bce3baeb9aa699d15904bcb38a","The Digitized First Byurakan Survey (DFBS) is the digitized version of the famous Markarian Survey. It is the largest low-dispersion spectroscopic survey of the sky, covering 17,000 square degrees at galactic latitudes |b|>15. DFBS provides images and extracted spectra for all objects present in the FBS plates. Programs were developed to compute astrometric solution, extract spectra, and apply wavelength and photometric calibration for objects. A DFBS database and catalog has been assembled containing data for nearly 20,000,000 objects. A classification scheme for the DFBS spectra is being developed. The Armenian Virtual Observatory is based on the DFBS database and other large-area surveys and catalogue data."
"10.2481/dsj.BR-03","https://www.scopus.com/inward/record.uri?eid=2-s2.0-76449115636&doi=10.2481%2fdsj.BR-03&partnerID=40&md5=d3ecdb97294dcf1fb892327d2f6f4648","The study goal was to investigate thyroid cancer morbidity in population groups affected by the Chernobyl catastrophe. The study period comprised 1994-2006 for clean-up workers and 1990-2006 for Chernobyl evacuees and residents of contaminated territories. A significant increase of thyroid cancer incidence was registered in all observed population groups. The most significant excess over the national level was identified in clean-up workers. This amounted to a factor of 5.9, while it was 5.5 for the evacuees and 1.7 for the residents. The highest thyroid cancer risk was observed in persons exposed to radioiodine in childhood and adolescence."
"10.2481/dsj.BR-06","https://www.scopus.com/inward/record.uri?eid=2-s2.0-76449098434&doi=10.2481%2fdsj.BR-06&partnerID=40&md5=86478a5bf9a402c585c6d388dab7044d","Tumor suppressor protein p53 is a transcription factor that plays important roles in biological responses to awide range of ionizing radiation doses. Elucidation of the molecular mechanisms fo r its function after actually relevant doses of radiation is therefore crucial for the accurate understanding of the biological effects of radiation. p21 is one of the p53 target genes, functioning in cell cycle checkpoint regulation. In this article, comprehensive analysis of the p21 gene promoter after clinically relevant doses of radiation is reviewed, especially emphasizing that a transcription factor Oct-1 plays a cooperative role in p53-mediated regulation of the p21 gene after irradiation. In addition, a potential application of the p21 gene promoter in the development of a low-dose radiation inducible vector for cancer gene therapy is also pointed out."
"10.2481/dsj.BR-07","https://www.scopus.com/inward/record.uri?eid=2-s2.0-76449091625&doi=10.2481%2fdsj.BR-07&partnerID=40&md5=e46f40c7233a1e08fd3e4bd96063fa5b","The effects of low dose chronic radiation on plant disease resistance and fungal and virus infections have been studied. The results obtained in the 10-km Chernobyl zone demonstrated a decrease in plant disease resistance and appearance of a ""new"" population of stem rust agents of cereal with a high frequency of more virulent clones. Radionuclide contamination and heavy metals lead to wider virus spread and a higher diversity of virus species. The Chernobyl zone is a territory of enhanced risk and potential threats for the environment. A special type of monitoring of microevolution processes in plant pathogens should provide better understanding of how serious these potential threats are."
"10.2481/dsj.007-071","https://www.scopus.com/inward/record.uri?eid=2-s2.0-76449089536&doi=10.2481%2fdsj.007-071&partnerID=40&md5=234c93bc8404063bf6125d744367ec34","Preserving digital objects requires preservation of not only their bit-level representation but also their intelligibility. To this end a digital object should be associated with metadata appropriate for interpreting that object; such metadata are often referred as representation information. Even such metadata may not be intelligible, however, so we may have to associate them with extra metadata, and so on. This paper approaches this problem by introducing an abstract model comprising modules and dependencies. Community knowledge is formalized over the same model by introducing the notion of profile. This notion is then exploited for deciding representation information adequacy (during input) and intelligibility (during output). Subsequently some general dependency management services for identifying and filling intelligibility gaps during input and output are described and analysed. Finally a prototype system based on these ideas is described."
"10.2481/dsj.008-018","https://www.scopus.com/inward/record.uri?eid=2-s2.0-76449083913&doi=10.2481%2fdsj.008-018&partnerID=40&md5=3bcbe378bffdac20d7809699b7ed041d","This paper suggests a ratio-cum-product estimator of finite population mean using a correlation coefficient between study variate and auxiliary variate in stratified random sampling. Bias and mean squared expressions of the suggested estimator are derived and compared with combined ratio estimator and several other estimators considered by Kadilar and Cingi (2003). An empirical study is also carried out to examine the performance of the proposed estimator."
"10.1504/IJBIDM.2009.026906","https://www.scopus.com/inward/record.uri?eid=2-s2.0-67650684970&doi=10.1504%2fIJBIDM.2009.026906&partnerID=40&md5=2717c02eab40980e7e0c25e2d340766a","This paper discusses the convenience of using data analysis techniques to monitor e-learning processes. It also proposes a model to monitor online students' academic activity and performance. Using data from log files and databases, this model determines which information has to be provided to online instructors and students. These reports allow instructors to: classify students according to their activity and learning outcomes, track their evolution, and identify those who might need immediate assistance. They also provide students with a periodical feedback which makes them aware of how they are performing as compared with the rest of the class. Copyright © 2009, Inderscience Publishers."
"10.1504/IJBIDM.2009.026907","https://www.scopus.com/inward/record.uri?eid=2-s2.0-67650684968&doi=10.1504%2fIJBIDM.2009.026907&partnerID=40&md5=9480dc77b67c48284702adf0c4e21a96","This paper reports on an experience of using an innovative CSCL tool to support real, collaborative learning by discussion. We base the success of online collaborative discussion on two crucial aspects: first, by providing an adequate dialogue structure and move types that promote meaningful contributions and achieve more effective interaction; second, by extracting relevant knowledge in order to provide learners and tutors with efficient awareness, feedback, and monitoring as regards learners' performance and collaboration. The ultimate goal is to achieve a more effective support and assessment of the discussion process. A real virtual learning environment based on asynchronous discussion is employed to validate the approach. Copyright © 2009, Inderscience Publishers."
"10.1504/IJBIDM.2009.026904","https://www.scopus.com/inward/record.uri?eid=2-s2.0-67650676773&doi=10.1504%2fIJBIDM.2009.026904&partnerID=40&md5=bf65bca5625407d6d6bc5b32c2052118","In the age of mobile computing where users can access learning materials while moving around different locations, a teacher, a student or a document might often face difficulties in being integrated within different e-learning systems. Focusing on the Computer Science field, we present a service-oriented solution for providing users with recommendations of interest documents and possible collaborators, enhancing their mobility within a pervasive e-learning system. Our solution is based on the ACM classification system, which is used to develop user competence profiles, to annotate materials, and to recommend them. Copyright © 2009, Inderscience Publishers."
"10.1504/IJBIDM.2009.026908","https://www.scopus.com/inward/record.uri?eid=2-s2.0-67650667439&doi=10.1504%2fIJBIDM.2009.026908&partnerID=40&md5=1c1d54c83daed16d73109a8fd3004ec0","The main research question is whether output of virtual teams is affected by the teams' structure. A model, suggesting that the team structure can be manipulated in order to increase the team's output will be presented. The study used an experiment, in which subjects, who were grouped into teams, had to share information in order to complete an intellective task. The findings indicated that virtual teams output were inferior to face-to-face teams output, and that structure did overcome two negative impacts of the virtuality - less effectiveness and less satisfaction; yet it did not raise the virtual team efficiency. Copyright © 2009, Inderscience Publishers."
"10.1504/IJBIDM.2009.026905","https://www.scopus.com/inward/record.uri?eid=2-s2.0-67650663174&doi=10.1504%2fIJBIDM.2009.026905&partnerID=40&md5=4e34a5951b8f3fe91dd5b92bdda51f24","This paper describes an approach for extracting and representing the knowledge generated from collaborative interaction of small learning teams that work together in distance to carry out a software project or a case study. Our approach is based on ontology, Social Network Analysis (SNA) and fuzzy classification, which has been initially developed as a mechanism to analyse and understand behavioural patterns from collaboration performed in different scenarios. Through the SNA we are able to define an ontological profile that provides a deep knowledge about the participants' roles, intentions and effects; hence a fuzzy model can perform inferences over individual indicators. Copyright © 2009, Inderscience Publishers."
"10.1504/IJBIDM.2009.025412","https://www.scopus.com/inward/record.uri?eid=2-s2.0-66349124662&doi=10.1504%2fIJBIDM.2009.025412&partnerID=40&md5=4685cf179423eb1104da809758612812","It is necessary to use expert knowledge of networks and computers to reduce the burden on users when providing high-quality communication services to non-expert users in ubiquitous information environments. We have been working on the systematic support of circulation of expert knowledge to meet that need. In this paper, we propose a new Knowledge Circulation Framework (KCF) for ubiquitous service provision: uKCF. We applied uKCF to service control for a ubiquitous videoconferencing system. The experimental results show that the proposed system can provide better and more stable service by improving adaptability to various ubiquitous computing environments. Copyright © 2009, Inderscience Publishers."
"10.1504/IJBIDM.2009.025410","https://www.scopus.com/inward/record.uri?eid=2-s2.0-66349123227&doi=10.1504%2fIJBIDM.2009.025410&partnerID=40&md5=237b89e2121989cf708077c355f49d8b","We present an approach to construct a networked online education system among physically separated participants targeted at teaching handwritten characters. The proposed system provides a haptic channel to intuitively learn remote instructor's fine motor skills through the sense of touch. The instructor's handwriting motions are sent to the learner's system and replayed via the learner's haptic device. The learner can intuitively acquire the instructor's writing skill through the haptic interactions. Additionally, the instructor can check the learner's writing exercise and give him or her some advices to improve the writing ability. We also describe experiments to quantitatively analyse how the haptic interactions improve the learning quality. Copyright © 2009, Inderscience Publishers."
"10.1504/IJBIDM.2009.025413","https://www.scopus.com/inward/record.uri?eid=2-s2.0-66349121611&doi=10.1504%2fIJBIDM.2009.025413&partnerID=40&md5=771c5a4e5f311cce492fa1b463189746","Multiple agents cooperate with each other through manipulating objects. A transaction is a unit of work issued by an agent. A transaction is assigned with a purpose which is a subfamily of roles granted to the agent. Even if transactions issue methods according to the purposes, illegal information flow might occur. We define legal, independent, illegal, and possibly illegal information flow relations among purposes. We discussed the purpose-based marking protocol to prevent illegal information flow. Then, we discussed the releasing mechanism of purpose marks to improve the throughput. Finally, we evaluated the Purpose-based Marking and Releasing (PMR) protocol. Copyright © 2009, Inderscience Publishers."
"10.1504/IJBIDM.2009.025409","https://www.scopus.com/inward/record.uri?eid=2-s2.0-66349113997&doi=10.1504%2fIJBIDM.2009.025409&partnerID=40&md5=d239f6509b695c0e1673deaf7e6d0a9b","This paper proposes a method that acquires a more appropriate classification model for a risk search system analysing corporate reputation information included in bulletin board sites. The method inductively acquires the model from textual examples composed of many negative examples and a few positive examples. It selects two kinds of important negative examples by referring to expressions related to a specific label. Here, the label represents the contents of the papers. Finally, the method uses the selected negative examples and all the positive examples to acquire the model. The paper verifies the effectiveness of the method through comparative experiments. Copyright © 2009, Inderscience Publishers."
"10.1504/IJBIDM.2009.025411","https://www.scopus.com/inward/record.uri?eid=2-s2.0-66349095657&doi=10.1504%2fIJBIDM.2009.025411&partnerID=40&md5=772db281f541991cffb201cd4da997a1","This paper presents a method for discriminating between personal and non-personal web pages. The method can support surveys of personal opinions about products and services. In the proposed method, subjective expressions are extracted from pages and then the pages are scored by quantitatively evaluating the subjectivity in the pages. We have evaluated performances of the proposed method using 1200 web pages collected from four categories of product, tourist spot, restaurant, and movie. Comparing the performances of the proposed method with categorisations by a general search engine, we have confirmed that the performances have been significantly better in every category. Copyright © 2009, Inderscience Publishers."
"10.1504/IJBIDM.2009.025408","https://www.scopus.com/inward/record.uri?eid=2-s2.0-66349093331&doi=10.1504%2fIJBIDM.2009.025408&partnerID=40&md5=8a13fda36e9bf3917dac152021b9d6f8","Telecom service providers are faced with an overwhelming flow of alarms, which makes good alarm classification and prioritisation very important. This paper first provides statistical analysis of data collected from a real-world alarm flow and then presents a quantitative characterisation of the alarm situation. Using data from the trouble ticketing system as a reference, we examine the relationship between mechanical classification of alarms and the human perception of them. Using this knowledge of alarm flow properties and trouble ticketing information, we suggest a neural network-based approach for alarm classification. Tests using live data show that our prototype assigns the same severity as a human expert in 50% of all cases, compared to 17% for a naïve approach. Copyright © 2009, Inderscience Publishers."
"10.2481/dsj.007-044","https://www.scopus.com/inward/record.uri?eid=2-s2.0-76649130950&doi=10.2481%2fdsj.007-044&partnerID=40&md5=83b36b45cbffa3fba32062d2c0bedea9","Attribute reduction aims to reduce the dimensionality of large scale data without losing useful information and is an important topic of knowledge discovery, data clustering, and classification. In this paper, we aim to solve the current problem that a continuous attribute in a clustering or classification algorithm must be made discrete. We propose a new algorithm of data reduction based on a correlation model with data discretization. It deals with selection of continuous attributes from a very large set of attributes. The proposed algorithm is an extended version of the Fast Correlation-based filter algorithm and is named FCBF+. The FCBF+ algorithm performs the discretization of continuous attributes in an efficient manner. Then it selects the relevant attributes from a very large set of attributes. Performance evaluation is done on clustering accuracy for all the features, and a reduced set of features is obtained using FCBF+. It is found that the proposed FCBF+ algorithm improves the clustering accuracy of various clustering algorithms."
"10.2481/dsj.007-006","https://www.scopus.com/inward/record.uri?eid=2-s2.0-76449118312&doi=10.2481%2fdsj.007-006&partnerID=40&md5=801e5605c85ee354fc93de73cd19361c","The prediction of product acceptability is often an additive effect of individual fuzzy impressions developed by consumer on certain underlying attributes characteristic of the product. In this paper we present the development of a data-driven fuzzy-rule-based approach for predicting the overall sensory acceptability of food products in this case composite cassava-wheat bread. The model was formulated using the Takagi-Sugeno and Kang (TSK) fuzzy modeling approach. Experiments with the model derived from sampled data were simulated on Windows 2000XP running on Intel 2Gh environment. The fuzzy membership function for the sensory scores is implemented in MATLAB 6.0 using the fuzzy logic toolkit and weights of each linguistic attribute were obtained using a Correlation Coefficient formula. The results obtained are compared to those of human judgments. Overall assessments suggest that if implemented this approach will facilitate a better acceptability of composite bread."
"10.2481/dsj.007-069","https://www.scopus.com/inward/record.uri?eid=2-s2.0-76449111991&doi=10.2481%2fdsj.007-069&partnerID=40&md5=3274b900bd94f54ddc8324125806afd7","Retrieval systems for 3D objects are required because 3D databases used around the web are growing. In this paper, we propose a visual similarity based search engine for 3D objects. The system is based on a new representation of 3D objects given by a 3D closed curve that captures all information about the surface of the 3D object. We propose a new 3D descriptor, which is a combination of three signatures of this new representation, and we implement it in our interactive web based search engine. Our method is compared to some state of the art methods, tested using the Princeton-Shape Benchmark as a large database of 3D objects. The experimental results show that the enhanced curve analysis descriptor performs well."
"10.2481/dsj.007-031","https://www.scopus.com/inward/record.uri?eid=2-s2.0-76449107156&doi=10.2481%2fdsj.007-031&partnerID=40&md5=f01da6eaab3081b8696d2865629c8e4c","In a grid and distributed computing environment, data replication is an effective way to improve data accessibility and data accessing efficiency. It is also significant in developing a real-time service monitoring system for a Chinese Scientific Data Grid to guarantee the system stability and data availability. Hierarchical data replication and service monitoring methods are proposed in this paper. The hierarchical data replication method divides the network into different domains and eplicates data in local domains. The nodes in a local domain are lassified into hierarchies to improve data accessibility according to bandwidth and storage memory space. An extensible agentbased prototype of a hierarchical service monitoring system is presented. The status information of services in the Chinese Scientific Data Grid is collected from the grid nodes based on agent technology and then is transformed into real-time operational pictres for management needs. This paper presents frameworks of the hierarchical data replication and service monitoring methods and gives detailed resolutions. Simulation analyses have demonstrated improved data accessing efficiency and verified the effectiveness of the methods at the same time."
"10.2481/dsj.007-051","https://www.scopus.com/inward/record.uri?eid=2-s2.0-76449105603&doi=10.2481%2fdsj.007-051&partnerID=40&md5=99f6fcecd539b1b3f282f8975a8e3775","Effective interatomic potentials are frequently utilized for large-scale simulations of materials. In this work, we generate an effective interatomic potential, with Niobium as an example, using the force-matching method derived from a material database which is created by the first-principle molecular dynamics. It is found that the potentials constructed in the present work are more transferable than other existing potential models. We further discuss how the first-principles material database should be organized for generation of additional potential."
"10.2481/dsj.007-042","https://www.scopus.com/inward/record.uri?eid=2-s2.0-76449095039&doi=10.2481%2fdsj.007-042&partnerID=40&md5=ba219f72309c97f1b3eb4a22dea45b4b","The explosion of non-structured materials science data makes it urgent for materials researchers to resolve the problem of how to effectively share this information. Materials science image data is an important class of non-structured data. This paper proposes a semantic annotation method to resolve the problem of materials science image data sharing. This method is implemented by a four-layer architecture, which includes ontology building, semantic annotation, reasoning service, and application. We take metallographic image data as an example and build a metallographic image OWL-ontology. Users can accomplish semantic annotation of metallographic image according to the ontology. Reasoning service is provided in a data sharing application to demonstrate the effective sharing of materials science image data through adding semantic annotation."
"10.2481/dsj.007-041","https://www.scopus.com/inward/record.uri?eid=2-s2.0-76449088967&doi=10.2481%2fdsj.007-041&partnerID=40&md5=5c765f61d326be804981d8f583be8928","Finding out how many parameters are necessary to explain and describe complex and various phenomena of nature has been a challenge in modern physics. This paper introduces a new formal system of units, which maintain compatibility with SI units, to express all seven SI base units by dimensionless numbers with acceptable uncertainties and to establish the number one as the fundamental parameter of everything. All seven SI base units are converted successfully into the unified dimensionless numerical values via normalization of s, c, h, k, e/me, NA, and b by unity (1). In the proposed system of units, even the unlike-dimensioned physical quantities can be convertible and hence added, subtracted, or compared to one another. It is very simple and easy to analyze and validate physical equations by substituting every unit with the corresponding number. Furthermore, it is expected to find new relationships among unlike-dimensioned physical quantities, which is extremely difficult or even impossible in SI units."
"10.2481/dsj.007-055","https://www.scopus.com/inward/record.uri?eid=2-s2.0-76449083713&doi=10.2481%2fdsj.007-055&partnerID=40&md5=f8f270ffe4d714a5db1e8bbe46f6e084","This paper reports the recent progress on mud volcanism data accumulation in the case of the Gulf of Cadiz area. The discovery of giant mud volcanoes, deep coral reefs, and gas hydrates in 1999, from the Guadalquivir Diapiric Ridge to the Larache Moroccan margin, launched a dynamic expansion of new projects (GeNesis, MoundForce, HERMES) and international oceanographic campaigns (R/V Sonne, Marion-Dufresne). The present monitoring of this Ibero-Moroccan oceanic zone is in need of a comprehensive database available in one site to make online search possible from a single interface. The database would constitute a reference point for a focused full scope collection."
"10.2481/dsj.007-020","https://www.scopus.com/inward/record.uri?eid=2-s2.0-76249111661&doi=10.2481%2fdsj.007-020&partnerID=40&md5=08a3e10706d9f040fcfec9773012acfd","Evaluation of clustering results (or cluster validation) is an important and necessary step in cluster analysis, but it is often time-consuming and complicated work. We present a visual cluster validation tool, the Cluster Validity Analysis Platform (CVAP), to facilitate cluster validation. The CVAP provides necessary methods (e.g., many validity indices, several clustering algorithms and procedures) and an analysis environment for clustering, evaluation of clustering results, estimation of the number of clusters, and performance comparison among different clustering algorithms. It can help users accomplish their clustering tasks faster and easier and help achieve good clustering quality when there is little prior knowledge about the cluster structure of a data set."
"10.2481/dsj.8.S45","https://www.scopus.com/inward/record.uri?eid=2-s2.0-76449122134&doi=10.2481%2fdsj.8.S45&partnerID=40&md5=9eb2e34ed1417488883543f7b68e0c4d","We summarize two projects representative of a developing movement in modern geosciences. By establishing a linkage between the developed coupling simulator and fusion prototype, we successfully incorporate geosciences with informatics, computer technologies and mathematics in the field of data processing. A challenging coupled inversion scheme is shining under such an integration."
"10.2481/dsj.8.S38","https://www.scopus.com/inward/record.uri?eid=2-s2.0-76449114999&doi=10.2481%2fdsj.8.S38&partnerID=40&md5=c4d45a78b3d48f229edc3898534ec622","The CASSIOPE Enhanced Polar Outflow Probe (e-POP) is a Canadian small-satellite mission dedicated to the study of polar ion outflows and related magnetosphere-ionosphere coupling processes in the topside ionosphere. Scheduled for launch in 2009, it will carry eight scientific instruments, including imaging plasma and neutral particle sensors, magnetometers, dual-frequency GPS receivers, CCD cameras, a radio wave receiver and a beacon transmitter, for in-situ particle, field, and current measurements and auroral imaging and radio measurements. We present an overview of the e-POP mission, its current status and plan of observations and data distribution during the operation phase of the mission."
"10.2481/dsj.8.S92","https://www.scopus.com/inward/record.uri?eid=2-s2.0-76449112387&doi=10.2481%2fdsj.8.S92&partnerID=40&md5=ee03ac54cc324417d6f250557f122903","We propose to establish ""data-showcase"" system to display the various geophysical data in one frame. Data-showcase is a system not to provide data but to show various types of the geophysical data in intuitable way. The newly developed Dagik, Daily geospace data in kml, is the first data-showcase system for the geospace data. It contains several types of data by ground-based and satellite measurements in addition to numerical models. We expect Dagik would make the combination and comparison of the geospace data easier, and foster new inter-disciplinary scientific researches."
"10.2481/dsj.8.S59","https://www.scopus.com/inward/record.uri?eid=2-s2.0-76449112185&doi=10.2481%2fdsj.8.S59&partnerID=40&md5=9fa8225794c04e1997a6f3aa5c3cc43c","In climate research, pressure patterns are often very important. When a climatologists need to know the days of a specific pressure pattern, for example ""low pressure in Western areas of Japan and high pressure in Eastern areas of Japan (Japanese winter-type weather),"" they have to visually check a huge number of surface weather charts. To overcome this problem, we propose an automatic classification system using a support vector machine (SVM), which is a machine-learning method. We attempted to classify pressure patterns into two classes: ""winter type"" and ""non-winter type"". For both training datasets and test datasets, we used the JRA-25 dataset from 1981 to 2000. An experimental evaluation showed that our method obtained a greater than 0.8 F-measure. We noted that variations in results were based on differences in training datasets."
"10.2481/dsj.8.S2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-76449110854&doi=10.2481%2fdsj.8.S2&partnerID=40&md5=4e6f48fd40dabd7d6e715302c62037b1","The International Geophysical Year (IGY) of 1957, a broad-based and all-encompassing effort to push the frontiers of geophysics, resulted in a tremendous increase of knowledge in space physics, Sun-Earth Connection, planetary science and the heliosphere in general. Now, 50 years later, we have the unique opportunity advance our knowledge of the global heliosphere and its interaction with planetary bodies and the interstellar medium through the International Heliophysical Year (IHY) in 2007. This was an international effort to coordinate scientific research, and the deployment of scientific instrument arrays, preserve the history of the IGY, and to raise the public awareness of space physics."
"10.2481/dsj.8.S35","https://www.scopus.com/inward/record.uri?eid=2-s2.0-76449109790&doi=10.2481%2fdsj.8.S35&partnerID=40&md5=393b1f3b4cfff6ceaf593500ca897b69","High frequency Doppler observations of the ionosphere began in August of 1957 in Kyoto. The number of the observation points worldwide were about 40 in 1980 and are about 20 at present. By this method the movement of the ionosphere reflection height and electron density below the height can be observed. Such variations are occurred by a wide variety of sources."
"10.2481/dsj.8.S85","https://www.scopus.com/inward/record.uri?eid=2-s2.0-76449109156&doi=10.2481%2fdsj.8.S85&partnerID=40&md5=6171d29decd9f9a98322e6f0dc77b04c","We have developed a new system using Google Earth as a data browser to visualize navigation data obtained by JAMSTEC research vessels. An XML-based language called Keyhole Markup Language (KML) is used to plot data on Google Earth. We developed a program, called a KML generator, to convert navigation data to KML. The generator enables us to quickly visualize on Google Earth the cruise track of any JAMSTEC vessel. The visualized image is a powerful tool for managing information on research cruises of various JAMSTEC vessels."
"10.2481/dsj.8.S68","https://www.scopus.com/inward/record.uri?eid=2-s2.0-76449100909&doi=10.2481%2fdsj.8.S68&partnerID=40&md5=3a9837de50c8562bf702f3aa11291b12","Ultra-low frequency acoustic waves called ""acoustic gravity waves"" or ""infrasounds"" are theoretically expected to resonate between the ground and the thermosphere. This resonance is a very important phenomenon causing the coupling of the solid Earth, neutral atmosphere, and ionospheric plasma. This acoustic resonance, however, has not been confirmed by direct observations. In this study, atmospheric perturbations on the ground and ionospheric disturbances were observed and compared with each other to confirm the existence of resonance. Atmospheric perturbations were observed with a barometer, and ionospheric disturbances were observed using the HF Doppler method. An end point of resonance is in the ionosphere, where conductivity is high and the dynamo effect occurs. Thus, geomagnetic observation is also useful, so the geomagnetic data were compared with other data. Power spectral density was calculated and averaged for each month. Peaks appeared at the theoretically expected resonance frequencies in the pressure and HF Doppler data. The frequencies of the peaks varied with the seasons. This is probably because the vertical temperature profile of the atmosphere varies with the seasons, as does the reflection height of infrasounds. These results indicate that acoustic resonance occurs frequently."
"10.2481/dsj.8.S78","https://www.scopus.com/inward/record.uri?eid=2-s2.0-76449097128&doi=10.2481%2fdsj.8.S78&partnerID=40&md5=28480a7baf11e840f8d7d5d2571f0dbe","Recent advance of information and communications technology enables to collect a large amount of ground-based and space-based observation data in real-time. The real-time data realize nowcast of space weather. This paper reports a history of space weather by the International Space Environment Service (ISES) in association with the International Geophysical Year (IGY) and importance of real-time monitoring in space weather."
"10.2481/dsj.8.S51","https://www.scopus.com/inward/record.uri?eid=2-s2.0-76449095911&doi=10.2481%2fdsj.8.S51&partnerID=40&md5=af5897a6d14be08c8539f656f64250c6","The Flare Monitoring Telescope (FMT) was constructed in 1992 at Hida Observatory in Japan to investigate the long-term variation of solar activity and explosive events. It has five solar imaging telescopes that simultaneously observe the full-disk Sun at different wavelengths around the H-alpha absorption line or in different modes. Therefore, the FMT can measure the three-dimensional velocity field of moving structures on the full solar disk. The science target of the FMT is to monitor solar flares and erupting filaments continuously all over the solar disk and to investigate correlation between the characteristics of the erupting phenomena and the geoeffectiveness of the corresponding coronal mass ejections (CMEs). We are planning to start up a new worldwide project, the Continuous H-alpha Imaging Network (CHAIN) project, as an important IHY project of our observatories. As part of this project, we are examining the possibility of installing telescopes similar to the FMT in developing countries. We have selected Peru and Algeria as the countries where the first and second overseas FMTs will be installed, and we are aiming to start operation of these FMTs by the end of 2010 before the maximum phase of solar cycle 24. To create such an international network, it will be necessary to improve the information technologies applied in our observation-system. In this paper, we explain the current status and future areas of work regarding our system."
"10.2481/dsj.8.S24","https://www.scopus.com/inward/record.uri?eid=2-s2.0-76449090342&doi=10.2481%2fdsj.8.S24&partnerID=40&md5=60a253c637a3ba4e08f9b847fa385da2","Fifty years have passed since the International Geophysical Year (IGY) of 1957-58, one of the most important and noble initiatives in the history of science and in the history of humanity in general. IGY became the model for subsequent international scientific initiatives in various fields of solid Earth research, including the Upper Mantle Project (1961-71), the Geodynamic Project (1971-80), the Geotraverse Project (1987-2003), and the ""InterMARGINS"" Project (2003). The Russian investigations as part of the Geotraverse Project and ""InterMARGINS"" were aimed at research into the deep structure of the continental margins of East Eurasia, which are characterized by high seismicity, volcanism, and natural cataclysms hazardous to people living there."
"10.2481/dsj.8.S14","https://www.scopus.com/inward/record.uri?eid=2-s2.0-76449087729&doi=10.2481%2fdsj.8.S14&partnerID=40&md5=b91d295833b389103855af9339398a82","The IHY Japanese National Steering Committee (STPP subcommittee of the Science Council of Japan) has been promoting and supporting (1) two satellite missions, (2) five ground-based networks, (3) public outreach, (4) international and domestic workshops, and (5) the nomination of IGY Gold Club members. In the present paper we introduce these IHY activities, briefly summarize them, and suggest several post-IHY activities."
"10.2481/dsj.8.34","https://www.scopus.com/inward/record.uri?eid=2-s2.0-76449117936&doi=10.2481%2fdsj.8.34&partnerID=40&md5=96fa6d19f715e412557ee17f387b4d4f","We describe the current status of CATS (astrophysical CATalogs Support system), a publicly accessible tool maintained at Special Astrophysical Observatory of the Russian Academy of Sciences (SAO RAS) (http://cats.sao.ru) allowing one to search hundreds of catalogs of astronomical objects discovered all along the electromagnetic spectrum. Our emphasis is mainly on catalogs of radio continuum sources observed from 10 MHz to 245 GHz, and secondly on catalogs of objects such as radio and active stars, X-ray binaries, planetary nebulae, HII regions, supernova remnants, pulsars, nearby and radio galaxies, AGN and quasars. CATS also includes the catalogs from the largest extragalactic surveys with non-radio waves. In 2008 CATS comprised a total of about 10 9 records from over 400 catalogs in the radio, IR, optical and X-ray windows, including most source catalogs deriving from observations with the Russian radio telescope RATAN-600. CATS offers several search tools through different ways of access, e.g. via Web-interface and e-mail. Since its creation in 1997 CATS has managed about 105 requests. Currently CATS is used by external users about 1500 times per day and since its opening to the public in 1997 has received about 4000 requests for its selection and matching tasks."
"10.2481/dsj.8.41","https://www.scopus.com/inward/record.uri?eid=2-s2.0-76449105602&doi=10.2481%2fdsj.8.41&partnerID=40&md5=21f01c87c27b95826f848e689b9e6065","Independent of established data centers, and partly for my own research, since 1989 I have been collecting the tabular data from over 2600 articles concerned with radio sources and extragalactic objects in general. Optical character recognition (OCR) was used to recover tables from 740 papers. Tables from only 41 percent of the 2600 articles are available in the CDS or CATS catalog collections, and only slightly better coverage is estimated for the NED database. This fraction is not better for articles published electronically since 2001. Both object databases (NED, SIMBAD, LEDA) as well as catalog browsers (VizieR, CATS) need to be consulted to obtain the most complete information on astronomical objects. More human resources at the data centers and better collaboration between authors, referees, editors, publishers, and data centers are required to improve data coverage and accessibility. The current efforts within the Virtual Observatory (VO) project, to provide retrieval and analysis tools for different types of published and archival data stored at various sites, should be balanced by an equal effort to recover and include large amounts of published data not currently available in this way."
"10.2481/dsj.8.1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-76449095237&doi=10.2481%2fdsj.8.1&partnerID=40&md5=3fd5eb5dba64a98d0e8bddcf8a08ac4c","MatML plays an important role in materials data applications while structure-aware query techniques (e.g., XPath and XQuery) are used to search the content of MatML. However, both XPath and XQuery cannot efficiently retrieve sets of MatML on a conceptual level. In this paper, we propose an approach to transform MatML-based materials data into an OWL ontology. As such, materials data can then be explored in a more semantic way. The proposed method formally defines a set of rules to extract the corresponding OWL ontology (named MatOWL) from a given MatML schema. The instance transformation from MatML to MatOWL is implemented with the help of an intermediate object model. The algorithm for instance transformation is also given. Further, MatOWL can be mapped to other ontologies with logic rules to provide more semantic context for domain experts, and more materials knowledge can be obtained by reasoning on the OWL ontology. An experimental prototype demonstrates the effectiveness of our proposed approach."
"10.2481/dsj.8.27","https://www.scopus.com/inward/record.uri?eid=2-s2.0-76449087536&doi=10.2481%2fdsj.8.27&partnerID=40&md5=9ce43a0c46194ec66355d3d42c7aa86b","In the Canadian research environment, it is difficult for researchers to effectively discover, access, and use data sets, except for those that are the most well known. Several recent reports have discussed the issues around ""lost"" data sets: those which are intended to be shared but cannot be identified and utilized effectively because of insufficient associated metadata. Both problems are approaching critical levels in Canada and internationally, a situation that is unacceptable because these data sets are often generated as a result of public funding. Solutions may involve providing support and training for researchers on how they can best collect and manage their data sets or developing gateways to scientific data sets. NRC-CISTI is the largest comprehensive source of scientific, technical, and medical information in North America, with a mandate to serve as Canada's national science library. Through its publishing arm, NRC Research Press, it is also Canada's foremost scientific publisher. NRC-CISTI is an organization with demonstrated expertise in metadata management, which, until recently, focused primarily on library and publishing contexts. However in November 2007, it formally committed to expand its agenda to address the management of scientific research data and the related critical needs of the research community. This paper presents NRC-CISTI's activities in this area. NRC-CISTI has begun by hosting forums in which the critical players (including the granting agencies) mapped out targets and approaches. It has strengthened its own internal expertise regarding metadata and management of scientific data sets. Finally, NRC-CISTI is developing a gateway Web site which will provide access to Canadian scientific data sets and related metadata, tools, educational resources, and other informative and collaborative tools urgently needed by Canadian and international researchers. NRC-CISTI is the sponsoring body for the Canadian National Committee for CODATA and is committed to promoting and supporting CNC/CODATA's initiatives."
"10.2481/dsj.8.18","https://www.scopus.com/inward/record.uri?eid=2-s2.0-73549101745&doi=10.2481%2fdsj.8.18&partnerID=40&md5=4b7d4ca89150977d86a48bb2c9ce3d8e","It is common practice that in teams working in the field of natural sciences all group members manage their primary data in highly individual systems. The consequence of this is that the data are usually lost shortly after the scientific results have been published or that they lose part of their value, as significant background information can no longer be found. To solve this problem in a simple way, we present a basic procedure that allows us to uniquely identify scientific data and their history at any time. We describe which requirements such a procedure has to meet (proper documentation, unique identification, and easy backtracking of the individual operations) and discuss on the basis of a timestamp approach how such a procedure can be integrated smoothly into the traditional scientificwork process and the existing IT infrastructure of a team. It does this by using established processes and developing them into a systematic information management covering both electronic and analogue media."
"10.2481/dsj.SS_IGY-009","https://www.scopus.com/inward/record.uri?eid=2-s2.0-76449113945&doi=10.2481%2fdsj.SS_IGY-009&partnerID=40&md5=26be391d2aab04b9255a48bc0d1bd38b","EMScope, a component of the NSF Earthscope project, is installing a 70 km-spaced grid of long-period magnetotelluric stations across the USA. Rapid data quality control and generation of derived data products provides an accessible resource openly available without charge or restriction through the IRIS Data Management Center and EMScope data portals via a set of data discovery tools. These data are available typically within two weeks of acquisition. Initial 3-d inversion of such EMScope data from the US Pacific Northwest shows substantial coherence between crust and mantle electrical conductivity variations, the boundaries of major physiographic provinces, and seismically delineated features."
"10.2481/dsj.007-034","https://www.scopus.com/inward/record.uri?eid=2-s2.0-76449106023&doi=10.2481%2fdsj.007-034&partnerID=40&md5=61311dc00564a50693f55f287190b79c","In this paper the authors underline the increasing importance of Chinese scientific information. They compare the results of a request launched on Western and Chinese databases concerning Chinese scientific paper publication. Although Chinese scientists are having more and more visibility in international scientific journals, their publications in English are far fewer than those in Chinese. The difference is so drastic that it emphasizes the need for westerners (researchers and observers) to access and master the use of Chinese sources of information. The aim of this paper is to define the specificity of Chinese scientific information and also to present some primary results concerning the automatic analysis of Chinese writing. Our method uses a specific core language close to the point of view of the expert and his or her knowledge that will permit accurate information retrieval from a huge quantity of documents."
"10.2481/dsj.008-017","https://www.scopus.com/inward/record.uri?eid=2-s2.0-76449103775&doi=10.2481%2fdsj.008-017&partnerID=40&md5=7cc9c60b56f8e390ae78340ad6ff82b9","In the present paper, a class of probability density functions is considered, and the properties of the Bayes estimator and the Bayes Shrinkage estimator of the parameters are studied. The loss functions used are asymmetric loss and squared error loss under different prior distributions. A Bayes estimate of Reliability function is also given."
"10.2481/dsj.IGY-002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-76449099064&doi=10.2481%2fdsj.IGY-002&partnerID=40&md5=e8e720b7a2851eb39ffdc2caae8c8771","The vision of the Electronic Geophysical Year (eGY) is that we can achieve a major step forward in geoscience capability, knowledge, and usage throughout the world for the benefit of humanity by accelerating the adoption of modern and visionary practices such as virtual observatories for managing and sharing data and information. eGY has found that the biggest challenges to implementing the vision are educating program mangers and senior scientists on the need for modern data management techniques and providing incentives for practitioners of the new field of geoinformatics."
"10.1504/IJBIDM.2008.020515","https://www.scopus.com/inward/record.uri?eid=2-s2.0-64549141061&doi=10.1504%2fIJBIDM.2008.020515&partnerID=40&md5=7a95e59cfd9079e70d2adde0cf0e6567","This paper extends the forward method plagiarism detection in finding the percentage of similarity between documents. We have developed an algorithm to quantify the similarity based on path patterns, and the method employed is simple, as it involves only ordinary mathematics, thus simplifying application programming and speed up processing time. The method simply converts words into steps, which walks on a mesh, using a new proposed hash function. The hash function guarantees that the number of steps for each different word is unique and thus the walk pattern on a mesh is unique. Hence, a plagiarised version document will display a unique pattern on a mesh that is similar to the original document. This extended paper presents the algorithm in detail, and results are compared with an available online plagiarism detection tool. Copyright © 2008, Inderscience Publishers."
"10.1504/IJBIDM.2008.020519","https://www.scopus.com/inward/record.uri?eid=2-s2.0-64249168351&doi=10.1504%2fIJBIDM.2008.020519&partnerID=40&md5=8823b1844155014895d0017ebfc2bbd8","Preprocessing is often required before using clustering or other data mining algorithms to analyse multivariate data sets. The approaches discussed in this paper are enhanced implementations of a preprocess that utilises an algorithm to cluster points in a data set based upon each attribute independently, resulting in additional information about the data points with respect to each of its dimensions. Noise, data boundaries, and likely representatives of data subsets can be more easily identified, thus significantly improving the performance of subsequent clustering or data mining algorithms by combining this additional information across all dimensions and querying the results. Copyright © 2008, Inderscience Publishers."
"10.1504/IJBIDM.2008.020516","https://www.scopus.com/inward/record.uri?eid=2-s2.0-64249146822&doi=10.1504%2fIJBIDM.2008.020516&partnerID=40&md5=ef3a7f441b927e22958b027a2c56ed06","Clustering is one of the techniques used to obtain useful information from web log file for better understanding of customer behaviour. Two clustering techniques that commonly used are Greedy Hierarchical Item Set-Based Clustering (GHIC) algorithm and Hierarchical Clustering Algorithm (HCA). The algorithms, however, have its weaknesses in terms of processing times and time complexity. This paper proposes a new approach called Hierarchical Pattern-Based Clustering (HPBC) algorithm to improve the processing times based on the difference of mean support values of each cluster. The simulation revealed that the proposed algorithm outperformed the HCA and GHIC up to 100% and 50% respectively, with less time complexity. Copyright © 2008, Inderscience Publishers."
"10.1504/IJBIDM.2008.020518","https://www.scopus.com/inward/record.uri?eid=2-s2.0-64249142621&doi=10.1504%2fIJBIDM.2008.020518&partnerID=40&md5=3428020f034e1202ddb46cf93d1c6b77","As business processes gain more importance in today's business environments, their unimpeded execution is crucial for a company's success. Corporate decision makers are faced with a wide spectrum of potential risks on the one hand and a plenitude of security safeguards on the other hand. Existing approaches for the evaluation of security measures often neglect the consideration of business needs under multiple objectives. This paper gives an overview of the Atana approach that supports decision makers with the elicitation of security safeguards based on corporate business processes and according to multiple objectives. It focuses on the description of a prototype by means of a case study from the social security sector and, thus, provides decision makers with an instrument for interactively exploring different security investment scenarios, while the system guarantees at the same time that only efficient solutions are considered. Copyright © 2008, Inderscience Publishers."
"10.1504/IJBIDM.2008.020517","https://www.scopus.com/inward/record.uri?eid=2-s2.0-64249141439&doi=10.1504%2fIJBIDM.2008.020517&partnerID=40&md5=b5d36082307cd1e2dc073b1b5375f188","Association Rule Mining (ARM) technique is one of the popular data mining techniques used to discover knowledge from a database. In this paper, we highlight the subsequent drawbacks to association rule mining without considering the absence of items. We propose a novel approach to mine both positive and negative association rules as rule pairs. Our approach ensures that impacts of negative associations are considered so that the drawbacks identified can be avoided. Our initial experiment results show that mining pairs of association rules invoking negative associations are small in number and easy to be appreciated for its implications for decision-making. Copyright © 2008, Inderscience Publishers."
"10.1504/IJBIDM.2008.020520","https://www.scopus.com/inward/record.uri?eid=2-s2.0-64249130041&doi=10.1504%2fIJBIDM.2008.020520&partnerID=40&md5=d6d1ae220384c62afbeb308201930ede","In this paper two categories of improvements are suggested that can be applied to most k-medoids-based algorithms - conceptual/algorithmic improvements, and implementational improvements. These include the revisiting of the accepted cases for swap comparison and the application of partial distance searching and previous medoid indexing to clustering. Various hybrids are then applied to a number of k-medoids-based algorithms and the method is shown to be generally applicable. Experimental results on both artificial and real datasets demonstrate that when applied to CLARANS the number of distance calculations can be reduced by up to 98%. Copyright © 2008, Inderscience Publishers."
"10.1504/IJBIDM.2008.020514","https://www.scopus.com/inward/record.uri?eid=2-s2.0-64249110504&doi=10.1504%2fIJBIDM.2008.020514&partnerID=40&md5=b95fba938865c5bbe76826107ed25a2f","The educational community across the world is facing the increasing problem of plagiarism. The proposed Plagiarism Detection Engine for Java (PDE4Java) detects code-plagiarism by applying data mining techniques. The engine consists of three main phases; Java tokenisation, similarity measurement and clustering. It has an optional default tokeniser that makes it flexible to be used with almost any programming language. The system provides a visualising representation for each cluster besides the textual representation. The simulation results of PDE4Java showed a comparable performance to that of JPlag and it outperformed the expectations when compared to the domain experts' findings. Copyright © 2008, Inderscience Publishers."
"10.1504/IJBIDM.2008.022736","https://www.scopus.com/inward/record.uri?eid=2-s2.0-59549107242&doi=10.1504%2fIJBIDM.2008.022736&partnerID=40&md5=1c56ffd616ff3c6b627f2f1d9c4773ce","Epistasis plays an important role in the genetic architecture of common human diseases. Most complex diseases are believed to have multiple contributing loci that often have subtle patterns which make them fairly difficult to find in large data sets. Disorders that follow purely epistatic models cannot be detected by cases/control studies based on individual analysis of susceptible loci. The computational complexity of performing exhaustive searches for detecting such models in genome-wide applications is practically unfeasible. Furthermore, with ever-increasing number of both genotypes and individuals on one side, and little knowledge of complex traits on the other, it is becoming fairly difficult and time consuming to perform systematic genome-wide studies on such traits. We present and discuss a convenient framework for modelling epistasis using information theoretic concepts and algorithms inspired by such an approach. These generalised algorithms, which are especially in favour of purely epistatic models, are applied to both simulated and real data. The real data represents the genotype-phenotype values for Age-Related Macular Degeneration (AMD) disease. Many two-locus purely epistatic patterns were found for AMD. A new visualisation approach is also presented for the purpose of better illustrating epistasy for cases where the number of loci is more than two or three. Copyright © 2008, Inderscience Publishers."
"10.1504/IJBIDM.2008.022734","https://www.scopus.com/inward/record.uri?eid=2-s2.0-59549107093&doi=10.1504%2fIJBIDM.2008.022734&partnerID=40&md5=e54029524922ee62975be9cd1e84e815","A cluster is a collection of data objects that are similar to one another within the same cluster and are dissimilar to the objects in other clusters. Moreover, there will exist more or less similarities among these large amounts of initial cluster results in a real-life data set. Accordingly, an analyser will have difficulty implementing further analysis if they know nothing about these similarities. Therefore, it is very valuable to analyse these similarities and construct the hierarchy structures of the initial clusters. The traditional cluster methods are unfit for this cluster postprocessing problem for their favour of finding the spherical shape clusters, impractical hypothesis and multiple scans of the data set. Based on multifractal theory, we propose the MultiFractal-based Cluster Hierarchy Optimisation (MFCHO) algorithm, which integrates the cluster similarity with cluster shape and cluster distribution to construct the cluster hierarchy tree from the disjoint initial clusters. The elementary time-space complexity of the MFCHO algorithm is presented. Several comparative experiments using synthetic and real-life data sets show the performance and the effectivity of MFCHO. Copyright © 2008, Inderscience Publishers."
"10.1504/IJBIDM.2008.022737","https://www.scopus.com/inward/record.uri?eid=2-s2.0-59549092880&doi=10.1504%2fIJBIDM.2008.022737&partnerID=40&md5=9269b1c601e34985d58e6f88f17a1a18","In this article, assignment reduction and approximation reduction are proposed for Inconsistent Ordered Information Systems (IOIS). The properties and relationships between assignment reduction and approximation reduction are discussed. The dominance matrix and decision assignment matrix are also proposed for information systems based on dominance relations. The algorithm of assignment reduction is introduced, from which we can provide an approach to knowledge reductions operated in inconsistent systems based on dominance relations. Finally, an example illustrates the validity of the given method, which shows that the method is effective in complicated information systems. Copyright © 2008, Inderscience Publishers."
"10.1504/IJBIDM.2008.022739","https://www.scopus.com/inward/record.uri?eid=2-s2.0-59549088415&doi=10.1504%2fIJBIDM.2008.022739&partnerID=40&md5=3b257475846acf8e3afe90bf14b9ab29","This paper considers sparse regression modelling using a generalised kernel model in which each kernel regressor has its individually tuned centre vector and diagonal covariance matrix. An Orthogonal Least Squares (OLS) forward selection procedure is employed to select the regressors one by one using a guided random search algorithm. In order to prevent the possible overfitting, a practical method to select the termination threshold is used. A novel hybrid wavelet is constructed to make the model sparser. The experimental results show that this generalised model outperforms the traditional methods in terms of precision and sparseness. The model with the wavelet and hybrid kernel has a much faster convergence rate compared to that with a conventional Radial Basis Function (RBF) kernel. Copyright © 2008, Inderscience Publishers."
"10.1504/IJBIDM.2008.022738","https://www.scopus.com/inward/record.uri?eid=2-s2.0-59549087144&doi=10.1504%2fIJBIDM.2008.022738&partnerID=40&md5=63ffa363b38ec137c1dad1752a2dfe1b","One of the most significant features of the Chinese language is that it displays a considerable amount of semantic information at the radical (components comprising Chinese characters) level. Chinese radicals show the important semantic impacts on Chinese characters. However, many previous studies have mainly focused on the word and phrase level of Chinese semantic analysis. In this paper, we quantitatively analyse Chinese radicals' semantics by mapping radicals and the corresponding characters' semantics to the WordNet synsets' hierarchy. Through the analysis, we find out the semantic strength of Chinese radicals and discover the entropy of the Chinese radicals set. Copyright © 2008, Inderscience Publishers."
"10.1504/IJBIDM.2008.022735","https://www.scopus.com/inward/record.uri?eid=2-s2.0-59549084797&doi=10.1504%2fIJBIDM.2008.022735&partnerID=40&md5=349d6463230e1fc2af5f7d198e1e8019","Spatial co-location patterns represent the subsets of Boolean spatial features whose instances often locate in close geographic proximity. The existing co-location pattern mining algorithms aim to find spatial relations based on the distance threshold. However, it is hard to decide the distance threshold for a spatial data set without any prior knowledge. Moreover, spatial data sets are usually not evenly distributed and a single distance value cannot fit an irregularly distributed spatial data set well. In this paper, we propose the notion of the k-nearest features (simply k-NF)-based co-location pattern. The k-NF set of a spatial feature's instances is used to evaluate the spatial relationship between this feature and any other feature. A k-NF-based co-location pattern mining algorithm by using T-tree (KNFCOM-T in short) is further presented to identify the co-location patterns in large spatial data sets. The experimental results show that the KNFCOM-T algorithm is effective and efficient and its complexity is O(n). Copyright © 2008, Inderscience Publishers."
"10.1504/IJBIDM.2008.022137","https://www.scopus.com/inward/record.uri?eid=2-s2.0-58149376842&doi=10.1504%2fIJBIDM.2008.022137&partnerID=40&md5=4fd764f7d352ce757ef94df91f9e3eda","Evaluating business solutions before their deployment is essential for any organisation. Risk is emerging as one of the most preeminent and accepted metrics for the evaluation of business solutions. In this paper, we present a comprehensive case study where the Tropos Goal-Risk framework is used to assess and treat risk on the basis of the likelihood and severity of failures within organisational settings. We present an analysis and evaluation of the business solutions within manufacturing enterprises. Copyright © 2008, Inderscience Publishers."
"10.1504/IJBIDM.2008.022138","https://www.scopus.com/inward/record.uri?eid=2-s2.0-58149357489&doi=10.1504%2fIJBIDM.2008.022138&partnerID=40&md5=675d717a1ea74ad92bd08d033bc15538","Change, they usually say, is the only constant in life. Everything rapidly changes around us, and increasingly, the key to survival is the ability to adapt rapidly to changes. This consideration applies to many aspects of our lives. Strangely enough, this nearly self-evident truth is not always considered by software engineers with the seriousness that it calls for: the assumptions we draw for our systems often do not take into due account that, for example, the run-time environments, the operational conditions, or the available resources will vary. Software is especially vulnerable to this threat, and with today's software-dominated systems controlling crucial services in nuclear plants, airborne equipment, healthcare systems and so forth, it becomes clear how this situation may potentially lead to catastrophes. This work discusses this problem and defines some of the requirements towards its effective solution, which we call 'new software development', as the software equivalent of the well-known concept of new product development. The paper also introduces and discusses a practical example of a software tool that was designed taking those requirements into account - an adaptive data integrity provision in which the degree of redundancy is not fixed once and for all at design time, but rather changes dynamically with respect to the disturbances experienced during run-time. Copyright © 2008, Inderscience Publishers."
"10.1504/IJBIDM.2008.022135","https://www.scopus.com/inward/record.uri?eid=2-s2.0-58149354681&doi=10.1504%2fIJBIDM.2008.022135&partnerID=40&md5=9fef00cfb06829f84595cf100831f52c","This paper investigates the privacy issues in the context of e-health and will especially consider e-health portals which provide patients access to Electronic Health Records (EHRs). Since e-health portals can be accessed via the internet, security and privacy issues arise that have to be considered carefully. Besides the traditional security properties, we focus mainly on additional threats, namely the disclosure attack, the anonymity set attack and statistical analysis of metadata. A disclosure attack takes place if a person 'motivates' or even forces another one to present her EHR. We propose so-called multiple identities, which help to eliminate this attack. In the context of anonymous authentication, we will point out weaknesses regarding the choice of anonymity sets. Additionally, by applying statistical analysis on the metadata of an e-health portal, it is possible to determine relevant information which could have negative effects on the patient. We present a concept that includes pseudonymisation of e-health portals, multiple identities, obfuscation of metadata and anonymity methods to prevent the above-mentioned attacks and make statistical analysis difficult. Furthermore, all privacy-enhancing methods do not rely on application-layer mechanisms (which in general can easily be bypassed by insiders), but are based on cryptographic primitives which are state of the art. Copyright © 2008, Inderscience Publishers."
"10.1504/IJBIDM.2008.022136","https://www.scopus.com/inward/record.uri?eid=2-s2.0-58149333687&doi=10.1504%2fIJBIDM.2008.022136&partnerID=40&md5=ce17ca26c6083fd3e70027815a584d02","Data Warehouses (DWs) manage historical information for the decision-making process and for enterprises. Online Analytical Processing Applications (OLAP) tools are the most used tools for implementing and consulting DWs and it is necessary to define security measures to avoid the accessing of unauthorised information by users by executing queries. It is vitally important to consider security requirements from the earliest stages of the development process. We have created a Model-Driven Architecture (MDA) to develop a secure DW and in this paper, we propose how to implement the security measures that are defined at upper abstraction levels using our approach to SQL Server Analysis Services (SSAS). Copyright © 2008, Inderscience Publishers."
"10.2481/dsj.7.106","https://www.scopus.com/inward/record.uri?eid=2-s2.0-55849130323&doi=10.2481%2fdsj.7.106&partnerID=40&md5=f12bad454f518ff8d6ec82b65c2e9039","This paper provides the Bayes estimators of the failure rate and reliability function for a one-parameter, exponential distribution by utilizing a point guess estimate of the parameter. For deriving the Bayes estimators, the prior distributions are chosen such that they are centered at the known prior values of parameters. The validity of proposed estimators is examined with respect to their maximum likelihood estimators (MLE) and Thompson's Shrinkage estimator on the basis of Monte Carlo simulations of 1000 samples."
"10.2481/dsj.7.96","https://www.scopus.com/inward/record.uri?eid=2-s2.0-55849113679&doi=10.2481%2fdsj.7.96&partnerID=40&md5=8e039ea78aad698c5e98c0ec5e16ddee","This paper describes an approach to visualizing concurrency control (CC) algorithms for real-time database systems (RTDBs). This approach is based on the principle of software visualization, which has been applied in related fields. The Model-View-controller (MVC) architecture is used to alleviate the black box syndrome associated with the study of algorithm behaviour for RTDBs Concurrency Controls. We propose a Visualization ""exploratory"" tool that assists the RTDBS designer in understanding the actual behaviour of the concurrency control algorithms of choice and also in evaluating the performance quality of the algorithm. We demonstrate the feasibility of our approach using an optimistic concurrency control model as our case study. The developed tool substantiates the earlier simulation-based performance studies by exposing spikes at some points when visualized dynamically that are not observed using usual static graphs. Eventually this tool helps solve the problem of contradictory assumptions of CC in RTDBs."
"10.2481/dsj.7.115","https://www.scopus.com/inward/record.uri?eid=2-s2.0-55849111510&doi=10.2481%2fdsj.7.115&partnerID=40&md5=92220a43d12e95b2856e6b558c3b82a4","On March 4-5, 2008, the CODATA Task Group for Exchangeable Material Data Representation to Support Research and Education held a two day seminar cum meeting at the National Physical Laboratory (NPL), New Delhi, India, with NPL materials researchers and task group members representing material activities and databases from seven countries: European Union (The Czech Republic, France, and the Netherlands), India, Korea, Japan, and the United States. The NPL seminar included presentations about the researchers 'work The Task Group meeting included presentations about current data related activities of the members. Joint discussions between NPL researchers and CODATA task group members began an exchange of viewpoints among materials data producers, users, and databases developers. The seminar cum meeting included plans to continue and expand Task Group activities at the 2008 CODATA 21st Meeting in Kyiv, Ukraine."
"10.2481/dsj.7.76","https://www.scopus.com/inward/record.uri?eid=2-s2.0-45949107186&doi=10.2481%2fdsj.7.76&partnerID=40&md5=51a4b0a579f80e825dbac0e82457bd73","This work proposes a data mining algorithm called Unordered Rule Sets using a continuous Ant-Miner algorithm. The goal of this work is to extract classification rules from data. Swarm intelligence (SI) is a technique whereby rules may be discovered through the study of collective behavior in decentralized, self-organized systems, such as ants. The Ant-Miner algorithm, first proposed by Parpinelli and his colleagues (2002), applies an ant colony optimization (ACO) heuristic to the classification task of data mining to discover an ordered list of classification rules. Ant-Miner is a rule-induction algorithm that uses SI techniques to form rules. Ant-Miner uses a discretization process to deal with continuous attributes in the data. Discretization transforms numeric attributes into nominal attributes. Discretization may suffer from a loss of information, as the real relationship underlying individual values of a numeric attribute is unknown. The objective of this work is to apply ACO heuristic techniques to discover unordered rule sets for mixed variables in a data set. The proposed algorithm handles both nominal and continuous attributes using multimodal functions. It has the advantage of discovering more modular rules, i.e., rules that can be interpreted independently from other rules - unlike the rules in an ordered list, where the interpretation of a rule requires knowledge of the previous rules in the list. The results provide evidence that the accuracy of the Unordered Rule Set Continuous Ant-Miner algorithm is competitive with other Ant-Miner versions and generates simpler rule sets."
"10.2481/dsj.7.57","https://www.scopus.com/inward/record.uri?eid=2-s2.0-45949101013&doi=10.2481%2fdsj.7.57&partnerID=40&md5=0d677afa75f049457df2f2c565b29dde","By distinguishing nested attributes as Decomposable and Non-Decomposable, it is proved that for all nested relations, unnesting and then renesting on the same attribute yields the original relation subject only to the elimination of duplicate data. Therefore, the statement that was popular in nested relations research: ""Unnesting and then nesting on the same attribute of a nested relation does not always yield the original relation"" is reconsidered."
"10.2481/dsj.7.65","https://www.scopus.com/inward/record.uri?eid=2-s2.0-45949092483&doi=10.2481%2fdsj.7.65&partnerID=40&md5=a8a4519911b8299f895ce3e5b74f56a7","In this paper, we have obtained the Bayes Estimator of Generalized- Exponential scale and shape parameter using Lindley 's approximation (L-approximation) under asymmetric loss functions. The proposed estimators have been compared with the corresponding MLE for their risks based on simulated samples from the Generalized-Exponential distribution."
"10.2481/dsj.7.46","https://www.scopus.com/inward/record.uri?eid=2-s2.0-45949092310&doi=10.2481%2fdsj.7.46&partnerID=40&md5=2ecca2f47390c6d84a59cc791faab1a4","With the development of computer graphics and digitalizing technologies, 3D model databases are becoming ubiquitous. This paper presents a method for content-based searching for similar 3D models in databases. To assess the similarity between 3D models, shape feature information of models must be extracted and compared. We propose a new 3D shape feature extraction algorithm. Experimental results show that the proposed method achieves good retrieval performance with short computation time."
"10.1504/IJBIDM.2008.017974","https://www.scopus.com/inward/record.uri?eid=2-s2.0-43149112091&doi=10.1504%2fIJBIDM.2008.017974&partnerID=40&md5=c8c629aaddd20382d667851294b148ba","This paper presents a new stochastic nature inspired methodology, which is based on the concepts of Particle Swarm Optimization (PSO) and Ant Colony Optimization (ACO), for optimally clustering N objects into K clusters. Due to the nature of stochastic and population-based search, the proposed algorithm can overcome the drawbacks of traditional clustering methods. Its performance is compared with other popular stochastic/metaheuristic methods like genetic algorithm and Tabu search. The proposed algorithm has been implemented and tested on several datasets with very good results. © 2008, Inderscience Publishers."
"10.1504/IJBIDM.2008.017977","https://www.scopus.com/inward/record.uri?eid=2-s2.0-43149102526&doi=10.1504%2fIJBIDM.2008.017977&partnerID=40&md5=2a8a1dde85e8acfc38a6c2144a19fc2e","The flow of data coming from modern sensing devices enables the development of novel research techniques related to data management and knowledge extraction. In this work, we undertake the problem of analysing traffic in a road network so as to help the city authorities to optimise traffic flow. A graph based modelling of the network traffic is presented which provides insights on the flow of movements within the network. We exploit this graph in order to analyse the traffic flow in the network and to discover traffic relationships like propagation, split and merge of traffic among the road segments. First experimental results illustrate the applicability and usefulness of our approach. © 2008, Inderscience Publishers."
"10.1504/IJBIDM.2008.017976","https://www.scopus.com/inward/record.uri?eid=2-s2.0-43149095386&doi=10.1504%2fIJBIDM.2008.017976&partnerID=40&md5=96d7d803ea3c2a9640e60ba266dda845","Regionalisation, a prominent problem from social geography, could be solved by a classification algorithm for grouping spatial objects. A typical task is to find spatially compact and dense regions of arbitrary shape with a homogeneous internal distribution of social variables. Grouping a set of homogeneous spatial units to compose a larger region can be useful for sampling procedures as well as many applications, e.g., direct mailing. It would be helpful to have specific purpose regions, depending on the kind of homogeneity one is interested in. In this paper, we propose an algorithm combining the 'spatial density' clustering approach and a covariance-based method to inductively find spatially dense and non spatially homogeneous clusters of arbitrary shape. © 2008, Inderscience Publishers."
"10.1504/IJBIDM.2008.017975","https://www.scopus.com/inward/record.uri?eid=2-s2.0-43149089143&doi=10.1504%2fIJBIDM.2008.017975&partnerID=40&md5=79c7638886db8520dd612aa632065c4e","Challenged by real-world clustering problems this paper proposes a novel fuzzy clustering scheme of datasets produced in the context of intuitionistic fuzzy set theory. More specifically, we introduce a variant of the Fuzzy C-Means (FCM) clustering algorithm that copes with uncertainty and a similarity measure between intuitionistic fuzzy sets, which is appropriately integrated in the clustering algorithm. We describe an intuitionistic fuzzification of colour digital images upon which we applied the proposed scheme. The experimental evaluation of the proposed scheme shows that it can be more efficient and more effective than the well-established FCM algorithm, opening perspectives for various applications. © 2008, Inderscience Publishers."
"10.1504/IJBIDM.2008.017978","https://www.scopus.com/inward/record.uri?eid=2-s2.0-43149088295&doi=10.1504%2fIJBIDM.2008.017978&partnerID=40&md5=8dd5a4b3cb12d8bba65330027b089c7e","Random samples are common in data streams applications due to limitations in data sources and transmission lines, or to load-shedding policies. Here we introduce a formal error model and show that, besides providing accurate estimates, it improves query answer accuracy by exploiting past statistics. The method is general, robust in the presence of concept drift, and minimises uncertainties due to sampling with negligible time and space overhead. We describe the application of the method, and the results obtained for SQL window aggregates, statistical aggregates such as quantiles, and data mining functions such as k-means clustering and naive Bayesian classifiers. © 2008, Inderscience Publishers."
"10.2481/dsj.7.31","https://www.scopus.com/inward/record.uri?eid=2-s2.0-45949085616&doi=10.2481%2fdsj.7.31&partnerID=40&md5=fd196a8ec640a4d2603de738e9603bb0","This research investigates the applicability of Davis's Technology Acceptance Model (TAM) to agriculturist's acceptance of a knowledge management system (KMS), developed by the authors. It is called AGROWIT. Although the authors used previous Technology Acceptance Model user acceptance research as a basis for investigation of user acceptance of AGROWIT, the model had to be extended and constructs from the Triandis model that were added increased the predictive results of the TAM, but only slightly. Relationships among primary TAM constructs used are in substantive agreement with those characteristic of previous TAM research. Significant positive relationships between perceived usefulness, ease of use, and system usage were consistent with previous TAM research. The observed mediating role of perceived usefulness in the relationship between ease of use and usage was also in consonance with earlier findings. The findings are significant because they suggest that the considerable body of previous TAM-related information technology research may be usefully applied to the knowledge management domain to promote further investigation of factors affecting the acceptance and usage of knowledge management information systems such as AGROWIT by farmers, extension workers, and agriculture researchers."
"10.2481/dsj.7.1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-45949097329&doi=10.2481%2fdsj.7.1&partnerID=40&md5=3acd927ab0e67ab10fb1df038183be74","Industrial and scientific datasets have been growing enormously in size and complexity in recent years. The largest transactional databases and data warehouses can no longer be hosted cost-effectively in off-the-shelf commercial database management systems. There are other forums for discussing databases and data warehouses, but they typically deal with problems occurring at smaller scales and do not always focus on practical solutions or influencing DBMS vendors. Given the relatively small (but highly influential and growing) number of users with these databases and the relatively small number of opportunities to exchange practical information related to DBMSes at extremely large scale, a workshop on extremely large databases was organized. This paper is the final report of the discussions and activities at the workshop."
"10.2481/dsj.7.23","https://www.scopus.com/inward/record.uri?eid=2-s2.0-45949096374&doi=10.2481%2fdsj.7.23&partnerID=40&md5=754a432225b6f822ba94c08f3cc86528","This paper is concerned with the problem of finding the minimax estimator of the parameter θ of the Rayleigh distribution for quadratic loss function by applying the theorem of Lehmann (1950)."
"10.2481/dsj.7.14","https://www.scopus.com/inward/record.uri?eid=2-s2.0-45949094353&doi=10.2481%2fdsj.7.14&partnerID=40&md5=f0d1939a105834d857a2193a64288189","Patent information is a derivative product from the legal patent system. This information, which includes patent applications, patent descriptions, patent gazettes, patent abstracts, and patent data, is prepared in exact compliance with the regulations and specifications of the patent acts. Patent information, different from other published circulating information, is legally well protected. For convenience, this study classifies patent information into bibliographic and numeric data to create a patent map."
"10.2481/dsj.6.154","https://www.scopus.com/inward/record.uri?eid=2-s2.0-39449123588&doi=10.2481%2fdsj.6.154&partnerID=40&md5=4edf176d6da5d23d815579a1fc941f9a","The CODATA Bureau's sole aim is to promote the data evolution and quality control of the data. The Bureau helped in increasing the awareness of the fact that much of the data in the primary literature was poorly documented, subjected to unknown systematic errors, and often inconsistent with measurements from other laboratories. Some of the CODATA projects such as Fundamental Constants and Key Values for Thermodynamics produced definitive datasets that have been adopted throughout the scientific community. Publication such as the series of CODATA Directories of Data Sources in Science and Technology pointed the way to reliable sources of high quality data in a wide range of disciplines. The CODATA Referral Database in the form of a list of selected websites that provide data of high quality could be carried out on the CODATA home site with appropriate links to restore data quality."
"10.2481/dsj.6.S930","https://www.scopus.com/inward/record.uri?eid=2-s2.0-38149137436&doi=10.2481%2fdsj.6.S930&partnerID=40&md5=5feb200c7dac16e62b8a6d251e78f290","The Overseas Sinology Database is composed of three databases: scholar, organization, and journal. The thesis database is regard as separate and is attached to the scholar database. The database information comes from major areas of the world, especially the countries adjacent to China, and updates are done continuously. The Sinology Database is in several different languages and should satisfy the differing needs of data collection and database application. The data quality is strictly controlled during the whole data life cycle, which includes data collection, processing, storage, and accessing. In addition, according to the standards and specifications of the metadata, metadata are created to accompany the data, which satisfies the cooperation among different databases. Finally, besides the function of searching, statistical calculation, and sorting, the database is also used for data mining and knowledge discovery. Through these methods, conclusions about changes in Sinology can be drawn, which will aid us in understanding the world and China in particular."
"10.2481/dsj.6.S962","https://www.scopus.com/inward/record.uri?eid=2-s2.0-38149131409&doi=10.2481%2fdsj.6.S962&partnerID=40&md5=f4a02d01e22775c3defa833386380d1e","It is historical that data development has its own mode (collect, treatment, delivery, store, and use), from Manual mode, Mechanism mode, and Electronic mode, now to the Network mode. And search engine plus self-learning is the advanced mode of data development. Network mode has also been changing, the underlying motivation exits in the development & progress of Internet itself. There are two huge trends force the mode of data development to face new challenge & make decision. One is the revolution resulted by the change of the user market need & represented by Web2.0. Another is the revolution resulted by technological developing tendency & represented by Grid. Squeezed by the two huge trends from opposite directions, the lagging, crude and inefficient mode will change revolutionarily forced by wise decision-making or silent market. As for data-development mode, the change of technology & operation need the change of game rule simultaneously. So eliminating barriers, promoting resource-sharing, rationalize relations of market/non-market is to be a big inescapable work."
"10.2481/dsj.6.S973","https://www.scopus.com/inward/record.uri?eid=2-s2.0-38149103303&doi=10.2481%2fdsj.6.S973&partnerID=40&md5=8176a27ec8a5e9695b1244e2b7219079","The development of semiconductive nanostructures for spinelectronics is discussed. Nanostructures constructed from inorganic solids such as semiconductors have new electronic and optical properties because of their size and quantization effects. The significance of spintronics is understood by perspectives of the development and creation of new types of non-volatile memory with random access, quantum single-electron logical structures, and ultra dense information storage media. The discovery of giant magnetoresistance effect (GMR) is considered as the beginning of the 'Spintronics Era'. Sensors operating with tunnel magnetic junctions (MTJ) fall into the second class of spintronics devices. The tunnel conductivity depends on the relative orientation of electrode magnetizations. High-temperature semiconductors with wide forbidden zones are also very promising materials for modern nanoelectronics."
"10.2481/dsj.6.S981","https://www.scopus.com/inward/record.uri?eid=2-s2.0-38149073387&doi=10.2481%2fdsj.6.S981&partnerID=40&md5=feb8aece64cc90c85e181ff22236b4d4","Data mining is a technique used to uncover previously unknown and potentially useful knowledge from large datasets. It turns data into actionable information for better decision making. Delphi incorporates a collection of high-performance data-analysis components known as the Decision Cube. The Decision Cube offers an easy path to powerful data analysis in an application. This paper describes a simple approach for data mining using the Decision Cube components of Delphi. It requires only some drag-and-dropping and property-setting, with no need for manual coding. It does not require special knowledge of data mining. At the end, an example is offered."
"10.2481/dsj.6.S941","https://www.scopus.com/inward/record.uri?eid=2-s2.0-38149029284&doi=10.2481%2fdsj.6.S941&partnerID=40&md5=a7a196b565b8019755cad0cd5b55d318","This article begins with attention to the digital divide. It gives a brief overview of the digital divide on a global basis and analyzes specific aspects of the digital divide in our country. It also introduces the informationization construction of Guizhou Province and points out problems with the digital divide in that province. Then it focuses on the practice of Guizhou Province to bridge the digital divide -- the practice and experience of GZNW. The final section gives a series of policy recommendations on how to bridge the digital divide, realize digital dividends, and how to build a new socialist countryside."
"10.2481/dsj.6.S968","https://www.scopus.com/inward/record.uri?eid=2-s2.0-38148999629&doi=10.2481%2fdsj.6.S968&partnerID=40&md5=fa2487192712b73d26ca119624ff96d4","Philosophy research used to rely mainly on the traditional published journals and newspapers for collecting or communicating data. However, because of financial limits or lack of capability to collect data, required published materials and even restricted materials and developing information from research projects often could not be obtained. The rise of digital techniques and Internet opportunities has allowed data resource sharing of philosophy research. However, although there are several ICPs with large-scale comprehensive commercial databases in the field in China, no real non-profit professional database for philosophy researchers exists. Therefore, in 2002, the Philosophy Institute of the Chinese Academy of Social Sciences began a project to build ""The Database of Philosophy Research."" Until Mar. 2006 the number of subsets had reached 30, with more than 30,000 records, retrieval services reached 6,000, and article-reading reached 30,000. Because of the concept of intellectual property, the service of the database is currently limited to the information held in CASS. Nevertheless, this is the first academic database for philosophy research, so its orientation is towards resource-sharing, leading users to data, and serving large number of demands from other provinces and departments."
"10.2481/dsj.6.S910","https://www.scopus.com/inward/record.uri?eid=2-s2.0-38149120551&doi=10.2481%2fdsj.6.S910&partnerID=40&md5=1c1f34046a1d65723073a5fb865798e1","Scientific data sharing is a long-term and complicated task. The related data sharing and distribution policies are prime concerns. By using both domestic and international experiences in scientific data sharing, the sources, distribution, and classification of scientific data in advanced manufacturing and automation are discussed. A primary data sharing and distribution policy in advanced manufacture and automation is introduced."
"10.2481/dsj.6.S926","https://www.scopus.com/inward/record.uri?eid=2-s2.0-38149078237&doi=10.2481%2fdsj.6.S926&partnerID=40&md5=348d79fb915f60ee71b69304408d7c31","We present recent research focused on the construction of a Web GIS-based GPS vehicle monitoring system to locate or navigate thousands of vehicles simultaneously."
"10.2481/dsj.6.S913","https://www.scopus.com/inward/record.uri?eid=2-s2.0-38149066867&doi=10.2481%2fdsj.6.S913&partnerID=40&md5=a5a718e3b4dc47f85d7e9df6c5b7ddae","This article discusses the key features of a newly developed national data-sharing online network for material environmental corrosion. Written in Java language and based on Oracle database technology, the central database in the network is supported with two unique series of corrosion failure data, both of which were accumulated during a long period of time. The first category of data, provided by national environment corrosion test sites, is corrosion failure data for different materials in typical environments (atmosphere, seawater and soil). The other category is corrosion data in production environments, provided by a variety of firms. This network system enables standardized management of environmental corrosion data, an effective data sharing process, and research and development support for new products and after-sale services. Moreover this network system provides a firm base and data-service platform for the evaluation of project bids, safety, and service life. This article also discusses issues including data quality management and evaluation in the material corrosion data sharing process, access authority of different users, compensation for providers of shared historical data, and finally, the related policy and law legal processes, which are required to protect the intellectual property rights of the database."
"10.2481/dsj.6.S904","https://www.scopus.com/inward/record.uri?eid=2-s2.0-38149032586&doi=10.2481%2fdsj.6.S904&partnerID=40&md5=f37f73cf8be99ddd360a746abdbdd710","Text mining deals with complex and unstructured texts. Usually a particular collection of texts that is specified to one or more domains is necessary. We have developed a customizable text classifier for users to mine the collection automatically. It derives from the sentence category of the HNC theory and corresponding techniques. It can start with a few texts, and it can adjust automatically or be adjusted by user. The user can also control the number of domains chosen and decide the standard with which to choose the texts based on demand and abundance of materials. The performance of the classifier varies with the user's choice."
"10.2481/dsj.6.S898","https://www.scopus.com/inward/record.uri?eid=2-s2.0-38149017948&doi=10.2481%2fdsj.6.S898&partnerID=40&md5=d74c0456166c28ce88d0ac6687e193c6","The administration and storage of environmental characteristic spectral data are highly relevant in many fields of environmental study, such as measurement of trace gases in the atmosphere and air quality estimation. For this reason, a web-accessible database has been developed, offering ready access to the main parameters of molecular absorption spectral data. Web-based and friendly interfaces allow for interactive queries as well as previews of plots and downloads of files of the resulting spectral data for thorough comparative analyses."
"10.2481/dsj.6.S853","https://www.scopus.com/inward/record.uri?eid=2-s2.0-38149132528&doi=10.2481%2fdsj.6.S853&partnerID=40&md5=722ec1a7786ff113025e99c2f8b517f8","With the development of computer and network technology, the study of metadata and the standards of metadata have become key research topics in recent years. Metadata design gives us a good tool to help with lexicography. Though it is indispensable for the external metadata of dictionaries, internal metadata design for entry content is even more important. Among these designs, those metadata for heads of character and headwords are still the basic work. These designs provide basic linguistic material and support finishing the work remaining in dictionary compilation. This paper describes a set of metadata of heads of character and headwords with the reference to the Temporary Chinese Dictionary."
"10.2481/dsj.6.S837","https://www.scopus.com/inward/record.uri?eid=2-s2.0-38149127293&doi=10.2481%2fdsj.6.S837&partnerID=40&md5=0e54fa6390f0d9112b2526ce1b35b72b","The formation of olefin complexes is of potential importance in the separation of olefins. The solvents affect the activation energies, and hence the rates and equilibrium constants of the complexing reactions, which performance should be well estimated for the purpose of industrial practice. The solvent effects on the complexing reaction of propylene and nickel dithiolene Ni(S2C2H2)2 + C2H 4=CH2 → Ni(S2C2H 2)·C2H4=CH2 are studied in this work, using density functional theory with B3LYP and an Onsager model. Complete optimizations of all the stagnation points are performed in benzene, toluene, tetrahydrofuran, dichloromethane, 1,2-dichloroethane, acetone, ethanol, methanol, 1,2,3-propanetriol, dimethylsulfoxide and water, respectively. The reaction of complexing nickel dithiolene with propylene is a two-step process: the first step coordinates the propylene to S atoms in dithiolene, forming a trans-structural intermediate. The second step then yields the cis-structural product. The activation energy of the first step is higher than that of the second, indicating that the first step is the rate-determining step. The solvents make slight changes in the geometries of the reactants, transition states, intermediates and products. However, the corresponding molecular dipole moment becomes large with increase of the solvent polarity, which is beneficial to accelerate the reaction. Furthermore, the activation energies of the first (or second) step will exponentially decrease from 125.0 to 113.0 kJ·mol-1 (or from 101.8 to 83.43 kJ·mol-1) when the dielectric constants of solvents increase from 1.00 to 78.39, while the reaction rates of the first (or second) step exponentially increase from 0.7673×10-9 to 96.20 ×10-9·s -1 (or from 0.5503 to 1.038·s-1), and the equilibrium constants rapidly increase from 0.5066 to 343.4 l·mol -1. The sharp variations of activation energies, rate constants, and equilibrium constants appear when the value of the dielectric constant of solvents lies between 1 and approximately 20, while these variations become mild when the dielectric constant of solvents is larger than 20. All of these results demonstrate that the complexing reaction of propylene and nickel dithiolene become much easier and faster to occur in polar solvents. The relationship between the equilibrium constants of the complexing reaction K eq and the dielectric constants of solvents ε can be presented mathematically as Keq = A · exp(- ε/t) + B with correlation parameters A = -378.4 l·mol-1, B = 350.7 l·mol-1 and t = 21.17. This relationship may be seen as a reference for solvent selection in olefin separation practice."
"10.2481/dsj.6.S867","https://www.scopus.com/inward/record.uri?eid=2-s2.0-38149109203&doi=10.2481%2fdsj.6.S867&partnerID=40&md5=c0ccec850a8e3eb5fdee7f837e609a9a","The comprehensive database system of the Northeast agro-ecology of black soil (CSDB_BL) is user-friendly software designed to store and manage large amounts of data on agriculture. The data was collected in an efficient and systematic way by long-term experiments and observations of black land and statistics information. It is based on the ORACLE database management system and the interface is written in PB language. The database has the following mainfacilities:(1) runs on Windows platforms; (2) facilitates data entry from *.dbf to ORACLE or creates ORACLE tables directly; (3)has a metadata facility that describes the methods used in the laboratory or in the observations; (4) data can be transferred to an expert system for simulation analysis and estimates made by Visual C++ and Visual Basic; (5) can be connected with GIS, so it is easy to analyze changes in land use ; and (6) allows metadata and data entity to be shared on the internet. The following dataseis are included in CSDBJL: long-term experiments and observations of water, soil, climate, biology, special research projects, and a natural resource survey of Hailun County in the 1980s; images from remote sensing, graphs of vectors and grids, and statistics from Northeast of China. CSDB_BL can be used in the research and evaluation of agricultural sustainability nationally, regionally, or locally. Also, it can be used as a tool to assist the government in planning for agricultural development. Expert systems connected with CSDB_BL can give farmers directions for farm planting management."
"10.2481/dsj.6.S857","https://www.scopus.com/inward/record.uri?eid=2-s2.0-38149103575&doi=10.2481%2fdsj.6.S857&partnerID=40&md5=6ce80968086215a3513db121938eabc6","Reading is the main way in which people acquire information; the function of a paper information reading room is to understand public habits, create an environment, and make a ""readers' space"" to link users with information. In English, the word ""information"" is used to indicate the Chinese word """" This word has several meanings in English: news, intelligence, knowledge, report, speech, lecture, notice, service platform, inform, tell, [computer] information. It is clear that ""information"" is a word with a wide range of connotations. According to Wang (1986), ""information is a signal from the universe that is combined with a media substance. Information is from a substance, but it is not the substance where it is from. It is a new substance, a combination of medium and signal. Such change occurred in the twinkling of an eye"" (Wang, 2005). In this paper, we discuss the meaning of information and present ideas about how paper-oriented reading rooms have become digital and how digital reading rooms can be extended to cybercafés, TV, radios, and telephones."
"10.2481/dsj.6.S889","https://www.scopus.com/inward/record.uri?eid=2-s2.0-38149090393&doi=10.2481%2fdsj.6.S889&partnerID=40&md5=37b7358183a5370932ae816b04188c17","Grid computing for resources sharing and distributed computing has been researched widely in the past. As for distributed spatial datasets, the current centralized administrative scheme may become the system performance bottleneck. This paper presents a distributed cooperative grid computing technology to facilitate complex spatial applications by collaboration among distributed spatial resources. A hierarchical spatial index and communication protocol has been designed for the collaboration, which enables a dynamical choice for the best quality nodes for specified subtasks, synchronized execution, and compensation for a failure to execute a subtask. Also, we present an approach for dynamic resource allocation and distributed transaction mechanics to ensure consistency."
"10.2481/dsj.6.S879","https://www.scopus.com/inward/record.uri?eid=2-s2.0-38149060152&doi=10.2481%2fdsj.6.S879&partnerID=40&md5=1d3ba06af288653444e2b30056a27dcc","Since the 1990s, our center's geophysics database has been available on the Internet (http://gp.wdc.cn and http://wdc.geophys.cn). Based on HTML language, the website offers simple data service and is being constantly upgraded and improved. We have adopted ORACLE as the database and use JSP (Java Server Pages) technology to create dynamic pages. The basic function of the network is to store, check, inquire, and renew the data. Users can query and download the data in txt form and view pictures generated by Matlab. At present the system is running well in both versions: Chinese and English, each having a unified style. The pages are simple and convenient to use. Its users come from most provinces of China, including Taiwan, and from developed or developing countries, such as the U.S.A., Germany, Japan, Singapore, Bulgaria, Canada, etc."
"10.2481/dsj.6.S847","https://www.scopus.com/inward/record.uri?eid=2-s2.0-38149042081&doi=10.2481%2fdsj.6.S847&partnerID=40&md5=1bef135c702859dc6bbc00d17c33ceb9","Every material database on the Internet has a different data schema. There are some trial attempts to define unified schema for material databases, but since the structure of scientific data is very complicated and changes dynamically, defining a complete data schema is an impossible task. There are two major approaches for material data standards: one is MatDB, an attempt to define precise and detailed metadata, and the other is MatML, which only defines a framework. However, there is a third way: loose syndication, such as blogs with RSS. RSS, RDF Site Summary, or Really Simple Syndication, was developed to summarize document pages, but it can be extended to describe metadata of factual databases. In this presentation, an RSS extension for material database summaries is discussed."
"10.2481/dsj.6.S884","https://www.scopus.com/inward/record.uri?eid=2-s2.0-38149041830&doi=10.2481%2fdsj.6.S884&partnerID=40&md5=7786045c21431dd5a7658c822ffa0614","Recently ontology research has received much attention in geo-information science and the concept of ontology is very important for spatial information concept modeling and data sharing, classification of geographical classes. More importantly, it enriches the semantic theory of spatial information. Geo-information services and geo-information interpretation and extraction are the two main applications of geo-ontology. Ontologies have great application potential for geo-information service."
"10.1504/IJBIDM.2007.016383","https://www.scopus.com/inward/record.uri?eid=2-s2.0-48649109837&doi=10.1504%2fIJBIDM.2007.016383&partnerID=40&md5=99f74c2f9fefbbb04ca26dd0c71478a0","Understanding human behaviour in electronic spaces is of central importance in modern business intelligence and web behaviour mining. We present a novel analytic framework that enables identification of the significant navigational elements, observation of higher order behavioural abstractions, and investigation of usability attributes. A case study of knowledge worker intranet browsing behaviour revealed elemental and complex browsing pattern formation. Frequent patterns were relatively easily executable. Knowledge workers accomplished browsing goals via few subtasks, had focused targets, and accessed narrow range of services and resources. A diminutive exploratory behaviour and significant underutilisation of available resources were exhibited. © 2007, Inderscience Publishers."
"10.1504/IJBIDM.2007.016384","https://www.scopus.com/inward/record.uri?eid=2-s2.0-48649106049&doi=10.1504%2fIJBIDM.2007.016384&partnerID=40&md5=12def07c7b04b57e11f0a69f79eacd8f","Instance selection is often used in case of lazy classifiers. This paper addresses the need of instance selection in case of neural network and decision tree classifiers and presents a novel Supervised Instance Selection (SIS) algorithm. Initially, a neural network classifier is constructed using all training instances. The algorithm then selects a few instances using the certainty values of the wrapped neural network to construct a compact classifier. Empirical study made with standard datasets shows that SIS save on 70% of storage space without degrading the accuracy. It is independent of nature of the dataset and the tool used. © 2007, Inderscience Publishers."
"10.1504/IJBIDM.2007.016385","https://www.scopus.com/inward/record.uri?eid=2-s2.0-48649104062&doi=10.1504%2fIJBIDM.2007.016385&partnerID=40&md5=a227ad09041fe34ad4913bf3560074da","Traditionally, knowledge actionability has been investigated mainly by developing and improving technical interestingness. Recently, initial work on technical subjective interestingness and business-oriented profit mining presents general potential, while it is a long-term mission to bridge the gap between technical significance and business expectation. In this paper, we propose a two-way significance framework for measuring knowledge actionability, which highlights both technical interestingness and domain-specific expectations. We further develop a fuzzy interestingness aggregation mechanism to generate a ranked final pattern set balancing technical and business interests. Real-life data mining applications show the proposed knowledge actionability framework can complement technical interestingness while satisfy real user needs. © 2007, Inderscience Publishers."
"10.1504/IJBIDM.2007.016382","https://www.scopus.com/inward/record.uri?eid=2-s2.0-48649098556&doi=10.1504%2fIJBIDM.2007.016382&partnerID=40&md5=23a5acb73dd7ae45f9aff0bfc63f5634","In this paper, we propose a new method, association rules mining for Named Entity Recognition (NER) and co-reference resolution. The method uses several morphological and lexical features such as Pronoun Class (PC) and Name Class (NC), String Similarity (SP) and Position (P) in the text, into a vector of attributes. Applied to a corpus of newspaper in the Indonesian language, the method outperforms state-of-the-art maximum entropy method in name entity recognition and is comparable with state-of-the-art machine learning methods, decision tree, for co-reference resolution. © 2007, Inderscience Publishers."
"10.1504/IJBIDM.2007.016380","https://www.scopus.com/inward/record.uri?eid=2-s2.0-48649096631&doi=10.1504%2fIJBIDM.2007.016380&partnerID=40&md5=6678334b3a482a138deadf3147a8d465","Both transactional and analytical systems store data which, being accessible to unauthorised persons may result in a privacy violation. This issue has become especially important nowadays, due to more restrictive legislation concerning personal data protection and preserving data privacy. We introduce distribution-based methods of preserving data confidentiality in distributed spatial data warehouses without encrypting all sensitive data. In the paper we present how a specified security policy can be implemented in the data warehouse system as well as how analytical applications can obtain protected data from the databases. Finally, we present test results verifying efficiency of the latter operations. © 2007, Inderscience Publishers."
"10.1504/IJBIDM.2007.016379","https://www.scopus.com/inward/record.uri?eid=2-s2.0-48649090732&doi=10.1504%2fIJBIDM.2007.016379&partnerID=40&md5=9cb517fd84a6944b5cd8ade1715ea95a","This paper first discusses security issues for data warehousing. In particular, issues on building a secure data warehouse, secure data warehousing technologies as well as design issues are discussed. Our design of a secure data warehouse that enforces an Extended RBAC Policy is described next. This will be followed by a discussion of privacy issues for data warehousing. Finally directions for secure data warehouses are discussed. © 2007, Inderscience Publishers."
"10.1504/IJBIDM.2007.016381","https://www.scopus.com/inward/record.uri?eid=2-s2.0-48649085273&doi=10.1504%2fIJBIDM.2007.016381&partnerID=40&md5=4ca78f3b3a90f3f9127ef34d7ec1f23b","This paper presents the study of structural cohesion which is discussed in Social Network Analysis, but can also useful in other important application areas including investigative data mining for destabilising terrorist networks. Structural cohesion is defined as the number of actors who, if removed from a group, would disconnect it. We discuss structural cohesion concepts, such as cliques, n-cliques, n-clans and k-plexes to determine familiarity, robustness and reachability within subgroups of the 9/11 terrorist network. We also propose a methodology of detecting critical regions in covert networks; and then compare them to the topological characteristics of those networks. © 2007, Inderscience Publishers."
"10.2481/dsj.6.S806","https://www.scopus.com/inward/record.uri?eid=2-s2.0-37249082912&doi=10.2481%2fdsj.6.S806&partnerID=40&md5=622c17ec506f6e4aa6eae9a2241d456d","Speech corpus is the basis for analyzing the characteristics of speech signals and developing speech synthesis and recognition systems. In China, almost all speech research and development affiliations are developing their own speech corpora. We have so many different kinds numbers of Chinese speech corpora that it is important to be able to conveniently share these speech corpora to avoid wasting time and money and to make research work more efficient. The primary goal of this research is to find a standard scheme which can make the corpus be established more efficiently and be used or shared more easily. A huge speech corpus on 10 regional accented Chinese, RASC863 (a Regional Accent Speech Corpus funded by National 863 Project) will be exemplified to illuminate the standardization of speech corpus production."
"10.2481/dsj.6.S797","https://www.scopus.com/inward/record.uri?eid=2-s2.0-37249080617&doi=10.2481%2fdsj.6.S797&partnerID=40&md5=a8c031732ea6ea48df7e4fba8b49a1ec","In this paper, a Bi-angle Model Management method (BiMM) is proposed to manage models in virtual reality research. One angle is based on the model itself, which includes the model, model scheme, and texture; another angle is based on the model sort - each sort has its child sorts except leaf nodes. Based on this method, we have developed a model management application that has the following major functions: model management, model sort management, model query, model statistics, model registration into database as a whole, etc. With this method, researchers can manage model data conveniently and efficiently."
"10.2481/dsj.6.S813","https://www.scopus.com/inward/record.uri?eid=2-s2.0-37249062091&doi=10.2481%2fdsj.6.S813&partnerID=40&md5=ae15c2b4bddfa707c49a171e62196d96","Focusing on common searches in Chinese, this paper analyzes the web search retrieval mechanism for Chinese words concerning library-related core periodicals and meta-library vocabulary. Going into further details by counting and sorting the search words appearing at given periods of time on the web, the impact of web search on words related to library and information sciences as well as its development trend are intended to be revealed."
"10.2481/dsj.6.S824","https://www.scopus.com/inward/record.uri?eid=2-s2.0-37249053743&doi=10.2481%2fdsj.6.S824&partnerID=40&md5=6aad11e87debd6a9000802d7b33ca188","In this paper, a method of extracting rules with immune algorithms from information systems is proposed. Designing an immune algorithm is based on a sharing mechanism to extract rules. The principle of sharing and competing resources in the sharing mechanism is consistent with the relationship of sharing and rivalry among rules. In order to extract rules efficiently, a new concept of flexible confidence and rule measurement is introduced. Experiments demonstrate that the proposed method is effective."
"10.2481/dsj.6.S831","https://www.scopus.com/inward/record.uri?eid=2-s2.0-37249035911&doi=10.2481%2fdsj.6.S831&partnerID=40&md5=4f5188be284ebe20bb9e031c219f6ede","There are massive amounts of process data in the usual course of doing engineering. How to choose and accumulate these data to provide reference for newly-built projects in designing and building is a question that project superintendents face. We propose to construct a knowledge management platform for engineering project management to realize the potential of the accumulated decision-making data and study data classification and knowledge management, using architectural engineering data as an example."
"10.2481/dsj.6.S792","https://www.scopus.com/inward/record.uri?eid=2-s2.0-37249011762&doi=10.2481%2fdsj.6.S792&partnerID=40&md5=107cd7cb4d8ee1d925861cff81ea159b","Sharing scientific data sharing about water resources is an indispensable component of national fundamental data-sharing construction and the science and technology innovation system. To implement data sharing about water resources, which covers a broad extent and a great variety of data, is a difficult task because of the technical complexity and, more critically, non-technical factors, such as data policy, standards, and sharing circumstances. In this paper, the status of water resources scientific data sharing is analyzed at home and abroad, and some concepts are presented regarding these analyses. Using data from the Scientific Data Sharing Project, the authors study the system of data sharing about water resources and propose technological standards and managing and servicing systems of data sharing of water resources for the public as well as for the scientific research sectors of the nation."
"10.2481/dsj.6.145","https://www.scopus.com/inward/record.uri?eid=2-s2.0-37249000765&doi=10.2481%2fdsj.6.145&partnerID=40&md5=88600b3c453e02ba28d3370770a5fa25","Malaria pandemic (MP) has been linked to a range of serious health problems including premature mortality. The main objective of this research is to quantify uncertainties about impacts of malaria on mortality. A multivariate spatial regression model was developed for estimation of the risk of mortality associated with malaria across Ogun State in Nigeria, West Africa. We characterize different local governments in the data and model the spatial structure of the mortality data in infants and pregnant women. A flexible Bayesian hierarchical model was considered for a space-time series of counts (mortality) by constructing a likelihood-based version of a generalized Poisson regression model that combines methods for point-level misaligned data and change of support regression. A simple two-stage procedure for producing maps of predicted risk is described. Logistic regression modeling was used to determine an approximate risk on a larger scale, and geo-statistical (""Kriging"" ) approaches were used to improve prediction at a local level. The results suggest improvement of risk prediction brought about in the second stage. The advantages and shortcomings of this approach highlight the need for further development of a better analytical methodology."
"10.2481/dsj.6.137","https://www.scopus.com/inward/record.uri?eid=2-s2.0-37249072749&doi=10.2481%2fdsj.6.137&partnerID=40&md5=942c64fc7be413ecaebe300d17480646","The purpose of this study is to develop a plausible method to code and compile Buddhist texts from original Tibetan scripts into Romanized form. Using GUI (Graphical User Interface) based on Object Oriented Design, a dictionary of Tibetan characters can be easily made for Buddhist literature researchers. It is hoped that a computer system capable of highly accurate character recognition will be actively used by all scholars engaged in Buddhist literature research. In the present study, an efficient automatic recognition method for Tibetan characters is established. The result of the experiments performed is that the recognition rate achieved is 99.4% for 28,954 characters."
"10.2481/dsj.6.S782","https://www.scopus.com/inward/record.uri?eid=2-s2.0-36248973127&doi=10.2481%2fdsj.6.S782&partnerID=40&md5=43ffab5739282cbb92b778195b68f78e","In this paper, a new model for suppressing jammers to GPS receivers is proposed. In the model, circular antenna arrays combining minimum norm (min-norm) and linearly constrained minimum variance (LCMV) algorithms have been used for signal anti-jamming. Six GPS signals ' and two jammers' original incident direction were assumed respectively. The simulation was performed with a variation of the power of the two jammers and the element number of the circular antenna array. The simulation result indicates that by utilizing this new signal suppression model, nulls depths assigned to the jammer reach -238dBW when the number of element of circular antenna array is assumed to be 30. It also indicates that the stronger power of the jammer, the deeper nulls depths can be assigned with this new signal processing structure."
"10.2481/dsj.6.S789","https://www.scopus.com/inward/record.uri?eid=2-s2.0-36248945094&doi=10.2481%2fdsj.6.S789&partnerID=40&md5=f7e8c87f469af6dcc4a9a640e1c67a42","A new method for digitizing letters and Chinese characters is proposed."
"10.2481/dsj.6.S770","https://www.scopus.com/inward/record.uri?eid=2-s2.0-36248993971&doi=10.2481%2fdsj.6.S770&partnerID=40&md5=e29616544265cbe9f9e5b5a218afd71f","The data sharing system for resource and environment science databases of the Chinese Academy of Science (CAS) is of an open three-tiered architecture, which integrates the geographical databases of about 9 institutes of CAS by the mechanism of distributive unstructured data management, metadata integration, catalogue services, and security control. The data tiers consist of several distributive data servers that are located in each CAS institute and support such unstructured data formats as vector files, remote sensing images or other raster files, documents, multi-media files, tables, and other format files. For the spatial data files, format transformation service is provided. The middle tier involves a centralized metadata server, which stores metadata records of data on all data servers. The primary function of this tier is catalog service, supporting the creation, search, browsing, updating, and deletion of catalogs. The client tier involves an integrated client that provides the end-users interfaces to search, browse, and download data or create a catalog and upload data."
"10.2481/dsj.6.S779","https://www.scopus.com/inward/record.uri?eid=2-s2.0-36248987257&doi=10.2481%2fdsj.6.S779&partnerID=40&md5=0d74635945afd97512c76f654b2d39d6","Solar physics and heliospheric study have arrived at the era of the Virtual Solar Observatory (VSO). The VSO gathers data from observation stations and data centers geographically distributed throughout the world, providing data from multiple spectral bands. In this paper, the author briefly analyzes the existing data model and presents a description of our local data (that is produced at the Huairou Solar Observation Station) in a well-defined way. The work lays the foundation for querying our data in VSO. It forms the beginning for adding additional search elements and categories to the VSO, therefore providing an infrastructure for a web site of the solar physics domain in China."
"10.2481/dsj.6.S760","https://www.scopus.com/inward/record.uri?eid=2-s2.0-36248980387&doi=10.2481%2fdsj.6.S760&partnerID=40&md5=2350026a6e24237f7a1f500872e3d85d","This article first discusses the characteristics of scientific research in the humanities and social sciences (HSS), and then lays out its basic e-management demands. In building a scientific research e-management platform (SREMP), attention must be paid to the characteristics of its data and structure. The data in the SREMP of HSS domains have multiformity. That is, their structure should be an integrative multi-functioned information subsystem with a clearly graded data management mode, flexible user jurisdiction management functions, accurate and common retrieval systems, reliable security design, and flexibility to improve its operations, and ease in maintenance and amelioration. The article also describes the development tendencies of a SREMP."
"10.2481/dsj.6.121","https://www.scopus.com/inward/record.uri?eid=2-s2.0-36248958147&doi=10.2481%2fdsj.6.121&partnerID=40&md5=8ab0fac282fa7338f398849fdd4f0416","Herein, an extension to the object query language (OQL) for incorporating binary relational expressions is investigated. The extended query language is suitable for query submissions to an object oriented database, whose functionality is based upon the algebra of binary relations. Algebraic expressions, consisting of simple and multiple merged chains of binary relations, are stated in SQL syntax-based object queries, which are utilized by a multiwavefront algorithm mapped on a multi-directional multi-functional engine(M2FE), for object oriented parallel query processing. The proposed extension also attempts to solve other object oriented database issues, such as inheritance, relationships between objects and literals, and recursive queries."
"10.2481/dsj.6.S756","https://www.scopus.com/inward/record.uri?eid=2-s2.0-36248939191&doi=10.2481%2fdsj.6.S756&partnerID=40&md5=eb98aab9a432651039738989910d8587","This paper presents results of applying a machine learning technique, the Support Vector Machine (SVM), to the astronomical problem of matching the Infra-Red Astronomical Satellite (IRAS) and Sloan Digital Sky Survey (SDSS) object catalogues. In this study, the IRAS catalogue has much larger positional uncertainties than those of the SDSS. A model was constructed by applying the supervised learning algorithm (SVM) to a set of training data. Validation of the model shows a good identification performance (∼ 90% correct), better than that derived from classical cross-matching algorithms, such as the likelihood-ratio method used in previous studies."
"10.2481/dsj.6.S749","https://www.scopus.com/inward/record.uri?eid=2-s2.0-36248935578&doi=10.2481%2fdsj.6.S749&partnerID=40&md5=5032cc533d10be220d8438345df47862","In this paper, we apply data mining technologies to a 100-year global land precipitation dataset and a 100-year Sea Surface Temperature (SST) dataset. Some interesting teleconnections are discovered, including well-known patterns and unknown patterns (to the best of our knowledge), such as teleconnections between the abnormally low temperature events of the North Atlantic and floods in Northern Bolivia, abnormally low temperatures of the Venezuelan Coast and floods in Northern Algeria and Tunisia, etc. In particular, we use a high dimensional clustering method and a method that mines episode association rules in event sequences. The former is used to cluster the original time series datasets into higher spatial granularity, and the later is used to discover teleconnection patterns among events sequences that are generated by the clustering method. In order to verify our method, we also do experiments on the SOI index and a 100-year global land precipitation dataset and find many well-known teleconnections, such as teleconnections between SOI lower events and drought events of Eastern Australia, South Africa, and North Brazil; SOI lower events and flood events of the middle-lower reaches of Yangtze River; etc. We also do explorative experiments to help domain scientists discover new knowledge."
"10.2481/dsj.6.S715","https://www.scopus.com/inward/record.uri?eid=2-s2.0-36249028712&doi=10.2481%2fdsj.6.S715&partnerID=40&md5=131e920c65f4617ab33b910fea8a504b","In this paper, we introduce in brief the basic conditions of the Sino-Tibetan data resources, the STEDT project (the Sino-Tibetan Etymological Dictionary and Thesaurus) at the University of California, Berkeley and the STDP (The Sino-Tibetan Database and Retrieval System Project) at the Chinese Academy of Social Sciences (CASS), including the data structures, data volumes, and retrieval methods. We also discuss interdisciplinary information on the origin of East Asian civilization, which consists of several disciplines, including linguistics, molecular biology, human genetics, and archaeology."
"10.2481/dsj.6.S676","https://www.scopus.com/inward/record.uri?eid=2-s2.0-36249027066&doi=10.2481%2fdsj.6.S676&partnerID=40&md5=89f10b39221028ab3a616694affd4fdf","A critical overview of the current doubtful practice on presentation of correlated data in the physics literature and in the scientific and technological databases is presented. The simple rules to calculate the rounding thresholds to preserve the positive definiteness of the covariance and correlation matrices as well as the rounding thresholds for the components of the mean vector to keep them inside the ""non-rounded"" scatter region are formulated. Evidence that in the multivariate case there are severe limitations on the applicability of the linear differential law of uncertainty propagation is presented. The explicit relation of the number of input random variables I, the number of output variables D, and the order T of Taylor polynomials sufficient to preserve the self-consistent numerical presentation of the mean value of the vector function and its covariance matrix under nonlinear differential propagation procedure is obtained. It is stressed that the rounding thresholds for the safe rounding of correlated data impose the severe requirements on the storage and exchange formats of the correlated data that could not be met in the traditional publications on the paper but could be realized in the electronic media."
"10.2481/dsj.6.S723","https://www.scopus.com/inward/record.uri?eid=2-s2.0-36248980933&doi=10.2481%2fdsj.6.S723&partnerID=40&md5=2d792da4862bc7fd04e805d178fe9c46","With the development of the global economy, the impact of tropical cyclones has become far-reaching. Thus they are a fundamental issue to be addressed both nationally and globally. The socio-economic impact is particularly noticeable in developing countries, especially China. This paper begins with the effects of cyclones on regional and global economies. Then a brief introduction to the past and current situations and progress in cyclones forecasting and warning in China are presented. Finally the paper gives recommendations about improving and perfecting the tropical cyclone forecasting and warning systems."
"10.2481/dsj.6.S743","https://www.scopus.com/inward/record.uri?eid=2-s2.0-36248978144&doi=10.2481%2fdsj.6.S743&partnerID=40&md5=2e44d465a4bbcdd5c59db537c0014e8c","Event-related potential (ERP) is the measurement of the brain's electrical activity in response to different types of events, such as attention, words, thinking, or sounds. By measuring the brain's response to such events, we can learn how different types of information are processed. As the mass of recorded ERP data explodes, an automatic and accurate tool to store, manage, and retrieve data readily is of increasing concern in neuroinformatics. In this paper, we describe a relational ERP database that has been constructed using the SQL server 2000 database management system and an IIS web server that has been setup for data retrieval through a custom web interface (http://202.113.232.103:8088/ erpdb/index.asp). A novel database structure has been used to store ERP data of different activity channels, which provides a rapid and accurate way for data retrieval within any given range on the time zone with various searching options. The database is divided into: (1) subjects' information and record information and (2) ERP data, which has been structured and standardized in a database table supplemented with unrestricted text files. It can integrate or exchange data with other clinical databases or computer-based information systems through a program based on ADO techniques. Users are able to readily retrieve ERP data through the user-friendly web page interface. All online resources of the database are freely available to the scientific community. As the database develops further, we anticipate it will become a valuable tool that will make a great contribution to everyday clinical practice, teaching, and research work inneuroscience and psychology in the future."
"10.2481/dsj.6.S738","https://www.scopus.com/inward/record.uri?eid=2-s2.0-36248961646&doi=10.2481%2fdsj.6.S738&partnerID=40&md5=d417c8a8cfda9e7ec4137fd532fc55b8","Currently there are many methods of collecting geoscience data, such as station observations, satellite images, sensor networks, etc. All of these data sources from different regions and time intervals are combined in geoscience research activities today. Using a mixture of several different data sources may have benefits but may also lead to severe data quality problems, such as inconsistent data and missing values. There have been efforts to produce more consistent data sets from multiple data sources. However, because of the huge gaps in data quality among the different sources, data quality inequality among different regions and time intervals has still occurred in the resultant data sets. As the construction methods of these data sets are quite complicated, it would be difficult for users to know the data quality of a dataset not to mention the data quality for a specified location or a given time interval. In this paper, the authors address the problem by generating a data quality measure for all regions and time intervals of a dataset. The data quality measure is computed by comparing the constructed datases and their sources or other relevant data, using data mining techniques. This paper also demonstrates how to handle major quality problems, such as outliers and missing values, by using data mining techniques in the geoscience data, especially in global climate data."
"10.2481/dsj.6.S690","https://www.scopus.com/inward/record.uri?eid=2-s2.0-36248954473&doi=10.2481%2fdsj.6.S690&partnerID=40&md5=219a19b67748b10a074a05f6c5b7a781","Because of the development of modern-day satellites and other data acquisition systems, global climate research often involves overwhelming volume and complexity of high dimensional dataseis. As a data preprocessing and analysis method, the clustering method is playing a more and more important role in these researches. In this paper, we propose a spatial clustering algorithm that, to some extent, cures the problem of dimensionality in high dimensional clustering. The similarity measure of our algorithm is based on the number of top-k nearest neighbors that two grids share. The neighbors of each grid are computed based on the time series associated with each grid, and computing the nearest neighbor of an object is the most time consuming step. According to Tabler's ""First Law of Geography,"" we add a spatial window constraint upon each grid to restrict the number of grids considered and greatly improve the efficiency of our algorithm. We apply this algorithm to a 100-year global climate dataset and partition the global surface into sub areas under various spatial granularities. Experiments indicate that our spatial clustering algorithm works well."
"10.2481/dsj.6.S698","https://www.scopus.com/inward/record.uri?eid=2-s2.0-36248944113&doi=10.2481%2fdsj.6.S698&partnerID=40&md5=65998e18739a4d10e76f722bf405bd53","Digital World History is a new expression of world history (or maybe ""a new method for world history expression"") and a paradigm of world history description, study, and application by virtual informatization and recovery. It is also a comprehensive systematic study through dynamic marks, integrated description, and retrieval of human society evolution and its causality dependant on the theory and methodology of digitization information. It aims at breaking the limitation of diachronic language attributed to the process of history cognition, summation, and recovery, addressing a possible scheme to fuse historical factors in relation to changing history, dynamically applying a multiplicity of results so that the discipline of world history can meet the needs of the information-equipped society of the 21st century. In this article, the author uses theoretical modelling methods, resulting in a blueprint of the quality issue, namely the Digital World History premise, and a paradigm for setting the foundation and scientific data strategy as a basis for its necessity."
"10.2481/dsj.6.S667","https://www.scopus.com/inward/record.uri?eid=2-s2.0-36249024786&doi=10.2481%2fdsj.6.S667&partnerID=40&md5=d5003c3b6e6cf396014e94e051efea9c","This paper adopts the Panel Data Model based on the combination of time sequence and data of cross sections and does research on data of seventeen institutes in the Chinese Academy of Sciences. It makes empirical analysis of the Intellectual Property Rights (IPR) authorization and the relationship between R&D input and R&D scientific and technological output. Research results show that the role played by the Intellectual Property Rights Authorization in scientific and technological growth is notable and positively correlated."
"10.2481/dsj.6.114","https://www.scopus.com/inward/record.uri?eid=2-s2.0-36248952690&doi=10.2481%2fdsj.6.114&partnerID=40&md5=4e8359bafe13c15b5c9957cdd14ce982","Patents in general contain much novel technological information. This paper demonstrates that the usage of patent analysis can facilitate a unique scheme for tracking technology development. In this paper, the walking technique of the Japanese biped robot is tracked as an example. The searching method of the FI (file index) and F-term classification system developed by JPO (Japan Patent Office) was employed in this study, where all the related patent data were searched from the IPDL (Intellectual Property Digital Library). This study investigated an important technique applied to the humanoid biped robot that imitates the walking behavior of the human beings on two legs. By analyzing the patent information obtained, the relative research capabilities, technical strengths, and patent citation conditions among patent competitors were compared. Furthermore, a formulated technical matrix of patent map is established in this paper to indicate that the ZMP (Zero Moment Point) control means is the main technology to achieve stabilized walking control of the humanoid biped robot. This study also incorporates relevant academic journal findings and industrial information. Results presented herein demonstrate that patents can function not only as a map for tracking a technology trajectory, but also as a guide to the main development of a new technology in years to come."
"10.2481/dsj.6.S658","https://www.scopus.com/inward/record.uri?eid=2-s2.0-36248952687&doi=10.2481%2fdsj.6.S658&partnerID=40&md5=adb33fb61f7234f48e0beea36c96364b","Data are important for making decisions. However, the quality of the data affects the quality of decisions. Data mining as one of the most important sources of knowledge needs high quality data to mine, but there are not enough good quality data in many enterprises. By analyzing the reasons for low data quality systematically, a new method called data mining consulting for improving data quality has been established. It defines data quality in a wider sense from the view of data mining, finds data quality problems, and solves data quality problems by a series of methods. Its application shows that it has good practicality and can increase data quality considerable."
"10.2481/dsj.6.S652","https://www.scopus.com/inward/record.uri?eid=2-s2.0-36248952165&doi=10.2481%2fdsj.6.S652&partnerID=40&md5=fadc0543b1914b4bb211b2667f053c8a","3D modeling and visualization of geology volume is very important to interpret accurately and locate subsurface geology volume for mining exploration and deep prospecting. However, it faces a lack of information because the target area is usually unexplored and lacks geological data. This paper presents our experience in applying a 3D model of geology volume based on geophysics. This work has researched and developed a 3D visualization system. It is based on an OO (orientated object) approach and modular programming, uses the C ++ language and Microsoft .NET platform. This system has built first a high resistivity method and MT database. The system uses irregular tetrahedrons to construct its model and then finally has built the 3D geological model itself."
"10.2481/dsj.6.S641","https://www.scopus.com/inward/record.uri?eid=2-s2.0-36248936093&doi=10.2481%2fdsj.6.S641&partnerID=40&md5=ec4921eda41827c19bd0220ecf69afbf","The authors analyze the developmental framework of digital libraries in China and point out their current demand characteristics, development requirements, and developmental period. They then conclude that it is necessary to start up a new paradigm evolution of a digital library, from a traditional digital library to a virtual digital library. On that basis, they describe in detail several problems and developmental approaches that developing a virtual digital library must deal with, drawing lessons from the prototype DILIGENT."
"10.1504/IJBIDM.2007.015486","https://www.scopus.com/inward/record.uri?eid=2-s2.0-42049115225&doi=10.1504%2fIJBIDM.2007.015486&partnerID=40&md5=72883766055d1ce42f31bd29508d6c07","This research addresses the effects of the neural network s-Sigmoid function on Knowledge Discovery of Databases (KDD) in the presence of imprecise data. ANOVA testing and Tukey's Honestly Significant Difference statistics are conducted to investigate the impact of two factors: level of data missingness and imputation method. Data mining is based upon searching the concatenation of multiple databases that usually contain some amount of missing data along with a percentage of inaccurate data and noise. Therefore, analysis depends heavily on the accuracy of the database and on the chosen sample data to be used for model training and testing. © 2007, Inderscience Publishers."
"10.1504/IJBIDM.2007.015484","https://www.scopus.com/inward/record.uri?eid=2-s2.0-42049103669&doi=10.1504%2fIJBIDM.2007.015484&partnerID=40&md5=7e17a92cb31356c59bc971f01faaeb29","Survey data are often incomplete. Incomplete data are often mistreated and damages of missing values in survey are often overlooked in data mining. This study proposes a classification-based data mining approach to assessing the extent of damage of missing values in survey. Using this approach, an incomplete observation is translated into fuzzy observations. These fuzzy observations are used to test the classifier that has been trained by the complete data set of the survey. The test results provide a base for discovering knowledge about the implication of missing data and the quality of the survey. © 2007, Inderscience Publishers."
"10.1504/IJBIDM.2007.015485","https://www.scopus.com/inward/record.uri?eid=2-s2.0-42049100767&doi=10.1504%2fIJBIDM.2007.015485&partnerID=40&md5=243c62b3697c7c810c8c53a56ae99b9f","Intelligent data analysis techniques are useful for better exploring real-world data sets. However, the real-world data sets always are accompanied by missing data that is one major factor affecting data quality. At the same time, good intelligent data exploration requires quality data. Fortunately, Missing Data Imputation Techniques (MDITs) can be used to improve data quality. However, no one method MDIT can be used in all conditions, each method has its own context. In this paper, we introduce the MDITs to the KDD and machine learning communities by presenting the basic idea and highlighting the advantages and limitations of each method. © 2007, Inderscience Publishers."
"10.1504/IJBIDM.2007.015489","https://www.scopus.com/inward/record.uri?eid=2-s2.0-42049093202&doi=10.1504%2fIJBIDM.2007.015489&partnerID=40&md5=5089e873ada149f65601b16b10eefc32","The rapid increase of publications in marketing and related areas increasingly hampers the realisation of a general idea of what is 'hot' in the respective fields of interest. Topic Detection (TD), based on unsupervised text clustering, is a promising approach to tackle this problem. We introduce a new methodology that facilitates the determination of the number of topics discussed in a given text collection. By applying this approach to a text corpus which includes 12 international marketing and business journals we identify hot spots in marketing science. The approach may help both scientists and practitioners to systematically discover topics in digital information environments, as provided by the internet for instance. © 2007, Inderscience Publishers."
"10.1504/IJBIDM.2007.015488","https://www.scopus.com/inward/record.uri?eid=2-s2.0-42049085057&doi=10.1504%2fIJBIDM.2007.015488&partnerID=40&md5=8fbe3c9cebe17bf76f0e897e23b40e72","Designing a website which is helpful to its users requires knowledge of the users' preferences and their motivations. Therefore, a designer requires to anticipate the users' needs and structures the website accordingly. This paper implements a novel approach for selecting the users' preferred web pages. In this approach, the navigated web pages are modelled as a finite state graph, where each visited web page is defined as a state. This graph then is used to provide the framework for determining the users' interest. The viability of this approach is demonstrated with a user-created website scenario. © 2007, Inderscience Publishers."
"10.1504/IJBIDM.2007.015487","https://www.scopus.com/inward/record.uri?eid=2-s2.0-42049083549&doi=10.1504%2fIJBIDM.2007.015487&partnerID=40&md5=156ff833a60154aea6f9cde66344f6f0","In data mining, target data selection is important. The symptom of ""garbage in and garbage out"" is avoided to derive effective business rules in knowledge discovery in database. Chi-Square test is useful to eliminate irrelevant data before data mining processing due to wrong degrees of freedom, untested hypothesis, inconsistent estimation, inefficient method, data redundancy, data overdue, and data heterogeneity. This paper offers an online analytical processing method to derive association rules for the filtered Chi-Square tested data. The process applies a Frame metadata to trigger the Chi-Square testing for the update of the source data, and to derive rules continuously. © 2007, Inderscience Publishers."
"10.2481/dsj.6.S636","https://www.scopus.com/inward/record.uri?eid=2-s2.0-35549008877&doi=10.2481%2fdsj.6.S636&partnerID=40&md5=626b5b6debd9de867584dcb8d4edbaad","This paper describes a real-time online prototype automobile and truck driver-fatigue monitor. It uses remotely located charge-coupled-device cameras equipped with active infrared illuminators to acquire video images of the driver. Various visual cues that typically characterize the level of alertness of a person are extracted in real time and systematically combined to infer the fatigue level of the driver. The visual cues employed characterize eyelid movement, gaze movement, head movement, and facial expression. A probabilistic model is developed to model human fatigue and to predict fatigue based on the visual cues obtained. The simultaneous use of multiple visual cues and their systematic combination yields a much more robust and accurate fatigue characterization than using a single visual cue. This system was validated under real-life fatigue conditions with human subjects of different ethnic backgrounds, genders, and ages; with/without glasses; and under different illumination conditions. It was found to be reasonably robust, reliable, and accurate in fatigue characterization."
"10.2481/dsj.6.S620","https://www.scopus.com/inward/record.uri?eid=2-s2.0-35148901011&doi=10.2481%2fdsj.6.S620&partnerID=40&md5=0d5a32a3d1145f44f0bbe9c9a331577f","Every country's economic development affects all levels of its society and thus the results of its social science research. To make social science research better serve their economic development, many countries have established social science research institutes, among which are management research institutes more related to economic research institutes. Through comparative research of the locations and founding dates of the institutes in different countries, this article analyses the development trends and the relationship between economics and management research, providing us with the relevant experience and background for planning purposes."
"10.2481/dsj.6.107","https://www.scopus.com/inward/record.uri?eid=2-s2.0-35148900764&doi=10.2481%2fdsj.6.107&partnerID=40&md5=0b5d39a42f67dc7061d4b600e4f91bf6","The Inverted Exponential Distribution is studied as a prospective life distribution. In this paper, we derive Bayes ' estimators for the parameter 9 of inverted exponential distribution. These estimators are obtained on the basis of squared error and LINEX loss functions. Comparisons in terms of risks with the estimate of 6 under squared error loss and LINEX loss functions have been made. Finally, numerical study is given to illustrate the results."
"10.2481/dsj.6.S603","https://www.scopus.com/inward/record.uri?eid=2-s2.0-35148858447&doi=10.2481%2fdsj.6.S603&partnerID=40&md5=4b7e15c5840563b8b1c82c2764fe5205","Based on the theory of actuarial present value, a pension fund investment goal can be formulated as an objective function. The mean-variance model is extended by defining the objective loss function. Furthermore, using the theory of stochastic optimal control, an optimal investment model is established under the minimum expectation of loss function. In the light of the Hamilton-Jacobi-Bellman (HJB) equation, the analytic solution of the optimal investment strategy problem is derived."
"10.2481/dsj.6.99","https://www.scopus.com/inward/record.uri?eid=2-s2.0-35148845049&doi=10.2481%2fdsj.6.99&partnerID=40&md5=6d89509d7154794cf464a0fc01c7cd28","Linear regression (LR) and support vector regression (SVR) are widely used in data analysis. Geometrical correlation learning (GcLearn) was proposed recently to improve the predictive ability of LR and SVR through mining and using correlations between data of a variable (inner correlation). This paper theoretically analyzes prediction performance of the GcLearn method and proves that GcLearn LR and SVR will have better prediction performance than traditional LR and SVR for prediction tasks when good inner correlations are obtained and predictions by traditional LR and SVR are far away from their neighbor training data under inner correlation. This gives the applicable condition of GcLearn method."
"10.2481/dsj.6.S611","https://www.scopus.com/inward/record.uri?eid=2-s2.0-35148841946&doi=10.2481%2fdsj.6.S611&partnerID=40&md5=8d6bafb22a3d1763119ae33c635074e3","Teleconnection is a linkage between two climate events that occur in widely separated regions of the globe on a monthly or longer timescale. In the past, statistical methods have been used to discover teleconnections. However, because of the overwhelming volume and high resolution of dataseis acquired by modern data acquisition systems, these methods are not sufficient. In this paper, we propose a novel approach to finding teleconnections in global climate datasets using data mining technologies. We present experiments on real datasets and find some interesting teleconnections, including well-known ones such as ENSO. The experiments indicate that our method is usable and efficient."
"10.2481/dsj.6.S581","https://www.scopus.com/inward/record.uri?eid=2-s2.0-35148883289&doi=10.2481%2fdsj.6.S581&partnerID=40&md5=bb37537b00f3c54021e27f9efb816e77","Heavy rains and typhoons cause major disasters, including great economic losses and casualties, in China. Therefore, it is very important to research these torrential rains and typhoons in order to improve forecasting accuracy to mitigate their consequences. The Center of Disaster Reduction of the Chinese Academy of Sciences has cooperated with the Department of Atmospheric and Oceanic Science at the University of Maryland in the US in studying heavy rains and typhoons for over 7 years. Some of the findings from the study of heavy rains and typhoons are introduced in this paper."
"10.2481/dsj.6.S589","https://www.scopus.com/inward/record.uri?eid=2-s2.0-35148860591&doi=10.2481%2fdsj.6.S589&partnerID=40&md5=bfa78591e0889be2e0271480404b5a94","Within folded strands of a protein, amino acids (AAs) on every adjacent two strands form a pair of AAs. To explore the interactions between strands in a protein sheet structure, we have established an Internet-accessible relational database named SheetsPairs based on SQL Server 2000. The database has collected AAs pairs in proteins with detailed information. Furthermore, it utilizes a non-freetext database structure to store protein sequences and a specific database table with a unique number to store strands, which provides more searching options and rapid and accurate access to data queries. An IIS web server has been set up for data retrieval through a custom web interface, which enables complex data queries. Also searchable are parallel or anti-parallel folded strands and the list of strands in a specified protein."
"10.2481/dsj.6.S596","https://www.scopus.com/inward/record.uri?eid=2-s2.0-35148856153&doi=10.2481%2fdsj.6.S596&partnerID=40&md5=c621c5926dab980bbcb38173e8a10415","Monitoring the condition of rolling element bearings and defect diagnosis has received considerable attention for many years because the majority of problems in rotating machines are caused by defective bearings. In order to monitor conditions and diagnose defects in a rolling element bearing, a new approach is developed, based on the characteristic defect octave frequencies. The characteristic defect frequencies make it possible to detect the presence of a defect and diagnose in what part of the bearing the defect appears. However, because the characteristic defect frequencies vary with rotational speed, it is difficult to extract feature information from data at variable rotational speeds. In this paper, the characteristic defect octave frequencies, which do not vary with rotation speed, are introduced to replace the characteristic defect frequencies. Therefore feature information can be easily extracted. Moreover, based on characteristic defect octave frequencies, an envelope spectrum array, which associates 3-D visualization technology with extremum envelope spectrum technology, is established. This method has great advantages in acquiring the characteristics and trends of the data and achieves a straightforward and creditable result."
"10.2481/dsj.6.S571","https://www.scopus.com/inward/record.uri?eid=2-s2.0-35148840957&doi=10.2481%2fdsj.6.S571&partnerID=40&md5=9705b324d09d7bc92929dfd1a976ccd1","Collaborative research is one of the most noteworthy trends in the development of scientific research, and co-authored papers are some of the most important results of this research. With the speed-up of globalization, wider adoption of computers and advanced communication technologies, and more frequent academic exchanges and co-operation, collaborative research across organizations, regions, and fields has provided greater access to Chinese researchers in the humanities and social sciences. Accordingly, co-authored papers have witnessed considerable growth in number and proportion. The Social Sciences Citation Index (SSCI) and the Arts & Humanities Citation Index (A&HCI), published by the Institute for Scientific Information (USA), enjoy a high reputation worldwide as large-scale and comprehensive retrieval systems for international large comprehensive papers and citations. This article aims to reveal the trends of Chinese collaborative research in the humanities and social sciences from the perspective of bibliometrics and offer advice for Chinese researchers and managers in these fields, by analyzing Chinese co-authored papers in the humanities and social sciences indexed in the SSCI and A&HCI in the last decade (1995-2004)."
"10.2481/dsj.6.S548","https://www.scopus.com/inward/record.uri?eid=2-s2.0-35148893098&doi=10.2481%2fdsj.6.S548&partnerID=40&md5=dacc20783608b1dcf2f38bfb30209cd5","Because remote sensing data can offer information on the geographical landscape of the earth's surface, these data can be widely used in researching types of land and monitoring vegetation. In this paper, we discuss how to extract land types and vegetation from remote sensing data using the method of Non-Supervised Classification. These data provide the means to study types of soil and vegetation along a freeway."
"10.2481/dsj.6.S559","https://www.scopus.com/inward/record.uri?eid=2-s2.0-35148877189&doi=10.2481%2fdsj.6.S559&partnerID=40&md5=ef5818908dde0294421e44267123fa91","Association rule mining is a very important research topic in the field of data mining. Discovering frequent itemsets is the key process in association rule mining. Traditional association rule algorithms adopt an iterative method to discovery, which requires very large calculations and a complicated transaction process. Because of this, a new association rule algorithm called ABBM is proposed in this paper. This new algorithm adopts a Boolean vector ""relational calculus"" method to discovering frequent itemsets. Experimental results show that this algorithm can quickly discover frequent itemsets and effectively mine potential association rules."
"10.2481/dsj.6.S566","https://www.scopus.com/inward/record.uri?eid=2-s2.0-35148876974&doi=10.2481%2fdsj.6.S566&partnerID=40&md5=0e0ccb852c86337ddf362cc3e4688703","There are many large-size and difficult computational problems in mathematics and computer science. For many of these problems, traditional computers cannot handle the mass of data in acceptable timeframes, which we call an NP problem. DNA computing is a means of solving a class of intractable computational problems in which the computing time grows exponentially with problem size. This paper proposes a parallel algorithm model for the universal 3-SAT problem based on the Adleman-Lipton model and applies biological operations to handling the mass of data in solution space. In this manner, we can control the run time of the algorithm to be finite and approximately constant."
,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-35148875067&partnerID=40&md5=ea6119028216fcc8711dc7cd29aa7edb","According to the fact that high order cumulants (HOC) retain the phase information of signals and the HOC of the Gaussian color noise is always equal to zero, a new method of wavelet reconstruction is provided in this paper, based on 4th-order cumulants of non-Gaussian seismic signals. The feasibility of this method is demonstrated by the simulation of wavelet estimation for synthetic seismic traces. Furthermore, the seismic wavelet of field data processed with this method can be reconstructed correctly."
"10.2481/dsj.6.91","https://www.scopus.com/inward/record.uri?eid=2-s2.0-34548585349&doi=10.2481%2fdsj.6.91&partnerID=40&md5=de018443c4899d9bba872fef738f6c43","We present an approach for improving the relevance of search results by clustering the search results obtained for a query string with the help of a Concept Clustering Algorithm. The Concept Clustering Algorithm combines common phrase discovery and latent semantic indexing techniques to separate search results into meaningful groups. It looks for meaningful phrases to use as cluster labels and then assigns documents to the labels to form groups. The labels assigned to each document cluster provide meaningful information on the various documents available under that cluster. This provides a more interactive and easier way to probe through search results and identifying the relevant documents for the users using the search engine."
"10.2481/dsj.6.S511","https://www.scopus.com/inward/record.uri?eid=2-s2.0-34548635228&doi=10.2481%2fdsj.6.S511&partnerID=40&md5=74c2b3f1355e2e28b18cb5dcb8336cb9","The first case of arsenicosis was reported in China in the 1950s. That incident was associated with the so-called ""black foot disease. "" In the late 1970s and early 1980s, arsenic specific coetaneous changes were diagnosed in the Xinjiang Autonomous Region and subsequently in the Inner Mongolia Autonomous Region and Shanxi Province. Recently, endemic arsenicosis was also found in Jilin, Ningxia, Qinghai, and Anhui Provinces. The prevalence of arsenicosis in China is becoming more and more serious. In order to prevent and control it, many departments and institutes have begun to work in this field. They have made a great progress including also the sharing of arsenicosis data within a limited area. But the limited nature of this data sharing is a barrier for preventing and controlling arsenicosis. Only once data sharing is realized within the whole nation, can we discover the best way of eliminating arsenicosis. With this goal in mind, we have set up a rudimentary platform ofasenicosis data sharing. This gradually needs to be improved and improved."
"10.2481/dsj.6.S535","https://www.scopus.com/inward/record.uri?eid=2-s2.0-34548630908&doi=10.2481%2fdsj.6.S535&partnerID=40&md5=6f3f97ccd4636570df3678d132747568","The objective of this work is to investigate the cognitive development of semantic process and arithmetic calculation in childhood using event-related potential tools. Sixty children of three age groups (8-, 9- and 11-year-old groups) participated in the experiment. Each group included 10 girls and 10 boys. Stimuli were presented in two separate lists: semantic priming list and mental arithmetic list. Participants were instructed to decide whether the target word was a real Chinese character or not in semantic priming task and decide whether the production of the arithmetic operation (addition, subtraction, multiplication) was correct or false in mental arithmetic task. The main new observation was that the cognitive process reflected by major ERP components changes with the age growing up. In the lexical decision task, the amplitude of N400 elicited by semantic non-related target was significantly larger than that of a related target in all the children groups. The latency and amplitude of N400 component in 8-year-old group were larger than that of 11-year-old group. A similar RP component was elicited by either a Chinese single-character word or pseudo-word as reported by other authors. In mental arithmetic task, similar results were observed that the latencies of P2, N2, P3 in 8-year-group were longer than those of the 11-year-old group. These was no significant differences in amplitude across the three operation and age groups. These results suggest that semantic priming effects and mental arithmetic are developmental processes even in the early childhood. These two cognitive processes may be used to evaluate the development of language and arithmetic abilities."
"10.2481/dsj.6.S518","https://www.scopus.com/inward/record.uri?eid=2-s2.0-34548624367&doi=10.2481%2fdsj.6.S518&partnerID=40&md5=b133b00d554f2690acab183b46cd00c2","The project of the Digital Science & Technology Museum of China is constructed by the China Association for Science and Technology, Ministry of Education and Chinese Academy of Sciences, of which the objective is to integrate the high quality digital science popularization resources in China to construct an Internet-based national science popularization resources sharing platform. This project complies with the objective requirement of Internet-based science popularization in China that has groundbreaking significance. The construction at Stage I includes the construction of Museum A (museum), Museum B (experiencing museum), Museum C (resources museum), and an information service platform. During construction the project has been faced with many problems, such as how to integrate numerous science popularization resources for sharing and how to realize the function of science experiencing of the digital science & technology museum to attract the public. These problems will be explored and resolved in practice. The units participated in this project will, by combining the requirement for science popularization at the grass roots level, aggressively launch the application and extended service of this project."
"10.2481/dsj.6.S500","https://www.scopus.com/inward/record.uri?eid=2-s2.0-34548623179&doi=10.2481%2fdsj.6.S500&partnerID=40&md5=82b171230a43ed7740247fe4a1fe2851","This paper presents an ontology learning architecture that reflects the interaction between ontology learning and other applications such as ontology-engineering tools and information systems. Based on this architecture, we have developed a prototype system CHOL: a Chinese ontology learning tool. CHOL learns domain ontology from Chinese domain specific texts. On the one hand, it supports a semi-automatic domain ontology acquisition and dynamic maintenance, and on the other hand, it supports an auto-indexing and auto-classification of Chinese scholarly literature. CHOL has been applied in ethnology and anthropology for Chinese information organization and knowledge discovery."
"10.2481/dsj.6.S492","https://www.scopus.com/inward/record.uri?eid=2-s2.0-34548250486&doi=10.2481%2fdsj.6.S492&partnerID=40&md5=2a81c089eecf624b79a9bedbedde9700","This paper presents a framework for distributed service discovery based on VIRGO P2P technologies. The services are classified as multi-layer, hierarchical catalogue domains according to their contents. The service providers, which have their own service registries such as UDDIs, register the services they provide and establish a virtual tree in a VIRGO network according to the domain of their service. The service location done by the proposed strategy is effective and guaranteed. This paper also discusses the primary implementation of service discovery based on Tomcat/Axis and jUDDI."
"10.2481/dsj.6.S474","https://www.scopus.com/inward/record.uri?eid=2-s2.0-34548241286&doi=10.2481%2fdsj.6.S474&partnerID=40&md5=b67e349e6e6e63b90ad6de12732d53a0","Photometric redshifts have been regarded as efficient and effective measures for studying the statistical properties of galaxies and their evolution. In this paper, we introduce SVM_Light, a freely available software package using support vector machines (SVM) for photometric redshift estimation. This technique shows its superiorities in accuracy and efficiency. It can be applied to huge volumes of dataseis, and its efficiency is acceptable. When a large representative training set is available, the results of this method are superior to the best ones obtained from template fitting. The method is used on a sample of 73,899 galaxies from the Sloan Digital Sky Survey Data Release 5. When applied to processed data sets, the RMS error in estimating redshifts is less than 0.03. The performances of various kernel functions and different parameter sets have been compared. Parameter selection and uniform data have also been discussed. Finally the strengths and weaknesses of the approach are summarized."
,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-34548230524&partnerID=40&md5=b161ebf74554eeeabeb4edcbab03495a","In recent years, Geographic Information Systems (GIS) have gradually changed from using the traditional 2D map expression to 3D visualization. The combination of visual techniques and GIS is a multi discipline, leading edge field, the development of which needs advancement in many fields. This paper introduces related theories and algorithms of Digital Elevation Model (DEM) visualization. Advantages of the Triangle Irregular Network (TIN) model and data structure are illustrated. The algorithms include the visualization process and methods to increase the realism of the DEM. Illumination models and a special technique to map remote sensing images onto DEM are also presented."
,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-34548221463&partnerID=40&md5=517b03e2364533f63448da9ad95362d2","An efficient method is proposed to diagnose a type of abnormal data. We first start with analyzing an example, carry through with development of the theory once more, and finally list the method steps and its application fields. Experiments show that we need more important and better ways to diagnose abnormal data and eliminate them along with the development of information technology and control technology. The quality of measured data is improved by the use of this technique."
"10.2481/dsj.6.S460","https://www.scopus.com/inward/record.uri?eid=2-s2.0-34548269079&doi=10.2481%2fdsj.6.S460&partnerID=40&md5=c715a68f04154d27e9d11e18efa37f3f","Coal is the dominant source of energy in China, but about 50% of the coal resource is left underground unmined. Because of this, the ""long-tunnel, large section, two-stage"" Underground Coal Gasification (UCG) technology has been put forward, and the UCG model platform has been built. Simulation tests are underway and some gasification parameters have been obtained. Five field trials have been completed, which have produced gas with a heating value of about 4.18MJ/m3. Gas containing more than 40% hydrogen and a heating value above 8.36MJ/m3 is produced at two-stage gasification."
"10.2481/dsj.6.S467","https://www.scopus.com/inward/record.uri?eid=2-s2.0-34548261368&doi=10.2481%2fdsj.6.S467&partnerID=40&md5=5c3da440c8e78a27f5c7ac8aa3c5fc54","As carriers and circulators of information, materials databases have been more and more extensively applied in the production of, scientific research on, and circulation and application of modern materials. In this paper, the history of materials databases is presented, the present status of domestic materials databases is discussed, and the development trends of Internet use, standardization, intelligence, and commercialization of materials databases are also discussed."
"10.2481/dsj.6.S441","https://www.scopus.com/inward/record.uri?eid=2-s2.0-34548283584&doi=10.2481%2fdsj.6.S441&partnerID=40&md5=beb43160a89a51ff354e439513bea3d1","Rapid increases of user data from terabytes to petabytes have created new challenges in data archiving. Modern data archive systems require higher adaptivity, reliability, and performance than traditional data archive systems can provide. Recently Hierarchical Storage Management (HSM) has been applied to a data archive that stores data in a multi-level storage system according to access frequency. In this paper, we describe the design and implementation of a novel HSM-based data archive system called T-Archive, which can meet the above requirements for order-of-magnitude scaling of storage."
"10.2481/dsj.6.S445","https://www.scopus.com/inward/record.uri?eid=2-s2.0-34548272141&doi=10.2481%2fdsj.6.S445&partnerID=40&md5=e1c1e6f50cf08eafbd3202fac6133f3e","Very high spatial resolution remote sensing images have applications in many fields. However, research on the intelligent interpretation of such images is insufficient partly because of their the complexity and large size. In this study, a high spatial resolution remote sensing image intelligent interpretation system (HSR-RSIIIs) was designed with image segmentation, a geographical information system, and a data-mining algorithm. Some key methods such as image segmentation, feature extraction, feature selection, and classification algorithm for interpreting high spatial resolution remote sensing image have been studied. A land cover classification experiment was performed in the Zhuzhou area using a Quickbird multi-spectral image. The classification results were consistent with the visual interpretation results. In additional, the proposed interpretation method was compared with the traditional pixelbased method. The results indicate that the method proposed in the literature is more effective and intelligent than that used previously."
"10.2481/dsj.6.S453","https://www.scopus.com/inward/record.uri?eid=2-s2.0-34548257310&doi=10.2481%2fdsj.6.S453&partnerID=40&md5=79051592f0651bb0629131bfdfbf9d3f","Supported by the World Bank, the Integrated Information System for Natural Disaster Mitigation (ISNDM), including the operational service system and network telecommunication system, has been in development for three years in the Center of Disaster Reduction, Chinese Academy of Sciences, based on the platform of the GIS software Arcview. It has five main modules: disaster background information, socio- economic information, disaster-induced factors database, disaster scenarios database, and disaster assessment. ISNDM has several significant functions, which include information collection, information processing, data storage, and information distribution. It is a simple but comprehensive demonstration system for our national center for natural disaster reduction."
"10.2481/dsj.6.S420","https://www.scopus.com/inward/record.uri?eid=2-s2.0-34548268578&doi=10.2481%2fdsj.6.S420&partnerID=40&md5=8e1fbad048946a40072127f104b99a95","The thermodynamic descriptions of the perovskite phase SrZrO 3 and the TbBr 3 -KBr molten salt system were carried out using the available experimental information. Special attention was paid to the structural behavior of SrZrO 3 and the decomposition of K 3 TbBr 6 at low temperature, respectively, to illustrate how to select an appropriate thermodynamic model based on crystal structure and chemistry information, how to identify and resolve the inconsistency between various kinds of experimental data, and how to use thermodynamic modeling as a basic tool in the development and optimization of materials and process. In the present work, different structures of SrZrO 3 were explained by thermodynamic calculation and confirmed with experimental technologies. The decomposition of the compound K 3 TbBr 6 at about 593 K was detected by the present thermodynamic calculation and the new complementary experimental measurements. Comparison between the calculated and measured phase diagrams as well as thermodynamic quantities provided the final test of the overall consistency between the reliable experimental information and the present modeling and thermodynamic computation."
"10.2481/dsj.6.S435","https://www.scopus.com/inward/record.uri?eid=2-s2.0-34548236406&doi=10.2481%2fdsj.6.S435&partnerID=40&md5=68909e1cef8b4c73fd9d5aabbfe441fd","In order to achieve the potential of existing power stations, increase the efficiency of waterpower, and change the running of power stations from an individual plant basis to centralization, many people have researched the automation of power stations over the years with excellent results. However, in reality there are no power stations that have been fully automated. This paper proposes a control system based on Multi-Agent Theory, including a management agent, communication agent, control agent, echo agent, and so on. These agents are able to harmonize automation and intelligence to provide effective control of the running of a hydropower station. The end goal is a real control system that can operate independently of human intervention."
"10.2481/dsj.6.S429","https://www.scopus.com/inward/record.uri?eid=2-s2.0-34548217747&doi=10.2481%2fdsj.6.S429&partnerID=40&md5=0d5ec216f7d69552af59b83688e9b9a7","Data mining is concerned with the extraction of useful knowledge from various types of data. Medical data mining has been a popular data mining topic of late. Compared with other data mining areas, medical data mining has some unique characteristics. Because medical files are related to human subjects, privacy concerns are taken more seriously than other data mining tasks. This paper applied data separation-based techniques to preserve privacy in classification of medical data. We take two approaches to protect privacy: one approach is to vertically partition the medical data and mine these partitioned data at multiple sites; the other approach is to horizontally split data across multiple sites. In the vertical partition approach, each site uses a portion of the attributes to compute its results, and the distributed results are assembled at a central trusted party using a majority-vote ensemble method. In the horizontal partition approach, data are distributed among several sites. Each site computes its own data, and a central trusted party is responsible to integrate these results. We implement these two approaches using medical datasets from UCI KDD archive and report the experimental results."
"10.2481/dsj.6.64","https://www.scopus.com/inward/record.uri?eid=2-s2.0-34548318695&doi=10.2481%2fdsj.6.64&partnerID=40&md5=3252be9db51be3311bec39886f666b14","Spatial Data Infrastructures (SDIs) have been developing in some countries for over 10 years but still suffer from having a relatively small installed base. Most SDIs will soon converge around a service-oriented-architecture (SOA) using IT standards promulgated primarily by the Open Geospatial Consortium (OGC) and ISO Technical Committee 211. There are very few examples of these types of architected SDIs in action, and as a result little detailed information exists on suitable governance models. This paper discusses the governance issues that are posed by SOA-based SDIs, particularly those issues surrounding standards and services management, with reference to an Australian marine case study and the general literature. A generalised governance framework is then postulated using an idealised use case model which is applicable for ""bottom-up,"" community-based initiatives. This model incorporates guiding principles and motivational and self-regulation instruments that are characteristically found in successful open source development activities. It is argued that harnessing an open development model, using a voluntary workforce, could rapidly increase the size of the SDI installed base and importantly defray infrastructure build costs."
"10.2481/dsj.6.S385","https://www.scopus.com/inward/record.uri?eid=2-s2.0-34547538334&doi=10.2481%2fdsj.6.S385&partnerID=40&md5=3a93133ee089e3cd3031bedc6486e73d","Based on the Small-World model of CAS e-Science and the power low of Internet, this paper presents a scalable CAS e-Science Grid framework based on virtual region called Virtual Region Grid Framework (VRGF). VRGF takes virtual region and layer as logic manage-unit. In VRGF, the mode of intra-virtual region is pure P2P, and the model of inter-virtual region is centralized. Therefore, VRGF is decentralized framework with some P2P properties. Further more, VRGF is able to achieve satisfactory performance on resource organizing and locating at a small cost, and is well adapted to the complicated and dynamic features of scientific collaborations. We have implemented a demonstration VRGF based Grid prototype-SDG."
"10.2481/dsj.6.S393","https://www.scopus.com/inward/record.uri?eid=2-s2.0-34547537998&doi=10.2481%2fdsj.6.S393&partnerID=40&md5=e5de9ad7913cb2ae4438f5ce286ff79b","This paper discusses the principle of electronic data and retrieval methods for the Secret History of the Mongols, which is a great classical historical work written in the 13th century with Chinese characters transliterated from Mongol. This handwritten work contains rather rich text information, which should be the contents of forming an electronic database. There are in the original book multi-types of information, including layouts, volumes, chapters, characters, interlinear translation, segments, and Chinese translation, each format of which has been approached in detail and divided separately with markers. On the basis of analysis, our project builds up a complete electronic retrieval system for this great book, which resolves the return to the original shape of the archaic handwriting form with three lines representing one content. The sorting methods of the system are also designed according to the original text formats, namely concordance technology, which can print out retrieved objects with their contexts, retrieve with statistical data, and freely browse search."
"10.2481/dsj.6.S408","https://www.scopus.com/inward/record.uri?eid=2-s2.0-34547516046&doi=10.2481%2fdsj.6.S408&partnerID=40&md5=2d721665c806348ad5196400a3335884","In this paper, the author analyzes the gap in knowledge-based economy development between China and the United States, explores its cause, and gives some constructive suggestions to promote Chinese knowledge-based economy development. The paper has three parts. The first is a brief literature review. The author concludes that at present the indicator model is more proper than the econometric model and statistical framework. In the second part, the author develops an indicator model with four dimensions: knowledge input, human capital, ICT application, and innovation performance. Each dimension has several different indicators. The Analytic Hierarchy Process (AHP) is used to give those indicators different weights and to compose them into a compound index in all hierarchies. On the basis of the above methodology, the third part calculates and compares the overall index and four dimension index differences of the development of Chinese and American, knowledge-based economies. There is a large gap between China and the United States. The dimension of innovation performance embodies this gap. The next dimensions are human capital, knowledge input, and ICT application in turn. The author then discusses reasons for such a great lag between China and the United States. The conclusion sums up the main challenges and puts forward some suggestions to promote the Chinese knowledge-based economy development."
,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-34547552643&partnerID=40&md5=b732da415253c09014cd2f7ab3a3b394","Many databases and platforms for human brain data have been established in China over the years, and metadata plays an important role in understanding and using them. The BrainBank Metadata Specification for the Human Brain Project and Neuroinformatics provides a structure for describing the context and content information of BrainBank databases and services. It includes six parts: identification, method, data schema, distribution of the database, metadata extension, and metadata reference The application of the BrainBank Metadata Specification will promote conservation and management of BrainBank databases and platforms, it will also greatly facilitate the retrieval, evaluation, acquisition, and application of the data."
"10.2481/dsj.6.S379","https://www.scopus.com/inward/record.uri?eid=2-s2.0-34547523124&doi=10.2481%2fdsj.6.S379&partnerID=40&md5=05442fb155859c846962ea7a53fa7f7f","In order to use distributed and heterogeneous scientific databases effectively, semantic heterogeneities have to be detected and resolved. To solve this problem, we propose architecture for managing metadata and metadata schema using a metadata registry. A metadata registry is a place to keep facts about characteristics of data that are necessary for data sharing and exchange in a specific domain. This paper will explore the role of metadata registries and describe some of the experiences of implementing the registry."
"10.2481/dsj.6.S400","https://www.scopus.com/inward/record.uri?eid=2-s2.0-34547519527&doi=10.2481%2fdsj.6.S400&partnerID=40&md5=aca78a0c1e468109e6dda30eed592225","This paper presents in detail the project ""Development and Service of World Data Center for Seismology, Beijing,"" including its background, construction tasks, main results, societal effects, and perspective for future development."
"10.2481/dsj.6.S404","https://www.scopus.com/inward/record.uri?eid=2-s2.0-34547512700&doi=10.2481%2fdsj.6.S404&partnerID=40&md5=1964d1a3eaf0f2528fe97065c4f19c1c","The World Data Center (WDC) for Geophysics, Beijing, was founded in 1988. Supported by The Chinese Academy of Science and The Ministry of Science and Technology, our center has made much progress in recent years. The center has not only established the database to restore data which contain heat flow data, geomagnetic data, gravity data, etc. but also put them on the Internet (http://gp.wdc.cn) to provide free data service. The center has expended a great deal of effort to rescue the magnetograms observed 100 years ago by the Sheshan Observatory, the earliest geomagnetic observatory in China. The geophysics data of our center are abundant, and the way to get the data and information from the website is very simple and easily obtainable. In the future, the center will edit more data and construct a strong, convenient database in order to provide the better service to users."
"10.1504/IJBIDM.2007.013935","https://www.scopus.com/inward/record.uri?eid=2-s2.0-34250885168&doi=10.1504%2fIJBIDM.2007.013935&partnerID=40&md5=24fb39dd191dbbfb88f349a32b3ce270","Data mining the American National Election Study (ANES), a rich but disparate source of information about Americans' vote choices, is the focus of this research. Specifically, we use data mining classification to construct a decision tree to select important predictors of the vote from the more than 900 items that compose the ANES. We use an iterative domain expert and data mining process to identify a limited number of survey questions intended to predict for which party an individual will vote in a presidential election or whether that individual will vote at all. Copyright © 2007 Inderscience Enterprises Ltd."
"10.1504/IJBIDM.2007.013938","https://www.scopus.com/inward/record.uri?eid=2-s2.0-34250866221&doi=10.1504%2fIJBIDM.2007.013938&partnerID=40&md5=3c9c72a1ff4d12fa2f1f80477c497fb2","We show that for binary classification variables, Gini and Pearson purity, measures yield exactly the same tree, provided all the other parameters of the algorithms are identical. A counterexample for ternary classification variables is given. Copyright © 2007 Inderscience Enterprises Ltd."
"10.1504/IJBIDM.2007.013937","https://www.scopus.com/inward/record.uri?eid=2-s2.0-34250858056&doi=10.1504%2fIJBIDM.2007.013937&partnerID=40&md5=01acbf55d5e8a02ea41ec10618593534","Data mining is a process to extract useful knowledge from large amounts of data. To conduct data mining, we often need to collect data. However, sometimes the data are distributed among various parties. Privacy concerns may prevent the parties from directly sharing the data and some types of information about the data. How multiple parties can collaboratively conduct data mining without breaching data privacy presents a grand challenge. In this paper, we propose a randomisation-based scheme for multi-parties to conduct data mining computations without disclosing their actual data sets to each other. Copyright © 2007 Inderscience Enterprises Ltd."
"10.1504/IJBIDM.2007.013939","https://www.scopus.com/inward/record.uri?eid=2-s2.0-34250847067&doi=10.1504%2fIJBIDM.2007.013939&partnerID=40&md5=4dd513e9af24d7ff443f487620bec583","In this paper, we present a model to extract the turning point from economies of scale to enhancement of marketable quality. We apply the real values of some leading manufacturing corporations in Japan to our proposed model to analyse its accuracy. By analysis, we found that the theoretical and real standard values of the marketable quality indicator for the rate of operation at the break event point were both 0.6. We evaluated the proposed model and verified that the fair relationship better reflects the real economical situation of corporations than the unfair relationship. The proposed model gives a good approximation of the economical trends of corporations. Copyright © 2007 Inderscience Enterprises Ltd."
"10.1504/IJBIDM.2007.013934","https://www.scopus.com/inward/record.uri?eid=2-s2.0-34250838052&doi=10.1504%2fIJBIDM.2007.013934&partnerID=40&md5=6928a613d13ef8e733e366cdc8521517","Clustering is a well-known method of data mining, which aims at extracting useful information from a data set. Clusters could be either crisp (having well-defined boundaries) or fuzzy (with vague boundaries) in nature. The present paper deals with fuzzy clustering of psychosis data. A set of statistically generated psychosis data are clustered using Fuzzy C-Means (FCM) algorithm and entropy-based method and its proposed extensions. From the clusters, we finally decide on patient distributions response-wise. Comparisons are made of the above algorithms, in terms of quality of clusters made and their computational complexity. Finally, the multidimensional best set of clusters are mapped into 2-D for visualisation, using a Self-Organising Map (SOM). Copyright © 2007 Inderscience Enterprises Ltd."
"10.1504/IJBIDM.2007.013936","https://www.scopus.com/inward/record.uri?eid=2-s2.0-34250817472&doi=10.1504%2fIJBIDM.2007.013936&partnerID=40&md5=911c00865592fa7bcd83a84396bdac13","Data Mining (DM) helps deliver tremendous insights for businesses into the problems they face and aids in identifying new opportunities. It further helps businesses to solve more complex problems and make smarter decisions. DM is a potentially powerful tool for companies; however, more research is needed to measure the benefits of DM. This paper represents a study of the effectiveness of DM in a commercial perspective. First, statistical issues are given. It is followed by data accuracy and standardisation. Diverse problems related to the information used for conducting a DM research are identified. Also, the technical challenges and potential roadblocks in an organisation itself are described. Copyright © 2007 Inderscience Enterprises Ltd."
"10.2481/dsj.6.GH37","https://www.scopus.com/inward/record.uri?eid=2-s2.0-34548452441&doi=10.2481%2fdsj.6.GH37&partnerID=40&md5=68f3e2894f55bdb9e59902056d35e12c","Gas hydrates, low molecular weight gas molecules trapped in water-ice ""cages,"" have gained international attention over the last few years for energy, safety, and environmental reasons. In 2000 CODATA (Committee on Data for Science and Technology, International Council for Science) established a task group for Data on Natural Gas Hydrates, which aims to develop a comprehensive information system of all aspects of natural gas hydrates. As important participants of this task group, Chinese scientists have constructed a gas hydrate information system in China. Now they are developing the data-based research support system of gas hydrates based on the existing gas hydrate information system. In this paper, the authors introduce the idea of constructing a data-based research support system of gas hydrates in detail, including the high level framework design of constructing this system, system platform and functions, and database designs. We believe this system will definitely facilitate the data sharing and scientific research of gas hydrates."
"10.2481/dsj.6.GH25","https://www.scopus.com/inward/record.uri?eid=2-s2.0-34548446682&doi=10.2481%2fdsj.6.GH25&partnerID=40&md5=a93c6f913ca07f4fc2e02b58b6904268","Natural gas hydrates, as an important potential fuels, flow assurance hazards, and possible factors initiating the submarine geo-hazard and global climate change, have attracted the interest of scientists all over the world. After two centuries of hydrate research, a great amount of scientific data on gas hydrates has been accumulated. Therefore the means to manage, share, and exchange these data have become an urgent task. At present, metadata (Markup Language) is recognized as one of the most efficient ways to facilitate data management, storage, integration, exchange, discovery and retrieval. Therefore the CODATA Gas Hydrate Data Task Group proposed and specified Gas Hydrate Markup Language (GHML) as an extensible conceptual metadata model to characterize the features of data on gas hydrate. This article introduces the details of modeling portion of GHML."
"10.2481/dsj.6.GH6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-34548441514&doi=10.2481%2fdsj.6.GH6&partnerID=40&md5=0d394ee4fa70296a9b301739f9ab0c8a","Data and information exchange are crucial for any kind of scientific research activities and are becoming more and more important. The comparison between different data sets and different disciplines creates new data, adds value, and finally accumulates knowledge. Also the distribution and accessibility of research results is an important factor for international work. The gas hydrate research community is dispersed across the globe and therefore, a common technical communication language or format is strongly demanded. The CODATA Gas Hydrate Data Task Group is creating the Gas Hydrate Markup Language (GHML), a standard based on the Extensible Markup Language (XML) to enable the transport, modeling, and storage of all manner of objects related to gas hydrate research. GHML initially offers an easily deducible content because of the text-based encoding of information, which does not use binary data. The result of these investigations is a custom-designed application schema, which describes the features, elements, and their properties, defining all aspects of Gas Hydrates. One of the components of GHML is the ""Field Data"" module, which is used for all data and information coming from the field. It considers international standards, particularly the standards defined by the W3C (World Wide Web Consortium) and the OGC (Open Geospatial Consortium). Various related standards were analyzed and compared with our requirements (in particular the Geographic Markup Language (ISO19136, GML) and the whole ISO19000 series). However, the requirements demanded a quick solution and an XML application schema readable for any scientist without a background in information technology. Therefore, ideas, concepts and definitions have been used to build up the modules of GHML without importing any of these Markup languages. This enables a comprehensive schema and simple use. An extensive documentation ensures the usability of the ""Field Data"" module consisting of a detailed explanation integrated in the application schema, an HTML-based document, and a detailed documentation. Because of the close collaboration of gas hydrate experts and specialists in Geoinformatics, the application schema of GHML is user-oriented and contains all possible aspects of this research field. The usability is the assessment factor for GHML."
"10.2481/dsj.6.GH18","https://www.scopus.com/inward/record.uri?eid=2-s2.0-34548408131&doi=10.2481%2fdsj.6.GH18&partnerID=40&md5=b45d456b7dd78297b6e23a3e2c355cc2","Laboratory Hydrate Data is one of the three constituent modules comprising the XML based Gas Hydrate Markup Language (a.k.a. GHML) schema, the others being Field Hydrate data by Löwner et al. and Hydrate Modeling by Wang et al. This module describes the characteristics of natural and synthetic gas hydrates as they pertain to data acquired via analysis within a laboratory environment. Such data include the preservation history (i.e.: technique, pressurization gas and pressure, etc), Macroscopic data (i.e.: water-sediment ratio, appearance, P-T behavior, etc) as well as that of the Microscopic realm."
"10.2481/dsj.6.GH1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-34548383789&doi=10.2481%2fdsj.6.GH1&partnerID=40&md5=2b677bc71ea7033cb609ccefc874be43","Natural gas hydrates may contain more energy than all the combined other fossil fuels, causing hydrates to be a potentially vital aspect of both energy and climate change. This article is an overview of the motivation, history, and future of hydrate data management using a CODATA vehicle to connect international hydrate databases. The basis is an introduction to the Gas Hydrate Markup Language (GHML) to connect various hydrate databases. The accompanying four articles on laboratory hydrate data by Smith et al., on field hydrate data by Löwner et al., on hydrate modeling by Wang et al., and on construction of a Chinese gas hydrate system by Xiao et al. provide details of GHML in their respective areas."
"10.2481/dsj.6.S353","https://www.scopus.com/inward/record.uri?eid=2-s2.0-34547525144&doi=10.2481%2fdsj.6.S353&partnerID=40&md5=b795ef5ca2d63707525dc1183af8f485","This paper analyzes the factors that influence the economic growth of the provinces of China by means of a Panel Data Model. Traditional analytical methods of economic growth are compared with a Panel Data Model. The results of empirical research indicate that the changes of fixed assets investment, gross domestic export, and macroeconomic policies will affect China's GDP. It is finally concluded that export is the driving force behind economic growth in China. This conclusion is quite different from traditional analysis."
"10.2481/dsj.6.S364","https://www.scopus.com/inward/record.uri?eid=2-s2.0-34547524826&doi=10.2481%2fdsj.6.S364&partnerID=40&md5=a46361ca38207f5b9be497006fc1741e","Based on data for the years 1995 to 2002, this paper has established a panel data model that reflects the relationship between China's foreign direct investment and China's exports, and regarding this, empirical analysis is made. The selected countries and regions include: Hong Kong of China, China's Taiwan, Japan, South Korea, the European Union, and the United States. We have found that the relationship between the accumulated FDI (FDE stock) of different countries and regions in China and Chinese exports to the target countries is quite strong."
"10.2481/dsj.6.S324","https://www.scopus.com/inward/record.uri?eid=2-s2.0-34547545946&doi=10.2481%2fdsj.6.S324&partnerID=40&md5=89768afe54cc1b16953ac8ed0d7b8a8e","Using 64 ms count data of long gamma-ray bursts (LBs, T90 &gt; 2.6 s), we analyze the quantity named relative spectral lag (RSL), τ31/FWHM(1) = τrel, 31. We investigate in detail the properties of the RSL for a sample of nine LBs, using the general cross-correlation technique that includes the lag between two different energy bands. We find that the distribution of RSLs is normal and has a mean value of 0.1. Our important discovery is that redshift (z) and peak luminosity (L p) are strongly correlated with the RSL, which can be measured easily and directly, making the RSL a good redshift and peak luminosity indicator. In addition, we find that the redshift and luminosity estimator can also hold for short gamma-ray bursts (SBs, T90 &lt; 2.6 s). With it, we estimate the median of redshift and peak luminosity of SBs to be about z≤P.06 and Lp∼1.68×1048 erg/s, which are in excellent agreement with the results suggested by some previous authors. We thus argue that the sources including SBs and LBs with positive spectral lags might be one united category with the same physical process."
"10.2481/dsj.6.S333","https://www.scopus.com/inward/record.uri?eid=2-s2.0-34547529206&doi=10.2481%2fdsj.6.S333&partnerID=40&md5=54c7c8036d57e70eb7620ee3be4c12a5","People believe what they can see. The Poles exist as a frozen dream to most people. The International Polar Year wants to break the ice (so to speak), open up the Poles to the general public, support current polar research, and encourage new research projects. The IPY officially begins in March, 2007. As part of this effort, the U.S. Geological Survey (USGS) and the British Antarctic Survey (BAS), with funding from the National Science Foundation (NSF), are developing three Landsat mosaics of Antarctica and an Antarctic Web Portal with a Community site and an online map viewer. When scientists are able to view the entire scope of polar research, they will be better able to collaborate and locate the resources they need. When the general public more readily sees what is happening in the polar environments, they will understand how changes to the polar areas affect everyone."
"10.1504/IJBIDM.2007.012947","https://www.scopus.com/inward/record.uri?eid=2-s2.0-34248682398&doi=10.1504%2fIJBIDM.2007.012947&partnerID=40&md5=1af485cd9803f152c414dfcc0438aa61","Database benchmarks can either help users in comparing the performances of different systems, or help engineers in testing the effect of various design choices. In the field of data warehouses, the Transaction Processing Performance Council's standard benchmarks address the first point, but they are not tunable enough to address the second one. We present in this paper the Data Warehouse Engineering Benchmark (DWEB), which allows generating various ad hoc synthetic data warehouses and workloads. We detail DWEB's full specifications, as well as the experiments we performed to illustrate how it may be used. DWEB is a Java free software. Copyright © 2007 Inderscience Enterprises Ltd."
"10.1504/IJBIDM.2007.012946","https://www.scopus.com/inward/record.uri?eid=2-s2.0-34248674370&doi=10.1504%2fIJBIDM.2007.012946&partnerID=40&md5=449892da8d03dbd57fa71934791d79c1","The k-nearest neighbour (KNN) technique is a simple yet effective method for classification. In this paper, we propose an efficient weighted nearest neighbour classification algorithm, called PINE, using vertical data representation. A metric called HOBBit is used as the distance metric. The PINE algorithm applies a Gaussian podium function to set weights to different neighbours. We compare PINE with classical KNN methods using horizontal and vertical representation with different distance metrics. The experimental results show that PINE outperforms other KNN methods in terms of classification accuracy and running time. Copyright © 2007 Inderscience Enterprises Ltd."
"10.1504/IJBIDM.2007.012948","https://www.scopus.com/inward/record.uri?eid=2-s2.0-34248654048&doi=10.1504%2fIJBIDM.2007.012948&partnerID=40&md5=0a57735ea559ba4570e3af3ad3938626","Our research project aims to build an infrastructure that can efficiently and seamlessly to meet the demanding requirements of business event stream-based analytical applications. We focus on the complex event analysis on event streams, QoS-aware event processing services, and a novel event stream processing model that seamlessly and efficiently combines event processing services. That model is built upon the Sense & Respond loops that support a complete Business Intelligence process to sense, interpret, predict, automate and respond to business processes and aim to decrease the time it takes to make the business decisions. We have developed ZELESSA, an event stream management system as a proof of concept. Copyright © 2007 Inderscience Enterprises Ltd."
"10.1504/IJBIDM.2007.012945","https://www.scopus.com/inward/record.uri?eid=2-s2.0-34248650696&doi=10.1504%2fIJBIDM.2007.012945&partnerID=40&md5=1bc3d4467b96757016dbda8a6173e7e6","To discover hidden correlations, association rule mining methods use two important constraints known as support and confidence. However, mining methods often unable to find the best value for these constraints: • large number of rules when these thresholds are low • very few r ules when these thresholds are high. In addition, regard less of these above thresholds, mining methods produce many rules that have identical meaning or, redundant roles. Indeed such redundant roles seem as a main impediment to efficient utilisation of discovered rules, and should be removed. To achieve this aim, here we present several methods that identify those roles that are redundant and eliminate them. Copyright © 2007 Inderscience Enterprises Ltd."
"10.1504/IJBIDM.2007.012944","https://www.scopus.com/inward/record.uri?eid=2-s2.0-34248644003&doi=10.1504%2fIJBIDM.2007.012944&partnerID=40&md5=79357bab291f4b01f2a8225290a9ff4a","The quality of data mining results is largely dependent on the ability to accommodate context and user requirements within the mining process. This is done effectively within the pre-processing and presentation stages, however the analysis (or mining) stage remains relatively autonomous and opaque with user input commonly limited to parameter setting. There is, at present, no direct manipulation of the analysis stage which results in the analysis of the domain space being statically constrained. This reduces the quality of results and increases the time needed for analysis. This paper presents a guided association mining environment, GAM, that enhances user-computer synergy by incorporating the user at fine level of granularity within the analysis stage. GAM extends the current state of the art and is based upon a generic guided knowledge discovery environment. Copyright © 2007 Inderscience Enterprises Ltd."
"10.2481/dsj.6.S310","https://www.scopus.com/inward/record.uri?eid=2-s2.0-34547538904&doi=10.2481%2fdsj.6.S310&partnerID=40&md5=186db2fec90a160e8b79002b1424a716","The forecasting of software failure data series by Genetic Programming (GP) can be realized without any assumptions before modeling. This discovery has transformed traditional statistical modeling methods as well as improved consistency for model applicability. The individuals' different characteristics during the evolution of generations, which are randomly changeable, are treated as Markov random processes. This paper also proposes that a GP algorithm with ""optimal individuals reserved strategy"" is the best solution to this problem, and therefore the adaptive individuals finally will be evolved. This will allow practical applications in software reliability modeling analysis and forecasting for failure behaviors. Moreover it can verify the feasibility and availability of the GP algorithm, which is applied to software failure data series forecasting on a theoretical basis. The results show that the GP algorithm is the best solution for software failure behaviors in a variety of disciplines."
"10.2481/dsj.6.S317","https://www.scopus.com/inward/record.uri?eid=2-s2.0-34547549306&doi=10.2481%2fdsj.6.S317&partnerID=40&md5=1c6a0f7d01712f8b6eb61c309c3a66c4","The research results of coal characterization using the XPS method are summarized. Microsoft Visual Studio.net is utilized to build a database functional group characterization for coal, which contains over 1000 records including the kind of functional group, binding energy value, coal specie, producing area, sample preparation, and literature information. The database can be used to search and analyze XPS data for coal conveniently and is also of significance to support further coal research using XPS."
"10.2481/dsj.6.S301","https://www.scopus.com/inward/record.uri?eid=2-s2.0-34547532831&doi=10.2481%2fdsj.6.S301&partnerID=40&md5=b7e4a60c0007b006e48e036345031ef8","A global multi-resolution image data model and a feasible solution for its seamless management and archiving remain a challenging vision. The traditional methods of the raster pixel data structure based on the idea of map projections are effective to support local or small-scale areas. However, if this structure is applied to large-scale or whole global image archiving, some significant drawbacks are unavoidable, such as data discontinuity (or overlapping), geometric distortions, etc. To overcome these deficiencies, in this paper the Quaternary Triangular Mesh (QTM) (Dutton, 1989), as a continuous, hierarchal quadtree data structure with uniform grids on a sphere, is proposed for global-scale seamless image archiving. First, the mapping relation between raster image pixels and QTM pixels is approached based on the QTM subdivision and Quaternary coding scheme (Bartholdi & Goldsman, 2001), and a corresponding algorithm of QTM pixel grey level calculation is also developed. Then, the storage structure of global-scale image archiving based on QTMpixels is presented in detail. In the end, an experiment is described using the Ihn resolution NOAA data for China, comparing the differences in pixel grey levels between original image pixels and QTM pixels. The result indicates that the QTM pixel data structure can keep global-scale images seamless, and the accuracy of transformation from the imaging pixel to the QTM pixel is a loss of less than 2 grey levels for 94.5% of all pixels, the loss from 2 to 4 is 1.9%, the loss from 4 to 10 is 2%, and the rest is 1.6%. The results are good and acceptable."
"10.2481/dsj.6.S297","https://www.scopus.com/inward/record.uri?eid=2-s2.0-34547497905&doi=10.2481%2fdsj.6.S297&partnerID=40&md5=56a5cf7fb28d22c27feaaf2900c0ccde","The sharing of scientific data is a problem that attracts worldwide attention. In answer, governments have started to establish many systems to provide the public with scientific data. However, sometimes the access does not consider the public's practical need but just stands as an ""official"" prototype. In this research, a questionnaire was devised to understand a Beijing citizen's practical need for scientific data and to explore the inconsistency between the needs of the public and the information published by the government. The research looks to find a more effective way to solve this problem."
"10.2481/dsj.6.S278","https://www.scopus.com/inward/record.uri?eid=2-s2.0-34547526393&doi=10.2481%2fdsj.6.S278&partnerID=40&md5=1f6e8d2290f9be7223ce2ec0435ffef1","Biological effects of low-dose radiation are studied by computational methods. Assessing the risks of low-dose radiation, i.e. radiation-induced cancer, is becoming important in the study of public health because of the many different types of exposures, medical exposures, and from radiation protection viewpoints. In general, radiation effects arise from damage done to DNA by ionizing radiation. Therefore, examining effects from the initial DNA damage to the risk assessment is a problem with a very wide spatiotemporal scale. We are studying this problem by dividing it into three parts: 1) the DNA strand is broken by ionizing radiation, 2) DNA lesion repair, and 3) the process of cell carcinogenesis and tumorigenesis. In this paper, we mainly focus on the third part, the study of modeling and simulation of cell carcinogenesis."
"10.2481/dsj.6.S285","https://www.scopus.com/inward/record.uri?eid=2-s2.0-34547521029&doi=10.2481%2fdsj.6.S285&partnerID=40&md5=7153e855e40fe54f9a5474203adac19a","In this paper, we present an evaluation of learning algorithms of a novel rule evaluation support method for post-processing of mined results with rule evaluation models based on objective indices. Post-processing of mined results is one of the key processes in a data mining process. However, it is difficult for human experts to completely evaluate several thousands of rules from a large dataset with noise. To reduce the costs in such rule evaluation task, we have developed a rule evaluation support method with rule evaluation models that learn from a dataset. This dataset comprises objective indices for mined classification rules and evaluation by a human expert for each rule. To evaluate performances of learning algorithms for constructing the rule evaluation models, we have done a case study on the meningitis data mining as an actual problem. Furthermore, we have also evaluated our method with ten rule sets obtained from ten UCI datasets. With regard to these results, we show the availability of our rule evaluation support method for human experts."
"10.2481/dsj.6.S255","https://www.scopus.com/inward/record.uri?eid=2-s2.0-34547532523&doi=10.2481%2fdsj.6.S255&partnerID=40&md5=053ce19bc00c06739b9ce114d330e821","Patents contain much novel technological information. In this paper, the searching methods of the file index (FI) and F-term classification system developed by the Japan Patent Office (JPO) were employed to find patents containing information on carbon nanotube technology. All related patent data were searched for in the Intellectual Property Digital Library (IPDL). Moreover, using theme codes and term codes in the two-dimensional structure of the F-term list, we investigated and analyzed the technical features expressed by carbon nanotubes in related documents in Boolean operations."
"10.2481/dsj.6.S270","https://www.scopus.com/inward/record.uri?eid=2-s2.0-34547514872&doi=10.2481%2fdsj.6.S270&partnerID=40&md5=ee0f5a5b53389a9ace7a1b190b19dad1","Using satellite cloud images to simulate clouds is one of the new visual simulation technologies in Virtual Reality (VR). Taking the original data of satellite cloud images as the source, this paper depicts specifically the technology of 3D satellite cloud imaging through the transforming of coordinates and projection, creating a DEM (Digital Elevation Model) of cloud imaging and 3D simulation. A Mercator projection was introduced to create a cloud image DEM, while solutions for geodetic problems were introduced to calculate distances, and the outer-trajectory science of rockets was introduced to obtain the elevation of clouds. For demonstration, we report on a computer program to simulate the 3D satellite cloud images."
"10.2481/dsj.6.S261","https://www.scopus.com/inward/record.uri?eid=2-s2.0-34547500172&doi=10.2481%2fdsj.6.S261&partnerID=40&md5=8a400f81ddc610575c08f2dbd65726e3","Defence, security, public works, and many other branches of government require small-scale, current geographical databases. With this aim, small-scale geographical databases in Turkey were produced in the past and are still in use today. There are, however, problems in using the available geographical databases. These problems informing, sharing, updating, and meeting requirements necessitate the reconfiguration of the present system. Classical approaches are preferred when updating the present geographical databases. Many errors and difficulties in recording new objects have occurred in the field, in scanning and collecting data from related institutions, and in recording and controlling nonstandard completion data. Thus it is difficult to achieve the desired high quality data with the present method. In this study, we introduce and discuss updating and completing small-scale maps of geographical bases. The Geographical Information System formation studies in Turkey are summarized, and a model is proposed for the formation, updating, and completion of systems of small-scale maps of significant databases."
"10.2481/dsj.6.S241","https://www.scopus.com/inward/record.uri?eid=2-s2.0-34547528277&doi=10.2481%2fdsj.6.S241&partnerID=40&md5=888d9f3e1902c02b3bc23dd0d968c82e","Information dissemination platforms for supporting voluntary collaboration among researchers should assure that controllable and verified information is being disseminated. However, previous related studies on this field narrowed their research scopes into information type and information specification. This paper focuses on the verification and the tracing of information using an information dissemination platform and other Semantic Web-based services. Services on our platform include information dissemination services to support reliable information exchange among researchers and knowledge service to provide unrevealed information. The latter is also divided into the two: knowledgization using ontology and inference using a Semantic Web-based inference engine. This paper discusses how this platform supports instant knowledge addition and inference. We demonstrate our approach by constructing an ontology for national R&D reference information using 37,656 RDF triples from about 2,300 KISTI (Korea Institute of Science and Technology Information) outcomes. Three knowledge services including 'Communities of Practice', 'Researcher Tracing,' and 'Research Map' were implemented on our platform using a Jena framework. Our study shows that information dissemination platforms will make a meaningful contribution to the possibility of realizing a practical Semantic Web-based information dissemination platform."
"10.2481/dsj.6.S250","https://www.scopus.com/inward/record.uri?eid=2-s2.0-34547502724&doi=10.2481%2fdsj.6.S250&partnerID=40&md5=5fc978a9529b1fad601d38a828a2e34d","China's geosciences data sharing has progressed significantly but is still facing some key questions that need to be solved Because of the data's insufficiency, it has limited use in modern scientific research. The documents and metadata for the data are insufficient. The scientific data service is dated. The data application procedure is troublesome, etc. All this has become a bottleneck affecting the progress of China's scientific data sharing. Considering the reality of China, some potential solution have been proposed, which include changing the scheme of data integration, perfecting metadata and documentation, emphasizing data service, simplifying the application procedure, enhancing the shared consciousness in science and technology fields, establishing national data centers, and realizing sustainable data sharing."
"10.2481/dsj.6.46","https://www.scopus.com/inward/record.uri?eid=2-s2.0-34547203926&doi=10.2481%2fdsj.6.46&partnerID=40&md5=47cfa0652ca7f19685d1cbae038cf8db","This paper presents data mining techniques that can be used to study voting patterns in the United States House of Representatives and shows how the results can be interpreted. We processed the raw data available at http://clerk.house.gov, performed t-weight calculations, an attribute relevance study, association rule mining, and decision tree analysis and present and interpret interesting results. WEKA and SQL Server 2005 were used for mining association rules and decision tree analysis."
"10.2481/dsj.6.S234","https://www.scopus.com/inward/record.uri?eid=2-s2.0-34547549578&doi=10.2481%2fdsj.6.S234&partnerID=40&md5=62d9af69228d75db3249ddd5440eec4f","The Grid today plays the role of a universal infrastructure for data processing with a great number of services that allow not only solving the concrete applied tasks but also helping to search for necessary resources to collect information about their state and to save and to deliver data. The grid-infrastructure in hand is able to provide the Ukrainian universities, research centres, and virtual laboratories with needed information and required computational resources. As an example, the system of ecological monitoring and telemedicine support for the Chernobyl Nuclear Power Plant area is considered."
"10.2481/dsj.6.S225","https://www.scopus.com/inward/record.uri?eid=2-s2.0-34547512672&doi=10.2481%2fdsj.6.S225&partnerID=40&md5=b0ac39ff3539dcf9f9edeb844d84370f","Decision support systems are nowadays used to disentangle all kinds of intricate situations and perform sophisticated analysis. Moreover, they are applied in areas where the knowledge can be heterogeneous, partially un-formalized, implicit, or diffuse. The representation and management of this knowledge becomes the key point to ensure the proper functioning of the system and to keep an intuitive view upon its expected behavior. This paper presents a generic architecture for implementing knowledge-base systems used in collaborative businesses, where the knowledge is organized into different databases, according to the usage, persistence, and quality of the information. This approach is illustrated with Cadrai, a customizable automated tool built on this architecture and used for processing family benefits applications at the National Family Benefits Fund of the Grand-Duchy of Luxembourg."
"10.2481/dsj.6.S220","https://www.scopus.com/inward/record.uri?eid=2-s2.0-34547552617&doi=10.2481%2fdsj.6.S220&partnerID=40&md5=6e337766d4c0321d8efbdbeea415d596","In Traditional Chinese Medicine (TCM), the functions of the brain are dispersed to five zang organs, and are maintained by comprehensive functional interactions among the five zang organs. Therefore, brain diseases are regarded as systematic diseases in TCM, and their treatments are aimed to normalize not only the activity of the organs, but also the balance of functional interaction. In addition, interestingly, the functional interaction between the five zang organs in TCM resembles a biological model based on chaos theory. These features of TCM derive from its theoretical basis in Yin-Yang and the five elements. In conclusion, TCM had co-opted the basic idea of a complex system for the diagnosis and treatment of human diseases thousands years ago. Research into TCM should not only evaluate the effects of herbal medicine or acupuncture, but should take into consideration the view of human beings in TCM."
"10.2481/dsj.6.S198","https://www.scopus.com/inward/record.uri?eid=2-s2.0-34547536667&doi=10.2481%2fdsj.6.S198&partnerID=40&md5=b18a6729d0bad6772b9e3b5715b12c33","The concept of information has become a crucial topic in several emerging scientific disciplines, as well as in organizations, in companies and in everyday life. Hence it is legitimate to speak of the so-called information society; but a scientific understanding of the Information Age has not had time to develop. Following this evolution we face the need of a new transdisciplinary understanding of information, encompassing many academic disciplines and new fields of interest. Therefore a Science of Information is required. The goal of this paper is to discuss the aims, the scope, and the tools of a Science of Information. Furthermore we describe the new Science of Information Institute (SOII), which will be established as an international and transdisciplinary organization that takes into consideration a larger perspective of information."
"10.2481/dsj.6.S206","https://www.scopus.com/inward/record.uri?eid=2-s2.0-34547515163&doi=10.2481%2fdsj.6.S206&partnerID=40&md5=fd842b4d3817e4d5cab96f7f4a39ed36","It is widely accepted that colloids play an important role in the contaminant migration process at present. However, the colloid deposition structure on rock surfaces has scarcely been studied. In this paper, preliminary results for a fractal characterization for colloid deposition in saturated fractures are presented, which consider the pH value, ionic strength, and flow rate of the solution. Under different chemical conditions, deposition behavior obviously changed, and fractal analysis appears to be an effective tool to capture the evolution and general behavior of depositions. Scanning Electron Microscopy (SEM) is used to observe the colloidal growth on granite surfaces and to acquire the visual image on a detailed level. The images are analyzed for their mass fractal dimensions. The influence on colloid fractal deposition is discussed."
"10.2481/dsj.6.S211","https://www.scopus.com/inward/record.uri?eid=2-s2.0-34547501750&doi=10.2481%2fdsj.6.S211&partnerID=40&md5=f698a3a6b328cca4afcb7cd2ef8ebdfd","The fast growing Open Content movement has profound consequences for pedagogical approaches to learning. This paper will explore the use of Open Content in higher education, including training for scientists and scholars at large, and consider its pedagogic implications. Relevance of these issues is expected to grow in the near future, involving the ability of scholars to cope with the increased need to access, search through, and fruitfully draw knowledge from data, especially for teams where cross-disciplinary competences are required to analyse, evaluate, and exchange data across a variety of research fields."
"10.2481/dsj.6.28","https://www.scopus.com/inward/record.uri?eid=2-s2.0-34547223966&doi=10.2481%2fdsj.6.28&partnerID=40&md5=16be6e4346a819ccf09d287d216a000d","This paper reviews the present status and major problems of the existing ISO standards related to imagery metadata. An imagery metadata model is proposed to facilitate the development of imagery metadata on the basis of conformance to these standards and combination with other ISO standards related to imagery. The model presents an integrated metadata structure and content description for any imagery data for finding data and data integration. Using the application of satellite data integration in CEOP as an example, satellite imagery metadata is developed, and the resulting satellite metadata list is given."
"10.2481/dsj.6.S172","https://www.scopus.com/inward/record.uri?eid=2-s2.0-34547524729&doi=10.2481%2fdsj.6.S172&partnerID=40&md5=200b20576b4970ab94f0830f56941051","This paper presents results in automated genre classification of digital documents in PDF format. It describes genre classification as an important ingredient in contextualising scientific data and in retrieving targetted material for improving research. The current paper compares the role of visual layout, stylistic features, and language model features in clustering documents and presents results in retrieving five selected genres (Scientific Article, Thesis, Periodicals, Business Report, and Form) from a pool of materials populated with documents of the nineteen most popular genres found in our experimental data set."
"10.2481/dsj.6.S184","https://www.scopus.com/inward/record.uri?eid=2-s2.0-34547497615&doi=10.2481%2fdsj.6.S184&partnerID=40&md5=b8de10bc83e1773c4215ff899b43f406","This paper proposes an efficient algorithm to compress the cubes in the progress of the parallel data cube generation. This low overhead compression mechanism provides block-by-block and record-by-record compression by using tuple difference coding techniques, thereby maximizing the compression ratio and minimizing the decompression penalty at run-time. The experimental results demonstrate that the typical compression ratio is about 30:1 without sacrificing running time. This paper also demonstrates that the compression method is suitable for Hilbert Space Filling Curve, a mechanism widely used in multi-dimensional indexing."
"10.2481/dsj.6.S164","https://www.scopus.com/inward/record.uri?eid=2-s2.0-34547552910&doi=10.2481%2fdsj.6.S164&partnerID=40&md5=f9d1a99d86d335fb3a466ed82604f8ac","Under the support of the National Digital Archive Program (NDAP), basic species information about most Taiwanese fishes, including their morphology, ecology, distribution, specimens with photos, and literatures have been compiled into the ""Fish Database of Taiwan"" (http://fishdb.sinica.edu.tw). We expect that the all Taiwanese fish species databank (RSD), with 2800+ species, and the digital ""Fish Fauna of Taiwan"" will be completed in 2007. Underwater ecological photos and video images for all 2,800+ fishes are quite difficult to achieve but will be collected continuously in the future. In the last year of NDAP, we have successfully integrated all fish specimen data deposited at 7 different institutes in Taiwan as well as their collection maps on the Google Map and Google Earth. Further, the database also provides the pronunciation of Latin scientific names and transliteration of Chinese common names by referring to the Romanization system for all Taiwanese fishes (2,902 species in 292 families so far). The Taiwanese fish species checklist with Chinese common/vernacular names and specimen data has been updated periodically and provided to the global FishBase as well as the Global Biodiversity Information Facility (GBIF) through the national portal of the Taiwan Biodiversity Information Facility (TaiBIF). Thus, Taiwanese fish data can be queried and browsed on the WWW. For contributing to the ""Barcode of Life"" and ""All Fishes"" international projects, alcohol-preserved specimens of more than 1,800 species and cryobanking tissues of 800 species have been accumulated at RCBAS in the past two years. Through this close collaboration between local and global databases, ""The Fish Database of Taiwan"" now attracts more than 250,000 visitors and achieves 5 million hits per month. We believe that this local database is becoming an important resource for education, research, conservation, and sustainable use of fish in Taiwan."
,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-34547541267&partnerID=40&md5=f035e6aba22e84dc8c532157c305c8d6","The National Institute of Information and Communications Technology (NICT) operates a Japanese space forecast center for the International Space Environment Service (ISES). Information on space weather is exchanged daily among the space weather forecast centers all over the world. Researches of space weather need data on large areas of space from the Sun to the Earth's upper atmosphere. It is necessary for researchers of space weather to access various data and to communicate among various other researchers using a network. We describe experiments on the space weather network using the Japan Gigabit Network 2 (JGN2) operated by the NICT."
"10.2481/dsj.6.S155","https://www.scopus.com/inward/record.uri?eid=2-s2.0-34547532170&doi=10.2481%2fdsj.6.S155&partnerID=40&md5=bbfc45c41096415e2ccb7c3af672e7ee","KISTI-ACOMS has been developed as a part of the national project for raising the information society index of academic societies that began in 2001. ACOMS automates almost every activity of academic societies, including membership management, journal processing, conference organization, and e-journal management and provides a search system. ACOMS can be customized easily by the system administrator of an academic society. The electronic databases built by ACOMS are serviced to the users through the KISTI website (http://www.yeskisti.net) along with other journal databases created in a conventional way. KISTI plans to raise the usage ratio of the ACOMS database to deliver society services up to 100% in the future."
"10.2481/dsj.6.S146","https://www.scopus.com/inward/record.uri?eid=2-s2.0-34547523745&doi=10.2481%2fdsj.6.S146&partnerID=40&md5=08b0d7145f13b6c47322baf346debeca","The National Institute of Standards and Technology (NIST) is developing a digital library to replace the widely used National Bureau of Standards Handbook of Mathematical Functions published in 1964. The NIST Digital Library of Mathematical Functions (DLMF) will include formulas, methods of computation, references, and links to software for over forty functions. It will be published both in hardcopy format and as a website featuring interactive navigation, a mathematical equation search, 2D graphics, and dynamic interactive 3D visualizations. This paper focuses on the development and accessibility of the 3D visualizations for the digital library. We examine the techniques needed to produce accurate computations of function data, and through a careful evaluation of several prototypes, we address the advantages and disadvantages of using various technologies, including the Virtual Reality Modeling Language (VRML), interactive embedded graphics, and video capture to render and disseminate the visualizations in an environment accessible to users on various platforms."
"10.2481/dsj.6.S137","https://www.scopus.com/inward/record.uri?eid=2-s2.0-34547508609&doi=10.2481%2fdsj.6.S137&partnerID=40&md5=b61f760951b0f55203feffecf455df3b","The concept of sustainable ecological-social-economic development is considered proceeding from the condition of obligatory coordination of economic, ecological, and human dimensions in such a way that from one generation to the other, the quality and safety of life should not decrease, the environmental conditions should not worsen, and social progress should meet the needs of every person. An approach of system coordination and balancing of these three constituents is suggested."
"10.2481/dsj.6.S132","https://www.scopus.com/inward/record.uri?eid=2-s2.0-34547549268&doi=10.2481%2fdsj.6.S132&partnerID=40&md5=67c95c3d6a328dd41cf0de76f3569fc9","The latent structure behind an observation often plays an important role in the dynamics of visible events. Such latent structure is composed of invisible events named dark events. Human-interactive annealing is developed to visualize and understand dark events. This paper presents an application of human-interactive annealing for extracting new scenarios for patent technology using the latent technology structure behind current patented technology."
"10.2481/dsj.6.S125","https://www.scopus.com/inward/record.uri?eid=2-s2.0-34547506710&doi=10.2481%2fdsj.6.S125&partnerID=40&md5=33b7b47c2123a1915c453cbf6a97ea8b","This paper describes a proper translation-selecting and translation-clustering algorithm for Korean translation of words automatically extracted from newspapers. As about 80% of the English words in Korean newspapers appear in abbreviated form, it is necessary to make clusters of translation words to construct easily bilingual knowledge bases such as dictionaries and translation patterns. As a seed to acquiring a translation cluster, we selected a proper translation word from a given translation set using bi-gram-based histograms. Translation words that share bi-grams with the chosen proper translation word are assigned to the cluster for the proper word. The given translation set then picks out the translation words of the cluster. These processes continue until the translation set becomes empty. Experimental results show that our algorithms are superior to bi-gram-based binary vectors including Dice coefficient and Jaccard coefficient in selecting the proper translation word for each translation cluster."
"10.2481/dsj.6.S104","https://www.scopus.com/inward/record.uri?eid=2-s2.0-34547177971&doi=10.2481%2fdsj.6.S104&partnerID=40&md5=5a175f53a1d31447aefe5e544cfdb45e","In this paper, we introduce integrated data mining. Because of recent rapid progress in medical science as well as clinical diagnosis and treatment, integrated and cooperative research among medical researchers, biology, engineering, cultural science, and sociology is required Therefore, we propose a framework called Cyber Integrated Medical Infrastructure (CIMI). Within this framework we can deal with various types of data and consequently need to integrate those data prior to analysis. In this study, for medical science, we analyze the features and relationships among various types of data and show the possibility of integrated data mining."
"10.2481/dsj.6.S92","https://www.scopus.com/inward/record.uri?eid=2-s2.0-34547175437&doi=10.2481%2fdsj.6.S92&partnerID=40&md5=cb6f429f73b7fdfc8f82cf0aacaa3a91","The UNESCO office in Venice (the Regional Bureau for Science and Culture in Europe) has promoted, in collaboration with the Italian Agency for New Technologies, Energy, and the Environment (ENEA), an e-learning project on renewable energy: the DESIRE-net project (Development and Sustainability with International Renewable Energies network). The project's aim is to share the best available knowledge on renewable energies among all the countries that have joined the project and exploit this knowledge at every level. Currently the project involves 30 Eastern European and Southern Mediterranean countries as well as Australia, Indonesia, and China."
"10.2481/dsj.6.S116","https://www.scopus.com/inward/record.uri?eid=2-s2.0-34547139940&doi=10.2481%2fdsj.6.S116&partnerID=40&md5=67ecb232805c5d1899eb38e77bd2e453","Astronomy is one of the most data-intensive of the sciences. Data technology is accelerating the quality and effectiveness of its research, and the rate of astronomical discovery is higher than ever. As a result, many view astronomy as being in a ""Golden Age,"" and projects such as the Virtual Observatory are amongst the most ambitious data projects in any field of science. But these powerful tools will be impotent unless the data on which they operate are of matching quality. Astronomy, like other fields of science, therefore needs to establish and agree on a set of guiding principles for the management of astronomical data. To focus this process, we are constructing a ""data manifesto,"" which proposes guidelines to maximise the rate and cost-effectiveness of scientific discovery."
"10.2481/dsj.6.S79","https://www.scopus.com/inward/record.uri?eid=2-s2.0-34547144932&doi=10.2481%2fdsj.6.S79&partnerID=40&md5=8ae389020f509fb6df45962d6581d70a","The construction of e-government entails the construction of an information system connecting the government with its society. The infrastructure and content of the applications determine the success of an e-government's implementation. Limitations of Internet access and people's awareness are the main problems in most developing countries, including Indonesia. Thus, the implementers of Indonesian e-government have to consider the social conditions and cultural behavior of their society. We propose the development of Community-based Information Systems (CIS) to empower the implementation of Indonesian e-government. A CIS is designed to take into account the Indonesian societal system, which is structured by communities. CIS is formatted based on one primary portal that groups numbers of community websites. In this paper, one pilot system is presented (www.nagari.org)."
"10.2481/dsj.6.S51","https://www.scopus.com/inward/record.uri?eid=2-s2.0-34547219593&doi=10.2481%2fdsj.6.S51&partnerID=40&md5=60048e66b0ef3a3f0f95210e298aa282","It is important for a collaborative community to decide its next action. The leader of a collaborative community must choose an action that increases rewards and reduces risks. When a leader cannot make this decision, action will be determined through community member discussion. However, this decision cannot be made in blind discussions, so systematic discussion is necessary to choose effective action in a limited time. In this paper, we propose a bulletin board system framework in which effective discussion is established through visualized discussion logs."
"10.2481/dsj.6.S39","https://www.scopus.com/inward/record.uri?eid=2-s2.0-34547177972&doi=10.2481%2fdsj.6.S39&partnerID=40&md5=6720561362eaa9e59a49c8bec4721f1e","In the present work, energy recovery and mechanical recycling, two treatment options for plastic wastes from discarded television sets, have been assessed and compared in the context of the life cycle assessment methodology (LCA). The environmental impact of each option was assessed by calculating the depletion of abiotic resources (ADP) and the global warming potential (GWP). Then, the indicators were compared, and the option with the smaller environmental impact was selected. The main finding of this study was that mechanical recycling of plastics is a more attractive treatment option in environmental terms than incineration for energy recovery."
"10.2481/dsj.6.S61","https://www.scopus.com/inward/record.uri?eid=2-s2.0-34547158282&doi=10.2481%2fdsj.6.S61&partnerID=40&md5=680354ae935a5cd66c2994634d677467","We propose an approach for understanding leadership behavior in dot-jp, a non-profit organization, by analyzing heterogeneous multi-data composed of questionnaires and mailing list archives. Attitudes toward leaders were obtained from the questionnaires, and human networks were extracted from the mailing list archives. By integrating the results, we discovered that leaders must receive messages from other people as well as send messages to construct reliable relationships."
"10.2481/dsj.6.19","https://www.scopus.com/inward/record.uri?eid=2-s2.0-34547315853&doi=10.2481%2fdsj.6.19&partnerID=40&md5=bdec167aabcdc5d80027e4fd7046091d","Recently, the use of the Burrows-Wheeler method for data compression has been expanded. A method of enhancing the compression efficiency of the common JPEG standard is presented in this paper, exploiting the Burrows-Wheeler compression technique. The paper suggests a replacement of the traditional Huffman compression used by JPEG by the Burrows-Wheeler compression. When using high quality images, this replacement will yield a better compression ratio. If the image is synthetic, even a poor quality image can he compressed better."
"10.2481/dsj.6.S18","https://www.scopus.com/inward/record.uri?eid=2-s2.0-34547235256&doi=10.2481%2fdsj.6.S18&partnerID=40&md5=63d409d52bea8d7a62b4cced6263404c","Anthropometric data are used by numerous types of organizations for health evaluation, ergonomics, apparel sizing, fitness training, and many other applications. Data have been collected and stored in electronic databases since at least the 1940s. These databases are owned by many organizations around the world. In addition, the anthropometric studies stored in these databases often employ different standards, terminology, procedures, or measurement sets. To promote the use and sharing of these databases, the World Engineering Anthropometry Resources (WEAR) group was formed and tasked with the integration and publishing of member resources. It is easy to see that organizing worldwide anthropometric data into a single database architecture could be a daunting and expensive undertaking. The challenges of WEAR integration reflect mainly in the areas of distributed and disparate data, different standards and formats, independent memberships, and limited development resources. Fortunately, XML schema and web services provide an alternative method for networking databases, referred to as the Loosely Coupled WEAR Integration. A standard XML schema can be defined and used as a type of Rosetta stone to translate the anthropometric data into a universal format, and a web services system can be set up to link the databases to one another. In this way, the originators of the data can keep their data locally along with their own data management system and user interface, but their data can be searched and accessed as part of the larger data network and even combined with the data of others. This paper will identify requirements for WEAR integration, review XML as the universal format, review different integration approaches, and propose a hybrid web services/data mart solution."
"10.2481/dsj.6.S28","https://www.scopus.com/inward/record.uri?eid=2-s2.0-34547201697&doi=10.2481%2fdsj.6.S28&partnerID=40&md5=6d361ae08963ada5866850112a2a4e37","The concept of a bulletin board system (BBS) equipped with information visualization techniques is proposed for supporting online data analysis. Although group discussion is known to be effective for analyzing data from various viewpoints, the number of participants is limited by time and space constraints. To solve that problem, this paper proposes to augment a BBS, a popular web based tool. In order for discussion participants to share data online, the system provides them with a visual representation of target data, which elicits comments from participants as well as compares these comments. In order to illustrate the concept's potential, a BBS equipped with KeyGraph is also developed for supporting online chance discovery. It has functions for making visual annotations on the KeyGraph as well as a function for retrieving similar scenarios. The experimental results show the effectiveness of the BBS in terms of the usefulness of scenario generation support functions as well as that of scenario retrieval engines."
"10.2481/dsj.6.S2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-34547146125&doi=10.2481%2fdsj.6.S2&partnerID=40&md5=fe4bb32b3c191cb82dfb367096282503","The biodiversity databases in Taiwan were dispersed to various institutions and colleges with limited amount of data by 2001. The Natural Resources and Ecology GIS Database sponsored by the Council of Agriculture, which is part of the National Geographic Information System planned by the Ministry of Interior, was the most well established biodiversity database in Taiwan. This database, however, mainly collected the distribution data of terrestrial animals and plants within the Taiwan area. In 2001, GBIF was formed, and Taiwan joined as an Associate Participant, starting the establishment and integration of animal and plant species databases; therefore, TaiBIF was able to co-operate with GBIE The information of Catalog of Life, specimens, and alien species were integrated by the Darwin core standard. These metadata standards allowed the biodiversity information of Taiwan to connect with global databases. Presently, more than 10 institutes and museums that collect the specimen data and distribution information of Taiwan are integrated, and they can be queried via the Union Catalog of National Digital Archives Program, TaiBIF, and other international nodes of GBIF. In TaiBIF, we have developed a platform and an exchange mechanism, which use the species and GIS distribution as the primary keys. The integrated databases allow users to search for various species web pages by the name and distribution of the species using GIS techniques and display species and specimen basic information and distribution data."
"10.2481/dsj.6.S11","https://www.scopus.com/inward/record.uri?eid=2-s2.0-34547212533&doi=10.2481%2fdsj.6.S11&partnerID=40&md5=176c45d752a27ba843647634f4aa1af0","Optical disks, DVDs and CDs, are convenient recording media on which to safely store data for a long period of time. However, the complete data erasure from recorded media is also important for the security of the data. After erasure of data from optical disks, recycling the material is needed in order to recover the valuable components of the optical disks. Here, data erasure methods for optical disks are discussed in the view of material recycling. The main finding of the study is that the explosion of optical disks in water is a very suitable method for complete erasure of data on the disks as well as recycling of their materials."
,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-33846680252&partnerID=40&md5=55ecfe90bc7e36e4db729652265e7df7","Chemical analyses of volcanic gases consist of: location of sampling, date of sampling, identification of the sampling, etc. Nowadays, these data are generally represented in different formats. All of these formats are inflexible and machine dependent. XML has become the most important method of transferring data between computers. VolcanoGasML is a new format, based on XML, for the chemical analyses of volcanic gases. Its definition is divided into several layers: the first one describes the general information concerning the sample, the second, which is organized in several sublayers, contains the chemical data."
"10.2481/dsj.6.1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-33846495092&doi=10.2481%2fdsj.6.1&partnerID=40&md5=4967744ef037a7153128565134f8580b","We explore the use of the LZW and LZSS data compression methods. These methods or some versions of them are very common in use of compressing different types of data. Even though on average LZSS gives better compression results, we determine the case in which the LZW perform best and when the compression efficiency gap between the LZW algorithm and its LZSS counterpart is the largest."
"10.1504/IJBIDM.2006.010784","https://www.scopus.com/inward/record.uri?eid=2-s2.0-33751019447&doi=10.1504%2fIJBIDM.2006.010784&partnerID=40&md5=c9fccdf9f6a4c18ebad77df56489f0d7","In our paper, we illustrate two kinds of product recommender algorithms to support e-commerce. For those commodities which a consumer seldom buys, user-rating methods are required to acquire the data set of the products rating in terms of the preference of the specific user. Thus, the combination of the Genetic Algorithm (GA) and k nearest neighbour method is proposed to infer the customer's personal preferences from rated products. On the other hand, for products that the consumers often buy, an interactive mode is provided for the users to evaluate the degree of interest for each feature of the products. We finally incorporate an intelligent agent model into the virtual shopping mall, which makes it easy for customers to fuse into the shopping experience. Copyright © 2006 Inderscience Enterprises Ltd."
"10.1504/IJBIDM.2006.010785","https://www.scopus.com/inward/record.uri?eid=2-s2.0-33751014534&doi=10.1504%2fIJBIDM.2006.010785&partnerID=40&md5=b9e42353f3a236ab9914d4b82b997fd8","In this paper, we present a model to evaluate both marketable quality and profitability of corporations. We apply the real values of some leading Japanese manufacturing corporations to our proposed model to analyse its accuracy. By analysis we found that the theoretical and real standard values of the marketable quality indicator for the rate of operation at the break even point were both 0.6 (that is 60%). We validate the proposed model and show that the proposed model gives a good approximation of the economical trends of Japanese corporations. We also discuss how to increase the relative annual profit by increasing marketable quality. Copyright © 2006 Inderscience Enterprises Ltd."
"10.1504/IJBIDM.2006.010783","https://www.scopus.com/inward/record.uri?eid=2-s2.0-33751002348&doi=10.1504%2fIJBIDM.2006.010783&partnerID=40&md5=ec8f809ac593674f6e3ffede867e9469","Due to increasing use of very large database and data warehouses, discovering useful knowledge from transactions is becoming an important research area. One of approaches is fuzzy classification. Hong and Lee (1996) proposed a learning method that automatically derives fuzzy if-then rules from a set of given training examples using a decision table. Hong and Chen (1999) improved it. Based on their heuristic algorithms and the well-known Apriori approach, this paper proposes a new fuzzy mining algorithm to explore association rules from given quantitative transactions. Experimental results on Iris data show that the proposed algorithm effectively induces more association rules. Copyright © 2006 Inderscience Enterprises Ltd."
"10.1504/IJBIDM.2006.010781","https://www.scopus.com/inward/record.uri?eid=2-s2.0-33750971738&doi=10.1504%2fIJBIDM.2006.010781&partnerID=40&md5=1aaf682d913572b50d528c339f6c8c61","Typical description logics are limited to dealing with crisp concepts and crisp roles. However, web applications based on description logics should allow the treatment of the inherent uncertainty. Extended Fuzzy Description Logics (EFDLs), which adopt a special fuzzify-method with more expressive power than the previous fuzzy description logics, are proposed to enable representation and reasoning for complex fuzzy information. They introduce the cut sets of fuzzy concepts and fuzzy roles as atomic concepts and atomic roles, and inherit the concept and role constructors from description logics. The definitions of syntax, semantics, reasoning tasks, reasoning properties, and reasoning algorithm are given for the extended fuzzy description logic. Copyright © 2006 Inderscience Enterprises Ltd."
"10.1504/IJBIDM.2006.010782","https://www.scopus.com/inward/record.uri?eid=2-s2.0-33750970557&doi=10.1504%2fIJBIDM.2006.010782&partnerID=40&md5=6bb1f8e4e4006d836653c99611dc8f7f","Data mining applications have been getting more attention in general business ares, but there is a need to use more of these applications in accounting areas where accounting deals with large amounts of both financial and non-financial data. The purpose of this research is to test the effectiveness of a Multiple Criteria Linear Programming (MCLP) approach to data mining for bankruptcy prediction using Japanese bankruptcy data. Our empirical results show that Ohlson's (1980) predictor variables perform better than Altman's (1968) predictor variables using 1990s Japanese financial data. Our Type I (misclassification of bankrupt as non-bankrupt firms) prediction rate using the MCLP approach, Ohlson's (1980) variables and 1990s Japanese financial data is much higher than that reported by Kwak et al. (2005) using the MCLP approach, Ohlson's (1980) variables and 1990s US data. Copyright © 2006 Inderscience Enterprises Ltd."
"10.1504/IJBIDM.2006.010780","https://www.scopus.com/inward/record.uri?eid=2-s2.0-33750967052&doi=10.1504%2fIJBIDM.2006.010780&partnerID=40&md5=1abf1363d3b0f7547929e5b6ebcd9a10","Advanced analytic and forecasting methodologies can enable organisations to more fully leverage the data resources available to them. In the healthcare industry, service providers can use data mining methods to enhance the decision-making process in optimising resource allocation by identifying the sources of future high-cost treatment in a given health plan population. The following paper includes a case study by Healthways Inc. that illustrates how predictive modelling techniques (e.g., neural networks) can help healthcare providers identify the sources of future high resource demand, enabling them to more effectively apply preemptive treatment to mitigate future high-cost treatment of fully developed cases of chronic illness. Copyright © 2006 Inderscience Enterprises Ltd."
"10.1109/TCSI.2005.856043","https://www.scopus.com/inward/record.uri?eid=2-s2.0-33646026414&doi=10.1109%2fTCSI.2005.856043&partnerID=40&md5=f8dc62fbc2060ff54195fd5ea44c18f2","Mining frequent traversal patterns is to discover the reference paths traversed by a sufficient number of users from web logs, which can be used for prefetching and suggestion for web users. However, the discovered frequent traversal patterns may become invalid or inappropriate when the user behaviours are changed. In this paper, we propose an incremental updating technique to maintain the discovered frequent traversal patterns when the traversal paths are inserted into or deleted from the database. The experimental results show that our algorithms are more efficient than other algorithms for the maintenance of mining frequent traversal patterns. Copyright © 2006 Inderscience Enterprises Ltd."
"10.2481/dsj.5.223","https://www.scopus.com/inward/record.uri?eid=2-s2.0-33845411686&doi=10.2481%2fdsj.5.223&partnerID=40&md5=99c5d3e3775e3e7158efb7ec334d8e41","There is clear demand for a global spatial public domain roads data set with improved geographic and temporal coverage, consistent coding of road types, and clear documentation of sources. The currently best available global public domain product covers only one-quarter to one-third of the existing road networks, and this varies considerably by region. Applications for such a data set span multiple sectors and would be particularly valuable for the international economic development, disaster relief, and biodiversity conservation communities, not to mention national and regional agencies and organizations around the world. The building blocks for such a global product are available for many countries and regions, yet thus far there has been neither strategy nor leadership for developing it. This paper evaluates the best available public domain and commercial data sets, assesses the gaps in global coverage, and proposes a number of strategies for filling them. It also identifies stakeholder organizations with an interest in such a data set that might either provide leadership or funding for its development. It closes with a proposed set of actions to begin the process."
"10.2481/dsj.5.143","https://www.scopus.com/inward/record.uri?eid=2-s2.0-33751545036&doi=10.2481%2fdsj.5.143&partnerID=40&md5=9eb525576726936f5a6dcb321d4176b5","It is always a major demand to provide efficient retrieving and storing of data and information in a large database system. For this purpose, many file organization techniques have already been developed, and much additional research is still going on. Hashing is one developed technique. In this paper we propose an enhanced hashing technique that uses a hash table combined with a binary tree, searching on the binary representation of a portion the primary key of records that is associated with each index of the hash table. The paper contains numerous examples to describe the technique. The technique shows significant improvements in searching, insertion, and deletion for systems with huge amounts of data. The paper also presents the mathematical analysis of the proposed technique and comparative results."
"10.2481/dsj.5.209","https://www.scopus.com/inward/record.uri?eid=2-s2.0-33751541178&doi=10.2481%2fdsj.5.209&partnerID=40&md5=91cb68b2bd7380d916177c0d116a00ac","This paper discusses MathML, an XML-based standard for expressing mathematics - everything from elementary mathematics to undergraduate college-level mathematics. Limitations of pre-existing options led to the creation of MathML. MathML is designed to be useful for authoring and publishing, for creating online interactive math resources, and as a non-proprietary approach for archiving. MathML is supported by the W3C and by multiple scholarly publishers and vendors of computer-based mathematics software. The question now is whether MathML can achieve greater acceptance among authors and better integration with standards in related domains, either through cross-walks or through direct incorporation into other domain schemas. MathML, XML, Digital libraries, Science and technology publishing."
"10.2481/dsj.5.119","https://www.scopus.com/inward/record.uri?eid=2-s2.0-33751538314&doi=10.2481%2fdsj.5.119&partnerID=40&md5=dab281e35bdf46f860b184b276fd4097","Medical data mining has great potential for exploring the hidden patterns in the data sets of the medical domain. These patterns can be utilized for clinical diagnosis. However, the available raw medical data are widely distributed, heterogeneous in nature, and voluminous. These data need to be collected in an organized form. This collected data can be then integrated to form a hospital information system. Data mining technology provides a user-oriented approach to novel and hidden patterns in the data. Data mining and statistics both strive towards discovering patterns and structures in data. Statistics deals with heterogeneous numbers only, whereas data mining deals with heterogeneous fields. We identify a few areas of healthcare where these techniques can be applied to healthcare databases for knowledge discovery. In this paper we briefly examine the impact of data mining techniques, including artificial neural networks, on medical diagnostics."
"10.2481/dsj.5.108","https://www.scopus.com/inward/record.uri?eid=2-s2.0-33751533020&doi=10.2481%2fdsj.5.108&partnerID=40&md5=ca3fd9ff7684f96d6582aa56bb6255cc","Scientific publications written in natural language still play a central role as our knowledge source. However, due to the flood of publications, the literature survey process has become a highly time-consuming and tangled process, especially for novices of the discipline. Therefore, tools supporting the literature-survey process may help the individual scientist to explore new useful domains. Natural language processing (NLP) is expected as one of the promising techniques to retrieve, abstract, and extract knowledge. In this contribution, NLP is firstly applied to the literature of chemical vapor deposition (CVD), which is a sub-discipline of materials science and is a complex and interdisciplinary field of research involving chemists, physicists, engineers, and materials scientists. Causal knowledge extraction from the literature is demonstrated using NLP."
"10.2481/dsj.5.127","https://www.scopus.com/inward/record.uri?eid=2-s2.0-33751512551&doi=10.2481%2fdsj.5.127&partnerID=40&md5=ccd3400b3fe6d2197ff865118a990b54","Redundant or duplicate data are the most troublesome problem in database management and applications. Approximate field matching is the key solution to resolve the problem by identifying semantically equivalent string values in syntactically different representations. This paper considers token-based solutions and proposes a general field matching framework to generalize the field matching problem in different domains. By introducing a concept of String Matching Points (SMP) in string comparison, string matching accuracy and efficiency are improved, compared with other commonly-applied field matching algorithms. The paper discusses the development of field matching algorithms from the developed general framework. The framework and corresponding algorithm are tested on a public data set of the NASA publication abstract database. The approach can be applied to address the similar problems in other databases."
"10.2481/dsj.5.168","https://www.scopus.com/inward/record.uri?eid=2-s2.0-33751506613&doi=10.2481%2fdsj.5.168&partnerID=40&md5=6e2a67bea375321d96ce57e23f34b87f","A primary goal of the International Virtual Observatory Alliance, which brings together Virtual Observatory Projects from 16 national and international development projects, is to develop, evaluate, test, and agree upon standards for astronomical data formatting, data discovery, and data delivery. In the three years that the IVOA has been in existence, substantial progress has been made on standards for tabular data, imaging data, spectroscopic data, and large-scale databases and on managing the metadata that describe data collections and data access services. In this paper, I describe how the IVOA operates and give my views as to why such a broadly based international collaboration has been able to make such rapid progress."
"10.2481/dsj.5.84","https://www.scopus.com/inward/record.uri?eid=2-s2.0-33746749405&doi=10.2481%2fdsj.5.84&partnerID=40&md5=81ad7453833059ad34e15dd24dc69aae","Access to information is necessary, but not sufficient in our digital era. The challenge is to objectively integrate digital resources based on user-defined objectives for the purpose of discovering information relationships that facilitate interpretations and decision making. The Antarctic Treaty Searchable Database (http://aspire.nvi.net), which is in its sixth edition, provides an example of digital integration based on the automated generation of information granules that can be dynamically combined to reveal objective relationships within and between digital information resources. This case study further demonstrates that automated granularity and dynamic integration can be accomplished simply by utilizing the inherent structure of the digital information resources. Such information integration is relevant to library and archival programs that require long-term preservation of authentic digital resources."
"10.2481/dsj.5.1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-33746661153&doi=10.2481%2fdsj.5.1&partnerID=40&md5=1d703919dd4acbfb5c5e96a6fb83da8b","This study investigates the effect of bias-corrected estimators in analyzing real-world skewed data where categorization and transformation are necessary. It also reports a small-scale simulation study to indicate factors which can influence the bias correction to be small or large. For the complete data-set, it is observed that the maximum likelihood estimates and Schaefer's bias-corrected estimates are not greatly different. However, when the original sample size is reduced by about 50%, the difference between the estimates is found to be much larger, possibly even large enough to influence the conclusions drawn. The impact of transformation and categorization is visibly present. However, the broad impression gained in categorization is the same though difference in types of categorizations can not be overlooked. A factor which seems to influence the size of the bias correction is identified."
"10.2481/dsj.5.79","https://www.scopus.com/inward/record.uri?eid=2-s2.0-33746652986&doi=10.2481%2fdsj.5.79&partnerID=40&md5=9a53ed5c6d8889fb4694c4960e70ae4a","The 'Berlin Declaration' was published in 2003 as a guideline to policy makers to promote the Internet as a functional instrument for a global scientific knowledge base. Because knowledge is derived from data, the principles of the 'Berlin Declaration ' should apply to data as well. Today, access to scientific data is hampered by structural deficits in the publication process. Data publication needs to offer authors an incentive to publish data through long-term repositories. Data publication also requires an adequate licence model that protects the intellectual property rights of the author while allowing further use of the data by the scientific community."
"10.2481/dsj.5.64","https://www.scopus.com/inward/record.uri?eid=2-s2.0-33746650526&doi=10.2481%2fdsj.5.64&partnerID=40&md5=dccdc32b338dfec506e4adc98c166c6c","Today's developed countries are networked societies, with strong specificities in the Health field. Health costs have strongly increased. Indeed, governments, local powers, public and private health or insurance organizations face difficult choices: they need the most specific and valuable data available. Data for Decision Making in Networked Health is necessary at 3 levels: between patients and physicians and within the organizations (micro), within the organizations (meso) and in regional powers or governments (macro) and with 3 major content dimensions: quality, ethics, economics (effectiveness). We point out some specific tools of e-Health Information Systems: EHR (Electronic Health Record), portals and call centers. Then we analyze the main issues about data for Decision Making in Networked Health: information sharing, coordination and evaluation. Lastly we use an Information System perspective to analyze ways of improving interoperability at the core of each context, and of advancing from one context to another."
"10.2481/dsj.5.18","https://www.scopus.com/inward/record.uri?eid=2-s2.0-33746642955&doi=10.2481%2fdsj.5.18&partnerID=40&md5=e1a8e7c8a07686a14778262b77a29131","The number of online resources for biodiversity information is growing. Names of organisms underpin access to information but present a number of unique problems when used as search terms. We examine these problems and assert that a taxonomic name-server or thesaurus is necessary to enable optimal retrieval of records from multiple datasets, A simple solution is presented, based upon our experience working with ""real-world"" data in the National Biodiversity Network (NBN) in the United Kingdom. The NBN provides access to over 18 million observational records and incorporates a nomenclator covering 198,000 names."
"10.2481/dsj.5.52","https://www.scopus.com/inward/record.uri?eid=2-s2.0-33746590309&doi=10.2481%2fdsj.5.52&partnerID=40&md5=4e2c42cbfdff2211219ed3cde02e9d2e","A standardized data schema for material properties in XML is under development to establish a common and exchangeable expression. The next stage toward the management of knowledge about material usage, selection or processing is to define an ontology that represents the structure of concepts related to materials, e.g., definition, classification or properties of material. Material selection for designing artifacts is a process that translates required material properties into material substances, which in turn requires a definition of data analysis and rules to interpret the result. In this paper, an ontology structure to formalize this kind of process is discussed using an example of the translation of creep property data into design data."
"10.1504/IJBIDM.2006.009135","https://www.scopus.com/inward/record.uri?eid=2-s2.0-33646056672&doi=10.1504%2fIJBIDM.2006.009135&partnerID=40&md5=bd02d3dd8e10338446f54806a4110caa","The sharing of association rules has been proven beneficial in business collaboration, but requires privacy safeguards. One may decide to disclose only part of the knowledge and conceal strategic patterns called sensitive rules. The challenge here is how to protect the sensitive rules without losing the benefit of mining. To address this problem, we propose a unified framework that combines: a set of algorithms to protect sensitive knowledge; retrieval facilities to speed up the process of knowledge protecting; and a set of metrics to evaluate the effectiveness of the proposed algorithms in terms of information loss and private information disclosure. Copyright © 2006 Inderscience Enterprises Ltd."
"10.1504/IJBIDM.2006.009138","https://www.scopus.com/inward/record.uri?eid=2-s2.0-33646050924&doi=10.1504%2fIJBIDM.2006.009138&partnerID=40&md5=b06e3a3a87b1cfa258f56cd22fb76f13","Appraisal of companies is an important business activity. We mainly apply Bajesian networks for this classification task for Japanese electric company data. Firstly, few standard statistical techniques are performed. Then Bayesian networks are applied in four steps; (1) for implementing a current procedure of economical experts, where economical variables are clustered and then summarised for computing a score for deciding the economical state of the company, (2) the same is done but with clustering of economical variables based on data, (3) the naive Bayes classifier and (4) an improved naive Bayes classifier through adjusting its conditional density of each feature variable given the class variable, which are initially obtained by maximum likelihood estimation. Adjustments are done by using the simulated annealing optimisation. Finally, a sensible way for appraisal of companies is discussed. Copyright © 2006 Inderscience Enterprises Ltd."
"10.1504/IJBIDM.2006.009137","https://www.scopus.com/inward/record.uri?eid=2-s2.0-33646042876&doi=10.1504%2fIJBIDM.2006.009137&partnerID=40&md5=f743c03fdc031c8c6f414bc13df0d237","Workflow Management Systems (WfMS) provide a fundamental technological infrastructure to define and manage business processes efficiently. WfMS logs contain valuable data that can be used to discover and extract knowledge about the execution of workflows and processes. One piece of important and useful information that can be discovered is related to the prediction of the path that will be followed during the execution of a workflow. We call this type of discovery, path mining. In this paper, we present and describe how path mining can be achieved using different data mining techniques including the Multimethod approach. Copyright © 2006 Inderscience Enterprises Ltd."
"10.1504/IJBIDM.2006.009139","https://www.scopus.com/inward/record.uri?eid=2-s2.0-33646032089&doi=10.1504%2fIJBIDM.2006.009139&partnerID=40&md5=aca6dbd15cf8bc9c03a5ab7d5d7e92fb","A human being can visualise only up to 3-dimensions. A mapping tool is essential to map the higher dimensional data to a lower dimension for visualization. Both linear as well as non-linear mapping methods had been used by various researchers for the said purpose. In the present work, a non-linear mapping method based on a genetic algorithm has been developed and its performance is compared to that of other methods, namely Sammon's NLM, VISOR and SOM, in terms of accuracy in mapping, visibility of the mapped data and computational complexity, for solving Schaffer's and DeJong's test functions. The proposed GA-like approach and VISOR algorithm are found to be the best and worst, respectively, in terms of accuracy in mapping, ease of visualization. Moreover, the GA-like approach and VISOR algorithm are seen to be the slowest and fastest, respectively, of all the methods. Copyright © 2006 Inderscience Enterprises Ltd."
"10.1504/IJBIDM.2005.008360","https://www.scopus.com/inward/record.uri?eid=2-s2.0-42049114236&doi=10.1504%2fIJBIDM.2005.008360&partnerID=40&md5=628b58f64a2bf6035b568835f4fe3480","High dimensional data pose challenges to traditional clustering algorithms due to their inherent sparseness and data tend to cluster in different and possibly overlapping subspaces of the entire feature space. Finding such subspaces is called subspace mining. We present SCHISM, a new algorithm for mining interesting subspaces, using the notions of support and Chernoff-Hoeffding bounds. We use a vertical representation of the dataset, and use a depth first search with backtracking to find maximal interesting subspaces. We test our algorithm on a number of high dimensional synthetic and real datasets to test its effectiveness. Copyright © 2005 Inderscience Enterprises Ltd."
"10.1504/IJBIDM.2005.008361","https://www.scopus.com/inward/record.uri?eid=2-s2.0-42049083789&doi=10.1504%2fIJBIDM.2005.008361&partnerID=40&md5=b2b70da38315c5d2b2bffccfb3e04244","In web classification, most researchers assume that the objects to be classified are individual web pages from one or more websites. In practice, the assumption is too restrictive since a web page itself may not carry sufficient information for it to be treated as an instance of some semantic class or concept. In this paper, we relax this assumption and allow a subgraph of web pages to represent an instance of the semantic concept. Such a subgraph of web pages is known as a web unit. To construct and classify web units, we formulate the web unit mining problem and propose an iterative web unit mining (iWUM) method. The iWUM method first finds subgraphs of web pages using knowledge about website structure and connectivity among the web pages. From these web subgraphs, web units are constructed and classified into categories in an iterative manner. Our experiments using the WebKB dataset showed that iWUM was able to construct web units and classify web units with high accuracy for the more structured parts of a website. Copyright © 2005 Inderscience Enterprises Ltd."
"10.1504/IJBIDM.2005.008362","https://www.scopus.com/inward/record.uri?eid=2-s2.0-34250167284&doi=10.1504%2fIJBIDM.2005.008362&partnerID=40&md5=130cd1abf424a13ab65d200d55238ccb","Extracting data on the Web is an important information extraction task. Most existing approaches rely on wrappers which require human knowledge and user interaction during extraction. This paper proposes the use of conditional models as an alternative solution to this task. Deriving the strength of conditional models like maximum entropy and maximum entropy Markov models our method offers three major advantages: the full automation, the ability to incorporate various non-independent, overlapping features of different hypertext representations, and the ability to deal with missing and disordered data fields. The experimental results on a wide range of e-commercial websites with different layouts show that our method can achieve a satisfactory trade-off between automation and accuracy, and also provide a practical application of automated data extraction from the Web. Copyright © 2005 Inderscience Enterprises Ltd."
"10.1504/IJBIDM.2005.008364","https://www.scopus.com/inward/record.uri?eid=2-s2.0-34248155340&doi=10.1504%2fIJBIDM.2005.008364&partnerID=40&md5=ccfa7128fed5e7701d1470ff5c0abed2","This paper applies an estimated distribution model to clustering problems. The proposed clustering method makes use of an inter-intra cluster metric and performs a conditional split-merge operation. With conditional splitting and merging, the proposed clustering method does not require the information of cluster number and an improved cluster vector is subsequently guaranteed. In addition, this paper compares movement conditions between inter-intra cluster metric and intra cluster metric. It proves that, under some conditions, the intersection of convergence space between inter-intra cluster metric and intra cluster metric is not empty, and neither is the other subset in the convergence space. This sheds light on how much a cluster metric can play in clustering convergence. Copyright © 2005 Inderscience Enterprises Ltd."
"10.1504/IJBIDM.2005.008363","https://www.scopus.com/inward/record.uri?eid=2-s2.0-33751016487&doi=10.1504%2fIJBIDM.2005.008363&partnerID=40&md5=132c1c5eb046acb8b8458a91f428a249","In this paper, we focus on similarity searching for similar acoustic data over unstructured decentralised P2P networks. Similarity is measured in terms of time warping, which can cope with distortion that is naturally present when 'query by content' is performed. We propose a novel framework, which takes advantage of the absence of overhead in unstructured P2P networks and minimises the required traffic for all operations with the use of an intelligent sampling scheme. Within the proposed framework we adapt several existing algorithms for searching in P2P networks. Detailed experimental results show the efficiency of the proposed framework and the comparison between similarity searching algorithms. Copyright © 2005 Inderscience Enterprises Ltd."
"10.1504/IJBIDM.2005.007318","https://www.scopus.com/inward/record.uri?eid=2-s2.0-33744503185&doi=10.1504%2fIJBIDM.2005.007318&partnerID=40&md5=37d6d1330df0b450dcd443d16ab062ae","Support vector machines (SVM) have been applied to build classifiers, which can help users make well-informed business decisions. Despite their high generalisation accuracy, the response time of SVM classifiers is still a concern when applied into real-time business intelligence systems, such as stock market surveillance and network intrusion detection. This paper speeds up the response of SVM classifiers by reducing the number of support vectors. This is done by the K-means SVM (KMSVM) algorithm proposed in this paper. The KMSVM algorithm combines the K-means clustering technique with SVM and requires one more input parameter to be determined: the number of clusters. The criterion and strategy to determine the input parameters in the KMSVM algorithm are given in this paper. Experiments compare the KMSVM algorithm with SVM on real-world databases, and the results show that the KMSVM algorithm can speed up the response time of classifiers by both reducing support vectors and maintaining a similar testing accuracy to SVM. Copyright © 2005 Inderscience Enterprises Ltd."
"10.1504/IJBIDM.2005.007317","https://www.scopus.com/inward/record.uri?eid=2-s2.0-33744488319&doi=10.1504%2fIJBIDM.2005.007317&partnerID=40&md5=4c6e75549571c70f6d9828835de30aa7","The VSM with TF-IDF is a popular approach to represent a document. But it is not very fit for clustering in a dynamic or changing corpus because we have to update the TF-IDF value of every dimension of every VSM vector when we add a new file into the corpus. Furthermore, popular feature selection methods, such as DF, IG and chi, need some global corpus information before clustering. We present the heavy frequency vector, which considers only the most frequent words in a document. Since an HFV does not contain any global corpus information, it is easy to implement incremental clustering, especially in dynamic or changing corpus. We compare the HFV-based K-means model with the traditional VSM-based K-means model with different feature selection methods. The results show that the HFV model has better precision than others. However, the complexity of HFV model is greater than others. Copyright © 2005 Inderscience Enterprises Ltd."
"10.1504/IJBIDM.2005.007322","https://www.scopus.com/inward/record.uri?eid=2-s2.0-33744474079&doi=10.1504%2fIJBIDM.2005.007322&partnerID=40&md5=73f65a456733a1add5a8b7023fe20849","Clinical outcomes analysis normally covers a particular time period. The sample under study is constantly changing as patients are censored, leave the study or die. In this paper, we present a novel data mining approach to mine temporal rules that reflect characteristics of outcomes analysis. We apply our temporal rule induction algorithm to a set of cancer patients. clinical records that were prospectively collected for 20 years. We analyse clinical data not only based on the static event, such as local recurrence for survival analysis, but also based on the temporal event with censored data for each time unit. The rules extracted from our temporal rule induction algorithm are compared to results from statistical analysis. The importance of this paper is that this novel temporal rule induction algorithm provides valuable insights for clinical data assessment and complements traditional statistical analysis. Copyright © 2005 Inderscience Enterprises Ltd."
"10.1504/IJBIDM.2005.007319","https://www.scopus.com/inward/record.uri?eid=2-s2.0-33744467993&doi=10.1504%2fIJBIDM.2005.007319&partnerID=40&md5=564c12beddcc829c68162221fd2a4fd5","Data dimensionality reduction is usually carried out before patterns are input to classifiers. In order to obtain good results in data mining, selecting relevant data is desirable. In many cases, irrelevant or redundant attributes are included in data sets, which interfere with knowledge discovery from data sets. In this paper, we propose a rule-extraction method based on a novel separability-correlation measure (SCM) ranking the importance of attributes. According to the attribute ranking results, the attribute subsets that lead to the best classification results are selected and used as inputs to a classifier, such as an RBF neural network in our paper. The complexity of the classifier can thus be reduced and its classification performance improved. Our method uses the classification results with reduced attribute sets to extract rules. Computer simulations show that our method leads to smaller rule sets with higher accuracies compared with other methods. Copyright © 2005 Inderscience Enterprises Ltd."
"10.1504/IJBIDM.2005.007320","https://www.scopus.com/inward/record.uri?eid=2-s2.0-33744459519&doi=10.1504%2fIJBIDM.2005.007320&partnerID=40&md5=bf32e4c02a1156a4753f5c4832099cb3","Data mining is the discovery of interesting and hidden patterns from a large amount of collected data. Applications can be found in many organisations with large databases, for many different purposes such as customer relationships, marketing, planning, scientific discovery, and other data analysis. In this paper, the problem of mining N-most interesting itemsets is addressed. We make use of the techniques of COFI-tree in order to tackle the problem. In our experiments, we find that our proposed algorithm based on COFI-tree performs faster than the previous approach BOMO based on the FP-tree. Copyright © 2005 Inderscience Enterprises Ltd."
"10.1504/IJBIDM.2005.007321","https://www.scopus.com/inward/record.uri?eid=2-s2.0-33744456038&doi=10.1504%2fIJBIDM.2005.007321&partnerID=40&md5=41c2dd70eba8dc758cd12e64027e3aa3","By exploiting the concept of fuzzy similarity measures, this paper presents a hybrid recommendation framework that builds on the strengths of knowledge-based and collaborative filtering techniques. Following a multi-criteria approach, the proposed framework is able to provide users with a ranked list of alternatives, while it also permits them to submit their evaluations on the existing items of the database. Much attention is given to the extent to which the user evaluation may affect the values of the stored items. The applicability of our approach is demonstrated through an already implemented web-based tool, namely CityGuide, which provides recommendations about visiting different cities of a country. Issues related to the robustness of our framework and the selection of the appropriate similarity measure are also discussed. Copyright © 2005 Inderscience Enterprises Ltd."
"10.1504/IJBIDM.2005.007316","https://www.scopus.com/inward/record.uri?eid=2-s2.0-33646520426&doi=10.1504%2fIJBIDM.2005.007316&partnerID=40&md5=b5ef9929d3828bc4bca87eb82369c42f","Inspired by the good work of Han et al. (1996) and Elfeky et al. (2001) on the design of data mining query languages for relational and object-oriented databases, in this paper, we develop an expressive XML-enabled data mining query language by extension of XQuery. We first describe some preliminaries on the extension of traditional rule mining to XML mining. The philosophy that guides the language design is then elaborated. We detail the syntax of the mining language and its usage with a number of examples. Copyright © 2005 Inderscience Enterprises Ltd."
"10.2481/dsj.2.128","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057092149&doi=10.2481%2fdsj.2.128&partnerID=40&md5=7e140cd83a3c847792b851709fbd0a52","The paper presents the core concepts of the Polish Spatial Information System, reflecting the results of investigations performed within the framework of the research project conducted at the Institute of Geodesy and Cartography. The architecture of the Polish Spatial Data Infrastructure with regard to data flow and data access is presented. Describing the System the author stresses the importance of data-sharing arrangements on spatial data usability. © 2018, Ubiquity Press. All rights reserved."
"10.2481/dsj.2.117","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84944795677&doi=10.2481%2fdsj.2.117&partnerID=40&md5=577678521971b1fcff62e1f311dea6e5","Recent population censuses have brought about arrays of high-resolution explicitly geo-referenced socioeconomic data stored in the framework of Geographic Information Systems. Geography and social science are not prepared for these new urban databases, and this paper considers their potential for investigating residential distribution, based on the data of the 1995 Israeli Census of Population and Households. We focus on the methodological problems: understanding the phenomena, formal analysis, and statistical inference. The methods for mapping high-resolution data, establishing spatial relationships between them, analyzing neighborhood structure, and exploring the significance of the results are proposed and illustrated by examples of the cities of Tel-Aviv (pop. 350,000) and Ashdod (pop. 100,000). © 2018, Ubiquity Press. All rights reserved."
"10.2481/dsj.2.100","https://www.scopus.com/inward/record.uri?eid=2-s2.0-33845427303&doi=10.2481%2fdsj.2.100&partnerID=40&md5=a347dd7ec54e5022a4346a537bc6f1e6","In this article, we intend to show how useful Exploratory Spatial Data Analysis is in improving spatial data usability. We first outlined a general framework about usability using conceptual modelling, including Data, Users and Methodologies. We then defined keywords into classes and their relations. A central ternary relation is enhanced to describe usability. In the second section, we present ESDA with its fundamental basics: i.e. robustness and way(s) to handle data and related graphic tools. We also described the software package ARPEGE’. Through a concrete example, we demonstrate and discuss its relevance for exploratory spatial data analysis and usability. © 2018, Ubiquity Press. All rights reserved."
"10.2481/dsj.2.79","https://www.scopus.com/inward/record.uri?eid=2-s2.0-33646183324&doi=10.2481%2fdsj.2.79&partnerID=40&md5=58728f57080704007068ec4e714653f2","In recent geographical information science literature, a number of researchers have made passing reference to an apparently new characteristic of spatial data known as ‘usability’. While this attribute is well-known to professionals engaged in software engineering and computer interface design and testing, extension of the concept to embrace information would seem to be a new development. Furthermore, while notions such as the use and value of spatial information, and the diffusion of spatial information systems, have been the subject of research since the late-1980s, the current references to usability clearly represent something which extends well beyond that initial research. Accordingly, the purposes of this paper are: (1) to understand what is meant by spatial data usability; (2) to identify the elements that might comprise usability; and (3) to consider what the related research questions might be. © 2018, Ubiquity Press. All rights reserved."
"10.2481/dsj.2.90","https://www.scopus.com/inward/record.uri?eid=2-s2.0-20944448370&doi=10.2481%2fdsj.2.90&partnerID=40&md5=d03f7fdfd49cffe79c2971b9529db7ca","The fact that many decisions need a combination of information sources makes easy integration of geospatial data an important data usability issue. Our vision is to achieve automated just-in-time integration. As a foundation, we present a system architecture with distributed data and services. Existing and evolving standards and technologies fitting into this architecture are presented along with their scope and shortcomings. A major point is the appropriate definition of data and operation semantics. Further research is needed here to make the automatic formation of service chains for data integration possible. © 2018, Ubiquity Press. All rights reserved."
,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-0033389889&partnerID=40&md5=79d5ed2c73f868a64f63e45ae5856923","The report from Woolmark Business Intelligence considers recent trends in world wool production and provides projections of production and supply to 2005. Information has been collected from official data sources, the Woolmark Company regional staff and from wool organisations in the major grower countries. Detailed information regarding global trends in key factors affecting wool production is followed by thorough reports of the top ten apparel wool producing countries."
,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-0033382992&partnerID=40&md5=cc62d374d5edc1540ef5a831fbd9e1f6","Changing consumer lifestyles and increased time pressures have heightened the need for Easy Care properties in the 1990's, leading to the expansion of commercial opportunities for Machine Washable and Total Easy Care wool garments. The aim of this publication is to help licensees/clients keep up with the rapidly expanding market for Easy Care Wool Knitwear and Wovens. A comprehensive sourcing guide is also included, covering processors involved with machine Washable Wool, from spinners to knitters, manufacturers, weavers and dyers."
